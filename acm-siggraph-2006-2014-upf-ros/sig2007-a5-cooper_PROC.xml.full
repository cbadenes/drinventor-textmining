{
  "uri" : "sig2007-a5-cooper_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2007/a5-cooper_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Active Learning for Real-Time Motion Controllers",
    "published" : "2007",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Seth-Cooper",
      "name" : "Seth",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Aaron-Hertzmann",
      "name" : "Aaron",
      "surname" : "Hertzmann"
    }, {
      "uri" : "http://drinventor/Zoran-Popovic",
      "name" : "Zoran",
      "surname" : "Popovic"
    } ]
  },
  "bagOfWords" : [ "942b3a009d6dd76d83f542a8a6ade1daca9d8ce30eabb3747cbe53fb8e516711", "mhj", "10.1145", "1239451.1239456", "name", "identification", "possible", "active", "Learning", "Real-Time", "Motion", "Controllers", "Seth", "Cooper", "Aaron", "Hertzmann", "University", "Washington", "paper", "describe", "approach", "build", "real-time", "highlycontrollable", "character", "kinematic", "character", "controller", "build", "on-the-fly", "during", "capture", "session", "update", "after", "each", "new", "motion", "clip", "acquire", "active", "learning", "use", "identify", "which", "motion", "sequence", "user", "should", "perform", "next", "order", "improve", "quality", "responsiveness", "controller", "because", "motion", "clip", "select", "adaptively", "we", "avoid", "difficulty", "manually", "determine", "which", "one", "capture", "can", "build", "complex", "controller", "from", "scratch", "while", "significantly", "reduce", "number", "necessary", "motion", "sample", "cr", "category", "i.", "3.7", "-lsb-", "Computer", "Graphics", "-rsb-", "three-dimensional", "graphic", "realism?animation", "Keywords", "Motion", "Capture", "human", "Motion", "active", "Learning", "figure", "catch", "controller", "character", "move", "through", "space", "catch", "two", "consecutive", "ball", "throw", "from", "different", "direction", "addition", "current", "character", "state", "controller", "have", "three-dimensional", "input", "space", "specify", "incoming", "position", "speed", "ball", "catch", "introduction", "human", "motion", "capture", "datum", "provide", "effective", "basis", "create", "new", "animation", "example", "interpolate", "concatenate", "motion", "realistic", "new", "animation", "can", "generate", "real-time", "response", "user", "control", "other", "input", "paper", "we", "consider", "animation", "model", "we", "refer", "motion", "controller", "controller", "generate", "animation", "real-time", "base", "user-specified", "task", "-lrb-", "e.g.", "user", "might", "press", "forward", "game", "controller", "specify", "task", "walk", "forward", "-rrb-", "each", "task", "parameterize", "control", "vector", "continuous", "space", "-lrb-", "e.g.", "catch", "ball", "fly", "from", "specific", "direction", "velocity", "-rrb-", "paper", "controller", "essentially", "function", "from", "combined", "space", "state", "task", "space", "motion", "we", "controller", "kinematic", "produce", "character?s", "motion", "interpolate", "motion", "capture", "rather", "than", "produce", "motion", "dynamically", "through", "force", "torque", "Motion", "capture", "datum", "very", "time-consuming", "expensive", "acquire", "thus", "desirable", "capture", "little", "possible", "when", "build", "simplest", "controller", "designer", "can", "minimize", "ACM", "Reference", "Format", "Cooper", "S.", "Hertzmann", "a.", "Popovic", "Z.", "2007", "active", "Learning", "Real-Time", "Motion", "Controllers", "ACM", "Trans", "graph", "26", "Article", "-lrb-", "July", "2007", "-rrb-", "page", "dous", "10.1145", "1239451.1239456", "http://doi.acm", "org/10", ".1145", "1239451.1239456", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "first", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2007", "ACM", "0730-0301/2007", "03-art5", "5.00", "DOI", "10.1145", "1239451.1239456", "http://doi.acm.org/10.1145/1239451.1239456", "Zoran", "Popovi", "University", "Toronto", "amount", "datum", "capture", "carefully", "plan", "datum", "sample", "acquire", "however", "non-trivial", "task", "space", "possible", "clip", "vast", "manual", "selection", "sample", "quickly", "become", "intractable", "example", "controller", "human", "can", "walk", "dodge", "projectile", "must", "parameterize", "direction", "speed", "walk", "direction", "speed", "projectile", "because", "task", "can", "change", "any", "point", "during", "animation", "controller", "also", "need", "parameterize", "all", "possible", "pose", "configuration", "character", "example", "controller", "must", "able", "dodge", "any", "projectile", "appear", "while", "character", "walk", "turn", "recover", "from", "previous", "dodge", "even", "moderate", "set", "task", "lead", "huge", "set", "initial", "configuration", "control", "vector", "determine", "which", "motion", "capture", "controller", "so", "can", "produce", "good", "motion", "huge", "space", "possible", "input", "daunting", "task", "we", "experience", "too", "difficult", "do", "manually", "uniform", "sampling", "input", "space", "would", "vastly", "inefficient", "require", "huge", "number", "sample", "capture", "store", "memory", "because", "space", "structure", "nearby", "motion", "space", "can", "often", "interpolate", "produce", "valid", "new", "motion", "selection", "which", "motion", "capture", "should", "greatly", "reduce", "number", "sample", "need", "however", "even", "small", "controller", "space", "nonlinearity", "space", "mean", "careful", "sampling", "require", "paper", "we", "propose", "use", "active", "learn", "address", "problem", "particular", "we", "build", "motion", "controller", "onthe-fly", "during", "datum", "acquisition", "process", "after", "each", "motion", "capture", "system", "automatically", "identify", "specific", "task", "controller", "perform", "poorly", "base", "suite", "error", "metric", "task", "present", "user", "candidate", "additional", "datum", "sample", "user", "choose", "one", "task", "perform", "controller", "update", "new", "motion", "clip", "way", "system", "continue", "refine", "controller", "until", "capable", "perform", "any", "task", "from", "any", "state", "any", "control", "vector", "time", "available", "capture", "have", "elapse", "else", "number", "datum", "sample", "have", "be", "reach", "predefined", "maximum", "process", "yield", "highly-controllable", "real-time", "motion", "controller", "realism", "motion", "capture", "datum", "while", "require", "only", "small", "amount", "motion", "capture", "datum", "we", "demonstrate", "feasibility", "approach", "use", "we", "system", "construct", "three", "example", "controller", "we", "validate", "active", "learning", "approach", "compare", "one", "controller", "manually-constructed", "controller", "we", "emphasize", "design", "we", "system", "do", "automate", "all", "decision", "rather", "compute", "aspect", "controller", "would", "difficult", "user", "handle", "user", "leave", "specific", "decision", "which", "hard", "quantitatively", "evaluate", "make", "decision", "normally", "entail", "selecting", "from", "among", "few", "alternative", "present", "system", "paper", "we", "refer", "generic", "user", "run", "system", "practice", "some", "role", "may", "perform", "separate", "individual", "example", "human", "operator", "might", "run", "active", "learning", "software", "have", "actor", "perform", "actual", "motion", "later", "separate", "nonspecialist", "user", "-lrb-", "e.g.", "game", "player", "-rrb-", "may", "control", "motion", "learn", "controller", "contribution", "key", "contribution", "work", "use", "active", "learning", "method", "animation", "produce", "compact", "controller", "complex", "task", "we", "also", "develop", "framework", "include", "user", "input", "key", "point", "process", "include", "provide", "motion", "capture", "sample", "select", "which", "motion", "capture", "from", "few", "automatically-determined", "option", "we", "also", "develop", "cluster-based", "learning", "model", "motion", "controller", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "5-2", "Cooper", "et", "al.", "related", "work", "common", "theme", "computer", "animation", "research", "create", "new", "motion", "from", "exist", "motion", "capture", "datum", "most", "method", "create", "animation", "off-line", "example", "interpolate", "similar", "set", "motion", "accord", "user-specified", "control", "parameter", "-lsb-", "Witkin", "Popovi", "1995", "Kovar", "Gleicher", "2004", "Mukai", "Kuriyama", "2005", "Rose", "et", "al.", "1998", "Wiley", "Hahn", "1997", "-rsb-", "optimize", "motion", "accord", "probabilistic", "time-series", "model", "-lsb-", "brand", "hertzmann", "2000", "Li", "et", "al.", "2002", "-rsb-", "concatenate", "blend", "example", "sequence", "-lsb-", "Arikan", "et", "al.", "2003", "Kovar", "et", "al.", "2002", "Torresani", "et", "al.", "2007", "-rsb-", "combine", "modeland", "data-driven", "technique", "-lsb-", "Yamane", "et", "al.", "2004", "Zordan", "et", "al.", "2005", "Liu", "Popovi", "2002", "Liu", "et", "al.", "2005", "-rsb-", "method", "generate", "motion", "off-line", "whereas", "we", "consider", "problem", "real-time", "synthesis", "worth", "note", "many", "method", "typically", "require", "large", "motion", "database", "need", "database", "could", "mitigate", "we", "active", "learning", "approach", "number", "real-time", "animation", "system", "build", "motion", "capture", "datum", "well", "one", "approach", "directly", "play", "transition", "between", "clip", "from", "motion", "database", "-lsb-", "Gleicher", "et", "al.", "2003", "Lee", "et", "al.", "2002", "Lee", "et", "al.", "2006", "-rsb-", "precomputation", "can", "use", "allow", "real-time", "planning", "which", "clip", "use", "-lsb-", "Lau", "Kuffner", "2006", "Lee", "Lee", "2006", "-rsb-", "Reitsma", "Pollard", "-lsb-", "2004", "-rsb-", "present", "method", "evaluate", "possible", "motion", "generate", "approach", "few", "author", "have", "describe", "method", "generate", "new", "pose", "response", "real-time", "input", "we", "motion", "controller", "model", "most", "similar", "method", "transition", "between", "interpolated", "sequence", "Park", "et", "al.", "-lsb-", "2004", "-rsb-", "Kwon", "Shin", "-lsb-", "2005", "-rsb-", "combine", "interpolation", "motion", "graph", "structure", "generate", "new", "locomotion", "sequence", "Shin", "oh", "-lsb-", "2006", "-rsb-", "perform", "interpolation", "graph", "edge", "simple", "model", "locomotion", "other", "repetitive", "motion", "previous", "work", "assume", "corpus", "motion", "datum", "available", "advance", "user", "manually", "select", "which", "motion", "capture", "paper", "we", "show", "how", "use", "adaptive", "selection", "motion", "sequence", "allow", "creation", "controller", "greater", "complexity", "while", "allow", "fine-scale", "parameterize", "control", "capture", "relatively", "few", "motion", "overall", "datum", "acquisition", "difficult", "expensive", "and/or", "time-consuming", "problem", "many", "discipline", "consequently", "automatic", "selection", "test", "case", "have", "be", "extensively", "study", "statistics", "optimal", "experimental", "design", "method", "seek", "most", "informative", "test", "point", "estimate", "unknown", "nonlinear", "function", "-lsb-", "Atkinson", "Donev", "1992", "Santner", "et", "al.", "2003", "-rsb-", "simplest", "method", "determine", "all", "test", "point", "advance", "e.g.", "via", "space-filling", "function", "optimize", "objective", "function", "however", "often", "possible", "determine", "advance", "which", "region", "input", "space", "need", "most", "datum", "active", "learning", "method", "select", "test", "datum", "sequentially", "after", "each", "datum", "point", "acquire", "next", "test", "point", "choose", "maximize", "objective", "function", "-lsb-", "Cohn", "et", "al.", "1994", "-rsb-", "active", "learning", "have", "be", "study", "most", "extensively", "classification", "problem", "-lrb-", "e.g.", "-lsb-", "Lindenbaum", "et", "al.", "2004", "-rsb-", "-rrb-", "paper", "we", "present", "active", "learning", "algorithm", "motion", "controller", "we", "approach", "distinct", "from", "exist", "active", "learning", "method", "two", "way", "first", "instead", "choose", "next", "sample", "capture", "we", "system", "identify", "set", "candidate", "from", "which", "user", "choose", "sample", "improve", "second", "we", "assume", "metric", "correctness", "provide", "which", "candidate", "may", "choose", "Motion", "Controllers", "before", "describe", "we", "active", "learning", "framework", "which", "main", "contribution", "paper", "we", "briefly", "discuss", "we", "model", "motion", "controller", "we", "framework", "kinematic", "controller", "generate", "motion", "clip", "character", "state", "perform", "task", "parameterize", "control", "vector", "U.", "define", "set", "all", "permissible", "character", "state", "discrete", "set", "controller", "task", "define", "operational", "range", "control", "input", "task", "define", "space", "output", "motion", "single", "controller", "can", "learn", "integrate", "several", "task", "each", "specify", "value", "argument", "t.", "example", "catch", "controller", "consist", "two", "task", "one", "catch", "ball", "come", "give", "speed", "direction", "idle", "controller", "invoke", "when", "ball", "catch", "we", "take", "advantage", "fact", "controller", "produce", "continuous", "motion", "clip", "rather", "individual", "state", "solve", "new", "motion", "only", "when", "task", "parameter", "change", "when", "current", "motion", "finish", "we", "represent", "motion", "clip", "sequence", "pose", "order", "compare", "blend", "visualize", "motion", "translationand", "rotationinvariant", "way", "we", "decouple", "each", "pose", "from", "its", "translation", "ground", "plane", "rotation", "its", "hip", "about", "up", "axis", "we", "represent", "change", "position", "rotation", "relative", "previous", "pose?s", "local", "frame", "we", "represent", "state", "character", "vector", "contain", "pose", "next", "ten", "frame", "currentlyplay", "clip", "use", "multiple", "frame", "state", "facilitate", "find", "smooth", "motion", "blend", "we", "determine", "distance", "between", "two", "state", "method", "inspire", "Kovar", "et", "al.", "-lsb-", "2002", "-rsb-", "evaluate", "distance", "between", "point", "cloud", "attach", "each", "corresponding", "pose", "we", "consider", "data-driven", "controller", "which", "create", "new", "clip", "interpolate", "example", "motion", "capture", "clip", "-lcb-", "-rcb-", "associate", "task", "parameter", "-lcb-", "-rcb-", "however", "all", "sample", "clip", "can", "blended", "together", "meaningful", "way", "consider", "tennis", "controller", "would", "make", "sense", "blend", "forehand", "backhand", "stroke", "motion", "sample", "reason", "each", "controller", "consist", "group", "blendable", "clip", "which", "we", "refer", "cluster", "each", "cluster", "contain", "set", "blendable", "motion", "sample", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "share", "common", "state", "continuous", "blend", "map", "-lrb-", "-rrb-", "which", "produce", "motion", "blend", "weight", "w.", "give", "state", "task", "control", "vector", "apply", "controller", "entail", "two", "step", "first", "we", "find", "t-specific", "cluster", "closest", "more", "than", "one", "we", "use", "one", "which", "have", "closest", "u.", "second", "output", "motion", "generate", "compute", "blend", "weight", "-lrb-", "-rrb-", "interpolate", "weight", "new", "clip", "interpolation", "perform", "time-aligned", "translationand", "rotation-invariant", "manner", "similar", "-lsb-", "Kovar", "Gleicher", "2003", "-rsb-", "new", "motion", "blended", "currently-playing", "motion", "however", "current", "character", "state", "very", "dissimilar", "beginning", "new", "controller", "motion", "ignore", "current", "motion", "continue", "until", "subsequent", "state", "produce", "successful", "controller", "motion", "we", "example", "we", "also", "use", "modify", "version", "controller", "employ", "inverse", "kinematic", "-lrb-", "ik", "-rrb-", "step", "further", "satisfy", "endeffector", "constraint", "-lrb-", "e.g.", "catch", "ball", "right", "hand", "-rrb-", "ik", "controller", "ik", "define", "term", "simple", "blend", "controller", "ik", "-lrb-", "-rrb-", "ik", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "function", "ik", "transform", "motion", "satisfy", "kinematic", "constraint", "define", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "active", "Learning", "Real-Time", "Motion", "Controllers", "5-3", "-lrb-", "-rrb-", "figure", "-lrb-", "-rrb-", "illustration", "catch", "controller", "single", "state", "2d", "controller", "space", "space", "have", "be", "partition", "two", "cluster", "each", "cross", "circle", "represent", "control", "vector", "example", "motion", "motion", "left", "cluster", "correspond", "catch", "place", "whereas", "motion", "right", "correspond", "take", "step", "during", "catch", "dash", "line", "represent", "boundary", "between", "cluster", "implicit", "nearest-neighbor", "assignment", "-lrb-", "-rrb-", "motion", "controller", "consist", "collection", "controller", "each", "specific", "task", "state", "joint", "angle", "relevant", "end-effector", "-lrb-", "i.e.", "shoulder", "elbow", "hand", "-rrb-", "first", "optimize", "satisfy", "kinematic", "constraint", "time", "instant", "neighbor", "frame", "linearly", "blended", "produce", "continuous", "motion", "use", "ik", "enable", "controller", "achieve", "much", "greater", "variety", "motion", "from", "few", "sample", "thus", "enable", "active", "learning", "use", "significantly", "fewer", "sample", "overall", "general", "more", "powerful", "controller", "fewer", "sample", "active", "learning", "require", "full", "coverage", "controller", "new", "clip", "usually", "play", "from", "beginning", "however", "some", "case", "may", "useful", "motion", "clip", "middle", "particular", "occur", "when", "new", "motion", "start", "from", "same", "state", "current", "motion", "would", "generate", "same", "cluster", "imply", "new", "motion", "would", "generate", "blend", "same", "example", "current", "motion", "case", "new", "motion", "can", "start", "time", "correspond", "current", "time", "index", "current", "clip", "since", "all", "motion", "belong", "same", "cluster", "can", "blended", "any", "portion", "clip", "process", "appropriate", "all", "task", "-lrb-", "e.g.", "one", "can", "change", "trajectory", "midair", "-rrb-", "whether", "allow", "indicate", "per-task", "basis", "cluster", "motion-blend", "model", "use", "we", "experiment", "motivate", "similar", "model", "use", "game", "industry", "real", "game", "controller", "specific", "cluster", "sample", "within", "each", "cluster", "all", "determine", "ad-hoc", "manual", "process", "manual", "process", "invariably", "produce", "suboptimal", "controller", "multiple", "trip", "motion", "capture", "lab", "more", "importantly", "manual", "process", "do", "scale", "well", "control", "problem", "large", "number", "input", "dimension", "those", "describe", "result", "section", "we", "point", "out", "active", "learning", "framework", "introduce", "follow", "section", "do", "depend", "specific", "detail", "controller", "specification", "addition", "datum", "sample", "need", "capture", "can", "easily", "come", "from", "alternate", "source", "animator-created", "motion", "clip", "active", "Learning", "we", "now", "describe", "how", "motion", "controller", "build", "interactively", "although", "multiple", "step", "process", "basic", "idea", "simple", "identify", "region", "control", "space", "can", "perform", "well", "improve", "they", "process", "require", "user", "first", "define", "control", "problem", "enumerate", "set", "task", "-lcb-", "-rcb-", "specify", "operational", "range", "each", "control", "vector", "u.", "each", "control", "vector", "must", "have", "finite", "valid", "domain", "-lrb-", "e.g.", "bound", "constraint", "-rrb-", "order", "limit", "range", "allowable", "task", "example", "might", "specify", "desire", "walk", "speed", "might", "range", "possible", "walking", "speed", "key", "assumption", "we", "input", "space", "controller", "define", "union", "taskspecific", "control", "space", "Task", "Task", "-lrb-", "-rrb-", "compute", "candidate", "Select", "motion", "compute", "motion", "improve", "pseudoexample", "active", "learning", "Accept", "Evaluate", "Identify", "cluster", "Perform", "motion", "pseudoexample", "reject", "Figure", "flowchart", "motion", "capture", "session", "use", "active", "learning", "Blue", "rectangle", "automatic", "process", "yellow", "round", "rectangle", "require", "user", "input", "approach", "motion", "controller", "do", "need", "able", "from", "any", "initial", "state", "only", "from", "state", "might", "generate", "controller", "hence", "we", "only", "consider", "possible", "start", "pose", "might", "arise", "from", "another", "clip", "generate", "controller", "order", "seed", "active", "learning", "user", "provide", "some", "initial", "state", "active", "learning", "process", "proceed", "follow", "outer", "loop", "system", "identify", "set", "controller", "input", "-lrb-", "-rrb-", "motion", "controller", "can", "handle", "well", "user", "select", "one", "improve", "system", "generate", "improve", "pseudoexample", "motion", "task", "pseudoexample", "approve", "user", "motion", "controller", "update", "step", "can", "often", "save", "user", "from", "have", "perform", "motion", "pseudoexample", "reject", "user", "perform", "task", "new", "motion", "clip", "use", "update", "motion", "controller", "we", "have", "set", "up", "we", "system", "motion", "capture", "lab", "system?s", "display", "project", "wall", "lab", "user", "see", "interface", "mouse-based", "so", "user", "can", "interact", "system", "use", "gyro", "mouse", "while", "capture", "motion", "we", "now", "describe", "step", "detail", "4.1", "select", "candidate", "task", "goal", "step", "identify", "control", "problem", "motion", "controller", "can", "handle", "well", "candidate", "problem", "fully", "specify", "state", "task", "control", "vector", "u.", "because", "evaluation", "motion", "can", "difficult", "do", "purely", "numerically", "we", "do", "have", "explicit", "mathematical", "function", "can", "precisely", "identify", "which", "motion", "most", "need", "improvement", "instead", "we", "find", "multiple", "candidate", "accord", "different", "motion", "metric", "each", "which", "provide", "different", "way", "evaluate", "motion", "generate", "controller", "we", "let", "user", "determine", "candidate", "improve", "we", "use", "follow", "metric", "Task", "error", "each", "task", "user", "specify", "one", "more", "metric", "measure", "how", "well", "motion", "perform", "task", "input", "u.", "example", "task", "require", "character?s", "hand", "position", "specific", "time", "metric", "might", "measure", "euclidean", "distance", "between", "character?s", "hand", "time", "multiple", "error", "metric", "may", "provide", "task", "goal", "all", "should", "satisfy", "likely", "some", "region", "control", "space", "simply", "harder", "solve", "than", "other", "direct", "application", "metric", "oversample", "space", "address", "we", "compute", "task", "error", "difference", "task-performing", "metric", "when", "compare", "nearest", "exist", "example", "sum", "task", "error", "when", "more", "than", "one", "task", "metric", "provide", "user", "weighted", "sum", "metric", "also", "use", "separate", "metric", "transition", "error", "order", "evaluate", "blend", "between", "clip", "we", "measure", "distance", "between", "current", "state", "state", "motion", "m.", "Distance", "from", "previous", "example", "order", "encourage", "exploration", "control", "space", "we", "use", "metric", "measure", "distance", "control", "vector", "from", "nearest", "exist", "example", "control", "vector", "find", "worst-performing", "region", "controller", "input", "space", "we", "need", "sample", "very", "large", "controller", "input", "space", "we", "reduce", "amount", "sample", "require", "use", "different", "sampling", "process", "different", "motion", "metric", "generate", "candidate", "use", "each", "task", "error", "metric", "we", "search", "motion", "perform", "task", "poorly", "each", "cluster", "state", "random", "control", "vector", "generate", "its", "task", "uniform", "random", "sampling", "we", "keep", "u?s", "which", "give", "worst", "result", "each", "motion", "metric", "each", "point", "refine", "Nelder-Mead", "search", "-lsb-", "1965", "-rsb-", "maximize", "badness", "respective", "metric", "Distance", "metric", "candidate", "generate", "similarly", "replace", "task", "error", "nearest-neighbor", "distance", "we", "use", "optimize", "along", "which", "generate", "they", "candidate", "generate", "candidate", "use", "transition", "error", "we", "search", "state", "blend", "poorly", "exist", "cluster", "average", "motion", "clip", "generate", "each", "cluster", "average", "example", "cluster", "we", "find", "state", "motion", "furthest", "from", "any", "exist", "cluster?s", "state", "we", "use", "state", "along", "set", "center", "domain", "candidate", "once", "all", "candidate", "have", "be", "generate", "user", "show", "candidate", "each", "metric", "sort", "score", "along", "information", "about", "they", "error", "corresponding", "user", "choose", "one", "candidate", "improve", "provide", "user", "several", "option", "have", "several", "important", "purpose", "first", "user", "able", "qualitatively", "judge", "which", "sample", "would", "best", "improve", "would", "very", "difficult", "do", "purely", "automatic", "evaluation", "metric", "second", "user", "able", "have", "some", "control", "over", "order", "sample", "collect", "example", "can", "more", "convenient", "user", "collect", "several", "consecutive", "sample", "from", "same", "state", "Third", "view", "candidate", "give", "user", "good", "sense", "performance", "controller", "more", "sample", "provide", "quality", "candidate", "improve", "once", "all", "candidate", "consider", "acceptable", "user", "have", "some", "confidence", "controller", "finish", "however", "formal", "guarantee", "how", "controller", "perform", "new", "task", "we", "experience", "although", "many", "candidate", "generate", "user", "usually", "find", "one", "worth", "refining", "after", "watch", "only", "few", "example", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "5-4", "Cooper", "et", "al.", "we", "also", "consider", "seed", "state", "well", "end", "state", "cluster", "which", "distant", "enough", "from", "any", "state", "Figure", "interactive", "setup", "user", "prepare", "perform", "walk", "motion", "projection", "display", "show", "desire", "task", "initial", "state", "4.2", "determine", "weight", "before", "proceeding", "we", "discuss", "detail", "we", "implementation", "per-cluster", "blend", "function", "we", "implement", "blend", "function", "use", "scheme", "similar", "radial", "basis", "function", "-lrb-", "rbf", "-rrb-", "base", "method", "propose", "Rose", "et", "al.", "-lsb-", "1998", "2001", "-rsb-", "we", "use", "different", "form", "linear", "basis", "replace", "residual", "basis", "basis", "non-uniform", "along", "each", "dimension", "-lrb-", "-rrb-", "where", "index", "over", "element", "control", "vector", "cubic", "profile", "curve", "??", "scale", "factor", "we", "believe", "other", "non-isotropic", "representation", "could", "perform", "equally", "well", "blend", "function", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "after", "weight", "produce", "function", "clamp", "range", "-lsb-", "-rsb-", "normalize", "sum", "each", "time", "new", "example", "datum", "point", "-lrb-", "-rrb-", "provide", "we", "solve", "linear", "weight", "use", "least", "square", "we", "initialize", "all", "element", "distance", "nearest", "neighbor", "solve", "Rose", "et", "al.", "we", "update", "each", "perform", "small", "optimization", "across", "each", "dimension", "we", "place", "number", "evaluation", "sample", "along", "dimension", "neighborhood", "regularly", "sample", "value", "range", "-lsb-", ".3", "-rsb-", "we", "scale", "solve", "evaluate", "sum", "task", "error", "metric", "evaluation", "sample", "best", "scale", "keep", "we", "update", "scale", "solve", "one", "last", "time", "4.3", "Generating", "pseudoexample", "system", "first", "attempt", "improve", "performance", "select", "task", "generate", "new", "motion", "call", "pseudoexample", "-lsb-", "Sloan", "et", "al.", "2001", "-rsb-", "pseudoexample", "new", "training", "motion", "define", "linear", "combination", "exist", "motion", "capable", "reshape", "without", "require", "additional", "motion", "sample", "pseudoexample", "can", "represent", "directly", "term", "weight", "use", "generate", "motion", "weight", "choose", "minimize", "sum", "task", "error", "metric", "task", "get", "initial", "guess", "correct", "cluster", "weight", "pseudoexample", "we", "iterate", "over", "all", "cluster", "give", "all", "motion", "cluster", "well", "currently", "predict", "motion", "u.", "all", "motion", "one", "which", "perform", "best", "task", "error", "metric", "select", "specifically", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3x", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "otherwise", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "active", "Learning", "Real-Time", "Motion", "Controllers", "5-5", "Figure", "dodge", "controller", "character", "maintain", "walk", "direction", "while", "avoid", "two", "consecutive", "projectile", "addition", "current", "character", "state", "controller", "have", "four-dimensional", "input", "space", "specify", "incoming", "position", "speed", "ball", "dodge", "angle", "turn", "while", "walk", "provide", "cluster", "weight", "pseudoexample", "we", "optimize", "weight", "within", "cluster", "accord", "sum", "task", "error", "metric", "use", "nelder-mead", "-lsb-", "1965", "-rsb-", "result", "weight", "together", "control", "vector", "constitute", "pseudoexample", "-lrb-", "-rrb-", "clip", "generate", "weight", "its", "evaluate", "task", "error", "show", "user", "user", "approve", "pseudoexample", "permanently", "add", "cluster?s", "rbf", "otherwise", "discard", "4.4", "perform", "new", "motion", "user", "present", "appropriate", "start", "state", "taskspecific", "visualization", "-lrb-", "e.g.", "show", "trajectory", "initial", "position", "ball", "catch", "-rrb-", "since", "motion", "determine", "relative", "character", "visualization", "translate", "rotate", "user?s", "movement", "prior", "begin", "capture", "user", "move", "eventually", "get", "-lrb-", "near", "-rrb-", "start", "state", "we", "system", "automatically", "determine", "when", "initial", "state", "condition", "meet", "record", "rest", "capture", "motion", "aspect", "interface", "significantly", "simplify", "task", "perform", "required", "motion", "sample", "once", "capture", "user", "can", "review", "motion", "its", "task", "error", "motion", "satisfactory", "user", "must", "repeat", "motion", "we", "experiment", "vast", "majority", "task", "could", "perform", "three", "fewer", "trial", "we", "find", "difficult", "perform", "task", "typically", "require", "only", "few", "more", "trial", "usually", "more", "than", "seven", "total", "although", "few", "rare", "case", "require", "up", "twelve", "total", "determine", "where", "desire", "motion", "sample", "end", "we", "seek", "local", "minimum", "state", "distance", "capture", "clip", "compare", "against", "beginning", "end", "exist", "cluster", "well", "consider", "actual", "end", "recorded", "motion", "we", "sort", "distance", "let", "user", "select", "appropriate", "ending", "typically", "involve", "nothing", "more", "than", "user", "confirm", "first", "choice", "system", "now", "need", "determine", "appropriate", "cluster", "new", "motion", "motion", "timewarp", "all", "cluster", "similar", "end", "state", "alignment", "result", "show", "user", "sort", "increase", "order", "alignment", "error", "user", "can", "select", "which", "cluster", "assign", "clip", "we", "experiment", "first", "choice", "frequently", "correct", "user", "decide", "cluster", "appropriate", "new", "cluster", "create", "approach", "remove", "need", "manually", "determine", "cluster", "any", "point", "during", "controller", "synthesis", "cluster", "create", "only", "when", "usersupplied", "sample", "sufficiently", "different", "from", "all", "exist", "cluster", "cluster", "update", "new", "clip", "new", "-lrb-", "-rrb-", "clip", "add", "cluster?s", "rbf", "active", "learning", "process", "repeat", "select", "new", "candidate", "task", "active", "learning", "process", "can", "have", "many", "termination", "criterion", "include", "time", "spend", "lab", "number", "sample", "allot", "controller", "well", "overall", "controller", "quality", "controllability", "user", "can", "estimate", "measure", "quality", "controllability", "current", "controller", "evaluate", "quality", "candidate", "task", "all", "poor-performing", "candidate", "appear", "high-quality", "controller", "have", "probably", "reach", "point", "where", "additional", "sample", "can", "significantly", "improve", "quality", "controller", "coverage", "result", "we", "have", "construct", "three", "example", "demonstrate", "we", "framework", "controller", "demonstrate", "accompany", "video", "each", "controller", "construct", "interactively", "short", "time", "motion", "capture", "lab", "we", "capture", "synthesize", "motion", "between", "20", "25", "frame", "per", "second", "computation", "time", "require", "negligible", "each", "active", "learning", "step", "take", "between", "30", "seconds", "depend", "size", "controller", "involve", "majority", "time", "spend", "capture", "especially", "since", "normally", "take", "two", "three", "try", "perform", "motion", "correct", "state", "match", "task", "we", "first", "example", "parameterized", "walk", "control", "space", "three-dimensional", "control", "stride", "length", "-lrb-", "distance", "between", "heel", "maximum", "spacing", "range", "from", "0.2", "0.9", "meter", "-rrb-", "speed", "-lrb-", "range", "from", "15", "25", "centimeter", "per", "frame", "-rrb-", "turn", "angle", "-lrb-", "change", "horizontal", "orientation", "per", "stride", "range", "from", "??", "radian", "-rrb-", "one", "task", "error", "metric", "each", "feature", "-lrb-", "e.g.", "stride", "length", "-rrb-", "measure", "square", "distance", "between", "desire", "feature", "maximum", "feature", "motion", "order", "determine", "stride", "metric", "assume", "sample", "contain", "complete", "walk", "cycle", "beginning", "right", "foot", "forward", "task", "allow", "motion", "from", "middle", "controller", "task", "produce", "only", "cluster", "use", "12", "example", "total", "843", "frame", "10", "pseudoexample", "we", "limit", "lab", "time", "roughly", "half", "hour", "include", "all", "active", "learning", "user", "interaction", "controller", "achieve", "89", "coverage", "-lrb-", "discuss", "section", "-rrb-", "we", "second", "example", "combine", "parameterized", "walk", "projectile", "dodging", "example", "have", "two", "task", "when", "nothing", "dodge", "control", "space", "one-dimensional", "control", "turn", "angle", "when", "walk", "dodge", "control", "space", "four-dimensional", "control", "turn", "angle", "well", "incoming", "height", "incoming", "angle", "distance", "front", "character", "incoming", "projectile", "walk", "turn", "parameter", "ratio", "radian", "per", "meter", "travel", "range", "from", "??", "task", "metric", "measure", "absolute", "value", "difference", "between", "ratio", "target", "value", "walk", "task", "allow", "motion", "from", "middle", "parameter", "incoming", "projectile", "specify", "relative", "character?s", "local", "coordinate", "system", "incoming", "height", "from", "0.25", "2.0", "meter", "distance", "front", "character", "aim", "from", "0.25", "1.0", "meter", "incoming", "angle", "from", "??", "radian", "parameter", "specify", "trajectory", "projectile", "error", "metric", "inverse", "distance", "between", "projectile", "any", "point", "character", "dodge", "task", "also", "include", "same", "turn", "control", "parameter", "use", "walk", "same", "error", "metric", "well", "synthesize", "controller", "contain", "10", "cluster", "use", "total", "30", "example", "total", "1121", "frame", "19", "pseudoexample", "we", "limit", "lab", "time", "roughly", "hour", "achieve", "76", "coverage", "dodge", "task", "we", "third", "example", "catch", "ball", "example", "have", "two", "task", "when", "catch", "zero-dimensional", "stand", "task", "when", "catch", "control", "space", "three-dimensional", "control", "incoming", "position", "plane", "front", "character", "well", "speed", "parameter", "incoming", "ball", "specify", "relative", "character?s", "local", "coordinate", "system", "incoming", "height", "from", "0.5", "2.0", "meter", "distance", "right", "character", "from", "-2.0", "2.0", "meter", "incoming", "speed", "from", "0.1", "0.2", "meter", "per", "frame", "parameter", "specify", "trajectory", "ball", "task", "error", "metric", "square", "distance", "between", "ball", "character?s", "right", "hand", "example", "we", "remove", "rotation", "invariance", "from", "all", "quantity", "order", "build", "model", "which", "character", "always", "face", "specific", "direction", "while", "wait", "next", "ball", "we", "have", "also", "allow", "user", "load", "several", "motion", "diving", "side", "place", "perform", "motion", "also", "example", "we", "use", "ik", "controller", "so", "we", "have", "can", "have", "greater", "reachability", "result", "motion", "result", "controller", "use", "12", "cluster", "use", "33", "example", "total", "1826", "frame", "23", "pseudoexample", "datum", "include", "diving", "motion", "capture", "lab", "session", "limit", "roughly", "hour", "we", "achieve", "57", "coverage", "catch", "task", "normalize", "manual", "controller", "discuss", "section", "worth", "note", "we", "also", "try", "create", "controller", "simpler", "cluster", "model", "do", "include", "IK", "find", "performance", "significantly", "poorer", "due", "nonlinearity", "underlie", "control", "space", "general", "more", "powerful", "controller", "model", "more", "active", "learning", "can", "take", "advantage", "require", "less", "sample", "produce", "effective", "controller", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "5-6", "Cooper", "et", "al.", "evaluation", "we", "have", "perform", "numerical", "evaluation", "active", "learning", "method", "propose", "use", "catch", "controller", "we", "compare", "active", "learning", "approach", "manual", "sample", "selection", "we", "have", "have", "four", "animation", "researcher", "-lrb-", "two", "associate", "project", "two", "-rrb-", "plan", "motion", "capture", "session", "catch", "controller", "like", "one", "we", "result", "be", "ask", "define", "blend-space", "cluster", "determine", "compact", "number", "sample", "best", "cover", "controller", "space", "all", "manual", "design", "require", "more", "sample", "than", "learn", "controller", "produce", "controller", "visually", "quantitatively", "be", "significantly", "inferior", "we", "learn", "controller", "Table", "we", "show", "number", "sample", "percentage", "input", "space", "coverage", "each", "controller", "comparison", "we", "only", "consider", "catch", "task", "ignore", "stand", "task", "coverage", "define", "percentage", "input", "which", "controller", "successfully", "match", "state", "complete", "task", "-lrb-", "catch", "ball", "mean", "hand", "within", "small", "distance", "ball", "-rrb-", "percentage", "compute", "randomly", "sampling", "state", "task", "similar", "scheme", "section", "4.1", "comparison", "we", "look", "percentage", "sample", "controllable", "give", "controller", "out", "sample", "controllable", "any", "controller", "remove", "sample", "physically", "impossible", "catch", "manual", "controller", "roughly", "80", "more", "sample", "produce", "significantly", "less", "coverage", "while", "controller", "20", "more", "sample", "cover", "roughly", "half", "space", "learn", "controller", "total", "mocap", "studio", "time", "roughly", "same", "course", "coverage", "perfect", "measure", "controller", "since", "do", "measure", "realism", "motion", "difference", "quality", "controller", "also", "apparent", "accompany", "video", "manual", "controller", "ball", "often", "catch", "very", "strange", "unrealistic", "way", "course", "evaluation", "depend", "skill", "level", "people", "who", "choose", "motion", "dive", "catch", "require", "significant", "motion", "cleanup", "also", "painful", "perform", "order", "facilitate", "prototyping", "testing", "we", "capture", "several", "representative", "dive", "advance", "when", "active", "learning", "system", "request", "dive", "one", "pre-captured", "motion", "provide", "-lrb-", "appropriate", "-rrb-", "instead", "perform", "new", "dive", "however", "mean", "controller", "limit", "dive", "example", "set", "can", "e.g.", "dive", "while", "run", "since", "include", "example", "method", "sample", "Coverage", "active", "Learning", "30", "57", "manual", "36", "38", "manual", "44", "56", "manual", "54", "48", "manual", "41", "48", "method", "sample", "Coverage", "active", "Learning", "30", "57", "manual", "36", "38", "manual", "44", "56", "manual", "54", "48", "manual", "41", "48", "Table", "comparison", "active", "learning", "manual", "method", "sample", "selection", "active", "learning", "capture", "less", "sample", "during", "one-hour", "achieve", "better", "coverage", "control", "space", "than", "manually-planned", "motion", "0.8", "controllable", "0.7", "0.6", "0.5", "input", "0.4", "0.3", "0.2", "percent", "0.1", "10", "15", "20", "25", "30", "number", "Samples", "Figure", "graph", "show", "percentage", "input", "controllable", "motion", "sample", "add", "controller", "during", "capture", "session", "Coverage", "may", "decrease", "when", "new", "cluster", "introduce", "because", "point", "controller", "may", "choose", "use", "new", "cluster", "because", "its", "state", "better", "match", "than", "exist", "one", "Coverage", "decrease", "temporarily", "until", "additional", "sample", "new", "cluster", "improve", "input", "coverage", "we", "believe", "nonetheless", "indicative", "difficulty", "problem", "attractiveness", "active", "learning", "approach", "we", "also", "show", "chart", "demonstrate", "improvement", "controller", "each", "sample", "add", "Figure", "we", "use", "same", "measure", "coverage", "above", "discussion", "future", "work", "we", "have", "introduce", "active", "learning", "framework", "create", "realtime", "motion", "controller", "adaptively", "determine", "which", "motion", "add", "model", "system", "create", "finely-controllable", "motion", "model", "reduce", "number", "datum", "clip", "little", "time", "spend", "motion", "capture", "studio", "addition", "always", "present", "worst-performing", "state", "sample", "user", "have", "continuous", "gauge", "quality", "result", "controller", "active", "learning", "framework", "both", "automatically", "determine", "parameter", "each", "individual", "cluster", "determine", "necessary", "number", "relationship", "between", "different", "cluster", "dynamically", "determine", "controller", "structure", "more", "sample", "appear", "although", "we", "system", "focus", "one", "specific", "model", "motion", "synthesis", "we", "believe", "general", "approach", "adaptive", "modelbuilding", "useful", "many", "type", "animation", "model", "any", "situation", "which", "minimize", "number", "motion", "sample", "important", "can", "potentially", "benefit", "from", "active", "learning", "we", "have", "design", "system", "fast", "flexible", "so", "relatively", "little", "time", "spend", "wait", "next", "candidate", "select", "hence", "we", "employ", "number", "heuristic", "incremental", "learning", "step", "we", "model", "generally", "optimal", "global", "sense", "we", "system", "do", "make", "formal", "guarantee", "be", "able", "perform", "every", "task", "from", "every", "state", "some", "task", "may", "impossible", "-lrb-", "e.g.", "start", "new", "task", "from", "midair", "-rrb-", "other", "may", "have", "be", "capture", "limited", "time", "available", "motion", "capture", "session", "failure", "mode", "controller", "during", "synthesis", "include", "have", "cluster", "appropriate", "state", "determine", "best", "cluster", "use", "determine", "best", "weight", "within", "cluster", "tradeoff", "between", "lab", "time", "number", "sample", "coverage", "user", "must", "evaluate", "we", "example", "we", "show", "possible", "get", "high", "amount", "coverage", "low", "number", "sample", "short", "lab", "time", "some", "difficulty", "arise", "during", "capture", "process", "necessary", "user", "look", "screen", "well", "mentally", "project", "motion", "onto", "on-screen", "visualization", "we", "believe", "difficulty", "can", "overcome", "through", "use", "augmented", "virtual", "reality", "system", "we", "have", "guarantee", "scalability", "we", "believe", "system", "scale", "well", "handle", "many", "different", "task", "perform", "sequentially", "create", "highly", "flexible", "character", "however", "we", "controller", "example", "do", "fully", "test", "effectiveness", "we", "approach", "very", "high", "dimension", "work", "certainly", "do", "solve", "fundamental", "problem", "curse", "dimensionality", "data-driven", "controller", "we", "believe", "active", "learning", "generalize", "controller", "higher", "dimension", "however", "controller", "10", "more", "task", "input", "together", "37", "dof", "character", "state", "space", "even", "though", "active", "learning", "drastically", "reduce", "number", "sample", "number", "require", "sample", "would", "still", "impractical", "very", "high", "dimension", "become", "imperative", "use", "sophisticated", "motion", "model", "accurately", "represent", "nonlinear", "dynamics", "since", "expressive", "power", "model", "greatly", "reduce", "number", "require", "sample", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007", "active", "Learning", "Real-Time", "Motion", "Controllers", "5-7", "acknowledgment", "author", "would", "like", "thank", "Gary", "Yngve", "he", "help", "video", "anonymous", "reviewer", "comment", "work", "support", "UW", "Animation", "Research", "Labs", "NSF", "grant", "ccr-0092970", "eia-0321235", "electronic", "art", "Sony", "Microsoft", "Research", "NSERC", "CFI", "Alfred", "P.", "Sloan", "Foundation", "reference", "rikan", "O.", "orsyth", "D.", "A.", "O?B", "RIEN", "J.", "F.", "2003", "Motion", "synthesis", "from", "annotation", "ACM", "transaction", "Graphics", "22", "-lrb-", "July", "-rrb-", "402", "408", "TKINSON", "A.", "C.", "onev", "a.", "N.", "1992", "Optimum", "experimental", "design", "Oxford", "University", "Press", "rand", "m.", "ertzmann", "a.", "2000", "style", "machine", "Proceedings", "SIGGRAPH", "2000", "-lrb-", "July", "-rrb-", "183", "192", "ohn", "D.", "TLAS", "L.", "adner", "R.", "1994", "improving", "generalization", "active", "Learning", "machine", "Learning", "201", "221", "leicher", "M.", "hin", "H.", "J.", "OVAR", "L.", "EPSEN", "A.", "2003", "Snap-Together", "Motion", "assemble", "Run-Time", "Animations", "Proc", "i3d", "OVAR", "L.", "LEICHER", "M.", "2003", "flexible", "Automatic", "Motion", "blend", "Registration", "Curves", "Proc", "SCA", "OVAR", "L.", "LEICHER", "M.", "2004", "automate", "extraction", "parameterization", "motion", "large", "datum", "set", "ACM", "Trans", "graphic", "-lrb-", "Aug.", "-rrb-", "OVAR", "L.", "LEICHER", "M.", "ighin", "F.", "2002", "Motion", "Graphs", "ACM", "Trans", "Graphics", "21", "-lrb-", "July", "-rrb-", "473", "482", "-lrb-", "Proceedings", "ACM", "SIGGRAPH", "2002", "-rrb-", "WON", "T.", "HIN", "S.", "Y.", "2005", "Motion", "Modeling", "On-Line", "Locomotion", "synthesis", "Proc", "SCA", "au", "m.", "UFFNER", "J.", "2006", "Precomputed", "search", "Trees", "Planning", "Interactive", "Goal-Driven", "Animation", "Proc", "SCA", "ee", "J.", "ee", "K.", "H.", "2006", "precompute", "avatar", "behavior", "from", "human", "motion", "datum", "Graphical", "model", "68", "-lrb-", "mar.", "-rrb-", "158", "174", "ee", "J.", "HAI", "J.", "EITSMA", "P.", "S.", "A.", "ODGINS", "J.", "K.", "ollard", "N.", "S.", "2002", "interactive", "control", "Avatars", "animated", "human", "Motion", "Data", "ACM", "Trans", "Graphics", "21", "-lrb-", "July", "-rrb-", "491", "500", "ee", "K.", "H.", "HOI", "M.", "G.", "ee", "J.", "2006", "Motion", "patch", "building", "block", "virtual", "environment", "annotated", "motion", "datum", "ACM", "transaction", "Graphics", "25", "-lrb-", "July", "-rrb-", "898", "906", "Y.", "ang", "T.", "hum", "h.-y", "2002", "Motion", "Texture", "Two-Level", "Statistical", "Model", "Character", "Motion", "synthesis", "ACM", "Trans", "Graphics", "21", "-lrb-", "July", "-rrb-", "465", "472", "INDENBAUM", "ARKOVITCH", "usakov", "m.", "S.", "D.", "2004", "selective", "sampling", "nearest", "neighbor", "classifier", "Mach", "learn", "54", "125", "152", "iu", "C.", "K.", "opovus", "Z.", "2002", "synthesis", "complex", "Dynamic", "Character", "Motion", "from", "simple", "animation", "ACM", "Trans", "Graphics", "21", "-lrb-", "July", "-rrb-", "408", "416", "iu", "C.", "K.", "ERTZMANN", "a.", "opovus", "Z.", "2005", "Learning", "Physics-Based", "Motion", "style", "nonlinear", "inverse", "optimization", "ACM", "transaction", "Graphics", "24", "-lrb-", "Aug.", "-rrb-", "1071", "1081", "UKAI", "URIYAMA", "ELDER", "EAD", "T.", "interpolation", "ACM", "Trans", "Graphics", "24", "J.", "A.", "function", "minimization", "Computer", "Journal", "S.", "2005", "geostatistical", "motion", "-lrb-", "Aug.", "-rrb-", "1062", "1070", "R.", "1965", "simplex", "method", "308", "313", "ark", "S.", "I.", "hin", "H.", "J.", "IM", "T.", "H.", "HIN", "S.", "Y.", "2004", "on-line", "motion", "blend", "real-time", "locomotion", "generation", "Comp", "Anim", "virtual", "world", "15", "125", "138", "EITSMA", "OLLARD", "2004", "evaluate", "P.", "S.", "A.", "N.", "S.", "motion", "graph", "character", "navigation", "Proc", "SCA", "OSE", "OHEN", "ODENHEIMER", "C.", "M.", "F.", "B.", "1998", "verb", "adverb", "multidimensional", "Motion", "Interpolation", "IEEE", "Computer", "Graphics", "application", "18", "32", "40", "ose", "iii", "C.", "F.", "LOAN", "p.-p", "J.", "OHEN", "M.", "F.", "2001", "artist-directed", "inverse-kinematic", "use", "radial", "basis", "function", "interpolation", "Computer", "Graphics", "Forum", "20", "239", "250", "antner", "illiam", "otz", "design", "analysis", "computer", "experiment", "T.", "J.", "B.", "J.", "W.", "I.", "2003", "Springer", "hin", "H.", "J.", "H.", "S.", "2006", "Fat", "Graphs", "construct", "interactive", "character", "continuous", "control", "Proc", "SCA", "LOAN", "OSE", "OHEN", "III", "C.", "F.", "M.", "F.", "2001", "shape", "p.-p", "J.", "Example", "Proc", "i3d", "orresanus", "L.", "ACKNEY", "P.", "REGLER", "C.", "2007", "Learning", "Motion", "Style", "synthesis", "from", "Perceptual", "observation", "Proc", "nip", "19", "ILEY", "D.", "J.", "ahn", "J.", "K.", "1997", "interpolation", "synthesis", "articulate", "figure", "motion", "IEEE", "Computer", "Graphics", "application", "17", "-lrb-", "Nov.", "Dec.", "-rrb-", "39", "45", "ITKIN", "opovus", "a.", "Z.", "1995", "Motion", "Warping", "Proc", "SIGGRAPH", "95", "-lrb-", "Aug.", "-rrb-", "105", "108", "amane", "K.", "UFFNER", "J.", "J.", "odgin", "J.", "K.", "2004", "synthesize", "animation", "human", "manipulation", "task", "ACM", "Trans", "Graphics", "23", "-lrb-", "Aug.", "-rrb-", "532", "539", "ORDAN", "V.", "B.", "AJKOWSKA", "A.", "HIU", "B.", "AST", "M.", "2005", "Dynamic", "Response", "Motion", "Capture", "Animation", "ACM", "transaction", "Graphics", "24", "-lrb-", "Aug.", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "26", "no.", "Article", "publication", "date", "July", "2007" ],
  "content" : "\n  \n    942b3a009d6dd76d83f542a8a6ade1daca9d8ce30eabb3747cbe53fb8e516711\n    mhj\n    10.1145/1239451.1239456\n    Name identification was not possible. \n  \n  \n    \n      \n        Active Learning for Real-Time Motion Controllers\n      \n      Seth Cooper Aaron Hertzmann University of Washington\n      This paper describes an approach to building real-time highlycontrollable characters. A kinematic character controller is built on-the-fly during a capture session, and updated after each new motion clip is acquired. Active learning is used to identify which motion sequence the user should perform next, in order to improve the quality and responsiveness of the controller. Because motion clips are selected adaptively, we avoid the difficulty of manually determining which ones to capture, and can build complex controllers from scratch while significantly reducing the number of necessary motion samples. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism?Animation; Keywords: Motion Capture, Human Motion, Active Learning\n    \n    \n      \n        1 1\n        \n          \n          Figure 1: Catching controller: the character moves through space to catch two consecutive balls thrown from different directions. In addition to the current character state, the controller has a three-dimensional input space that specifies the incoming position and speed of the ball to be caught.\n        \n      \n      \n        1 Introduction\n      \n      Human motion capture data provides an effective basis for creating new animations. For example, by interpolating and concatenating motions, realistic new animations can be generated in real-time in response to user control and other inputs. In this paper, we consider such an animation model that we refer to as a motion controller: a controller generates animation in real-time, based on user-specified tasks (e.g., a user might press forward on a game controller to specify the task ?walk forward,?). Each task is parameterized by a control vector in a continuous space (e.g., ?catch the ball flying from a specific direction and velocity?). In this paper, a controller is essentially a function from the combined space of states and tasks to the space of motions. Our controllers are kinematic: they produce the character?s motion by interpolating motion capture, rather than producing motion dynamically through forces and torques. Motion capture data is very time-consuming and expensive to acquire, and thus it is desirable to capture as little as possible. When building the simplest controllers, a designer can minimize\n      ACM Reference Format Cooper, S., Hertzmann, A., Popovic  ?, Z. 2007. Active Learning for Real-Time Motion Controllers. ACM Trans. Graph. 26, 3, Article 5 (July 2007), 7 pages. DOI = 10.1145/1239451.1239456 http://doi.acm . org/10.1145/1239451.1239456. Copyright Notice Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2007 ACM 0730-0301/2007/03-ART5 $5.00 DOI 10.1145/1239451.1239456 http://doi.acm.org/10.1145/1239451.1239456\n      \n        2 1 2\n      \n      Zoran Popovi? University of Toronto\n      the amount of data captured by carefully planning the data samples to be acquired. However, for non-trivial tasks, the space of possible clips is vast, and manual selection of samples quickly becomes intractable. For example, a controller for a human that can walk and dodge projectiles must be parameterized by the direction and speed of walking, the direction and speed of the projectiles. Because the task can be changed at any point during the animation, the controller also needs to be parameterized by all possible pose configurations of the character. For example, the controller must be able to dodge any projectile that appears while the character is walking, turning, or recovering from the previous dodge. Even this moderate set of tasks leads to a huge set of initial configurations and control vectors. Determining which motions to capture for this controller ? so that it can produce good motions in this huge space of possible inputs ? is a daunting task, and, in our experience, is too difficult to do manually. Uniform sampling of the input space would be vastly inefficient, requiring a huge number of samples to be captured and then stored in memory. Because the space is structured ? that is, nearby motions in the space can often be interpolated to produce valid new motions ? selection of which motions to capture should greatly reduce the number of samples needed. However, even in a small controller space, nonlinearities in the space will mean that careful sampling is required. In this paper, we propose the use of active learning to address these problems. In particular, we build the motion controller onthe-fly during the data acquisition process. After each motion is captured, the system automatically identifies specific tasks that the controller performs poorly, based on a suite of error metrics. These tasks are presented to the user as candidates for additional data samples. The user chooses one of the tasks to perform, and the controller is updated with this new motion clip. In this way, the system continues to refine the controller until it is capable of performing any of the tasks from any state with any control vector, the time available for capture has elapsed, or else the number of data samples has been reached a predefined maximum. This process yields highly-controllable, real-time motion controllers with the realism of motion capture data, while requiring only a small amount of motion capture data. We demonstrate the feasibility of this approach by using our system to construct three example controllers. We validate the active learning approach by comparing one of these controllers to manually-constructed controllers. We emphasize that, by design, our system does not automate all decisions, but, rather, computes aspects of the controller that would be difficult for a user to handle. The user is left with specific decisions which are hard to quantitatively evaluate. Making these decisions normally entails selecting from among a few alternatives  presented by the system. In this paper, we refer to a generic ?user? that runs the system. In practice, some roles may be performed by separate individuals. For example, a human operator might run the active learning software, but have an actor perform the actual motions; later, a separate nonspecialist user (e.g., a game player) may control the motions with the learned controller. Contributions. A key contribution of this work is the use of an active learning method for animation that produces compact controllers for complex tasks. We also develop a framework that includes user input at key points of the process, including providing motion capture samples, and in selecting which motions to capture from a few automatically-determined options. We also develop a cluster-based learning model for motion controllers.\n      ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n      5-2\n      ?\n      Cooper et al.\n      \n        2 Related Work\n        A common theme in computer animation research is to create new motion from existing motion capture data. Most methods create animation off-line, for example, by interpolating a similar set of motions according to user-specified control parameters [Witkin and Popovi? 1995; Kovar and Gleicher 2004; Mukai and Kuriyama 2005; Rose et al. 1998; Wiley and Hahn 1997], by optimizing motion according to probabilistic time-series models [Brand and Hertzmann 2000; Li et al. 2002], by concatenating and blending example sequences [Arikan et al. 2003; Kovar et al. 2002; Torresani et al. 2007], or by combining modeland data-driven techniques [Yamane et al. 2004; Zordan et al. 2005; Liu and Popovi? 2002; Liu et al. 2005]. These methods generate motion off-line, whereas we consider the problem of real-time synthesis. It is worth noting that many of these methods typically require large motion databases; the need for such databases could be mitigated by our active learning approach. A number of real-time animation systems build on motion capture data as well. One approach is to directly play and transition between clips from a motion database [Gleicher et al. 2003; Lee et al. 2002; Lee et al. 2006]; precomputation can be used to allow real-time planning of which clips to use [Lau and Kuffner 2006; Lee and Lee 2006]. Reitsma and Pollard [2004] present a method for evaluating the possible motions generated by such approaches. A few authors have described methods that generate new poses in response to real-time input. Our motion controller model is most similar to methods that transition between interpolated sequences. Park et al. [2004] and Kwon and Shin [2005] combine interpolation of motions with a graph structure to generate new locomotion sequences. Shin and Oh [2006] perform interpolation on graph edges for simple models of locomotion and other repetitive motions. In previous work, it is assumed that a corpus of motion data is available in advance, or that a user will manually select which motions to capture. In this paper, we show how the use of adaptive selection of motion sequences allows the creation of controllers with greater complexity, while allowing fine-scale parameterized control and capturing relatively few motions overall. Data acquisition is difficult, expensive, and/or time-consuming for problems in many disciplines. Consequently, automatic selection of test cases has been extensively studied. In statistics, optimal experimental design methods seek the most informative test points to estimate unknown nonlinear functions [Atkinson and Donev 1992; Santner et al. 2003]. The simplest methods determine all test points in advance, e.g., via space-filling functions, or by optimizing an objective function. However, it is often not possible to determine in advance which regions of input space will need the most data. Active learning methods select test data sequentially: after each data point is acquired, the next test point is chosen to maximize an objective function [Cohn et al. 1994]. Active learning has been studied most extensively for classification problems  (e.g., [Lindenbaum et al. 2004]). In this paper, we present an active learning algorithm for motion controllers. Our approach is distinct from existing active learning methods in two ways: first, instead of choosing the next sample to capture, our system identifies a set of candidates, from which a user chooses a sample to improve; second, we assume that a metric of correctness is provided by which candidates may be chosen.\n      \n      \n        3 Motion Controllers\n        Before describing our active learning framework ? which is the main contribution of this paper ? we will briefly discuss our model for motion controllers. In our framework, a kinematic controller C : S ? T ? U ? M generates a motion clip m ? M that starts at character state s ? S and performs task t ? T parameterized by the control vector u ? U. S defines a set of all permissible character states, T is a discrete set of controller tasks, U defines the operational range of the control inputs for task t, and M defines the space of output motions. A single controller can learn and integrate several tasks, each specified by a value of the argument t. For example, a catching controller consists of two tasks: one that catches the ball coming at given speed and direction, and the idle controller that is invoked when there is no ball to be caught. We take advantage of the fact that the controller produces continuous motion clips rather individual states, and solve for the new motion m only when the task parameters u change or when the current motion m finishes. We represent a motion clip m ? M as a sequence of poses. In order to compare, blend, and visualize motions in a translationand rotationinvariant way, we decouple each pose from its translation in the ground plane and the rotation of its hips about the up axis. We represent changes in position and rotation relative to the previous pose?s local frame. We represent the state s of the character as a vector containing the poses in the next ten frames of the currentlyplaying clip. The use of multiple frames for state facilitates finding smooth motion blends. We determine the distance between two states with a method inspired by Kovar et al. [2002], by evaluating the distance between point clouds attached to each corresponding pose. We consider data-driven controllers which create a new clip m by interpolating example motion capture clips {m i } associated with task parameters {u i }. However, not all sampled clips can be blended together in a meaningful way. Consider a tennis controller: it would not make sense to blend forehand and backhand stroke motion samples. For this reason, each controller consists of groups of blendable clips which we refer to as ?clusters.? Each cluster C j contains a set of blendable motion samples {(m k, j , u k, j )} that share a common start state s j , and a continuous blend map w = b j (u) which produces motion blend weights w. Given a state s, a task t, and a control vector u, applying a controller entails two steps. First, we find the t-specific clusters with s j closest to s; if there is more than one, we use the one which has a u k, j closest to u. Second, the output motion is generated by computing the blend weights w = b j (u), and then interpolating with these weights: the new clip is ? k w k m k, j . Interpolation is performed in a time-aligned, translationand rotation-invariant manner similar to [Kovar and Gleicher 2003]. The new motion is then blended in with the currently-playing motion. However, if the current character state is very dissimilar to the beginning of m, then the new controller motion is ignored, and the current motion continues until the subsequent states s produce successful controller motion. In our examples, we also use a modified version of the controller that employs an inverse kinematics (IK) step to further satisfy endeffector constraints (e.g., catching a ball with the right hand). An IK controller C IK : S ? T ? U ? M is defined in terms of the simple blend controller as C IK (s, t, u) = IK(C (s, t, u), u). The function IK transforms the motion to satisfy kinematic constraints defined\n        ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n        Active Learning for Real-Time Motion Controllers\n        ?\n        5-3\n        \n          \n          \n        \n        u\n        \n          \n          \n        \n        (a)\n        \n          Figure 2: (a) Illustration of a catching controller for a single start state and a 2D controller space U. The space has been partitioned into two clusters, with each cross and circle representing the control vector for an example motion. The motions in the left cluster correspond to catching in place, whereas the motions on the right correspond to taking a step during the catch. The dashed line represents the boundary between clusters implicit in the nearest-neighbor assignment. (b) A motion controller consists of a collection of controllers, each for a specific task and start state.\n        \n        by u. The joint angles for the relevant end-effector (i.e., shoulder and elbow for the hand) are first optimized to satisfy the kinematic constraint in that time instant. The neighboring frames are linearly blended to produce continuous motion. The use of IK enables the controller to achieve a much greater variety of motions from a few samples and thus enables the active learning to use significantly fewer samples overall. In general, the more powerful the controller, the fewer samples active learning requires for full coverage of the controller. A new clip m is usually played from the beginning. However, in some cases, it may be useful for a motion clip to start in the middle. In particular, this occurs when the new motion, if started from the same state as the current motion, would be generated by the same cluster. This implies that the new motion would be generated by blending the same examples as the current motion. In this case, the new motion can be started at the time corresponding to the current time index of the current clip, since all motions belonging to the same cluster can be blended at any portion of the clip. This process is not appropriate for all tasks (e.g. , one cannot change trajectory in midair) and whether or not to allow this is indicated on a per-task basis. The clustered motion-blend model used in our experiments was motivated by similar models used in the game industry. In real game controllers, specific clusters and the samples within each clusters are all determined in an ad-hoc manual process. This manual process invariably produces suboptimal controllers and multiple trips to the motion capture lab. More importantly, the manual process does not scale well for control problems with a large number of input dimensions, such as those described in the results section. We point out that the active learning framework introduced in the following sections does not depend on the specific details of the controller specification. In addition, the data samples need not be captured; they can easily come from alternate sources, such as animator-created motion clips.\n      \n      \n        4 Active Learning\n        We now describe how motion controllers are built interactively. Although there are multiple steps to the process, the basic idea is simple: identify the regions of the control space that cannot be performed well, and improve them. The process requires the user to first define the control problem by enumerating the set of tasks {t k } and to specify the operational range of each control vector u. Each control vector must have a finite valid domain u ? U k (e.g., bounds constraints) in order to limit the range of allowable tasks 1 . For example, u might specify the desired walking speed, and U k might be the range of possible walking speeds. A key assumption of our\n        1 The input space U for the controller is defined as a union of the taskspecific control spaces: U = U k . k\n        Task 1 Task 2 s 1 s 1 s 2 s 2 (b)\n        Compute candidate Select motion Compute motions by to improve pseudoexample active learning Accept Evaluate Identify cluster Perform motion pseudoexample Reject\n        \n          Figure 3: Flowchart of a motion capture session using active learning. Blue rectangles are automatic processes and yellow rounded rectangles require user input.\n        \n        approach is that the motion controller does not need to be able to start from any initial state s, but only from states that might be generated by the controller. Hence, we only consider possible starting poses that might arise from another clip generated by the controller. In order to ?seed? the active learning, the user provides some initial state s. The active learning process then proceeds in the following outer loop: 1. The system identifies a set of controller inputs (s i , t i , u i ) that the motion controller cannot handle well. The user selects one to be improved. 2. The system generates an improved ?pseudoexample? motion for this task. If the pseudoexample is approved by the user, the motion controller is updated. This step can often save the user from having to perform the motion. 3. If the pseudoexample is rejected, the user performs the task, and the new motion clip is used to update the motion controller. We have set up our system in a motion capture lab. The system?s display is projected on the wall of the lab for the user to see. The interface is mouse-based so the user can interact with the system using a gyro mouse while capturing motions. We now describe these steps in detail.\n        \n          4.1 Selecting candidate tasks\n          The goal of this step is to identify control problems that the motion controller cannot handle well. A candidate problem is fully specified by a start state s, a task t, and control vector u. Because the evaluation of motions can be difficult to do purely numerically, we do not have an explicit mathematical function that can precisely identify which motions are most in need of improvement. Instead, we find multiple candidates according to different motion metrics, each of which provides a different way of evaluating the motion generated by a controller, and we let the user determine the candidate to improve. We use the following metrics:  ? Task error. For each task, the user specifies one or more metrics to measure how well a motion m performs task t with inputs u. For example, if the task requires the character?s hand to be at position u at a specific time, the metric might measure the Euclidean distance between the character?s hand and u at that time. Multiple error metrics may be provided for a task, with the goal that they all should be satisfied. It is likely that some regions of control space are simply harder to solve than others, and direct application of this metric will oversample these spaces. To address this, we compute the task error as the difference of the task-performing metric when compared to the nearest existing example. ? Sum of task errors. When more than one task metric is provided by the user, a weighted sum of these metrics is also used as a separate metric. ? Transition error. In order to evaluate blending between clips, we measure the distance between a current state and the start state s of a motion m. ? Distance from previous examples. In order to encourage exploration of the control space, we use a metric that measures the distance of a control vector from the nearest existing example control vector. To find the worst-performing regions of controller input space, we need to sample the very large controller input space. We reduce the amount of samples required by using different sampling processes for different motion metrics. To generate candidates using each task error metric, we search for motions that perform their tasks poorly. For each cluster start state, random control vectors u are generated for its task by uniform random sampling in U k . We keep the u?s which give the worst result for each motion metric. Each of these points is refined by Nelder-Mead search [1965], maximizing the badness of their respective metrics. Distance metric candidates are generated similarly, replacing task error with nearest-neighbor distance in U k . We use the optimized u, along with the t and s which generated them, as candidates. To generate candidates using transition error, we search for states that will blend poorly with existing clusters. The average motion clip is generated for each cluster by averaging the examples in that cluster. We then find the state s in this motion that is the furthest from any existing cluster?s start state. We use these states 2 , along with t and u set to the center of the domain U k , as candidates. Once all candidates have been generated, the user is then shown the candidates for each metric, sorted by score, along with information about them, such as their error and corresponding u. The user then chooses one of the candidates to be improved. Providing the user with several options has several important purposes. First, the user is able to qualitatively judge which sample would be best to improve. This would be very difficult to do with the purely automatic evaluation metrics. Second, the user is able to have some control over the order samples are collected in. For example, it can be more convenient for the user to collect several consecutive samples from the same start state. Third, viewing the candidates gives the user a good sense of the performance of the controller. As more samples are provided, the quality of the candidates improves. Once all of the candidates are considered acceptable, the user has some confidence that the controller is finished. However, there is no formal guarantee as to how the controller will perform in new tasks. In our experience, although many candidates are generated, the user usually finds one worth refining after watching only a few examples.\n          ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n          5-4\n          ?\n          Cooper et al.\n          2 We also consider the ?seed? start state, as well as the ending states of clusters which are distant enough from any start state.\n          \n            \n            Figure 4: The interactive setup. The user is preparing to perform a walking motion. The projection display shows the desired task and initial state.\n          \n        \n        \n          4.2 Determining weights\n          Before proceeding, we will discuss the details of our implementation of the per-cluster blend function, b. We implement blend functions using a scheme similar to Radial Basis Functions (RBFs), based on the method proposed by Rose et al. [1998; 2001]. We use a different form of linear basis, and replace their residual bases R j with bases that are non-uniform along each dimension:\n          \n            1\n            |u ? u j, | R j (u) = ? h ? j,\n          \n          where indexes over the elements of a control vector, and h is a cubic profile curve 3 , and the ??s are scale factors. We believe that other non-isotropic representations could perform equally well. The blend function is then\n          \n            2\n            N b i (u) = a i + c i T u + ? r j,i R j (u) j=1\n          \n          After weights are produced by this function, they are clamped to the range [0, 1] and then normalized to sum to 1. Each time a new example data point (u j , w j ) is provided, we solve for the linear weights a i and c i using least squares. We then initialize all elements of ? j to the distance to the nearest neighbor of u j , and solve for r as in Rose et al. We then update each ? j by performing a small optimization across each dimension . We place a number of evaluation samples along dimension in the neighborhood of u j . Then, for regularly sampled values in the range [1..3], we scale ? j, , solve for r, and evaluate the sum of the task error metrics at the evaluation samples; the best scale is kept. We then update ? j by these scales and solve for r one last time.\n        \n        \n          4.3 Generating a pseudoexample\n          The system first attempts to improve performance at the selected task by generating a new motion called a ?pseudoexample? [Sloan et al. 2001]. A pseudoexample is a new training motion defined as a linear combination of existing motions. It is capable of reshaping b without requiring an additional motion sample. A pseudoexample can be represented directly in terms of the weights w used to generate the motion. These weights are chosen to minimize the sum of the task error metrics for this task. To get an initial guess for the correct cluster and weights for the pseudoexample, we iterate over all clusters that start at the given s, and all the motions in the cluster as well as the currently predicted motion at u. Of all these motions, the one which performs best on the task error metric is selected as\n          3 Specifically, h(x) = (1 ? x) 3 + 3x(1 ? x) 2 for 0 ? x ? 1 and h(x) = 0 otherwise.\n          ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n          Active Learning for Real-Time Motion Controllers\n          ?\n          5-5\n          \n            \n            Figure 5: Dodging controller: the character maintains a walking direction while avoiding two consecutive projectiles. In addition to the current character state, the controller has a four-dimensional input space that specifies the incoming position and speed of the ball to be dodged and angle to turn while walking.\n          \n          providing the cluster and weights for the pseudoexample. We then optimize the weights within the cluster according to the sum of task errors metric using Nelder-Mead [1965]. The resulting weights, together with the control vector, constitute the pseudoexample (u, w). The clip generated by these weights and its evaluated task error is then shown to the user. If the user approves the pseudoexample, it is permanently added to this cluster?s RBF; otherwise, it is discarded.\n        \n        \n          4.4 Performing a new motion\n          The user is presented with the appropriate starting state and a taskspecific visualization (e.g., showing the trajectory and initial position of a ball to be caught). Since the motion is determined relative to the character, the visualizations translate and rotate with the user?s movement prior to beginning the capture. The user starts moving and eventually gets to (or near) the starting state. Our system automatically determines when the initial state conditions are met, and records the rest of the captured motion. This aspect of the interface significantly simplifies the task of performing the required motion sample. Once captured, the user can review the motion and its task error. If the motion is not satisfactory, the user must repeat the motion. In our experiments, the vast majority of tasks could be performed in three or fewer trials. We found that difficult to perform tasks typically required only a few more trials, usually no more than seven in total, although a few rare cases required up to twelve in total. To determine where the desired motion sample ends, we seek local minima of state distance in the captured clip, comparing against the beginnings and ends of the existing clusters, as well as considering the actual end of the recorded motion. We sort these by their distance, and let the user select the appropriate ending. This typically involves nothing more than the user confirming the first choice. The system now needs to determine the appropriate cluster for the new motion. The motion is timewarped to all clusters with similar start and end states, and the alignment results are shown to the user, sorted in increasing order of alignment error. The user can then select which cluster to assign the clip to. In our experiments, the first choice is frequently correct. If the user decides that no cluster is appropriate, a new cluster is created. This approach removes the need for manually determining the clusters at any point during controller synthesis. Clusters are created only when the usersupplied samples are sufficiently different from all existing clusters. The cluster is then updated with the new clip, and a new (u, w) for that clip is added to the cluster?s RBF. The active learning process is repeated by selecting a new candidate tasks. The active learning process can have many termination criteria, including the time spent in the lab, number of samples allotted to the controller, as well as the overall controller quality and controllability. The user can estimate the measure the quality and controllability of the current controller by evaluating the quality of the candidate tasks: if all ?poor-performing? candidates appear to be of high-quality, the controller has probably reached the point where no additional samples can significantly improve the quality and controller coverage.\n        \n      \n      \n        5 Results\n        We have constructed three examples to demonstrate our framework. The controllers are demonstrated in the accompanying video. Each of these controllers was constructed interactively in a short time in the motion capture lab. We capture and synthesize motions at between 20 and 25 frames per second. The computation time required is negligible: each active learning step took between 5 and 30 seconds, depending on the size of the controller involved. The majority of the time was spent in capture, especially since it normally takes two or three tries to perform a motion with the correct start state that matches the task. Our first example is a parameterized walk. The control space U is three-dimensional, controlling stride length (the distance between the heels at maximum spacing, ranging from 0.2 ? 0.9 meters), speed (ranging from 15 ? 25 centimeters per frame), and turning angle (change in horizontal orientation per stride, ranging from ??/4 to ?/4 radians). There is one task error metric for each of these features (e.g., stride length), measured as the the squared distance between the desired feature and the maximum feature in the motion. In order to determine strides, the metric assumes that a sample contains a complete walk cycle beginning with the right foot forward. This task allows motions to start from the middle. The controller for this task produced only 1 cluster, using 12 examples totaling 843 frames, and 10 pseudoexamples. We limited lab time to roughly half an hour, including all active learning and user interaction. This controller achieved 89% coverage (discussed in section 6). Our second example combines a parameterized walk with projectile dodging. This example has two tasks. When there is nothing to dodge, the control space is one-dimensional, controlling turning angle. When walking and dodging, the control space is four-dimensional, controlling turning angle as well as the incoming height, incoming angle, and distance in front of the character of the incoming projectile. The walk turning parameter as a ratio of radians per meter traveled, ranging from ??/3 to ?/3. The task metric measures the absolute value of the difference between this ratio and a the target value. The walk task allows motions to start from the middle. The parameters of the incoming projectile are specified relative to the character?s local coordinate system. They are the incoming height, from 0.25 ? 2.0 meters, the distance in front of the character to be aimed at, from 0.25 ? 1.0 meters, and the incoming angle, from ??/2 to ?/2 radians. These parameters specify a trajectory for the projectile. The error metric is the inverse of the distance between the projectile and any point on the character. The dodge task also includes the same turning control parameter as used for walking, with the same error metric as well. The synthesized controller contains 10 clusters, using total of 30 examples totaling 1121 frames, and 19 pseudoexamples. We limited lab time to roughly an hour, and achieved 76% coverage of the dodge task. Our third example is catching a ball. This example has two tasks. When not catching, there is a zero-dimensional stand task. When catching, the control space is three-dimensional, controlling the incoming position on the plane in front of the character as well as  speed. The parameters of the incoming ball are specified relative to the character?s local coordinate system. They are the incoming height, from 0.5 to 2.0 meters, the distance to the right of the character, from -2.0 to 2.0 meters, and the incoming speed, from 0.1 to 0.2 meters per frame. These parameters specify a trajectory for the ball. The task error metric is the squared distance between the ball and the character?s right hand. For this example, we removed rotation invariance from all quantities, in order to build a model in which the character is always facing a specific direction while waiting for the next ball. We have also allowed the user to load in several motions of diving to the side in place of performing these motions. Also, in this example, we use the IK controller so that we have can have greater reachability of the resulting motion. The resulting controller used 12 clusters, using 33 examples totaling 1826 frames, and 23 pseudoexamples. The data, not including the diving motions 4 , was captured in the lab session limited to roughly an hour. We achieved 57% coverage of the catch task, normalized by the manual controllers discussed in section 6. It is worth noting that we also tried creating the controller with a simpler cluster model that did not include IK, and found that the performance was significantly poorer, due to the nonlinearity of the underlying control space. In general, the more powerful the controller model, the more active learning can take advantage of it and require less samples to produce an effective controller.\n        ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n        5-6\n        ?\n        Cooper et al.\n      \n      \n        6 Evaluation\n        We have performed a numerical evaluation of the active learning method proposed using the catching controller. We compare the active learning approach with manual sample selection. We have had four animation researchers (two associated with this project and two not) plan a motion capture session for a catching controller like the one in our results. They were asked to define the blend-space clusters, and determine a compact number of samples to best cover the controller space. All manual designs required more samples than the learned controller, but produced controllers that, visually and quantitatively, were significantly inferior to our learned controller. In Table 1 , we show the number of samples and the percentage of input space coverage of each controller. For this comparison, we only consider the catch task, ignoring the stand task. The coverage is defined as the percentage of inputs for which the controller successfully matches the start state and completes the task (for catching the ball, this means the hand is within a small distance of the ball). The percentage is computed by randomly sampling state and task, similar to the scheme in Section 4.1. For this comparison, we look at the percentage of samples controllable by a given controller out of the samples controllable by any of the controllers, to remove samples physically impossible to catch. The manual controller with roughly 80% more samples produced significantly less coverage, while the controller with 20% more samples covers roughly half the space of learned controller. The total mocap studio time was roughly the same. Of course, coverage is not a perfect measure of a controller, since it does not measure realism of the motion. The difference in the quality of controllers is also apparent in the accompanying video: with the manual controllers, the balls are often caught in very strange and unrealistic ways. Of course, the evaluation depends on the skill level of people who chose the motions,\n        4 The dive catch required significant motion cleanup and was also painful to perform. In order to facilitate prototyping and testing, we captured several representative dives in advance. Then, when the active learning system requested a dive, one of these pre-captured motions was provided (if appropriate) instead of performing a new dive. However, this meant that the controller is limited to the dives in this example set, and cannot, e.g., dive while running, since this was not included in the examples.\n        \n          \n            \n              \n                \n                   Method\n                   Samples\n                   Coverage\n                \n              \n              \n                \n                   Active Learning\n                   30\n                   57%\n                \n                \n                   Manual 1\n                   36\n                   38%\n                \n                \n                   Manual 2\n                   44\n                   56%\n                \n                \n                   Manual 3\n                   54\n                   48%\n                \n                \n                   Manual 4\n                   41\n                   48%\n                \n              \n            \n          \n          Method Samples Coverage Active Learning 30 57% Manual 1 36 38% Manual 2 44 56% Manual 3 54 48% Manual 4 41 48%\n        \n        of\n        \n          Table 1: Comparison of active learning and manual methods of sample selection. Active learning captured less samples during a one-hour period, but achieved better coverage of the control space than manually-planned motions. 0.8 Controllable 0.7 0.6 0.5 Inputs 0.4 0.3 0.2 Percent 0.1 0 5 10 15 20 25 30 Number of Samples\n        \n        \n          Figure 6: This graph shows the percentage of inputs controllable as motion samples are added to the controller during a capture session. Coverage may decrease when a new cluster is introduced, because at that point, the controller may choose to use this new cluster because its start state is a better match than an existing one. Coverage decreases temporarily until additional samples in the new cluster improve the input coverage.\n        \n        but we believe that it is nonetheless indicative of the difficulty of the problem and the attractiveness of the active learning approach. We also show a chart demonstrating the improvement of the controller as each sample is added in Figure 6 . We use the same measure of coverage as above.\n      \n      \n        7 Discussion and Future Work\n        We have introduced an active learning framework for creating realtime motion controllers. By adaptively determining which motions to add to the model, the system creates finely-controllable motion models with a reduced number of data clips and little time spent in the motion capture studio. In addition, by always presenting the worst-performing state samples, the user has a continuous gauge of the quality of the resulting controller. The active learning framework both automatically determines the parameters of each individual cluster and determines the necessary number and relationship between different clusters, dynamically determining the controller structure as more samples appear. Although our system focuses on one specific model for motion synthesis, we believe that the general approach of adaptive modelbuilding will be useful for many types of animation models ? any situation in which minimizing the number of motion samples is important can potentially benefit from active learning. We have designed the system to be fast and flexible, so that relatively little time is spent waiting for the next candidate to be selected. Hence, we employed a number of heuristic, incremental learning steps, and our models are not generally ?optimal? in a global sense. Our system does not make formal guarantees of being able to perform every task from every start state. Some of these tasks may be impossible (e.g., starting a new task from midair); others may not have been captured in the limited time available in a motion capture session. The failure modes of the controller during synthesis include not having a cluster with the appropriate start state, not determining the best cluster to use, and not determining the best weights within a cluster. There is a tradeoff between lab time, number of samples, and coverage that the user must evaluate. In our examples we show that it is possible to get a high amount of coverage with a low number of samples and short lab time. Some difficulties arise during the capture process. It is necessary for the user to look at the screen as well as mentally project their  motion onto the on-screen visualization. We believe that these difficulties can be overcome through the use of an augmented or virtual reality system. We have no guarantee of scalability, but we believe that this system will scale well to handle many different tasks performed sequentially, creating highly flexible characters. However, our controller examples do not fully test the effectiveness of our approach in very high dimensions, and this work certainly does not solve the fundamental problem of ?curse of dimensionality? for data-driven controllers. We believe that active learning will generalize to controllers with higher dimensions. However, for controllers with 10 or more task inputs together with 37 DOF characters state space, even though active learning will drastically reduce the number of samples, the number of required samples would still be impractical. In very high dimensions, it becomes an imperative to use sophisticated motion models that accurately represent nonlinear dynamics, since the expressive power of such models greatly reduces the number of required samples.\n        ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n        Active Learning for Real-Time Motion Controllers\n        ?\n        5-7\n      \n      \n        Acknowledgments\n        The authors would like to thank Gary Yngve for his help with the video, and the anonymous reviewers for their comments. This work was supported by the UW Animation Research Labs, NSF grants CCR-0092970, EIA-0321235, Electronic Arts, Sony, Microsoft Research, NSERC, CFI, and the Alfred P. Sloan Foundation.\n      \n      \n        References\n        \n          A RIKAN , O., F ORSYTH , D. A., AND O?B RIEN , J. F. 2003. Motion synthesis from annotations. ACM Transactions on Graphics 22, 3 (July), 402?408.\n          A TKINSON , A. C., AND D ONEV , A. N. 1992. Optimum Experimental Designs. Oxford University Press.\n          B RAND , M., AND H ERTZMANN , A. 2000. Style machines. Proceedings of SIGGRAPH 2000 (July), 183?192.\n          C OHN , D., A TLAS , L., AND L ADNER , R. 1994. Improving Generalization with Active Learning. Machine Learning 5, 2, 201? 221.\n          G LEICHER , M., S HIN , H. J., K OVAR , L., AND J EPSEN , A. 2003. Snap-Together Motion: Assembling Run-Time Animations. In Proc. I3D.\n          K OVAR , L., AND G LEICHER , M. 2003. Flexible Automatic Motion Blending with Registration Curves. In Proc. SCA.\n          K OVAR , L., AND G LEICHER , M. 2004. Automated Extraction and Parameterization of Motions in Large Data Sets. ACM Trans. on Graphics (Aug.).\n          K OVAR , L., G LEICHER , M., AND P IGHIN , F. 2002. Motion Graphs. ACM Trans. on Graphics 21, 3 (July), 473?482. (Proceedings of ACM SIGGRAPH 2002).\n          K WON , T., AND S HIN , S. Y. 2005. Motion Modeling for On-Line Locomotion Synthesis. In Proc. SCA.\n          L AU , M., AND K UFFNER , J. 2006. Precomputed Search Trees: Planning for Interactive Goal-Driven Animation. In Proc. SCA.\n          L EE , J., AND L EE , K. H. 2006. Precomputing avatar behavior from human motion data. Graphical Models 68, 2 (Mar.), 158? 174.\n          L EE , J., C HAI , J., R EITSMA , P. S. A., H ODGINS , J. K., AND P OLLARD , N. S. 2002. Interactive Control of Avatars Animated With Human Motion Data. ACM Trans. on Graphics 21, 3 (July), 491?500.\n          L EE , K. H., C HOI , M. G., AND L EE , J. 2006. Motion patches: building blocks for virtual environments annotated with motion data. ACM Transactions on Graphics 25, 3 (July), 898?906.\n          L I , Y., W ANG , T., AND S HUM , H.-Y. 2002. Motion Texture: A Two-Level Statistical Model for Character Motion Synthesis. ACM Trans. on Graphics 21, 3 (July), 465?472.\n        \n      \n      \n        INDENBAUM ARKOVITCH AND USAKOV\n        L , M., M , S., R , D. 2004. Selective sampling for nearest neighbor classifiers. Mach. Learn. 54, 2, 125?152.\n        L IU , C. K., AND P OPOVI C  ? , Z. 2002. Synthesis of Complex Dynamic Character Motion from Simple Animations. ACM Trans. on Graphics 21, 3 (July), 408?416. L IU , C. K., H ERTZMANN , A., AND P OPOVI C  ? , Z. 2005. Learning Physics-Based Motion Style with Nonlinear Inverse Optimization. ACM Transactions on Graphics 24, 3 (Aug.), 1071?1081.\n      \n      \n        UKAI AND URIYAMA\n        K\n      \n      \n        ELDER AND EAD\n        M , T., interpolation. ACM Trans. on Graphics 24, N , J. A., M function minimization. Computer Journal\n        , S. 2005. Geostatistical motion 3 (Aug.), 1062?1070. , R. 1965. A simplex method for 7, 4, 308?313.\n        P ARK , S. I., S HIN , H. J., K IM , T. H., AND S HIN , S. Y. 2004. On-line motion blending for real-time locomotion generation. Comp. Anim. Virtual Worlds 15, 125?138.\n      \n      \n        EITSMA AND OLLARD\n        2004. Evaluating\n        R , P. S. A., P , N. S. motion graphs for character navigation. In Proc. SCA.\n      \n      \n        OSE OHEN AND ODENHEIMER\n        R , C., C , M. F., B , B. 1998. Verbs and Adverbs: Multidimensional Motion Interpolation. IEEE Computer Graphics & Applications 18, 5, 32?40.\n        R OSE III, C. F., S LOAN , P.-P. J., AND C OHEN , M. F. 2001. Artist-Directed Inverse-Kinematics Using Radial Basis Function Interpolation. Computer Graphics Forum 20, 3, 239?250.\n        S\n      \n      \n        ANTNER ILLIAMS AND OTZ\n        Design and Analysis of Computer Experiments.\n        , T. J., W , B. J., N , W. I. 2003. The Springer.\n        S HIN , H. J., AND O H , H. S. 2006. Fat Graphs: Constructing an interactive character with continuous controls. In Proc. SCA.\n      \n      \n        LOAN OSE AND OHEN\n        III, C. F., C , M. F. 2001. Shape\n        S , P.-P. J., R by Example. In Proc. I3D.\n        T ORRESANI , L., H ACKNEY , P., AND B REGLER , C. 2007. Learning Motion Style Synthesis from Perceptual Observations. In Proc. NIPS 19.  W ILEY , D. J., AND H AHN , J. K. 1997. Interpolation synthesis of articulated figure motion. IEEE Computer Graphics and Applications 17, 6 (Nov./Dec.), 39?45.\n        W\n      \n      \n        ITKIN AND OPOVI C\n        , A.,\n        P  ? , Z. 1995. Motion Warping. Proc. SIGGRAPH 95 (Aug.), 105?108.\n        Y AMANE , K., K UFFNER , J. J., AND H ODGINS , J. K. 2004. Synthesizing animations of human manipulation tasks. ACM Trans. on Graphics 23, 3 (Aug.), 532?539. Z ORDAN , V. B., M AJKOWSKA , A., C HIU , B., AND F AST , M. 2005. Dynamic Response for Motion Capture Animation. ACM Transactions on Graphics 24, 3 (Aug.).\n        ACM Transactions on Graphics, Vol. 26, No. 3, Article 5, Publication date: July 2007.\n      \n    \n  ",
  "resources" : [ ]
}