{
  "uri" : "sig2011-a31-shiratori_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a31-shiratori_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Motion Capture from Body-Mounted Cameras",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Takaaki-Shiratori",
      "name" : "Takaaki",
      "surname" : "Shiratori"
    }, {
      "uri" : "http://drinventor/Hyun Soo-Park",
      "name" : "Hyun Soo",
      "surname" : "Park"
    }, {
      "uri" : "http://drinventor/Leonid-Sigal",
      "name" : "Leonid",
      "surname" : "Sigal"
    }, {
      "uri" : "http://drinventor/Yaser-Sheikh",
      "name" : "Yaser",
      "surname" : "Sheikh"
    }, {
      "uri" : "http://drinventor/Jessica K.-Hodgins",
      "name" : "Jessica K.",
      "surname" : "Hodgins"
    } ]
  },
  "bagOfWords" : [ "Motion", "capture", "have", "be", "use", "provide", "much", "character", "motion", "several", "recent", "theatrical", "release", "Avatar", "motion", "capture", "use", "animate", "character", "ride", "direhorse", "fly", "back", "mountain", "banshee", "-lsb-", "Duncan", "2010", "-rsb-", "capture", "realistic", "motion", "scene", "actor", "ride", "horse", "robotic", "mockup", "expansive", "motion", "capture", "studio", "require", "large", "number", "camera", "Coverage", "lighting", "problem", "often", "prevent", "director", "from", "capture", "motion", "natural", "setting", "other", "large", "environment", "inertial", "system", "one", "describe", "Vlasic", "colleague", "-lsb-", "2007", "-rsb-", "allow", "capture", "occur", "outdoor", "space", "design", "recover", "only", "relative", "motion", "joint", "global", "root", "motion", "paper", "we", "present", "wearable", "system", "outward-looking", "camera", "allow", "reconstruction", "relative", "global", "motion", "actor", "outside", "laboratory", "closed", "stage", "camera", "can", "mount", "casual", "clothing", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "easily", "mount", "remove", "use", "velcro", "attachment", "lightweight", "enough", "allow", "unimpeded", "movement", "structurefrom-motion", "-lrb-", "sfm", "-rrb-", "use", "estimate", "pose", "camera", "throughout", "capture", "estimate", "camera", "movement", "from", "range-of-motion", "sequence", "use", "automatically", "build", "skeleton", "use", "co-occur", "transformation", "limb", "connect", "each", "joint", "reconstruct", "camera", "skeleton", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "use", "initialization", "overall", "optimization", "compute", "root", "position", "orientation", "joint", "angle", "while", "minimize", "image", "match", "error", "Reference", "imagery", "capture", "area", "leverage", "reduce", "drift", "we", "render", "motion", "skin", "character", "apply", "recover", "skeletal", "motion", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "estimate", "camera", "pose", "global", "relative", "motion", "actor", "can", "capture", "outdoors", "under", "wide", "variety", "lighting", "condition", "extended", "indoor", "region", "without", "any", "additional", "equipment", "we", "also", "avoid", "some", "miss", "datum", "problem", "introduce", "occlusion", "between", "marker", "camera", "traditional", "optical", "motion", "capture", "because", "we", "system", "any", "visually", "distinctive", "feature", "world", "can", "serve", "marker", "traditional", "system", "by-product", "capture", "process", "sparse", "3d", "structure", "scene", "structure", "useful", "guide", "define", "ground", "geometry", "first", "sketch", "scene", "3d", "animator", "director", "we", "evaluate", "we", "approach", "against", "motion", "capture", "datum", "generate", "Vicon", "optical", "motion", "capture", "system", "report", "mean", "joint", "position", "error", "1.76", "cm", "mean", "joint", "angle", "error", "3.01", "full", "range-of-motion", "sequence", "use", "skeleton", "estimation", "we", "result", "demonstrate", "system", "can", "reconstruct", "action", "difficult", "capture", "traditional", "motion", "capture", "system", "include", "outdoor", "activity", "direct", "sunlight", "activity", "occlude", "near", "proximal", "structure", "extend", "indoor", "activity", "we", "prototype", "first", "we", "knowledge", "employ", "camera", "sensor", "motion", "capture", "measure", "environment", "estimate", "motion", "set", "camera", "relate", "underlie", "articulate", "structure", "current", "camera", "inexpensive", "have", "form", "factor", "rival", "inertial", "measurement", "unit", "-lrb-", "imus", "-rrb-", "already", "embed", "everyday", "handheld", "device", "we", "approach", "continue", "benefit", "from", "consumer", "trend", "drive", "camera", "become", "cheaper", "smaller", "faster", "more", "pervasive", "give", "expect", "continuation", "technological", "trend", "we", "believe", "system", "one", "propose", "here", "become", "viable", "alternative", "traditional", "motion", "capture", "technology", "variety", "motion", "capture", "technology", "currently", "available", "both", "commercially", "prototype", "advantage", "disadvantage", "different", "design", "discuss", "several", "survey", "-lrb-", "e.g.", "-lsb-", "Welch", "Foxlin", "2002", "Moeslund", "et", "al.", "2006", "-rsb-", "-rrb-", "Motion", "capture", "system", "can", "classify", "outside-in", "-lsb-", "Welch", "Foxlin", "2002", "-rsb-", "rely", "sensor", "mount", "environment", "passive", "any", "marker", "body", "definition", "requirement", "restrict", "use", "laboratory", "environment", "closed", "stage", "setting", "because", "capture", "space", "have", "instrument", "sensor", "inside-out", "system", "-lsb-", "Welch", "Foxlin", "2002", "-rsb-", "rely", "sensor", "body", "recover", "3d", "pose", "portability", "allow", "use", "both", "indoor", "outdoor", "environment", "we", "approach", "fall", "latter", "category", "here", "we", "review", "most", "relevant", "method", "system", "Optical", "motion", "capture", "system", "-lsb-", "woltring", "1974", "-rsb-", "among", "most", "widely", "use", "industry", "today", "commercial", "system", "available", "from", "Vicon", "-lrb-", "www.vicon.com", "-rrb-", "qualisy", "-lrb-", "www.qualisys.com", "-rrb-", "among", "other", "Optical", "motion", "capture", "system", "use", "set", "specialize", "high-resolution", "video", "camera", "track", "retro-reflective", "marker", "light-emitting", "diode", "-lrb-", "led", "-rrb-", "place", "key", "point", "body", "Triangulation", "use", "recover", "3d", "position", "marker", "space", "3d", "marker", "position", "turn", "use", "fit", "skeletal", "model", "observe", "motion", "system", "popular", "due", "accuracy", "major", "disadvantage", "cost", "portability", "intrusiveness", "Optical", "system", "require", "indoor", "setup", "typically", "cost", "between", "ten", "hundred", "thousand", "dollar", "use", "photosensor", "explore", "raskar", "colleague", "-lsb-", "2007", "-rsb-", "propose", "system", "rely", "measure", "spatio-temporal", "light", "modulation", "produce", "multiple", "led", "transmitter", "emit", "gray", "code", "pattern", "receiver", "module", "equip", "infrared", "rgb", "photosensor", "be", "task", "decoding", "-lrb-", "demultiplexing", "-rrb-", "observe", "pattern", "do", "so", "directly", "produce", "3d", "spatial", "location", "-lrb-", "side", "effect", "measure", "incident", "light", "scene", "light", "matching", "-rrb-", "while", "system", "inspirational", "we", "utilize", "simplify", "photosensor", "camera", "wear", "body", "fundamentally", "different", "from", "we", "approach", "because", "require", "transmitter", "environment", "alleviate", "intrusive", "characteristic", "marker-based", "motion", "capture", "system", "marker-less", "motion", "capture", "technology", "have", "be", "develop", "number", "researcher", "-lsb-", "Cheung", "et", "al.", "2003", "Deutscher", "Reid", "2005", "Moeslund", "et", "al.", "2006", "Hasler", "et", "al.", "2009", "-rsb-", "marker-less", "method", "most", "often", "use", "regular", "video", "camera", "simple", "-lrb-", "e.g.", "chromakey", "-rrb-", "background", "reconstruct", "voxel", "representation", "body", "over", "time", "fit", "skeletal", "model", "voxel", "representation", "similar", "paradigm", "use", "system", "develop", "Organic", "Motion", "-lrb-", "www.organicmotion.com", "-rrb-", "recent", "study", "-lsb-", "Corazza", "et", "al.", "2006", "Corazza", "et", "al.", "2010", "-rsb-", "suggest", "sufficient", "number", "camera", "favorable", "imaging", "condition", "accuracy", "marker-less", "method", "can", "rival", "traditional", "optical", "motion", "capture", "Hasler", "colleague", "-lsb-", "2009", "-rsb-", "introduce", "approach", "capture", "motion", "actor", "outdoor", "environment", "from", "multiple", "inward-looking", "move", "camera", "method", "use", "audio", "synchronize", "camera", "fit", "3d", "scan", "actor", "silhouette", "estimate", "each", "move", "camera", "markerless", "method", "require", "image", "segmentation", "3d", "scan", "actor", "most", "direct", "approach", "measure", "human", "motion", "through", "use", "wearable", "electro-mechanical", "system", "e.g.", "gypsy", "-lrb-", "www.animazoo.com", "-rrb-", "system", "consist", "exoskeleton", "suit", "embedded", "lightweight", "rod", "articulate", "performer?s", "bone", "potentiometer", "joint", "measure", "angular", "rotation", "rod", "convert", "joint", "angle", "use", "kinematic", "model", "system", "while", "capable", "directly", "measure", "motion", "subject", "intrusive", "uncomfortable", "wear", "recently", "have", "be", "number", "self-contained", "wearable", "experimental", "system", "develop", "base", "variety", "sensor", "technology", "-lrb-", "e.g.", "-lsb-", "Schwarz", "et", "al.", "2010", "Zhang", "et", "al.", "2009", "-rsb-", "-rrb-", "include", "ultrasound", "imus", "tri-axial", "accelerometer", "inertial", "motion", "capture", "system", "-lrb-", "e.g.", "Xsens", "MVN", "www.xsens.com", "-rrb-", "measure", "rotation", "body", "part", "world", "use", "accelerometer", "gyroscope", "system", "portable", "can", "take", "outside", "however", "only", "able", "measure", "orientation", "body", "part", "motion", "body", "world", "multiple", "sensor", "can", "combine", "alleviate", "drift", "example", "Vlasic", "colleague", "-lsb-", "2007", "-rsb-", "add", "ultrasonic", "sensor", "imus", "Alternatives", "battle", "drift", "include", "data-driven", "approach", "base", "motion", "capture", "datum", "stabilize", "accelerometer", "estimate", "-lsb-", "Slyper", "Hodgins", "2008", "Xie", "et", "al.", "2008", "Kelly", "et", "al.", "2010", "Tautges", "et", "al.", "2011", "-rsb-", "we", "system", "camera-based", "therefore", "rely", "rich", "datum", "detailed", "view", "environment", "we", "use", "image", "from", "camera", "along", "estimate", "3d", "geometry", "environment", "recover", "3d", "limb", "position", "orientation", "world", "over", "time", "thus", "we", "build", "substantial", "prior", "work", "sfm", "-lsb-", "Hartley", "Zisserman", "2004", "Pollefeys", "et", "al.", "2004", "Snavely", "et", "al.", "2006", "-rsb-", "visual", "simultaneous", "localization", "mapping", "-lrb-", "slam", "-rrb-", "-lsb-", "Welch", "et", "al.", "1999", "Davison", "et", "al.", "2007", "Klein", "Murray", "2007", "-rsb-", "approach", "have", "be", "use", "estimate", "motion", "move", "platform", "-lsb-", "Ballan", "et", "al.", "2010", "N?ster", "et", "al.", "2006", "-rsb-", "even", "human", "-lsb-", "Oskiper", "et", "al.", "2007", "Zhu", "et", "al.", "2007", "Zhu", "et", "al.", "2008", "-rsb-", "however", "recover", "only", "independent", "ego-motion", "individual", "camera", "platform", "we", "work", "first", "reconstruct", "3d", "motion", "set", "camera", "relate", "underlie", "articulate", "structure" ],
  "content" : "Motion capture has been used to provide much of the character motion in several recent theatrical releases. In Avatar, motion capture was used to animate characters riding on direhorses and flying on the back of mountain banshees [Duncan 2010]. To capture realistic motion for such scenes, the actors rode horses and robotic mockups in an expansive motion capture studio requiring a large number of cameras. Coverage and lighting problems often prevent directors from capturing motion in natural settings or in other large environments. Inertial systems, such as the one described by Vlasic and colleagues [2007], allow capture to occur in outdoor spaces but are designed to recover only the relative motion of the joints, not the global root motion. In this paper, we present a wearable system of outward-looking cameras that allow the reconstruction of the relative and the global motion of an actor outside of a laboratory or closed stage. The cameras can be mounted on casual clothing ( Figure 1(a) ), are easily mounted and removed using Velcro attachments, and are lightweight enough to allow unimpeded movement. Structurefrom-motion (SfM) is used to estimate the pose of the cameras throughout the capture. The estimated camera movements from a range-of-motion sequence are used to automatically build a skeleton using co-occurring transformations of the limbs connecting each joint. The reconstructed cameras and skeleton ( Figure 1(b) ) are used as an initialization for an overall optimization to compute the root position, orientation, and joint angles while minimizing the image matching error. Reference imagery of the capture area is leveraged to reduce drift. We render the motion of a skinned character by applying the recovered skeletal motion ( Figure 1(c) ). By estimating the camera poses, the global and relative motion of an actor can be captured outdoors under a wide variety of lighting conditions or in extended indoor regions without any additional equipment. We also avoid some of the missing data problems introduced by occlusions between the markers and cameras in traditional optical motion capture, because, in our system, any visually distinctive feature in the world can serve as a marker in the traditional systems. A by-product of the capture process is a sparse 3D structure of the scene. This structure is useful as a guide for defining the ground geometry and as a first sketch of the scene for 3D animators and directors. We evaluate our approach against motion capture data generated by a Vicon optical motion capture system and report a mean joint position error of 1.76 cm and a mean joint angle error of 3.01 ? on the full range-of-motion sequence used for skeleton estimation. Our results demonstrate that the system can reconstruct actions that are difficult to capture with traditional motion capture systems, including outdoor activities in direct sunlight, activities that are occluded by near by proximal structures, and extended indoor activities. Our prototype is the first, to our knowledge, to employ camera sensors for motion capture by measuring the environment and to estimate the motion of a set of cameras that are related by an underlying articulated structure. Current cameras are inexpensive, have form factors that rival inertial measurement units (IMUs), and are already embedded in everyday handheld devices. Our approach will continue to benefit from consumer trends that are driving cameras to become cheaper, smaller, faster, and more pervasive. Given the expected continuation of these technological trends, we believe that systems such as the one proposed here, will become viable alternatives to traditional motion capture technologies. There are a variety of motion capture technologies currently available both commercially and as prototypes. The advantages and disadvantages of the different designs are discussed in several surveys (e.g., [Welch and Foxlin 2002; Moeslund et al. 2006]). Motion capture systems can be classified as outside-in [Welch and Foxlin 2002], in that they rely on sensors mounted in the environment and passive, if any, markers on the body. By definition, this requirement restricts their use to laboratory environments or closed stage settings, because the capture space has to be instrumented with the sensors. Inside-out systems [Welch and Foxlin 2002] rely on sensors on the body to recover the 3D pose. This portability allows their use in both indoor and outdoor environments. Our approach falls into the latter category. Here, we review the most relevant methods and systems. Optical motion capture systems [Woltring 1974] are among the most widely used in the industry today; commercial systems are available from Vicon (www.vicon.com) and Qualisys (www.qualisys.com), among others. Optical motion capture systems use a set of specialized high-resolution video cameras to track retro-reflective markers or light-emitting diodes (LEDs) placed at key points on the body. Triangulation is used to recover the 3D position of these markers in space, and the 3D marker positions, in turn, are used to fit a skeletal model to the observed motion. These systems are popular due to their accuracy; their major disadvantages are cost, portability, and intrusiveness. Optical systems require indoor setups that typically cost between tens and hundreds of thousands of dollars. The use of photosensors was explored by Raskar and colleagues [2007]. Their proposed system relied on measuring the spatio-temporal light modulations produced by multiple LED transmitters that emitted gray coded patterns. The receiver modules, equipped with infrared and RGB photosensors, were tasked with decoding (demultiplexing) the observed patterns and, in doing so, directly producing the 3D spatial location (and as a side effect measuring incident light for scene light matching). While their system was inspirational for us in that it utilized a simplified photosensor as a ?camera? worn on the body, it is fundamentally different from our approach, because it requires transmitters in the environment. To alleviate the intrusive characteristics of marker-based motion capture systems, marker-less motion capture technologies have been developed by a number of researchers [Cheung et al. 2003;  Deutscher and Reid 2005; Moeslund et al. 2006; Hasler et al. 2009]. Marker-less methods most often use regular video cameras with simple (e.g., chromakey) backgrounds to reconstruct a voxel representation of the body over time and then fit a skeletal model to the voxel representations. A similar paradigm is used by the system developed by Organic Motion (www.organicmotion.com). Recent studies [Corazza et al. 2006; Corazza et al. 2010] suggest that with a sufficient number of cameras and favorable imaging conditions, the accuracy of marker-less methods can rival that of traditional optical motion capture. Hasler and colleagues [2009] introduced an approach to capture the motion of an actor in outdoor environments from multiple inward-looking moving cameras. The method uses audio to synchronize the cameras and fits a 3D scan of the actor to silhouettes estimated in each of the moving cameras. The markerless methods require image segmentation, or a 3D scan of the actor. The most direct approach to measuring human motion is through the use of a wearable electro-mechanical system; e.g., Gypsy (www.animazoo.com). Such systems consist of an exoskeleton suit with embedded lightweight rods that articulate with the performer?s bones. Potentiometers at the joints measure the angular rotation of the rods, and are converted to joint angles using a kinematic model. Such systems, while capable of directly measuring the motion of the subject, are intrusive and uncomfortable to wear. Recently, there have been a number of self-contained, wearable experimental systems developed based on a variety of sensor technologies (e.g., [Schwarz et al. 2010; Zhang et al. 2009]), including ultrasound, IMUs, and tri-axial accelerometers. Inertial motion capture systems (e.g., Xsens MVN, www.xsens.com) measure the rotation of body parts in the world using accelerometers and gyroscopes. These systems are portable and can be taken outside; however, they are only able to measure the orientation of body parts, not the motion of the body in the world. Multiple sensors can be combined to alleviate drift. For example, Vlasic and colleagues [2007] added ultrasonic sensors to IMUs. Alternatives for battling drift include data-driven approaches based on motion capture data to stabilize accelerometer estimates [Slyper and Hodgins 2008; Xie et al. 2008; Kelly et al. 2010; Tautges et al. 2011]. Our system is camera-based and therefore relies on the rich data in a detailed view of the environment. We use the images from the cameras along with the estimated 3D geometry of the environment to recover the 3D limb positions and orientations in the world over time. Thus, we build on substantial prior work in SfM [Hartley and Zisserman 2004; Pollefeys et al. 2004; Snavely et al. 2006] and visual Simultaneous Localization and Mapping (SLAM) [Welch et al. 1999; Davison et al. 2007; Klein and Murray 2007]. These approaches have been used for estimating the motion of moving platforms [Ballan et al. 2010; N?ster et al. 2006] and even humans [Oskiper et al. 2007; Zhu et al. 2007; Zhu et al. 2008]. However, they recovered only the independent ego-motion of individual camera platforms. Our work is the first to reconstruct the 3D motion of a set of cameras related by an underlying articulated structure.",
  "resources" : [ ]
}