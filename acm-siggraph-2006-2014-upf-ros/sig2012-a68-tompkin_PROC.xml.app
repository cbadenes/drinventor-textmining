{
  "uri" : "sig2012-a68-tompkin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a68-tompkin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Videoscapes: Exploring Sparse, Unstructured Video Collections",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/James-Tompkin",
      "name" : "James",
      "surname" : "Tompkin"
    }, {
      "uri" : "http://drinventor/Kwang In-Kim",
      "name" : "Kwang In",
      "surname" : "Kim"
    }, {
      "uri" : "http://drinventor/Jan-Kautz",
      "name" : "Jan",
      "surname" : "Kautz"
    }, {
      "uri" : "http://drinventor/Christian-Theobalt",
      "name" : "Christian",
      "surname" : "Theobalt"
    } ]
  },
  "bagOfWords" : [ "abundance", "mobile", "device", "digital", "camera", "video", "capture", "make", "easy", "obtain", "large", "collection", "video", "clip", "contain", "same", "location", "environment", "event", "however", "unstructured", "collection", "difficult", "comprehend", "explore", "we", "propose", "system", "analyze", "collection", "unstructured", "related", "video", "datum", "create", "Videoscape", "datum", "structure", "enable", "interactive", "exploration", "video", "collection", "visually", "navigate", "spatially", "and/or", "temporally", "between", "different", "clip", "we", "automatically", "identify", "transition", "opportunity", "portal", "now", "structure", "video", "can", "interactively", "explore", "walk", "graph", "geographic", "map", "give", "system", "we", "gauge", "preference", "different", "video", "transition", "style", "user", "study", "generate", "heuristic", "automatically", "choose", "appropriate", "transition", "style", "recent", "year", "have", "be", "explosion", "mobile", "device", "capable", "record", "photograph", "can", "share", "community", "platform", "research", "community", "have", "start", "harvest", "immense", "amount", "datum", "from", "community", "photo", "collection", "have", "develop", "tool", "estimate", "spatial", "relation", "between", "photograph", "reconstruct", "3d", "geometry", "certain", "landmark", "sufficiently", "dense", "set", "photo", "available", "-lsb-", "Snavely", "et", "al.", "2006", "Goesele", "et", "al.", "2007", "Agarwal", "et", "al.", "2009", "Frahm", "et", "al.", "2010b", "-rsb-", "user", "can", "interactively", "explore", "location", "view", "reconstruct", "3d", "model", "spatially", "transition", "between", "photograph", "Navigation", "tool", "like", "Google", "Street", "View", "Bing", "Maps", "also", "use", "exploration", "paradigm", "reconstruct", "entire", "street", "network", "through", "alignment", "purposefully", "capture", "imagery", "via", "additionally", "record", "localization", "depth", "sensor", "datum", "photo", "exploration", "tool", "ideal", "view", "navigate", "static", "landmark", "Notre", "Dame", "can", "convey", "dynamics", "liveliness", "spatio-temporal", "relationship", "location", "event", "one", "solution", "employ", "video", "datum", "yet", "comparable", "browse", "experience", "casually", "capture", "video", "generation", "still", "open", "challenge", "one", "may", "tempt", "think", "video", "simply", "series", "image", "so", "straightforward", "extension", "image-based", "approach", "should", "serve", "purpose", "enable", "video", "tour", "casually", "capture", "video", "collection", "usually", "sparse", "largely", "unstructured", "unlike", "dense", "photo", "collection", "use", "approach", "mention", "above", "preclude", "dense", "reconstruction", "registration", "all", "frame", "furthermore", "exploration", "interface", "should", "reflect", "dynamic", "temporal", "nature", "video", "paper", "we", "propose", "system", "explore", "unstructured", "video", "collection", "immersive", "visually", "compelling", "manner", "give", "sparse", "video", "collection", "certain", "-lrb-", "possibly", "large", "-rrb-", "area", "e.g.", "inner", "city", "London", "user", "can", "tour", "through", "video", "collection", "follow", "video", "transition", "between", "they", "correspond", "view", "end", "we", "compute", "videoscape", "graph", "structure", "from", "collection", "video", "-lrb-", "figure", "-rrb-", "edge", "Videoscape", "video", "segment", "node", "mark", "possible", "transition", "point", "portal", "between", "video", "Videoscape", "can", "explore", "interactively", "play", "video", "clip", "transition", "other", "clip", "when", "portal", "arise", "when", "temporal", "context", "relevant", "temporal", "awareness", "event", "provide", "offer", "correctly", "order", "transition", "between", "temporally", "align", "video", "yield", "meaningful", "spatio-temporal", "viewing", "experience", "large", "unstructured", "video", "collection", "map-based", "viewing", "mode", "let", "user", "choose", "end", "video", "automatically", "find", "path", "video", "transition", "join", "they", "gp", "orientation", "datum", "enhance", "map", "view", "when", "available", "furthermore", "image", "can", "give", "system", "from", "which", "closest", "matching", "portal", "form", "path", "through", "Videoscape", "enhance", "experience", "when", "transition", "through", "portal", "we", "develop", "different", "video", "transition", "mode", "appropriate", "transition", "select", "base", "preference", "participant", "user", "study", "finally", "we", "evaluate", "videoscape", "system", "three", "further", "user", "study", "videoscape", "exploration", "explorer", "application", "enable", "intuitive", "seamless", "spatio-temporal", "exploration", "Videoscape", "base", "several", "novel", "exploration", "paradigm", "videoscape", "evaluation", "four", "user", "study", "provide", "quantitative", "qualitative", "datum", "compare", "Videoscapes", "exist", "system", "include", "user", "study", "analyze", "preferred", "transition", "type", "heuristic", "appropriate", "use", "Video", "Google", "-lsb-", "Sivic", "Zisserman", "2003", "-rsb-", "one", "first", "system", "enable", "video", "retrieval", "can", "robustly", "detect", "recognize", "object", "from", "different", "viewpoint", "so", "provide", "image-based", "retrieval", "contents", "video", "database", "goal", "we", "work", "pure", "content", "retrieval", "instead", "we", "want", "structure", "video", "datum", "can", "explore", "intuitively", "seamlessly", "through", "spectral", "refinement", "we", "also", "filter", "out", "erroneous", "portal", "we", "Videoscape", "graph", "which", "relate", "spirit", "identify", "iconic", "image", "structure", "Media", "Collections", "since", "casually", "capture", "community", "photo", "video", "collection", "stem", "largely", "from", "unconstrained", "environment", "analyze", "connection", "spatial", "arrangement", "camera", "challenging", "problem", "set", "image", "arrange", "space", "spatially", "confine", "location", "can", "interactively", "navigate", "other", "work", "find", "novel", "strategy", "scale", "basic", "concept", "larger", "image", "set", "reconstruction", "-lsb-", "Frahm", "et", "al.", "2010b", "-rsb-", "include", "reconstruct", "geometry", "from", "frame", "video", "capture", "from", "roof", "vehicle", "additional", "sensor", "-lsb-", "Frahm", "et", "al.", "2010a", "-rsb-", "while", "some", "problem", "parallel", "ours", "transfer", "approach", "casually", "capture", "video", "non-trivial", "contrast", "previous", "system", "which", "attempt", "reconstruct", "dense", "geometry", "confine", "location", "we", "approach", "aim", "recover", "navigate", "linkage", "structure", "video", "cover", "much", "larger", "area", "we", "system", "go", "farther", "than", "application", "scenario", "automatically", "link", "network", "video", "from", "unknown", "location", "compute", "immersive", "3d", "transition", "however", "oppose", "image", "web", "we", "want", "filter", "out", "unreliable", "match", "rather", "than", "increase", "graph", "connectivity", "rendering", "explore", "Media", "Collections", "image/videobased", "rendering", "method", "synthesize", "new", "view", "from", "photos/videos", "scene", "pioneering", "work", "Andrew", "Lippman", "-lsb-", "1980", "-rsb-", "realize", "one", "first", "system", "interactive", "navigation", "through", "database", "image", "user", "choose", "path", "through", "constrain", "set", "automatically", "pre-computed", "branch", "point", "point", "only", "novel", "view", "synthesis", "require", "we", "describe", "heuristic", "investigate", "through", "user", "study", "select", "appropriate", "transition", "render", "style", "however", "method", "rely", "constrain", "capture", "environment", "-lrb-", "e.g.", "special", "hardware", "confine", "spatial", "location", "-rrb-", "which", "facilitate", "processing", "rendering", "contrast", "we", "work", "we", "exploit", "vision", "technique", "automatically", "find", "connection", "between", "video", "capture", "under", "less", "constrain", "condition", "video", "browse", "system", "propose", "Pongnumkul", "et", "al.", "-lsb-", "2008", "-rsb-", "provide", "interface", "create", "geographical", "storyboard", "from", "single", "continuous", "video", "manually", "connect", "frame", "map", "landmark", "we", "also", "exploit", "sensor", "datum", "provide", "richer", "viewing", "interface", "technique", "propose", "Ballan", "et", "al.", "-lsb-", "2010", "-rsb-", "enable", "blend", "between", "different", "video", "show", "single", "spatially", "confine", "scene", "event", "here", "we", "intuit", "people", "naturally", "choose", "capture", "prominent", "feature", "scene", "landmark", "building", "city", "videoscape", "construction", "commence", "identify", "possible", "portal", "between", "all", "pair", "video", "clip", "-lrb-", "section", "4.1", "-rrb-", "Henceforth", "we", "term", "cluster", "support", "set", "portal", "section", "we", "detail", "step", "take", "find", "portal", "reconstruct", "portal", "geometry", "later", "use", "render", "transition", "support", "set", "we", "reconstruct", "3d", "geometry", "provide", "various", "different", "video", "transition", "between", "portal", "-lrb-", "section", "4.2", "-rrb-", "Video", "time", "synchronization", "detail", "provide", "supplemental", "material", "incorrect", "match", "less", "likely", "we", "associate", "each", "edge", "real", "value", "score", "represent", "match?s", "quality", "-lsb-", "Philbin", "et", "al.", "2011", "-rsb-", "ensure", "number", "sift", "descriptor", "extract", "from", "any", "pair", "frame", "comparable", "all", "frame", "scale", "height", "identical", "-lrb-", "480", "pixel", "-rrb-", "Portal", "selection", "matching", "refinement", "phase", "may", "produce", "many", "multiple", "matching", "portal", "frame", "-lrb-", "-rrb-", "between", "two", "video", "however", "all", "portal", "necessarily", "represent", "good", "transition", "opportunity", "now", "we", "know", "frame", "we", "video", "which", "connect", "portal", "we", "wish", "able", "visually", "transition", "from", "one", "video", "next", "we", "perform", "multi-view", "stereo", "-lsb-", "Furukawa", "Ponce", "2010", "Furukawa", "et", "al.", "2010", "-rsb-", "support", "set", "reconstruct", "dense", "point", "cloud", "portal", "scene", "often", "hand-held", "video", "include", "distract", "camera", "shake", "which", "we", "may", "wish", "remove", "one", "might", "think", "smoothly", "turn", "off", "stabilization", "portal", "approach", "time", "leave", "critical", "part", "video", "unstabilized", "we", "identify", "three", "workflow", "interact", "Videoscape", "application", "itself", "seamlessly", "transition", "via", "animation", "accommodate", "three", "way", "work", "datum", "new", "video", "current", "scene", "view", "from", "different", "spatio-temporal", "location", "we", "add", "clock", "icon", "choice", "thumbnail", "when", "view", "timesynchronous", "represent", "move", "only", "spatially", "temporally", "different", "video", "however", "lower", "recall", "considerably", "indicate", "reduction", "size", "support", "set", "hence", "reduce", "ability", "reconstruct", "3d", "model", "transition", "we", "goal", "derive", "criterion", "automatically", "choose", "most", "appropriate", "transition", "type", "give", "portal", "static", "3d", "transition", "significantly", "better", "than", "all", "other", "technique", "considerable", "view", "change", "-lrb-", "0.05", "t-test", "-rrb-", "have", "large", "variance", "slight", "view", "change", "case", "many", "factor", "may", "have", "contribute", "participant", "preference", "slight", "vs.", "considerable", "view", "change", "key", "factor", "which", "we", "straightforwardly", "exploit", "we", "employ", "warp", "view", "rotation", "slight", "i.e.", "less", "than", "10", "should", "portal", "fail", "reconstruct", "-lrb-", "e.g.", "from", "insufficient", "context", "bad", "camera", "tracking", "-rrb-", "we", "always", "fall", "back", "dissolve", "instead", "cut", "Video", "Tour", "Experiment", "we", "wish", "compare", "we", "system", "exist", "method", "method", "do", "produce", "comparable", "output", "from", "comparable", "input", "we", "do", "show", "effect", "any", "interface", "element", "so", "video", "view", "independently", "any", "other", "system", "functionality", "we", "suggest", "because", "we", "video", "database", "structure", "sort", "group", "similar", "material", "mean", "video", "browsing", "more", "accurate", "more", "efficient", "tour", "go", "through", "two", "considerable", "view", "change", "before", "end", "view", "river", "bank", "broad", "problem", "space", "require", "significant", "investigation", "some", "challenge", "remain", "some", "hand-held", "footage", "obtain", "camera", "tracking", "challenging", "especially", "roll", "shutter", "artifact", "occur", "well" ],
  "content" : "The abundance of mobile devices and digital cameras with video capture makes it easy to obtain large collections of video clips that contain the same location, environment, or event. However, such an unstructured collection is difficult to comprehend and explore. We propose a system that analyzes collections of unstructured but related video data to create a Videoscape: a data structure that enables interactive exploration of video collections by visually navigating ? spatially and/or temporally ? between different clips. We automatically identify transition opportunities, or portals. Now structured, the videos can be interactively explored by walking the graph or by geographic map. Given this system, we gauge preference for different video transition styles in a user study, and generate heuristics that automatically choose an appropriate transition style. In recent years, there has been an explosion of mobile devices capable of recording photographs that can be shared on community platforms. The research community has started to harvest the immense amount of data from community photo collections, and has developed tools to estimate the spatial relation between photographs, or to reconstruct 3D geometry of certain landmarks if a sufficiently dense set of photos is available [Snavely et al. 2006; Goesele et al. 2007; Agarwal et al. 2009; Frahm et al. 2010b]. Users can then interactively explore these locations by viewing the reconstructed 3D models or spatially transitioning between photographs. Navigation tools like Google Street View or Bing Maps also use this exploration paradigm and reconstruct entire street networks through alignment of purposefully captured imagery via additionally recorded localization and depth sensor data. These photo exploration tools are ideal for viewing and navigating static landmarks, such as Notre Dame, but cannot convey the dynamics, liveliness, and spatio-temporal relationships of a location or an event. One solution is to employ video data; yet, there are no comparable browsing experiences for casually captured videos and their generation is still an open challenge. One may be tempted to think that videos are simply series of images, so straightforward extensions of image-based approaches should serve the purpose and enable video tours. Casually captured video collections are usually sparse and largely unstructured, unlike the dense photo collections used in the approaches mentioned above. This precludes a dense reconstruction or registration of all frames. Furthermore, the exploration interface should reflect the dynamic and temporal nature of video. In this paper, we propose a system to explore unstructured video collections in an immersive and visually compelling manner. Given a sparse video collection of a certain (possibly large) area, e.g., the inner city of London, the user can tour through the video collection by following videos and transitioning between them at corresponding views. To this end, we compute a Videoscape graph structure from a collection of videos ( Figure 1 ). The edges of the Videoscape are video segments and the nodes mark possible transition points, or portals, between videos. The Videoscape can be explored interactively by playing video clips and transitioning to other clips when a portal arises. When temporal context is relevant, temporal awareness of an event is provided by offering correctly ordered transitions between temporally aligned videos. This yields a meaningful spatio-temporal viewing experience of large, unstructured video collections. A map-based viewing mode lets the user choose start and end videos, and automatically find a path of videos and transitions that join them. GPS and orientation data enhances the map view when available. Furthermore, images can be given to the system, from which the closest matching portals form a path through the Videoscape. To enhance the experience when transitioning through a portal, we develop different video transition modes, with appropriate transitions selected based on the preference of participants in a user study. Finally, we evaluate the Videoscape system with three further user studies. ? Videoscape exploration: an explorer application that enables intuitive and seamless spatio-temporal exploration of the Videoscape, based on several novel exploration paradigms. ? Videoscape evaluation: four user studies providing quantitative and qualitative data comparing Videoscapes to existing systems, including a user study analyzing preferred transition types and heuristics for their appropriate use. Video Google [Sivic and Zisserman 2003] is one of the first systems that enables video retrieval. It can robustly detect and recognize objects from different viewpoints and so provides image-based retrieval of contents in a video database. The goal of our work is not pure content retrieval; instead, we want to structure video data such that it can be explored intuitively and seamlessly. Through spectral refinement we also filter out erroneous portals in our Videoscape graph, which is related in spirit to identifying iconic images. Structuring Media Collections Since casually captured community photo and video collections stem largely from unconstrained environments, analyzing their connections and the spatial arrangement of cameras is a challenging problem. The set of images is arranged in space such that spatially confined locations can be interactively navigated. Other work finds novel strategies to scale the basic concepts to larger image sets for reconstruction [Frahm et al. 2010b], including reconstructing geometry from frames of videos captured from the roof of a vehicle with additional sensors [Frahm et al. 2010a]. While some of these problems are parallel to ours, transfer of their approaches to casually captured videos is non-trivial. In contrast to previous systems, which attempt to reconstruct a dense geometry for a confined location, our approach aims to recover and navigate the linkage structure of videos covering a much larger area. Our system goes farther than this application scenario by automatically linking networks of videos from unknown locations, and computing immersive 3D transitions. However, as opposed Image Webs, we want to filter out unreliable matches rather than to increase the graph connectivity. Rendering and Exploring Media Collections Image/videobased rendering methods synthesize new views from photos/videos of a scene. The pioneering work of Andrew Lippman [1980] realized one of the first systems for interactive navigation through a database of images. Users chose a path through a constrained set of automatically pre-computed branching points, and at these points only novel view synthesis is required. We describe heuristics, investigated through a user study, to select appropriate transition rendering styles. However, these methods rely on a constrained capture environment (e.g., special hardware or confined spatial locations), which facilitates processing and rendering. In contrast, in our work we exploit vision techniques to automatically find the connections between videos captured under less constrained conditions. The video browsing system proposed by Pongnumkul et al. [2008] provides an interface to create a geographical storyboard from a single continuous video by manually connecting frames to map landmarks. We also exploit sensor data to provide a richer viewing interface. The technique proposed by Ballan et al. [2010] enables blending between different videos showing a single spatially confined scene or event. Here, we intuit that people will naturally choose to capture prominent features in a scene, such as landmark buildings in a city. Videoscape construction commences by identifying possible portals between all pairs of video clips (Section 4.1). Henceforth, we term this cluster the support set for a portal. In this section we detail the steps taken to find portals and to reconstruct the portal geometry that is later used for rendering transitions. With the support set, we reconstruct 3D geometry and provide various different video transitions between portals (Section 4.2). Video time synchronization details are provided in the supplemental material. For incorrect matches, this is less likely. We associate each edge with a real valued score representing the match?s quality [Philbin et al. 2011]: To ensure that the numbers of SIFT descriptors extracted from any pair of frames are comparable, all frames are scaled such that their heights are identical (480 pixels). Portal Selection The matching and refinement phases may produce many multiple matching portal frames (I i , I j ) between two videos. However, not all portals necessarily represent good transition opportunities. Now that we know the frames in our videos which are connected as portals, we wish to be able to visually transition from one video to the next. We perform multi-view stereo [Furukawa and Ponce 2010; Furukawa et al. 2010] on the support set to reconstruct a dense point cloud of the portal scene. Often, hand-held video includes distracting camera shake which we may wish to remove. One might think to smoothly ?turn off? stabilization as portals approach in time, but this leaves critical parts of the video  unstabilized. We identify three workflows in interacting with the Videoscape, and the application itself seamlessly transitions via animations to accommodate these three ways of working with the data. This new video starts with the current scene viewed from a different spatio-temporal location. We add a clock icon to the choice thumbnails when views are timesynchronous, and this represents moving only spatially but not temporally to a different video. However, this lowers the recall considerably, indicating the reduction of the size of the support sets and hence reducing the ability to reconstruct 3D models for the transitions. Our goal is to derive criteria to automatically choose the most appropriate transition type for a given portal. Static 3D transitions are significantly better than all other techniques for considerable view changes (p < 0.05, t-test), but have large variance in slight view change cases. There are many factors that may have contributed to participant preferences, but slight vs. considerable view changes is a key factor which we straightforwardly exploit. We employ a warp if the view rotation is slight, i.e., less than 10 ? . Should portals fail to reconstruct (e.g., from insufficient context or bad camera tracking), we always fall back to a dissolve instead of a cut. Video Tour Experiment We wish to compare our system to existing methods, but these methods do not produce comparable output from comparable input. We do not show effects or any interface elements so that the videos are viewed independently of any other system functionality 1 . We suggest that this is because our video database structuring sorts and groups similar material, meaning that video browsing is more accurate and more efficient. The tour then goes through two considerable view changes before ending with a view of the river bank. Such a broad problem space requires significant investigation, and some challenges remain. For some hand-held footage, obtaining camera tracking is challenging, especially if rolling shutter artifacts occur as well.",
  "resources" : [ ]
}