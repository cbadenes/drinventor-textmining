{
  "uri" : "sig2012-a68-tompkin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a68-tompkin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Videoscapes: Exploring Sparse, Unstructured Video Collections",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/James-Tompkin",
      "name" : "James",
      "surname" : "Tompkin"
    }, {
      "uri" : "http://drinventor/Kwang In-Kim",
      "name" : "Kwang In",
      "surname" : "Kim"
    }, {
      "uri" : "http://drinventor/Jan-Kautz",
      "name" : "Jan",
      "surname" : "Kautz"
    }, {
      "uri" : "http://drinventor/Christian-Theobalt",
      "name" : "Christian",
      "surname" : "Theobalt"
    } ]
  },
  "bagOfWords" : [ "we", "evaluate", "we", "system", "use", "three", "further", "user", "study", "which", "allow", "we", "conclude", "Videoscapes", "provide", "significant", "benefit", "over", "related", "method", "we", "system", "lead", "previously", "unseen", "way", "interactive", "spatio-temporal", "exploration", "casually", "capture", "video", "we", "demonstrate", "several", "video", "collection", "while", "we", "system", "can", "provide", "direction", "from", "location", "sparse", "video", "collection", "may", "contain", "sufficient", "input", "do", "provide", "spatial", "arrangement", "landmark", "contain", "within", "video", "collection", "-lrb-", "distinct", "from", "geolocation", "video", "capture", "-rrb-", "unlike", "tour", "through", "image", "we", "system", "convey", "sense", "place", "dynamics", "liveliness", "while", "still", "maintain", "seamless", "browse", "video", "transition", "we", "automatically", "identify", "portal", "from", "appropriate", "subset", "video", "frame", "often", "great", "redundancy", "video", "process", "portal", "-lrb-", "corresponding", "video", "frame", "-rrb-", "enable", "smooth", "transition", "between", "video", "we", "exemplify", "use", "we", "system", "database", "part", "London", "set", "video", "capture", "different", "day", "be", "process", "videoscape", "can", "interactively", "explore", "we", "demonstrate", "we", "interactive", "interface", "we", "supplemental", "video", "we", "employ", "robust", "key-point", "matching", "portal", "identification", "-lrb-", "section", "-rrb-", "approach", "have", "also", "be", "use", "recent", "work", "content-based", "geolocation", "image", "-lsb-", "Baatz", "et", "al.", "2010", "Zamir", "Shah", "2010", "Li", "et", "al.", "2008", "-rsb-", "however", "we", "setting", "different", "since", "we", "graph", "model", "entire", "video", "collection", "cover", "many", "landmark", "we", "filter", "match", "technique", "adapt", "specifically", "we", "sparse", "video", "datum", "instance", "naive", "application", "-lsb-", "Frahm", "et", "al.", "2010b", "-rsb-", "we", "London", "video", "collection", "can", "yield", "full", "3d", "reconstruction", "depict", "environment", "video", "datum", "sparse", "video", "coverage", "sporadic", "we", "reconstruct", "scene", "camera", "geometry", "only", "specific", "location", "-lrb-", "i.e.", "portal", "-rrb-", "discuss", "section", "can", "all", "benefit", "from", "we", "analysis", "global", "context", "graph", "structure", "system", "first", "build", "sparsely", "connect", "graph", "perform", "feature-based", "matching", "which", "make", "incrementally", "denser", "via", "connectivity", "analysis", "we", "portal", "identification", "scheme", "also", "rely", "key", "point", "matching", "follow", "connectivity", "analysis", "base", "graph", "laplacian", "we", "capitalize", "previous", "work", "area", "render", "portal", "while", "navigate", "Videoscape", "consequence", "system", "operate", "spatially", "confine", "environment", "where", "dense", "set", "view", "can", "easily", "capture", "we", "system", "improve", "upon", "method", "automatically", "identify", "connection", "between", "many", "video", "join", "they", "visual", "transition", "background", "reconstruct", "from", "additional", "community", "photo", "scene", "video", "camera", "calibrate", "w.r.t.", "system", "state", "art", "tailor", "spatially", "confine", "set", "video", "all", "see", "same", "event", "same", "time", "from", "converge", "camera", "angle", "contrast", "we", "system", "operate", "collection", "show", "variety", "general", "scene", "film", "from", "much", "less", "constrain", "set", "camera", "position", "different", "time", "we", "system", "have", "both", "on-line", "off-line", "component", "graph", "can", "either", "direct", "undirected", "difference", "be", "undirected", "graph", "allow", "video", "play", "backwards", "necessary", "graph", "can", "maintain", "temporal", "consistency", "only", "allow", "edge", "portal", "forward", "time", "graph", "can", "also", "include", "portal", "join", "single", "video", "different", "time", "-lrb-", "loop", "within", "video", "-rrb-", "along", "portal", "node", "we", "also", "add", "node", "represent", "end", "each", "input", "video", "ensure", "all", "connect", "video", "content", "navigable", "we", "approach", "suitable", "indoor", "outdoor", "scene", "online", "component", "provide", "interface", "navigate", "videoscape", "watch", "video", "render", "transition", "between", "they", "portal", "portal", "span", "video", "frame", "either", "video", "show", "same", "physical", "location", "possibly", "film", "from", "different", "viewpoint", "different", "time", "practice", "we", "represent", "portal", "single", "pair", "portal", "frame", "from", "span", "one", "frame", "from", "each", "video", "through", "which", "visual", "transition", "other", "video", "can", "render", "-lrb-", "figure", "-rrb-", "addition", "portal", "we", "also", "identify", "all", "frame", "across", "all", "video", "which", "broadly", "match", "portal", "frame", "produce", "cluster", "frame", "around", "visual", "target", "enable", "3d", "reconstruction", "portal", "geometry", "after", "portal", "its", "support", "set", "have", "be", "identify", "portal", "geometry", "reconstruct", "3d", "model", "environment", "first", "we", "identify", "candidate", "portal", "match", "suitable", "frame", "between", "video", "contain", "similar", "content", "-lrb-", "section", "4.1", "-rrb-", "out", "candidate", "we", "select", "most", "appropriate", "portal", "deduce", "support", "set", "each", "filter", "naively", "match", "all", "frame", "database", "against", "each", "other", "computationally", "prohibitive", "ideally", "system", "would", "select", "just", "enough", "frame", "per", "video", "all", "visual", "content", "be", "represent", "all", "possible", "transition", "be", "still", "find", "Optical", "flow", "analysis", "-lsb-", "farneb?ck", "2003", "-rsb-", "provide", "good", "indication", "camera", "motion", "allow", "we", "find", "appropriate", "video", "frame", "representative", "visual", "content", "we", "analyze", "frame-to-frame", "flow", "pick", "one", "frame", "every", "time", "cumulative", "flow", "-lrb-", "-rrb-", "exceed", "25", "width", "-lrb-", "height", "-rrb-", "video", "whenever", "scene", "have", "move", "25", "frame", "gp", "orientation", "sensor", "datum", "provide", "we", "further", "cull", "candidate", "frame", "unlikely", "provide", "match", "however", "even", "though", "we", "perform", "sensor", "fusion", "complementary", "filter", "we", "still", "cull", "respect", "sensor", "error", "sensor", "datum", "often", "unreliable", "additional", "sensor", "filter", "step", "allow", "we", "process", "dataset", "larger", "same", "computational", "cost", "Holistic", "Matching", "Feature", "match", "holistic", "matching", "phase", "examine", "global", "structural", "similarity", "frame", "base", "spatial", "pyramid", "matching", "-lsb-", "Lazebnik", "et", "al.", "2006", "-rsb-", "we", "use", "bag-ofvisual-word-type", "histogram", "sift", "feature", "-lsb-", "Csurka", "et", "al.", "2004", "Leung", "Malik", "2001", "-rsb-", "standard", "set", "parameter", "-lrb-", "#pyramid", "level", "codebook", "size", "200", "-rrb-", "result", "matching", "score", "between", "each", "pair", "frame", "compare", "pair", "score", "lower", "than", "threshold", "discard", "use", "holistic", "match", "before", "subsequent", "feature", "matching", "have", "advantage", "reduce", "overall", "time", "complexity", "while", "severely", "degrading", "matching", "result", "-lsb-", "Heath", "et", "al.", "2010", "Frahm", "et", "al.", "2010a", "Frahm", "et", "al.", "2010b", "-rsb-", "output", "from", "holistic", "matching", "phase", "set", "candidate", "match", "-lrb-", "i.e.", "pair", "frame", "-rrb-", "some", "which", "may", "incorrect", "we", "improve", "result", "through", "feature", "matching", "match", "local", "frame", "context", "through", "SIFT", "feature", "detector", "descriptor", "context", "refinement", "output", "feature", "matching", "stage", "may", "still", "include", "false", "positive", "match", "which", "hard", "remove", "use", "only", "result", "pairwise", "feature", "matching", "Figure", "show", "example", "incorrect", "match", "preliminary", "experiment", "we", "observe", "when", "simultaneously", "examine", "more", "than", "two", "pair", "frame", "correct", "match", "more", "consistent", "other", "correct", "match", "than", "incorrect", "match", "example", "when", "frame", "correctly", "match", "frame", "frame", "form", "another", "correct", "match", "very", "likely", "also", "match", "we", "exploit", "context", "information", "perform", "novel", "graph-based", "match", "refinement", "prune", "false", "positive", "we", "first", "build", "graph", "represent", "all", "pairwise", "match", "where", "node", "frame", "edge", "connect", "match", "frame", "where", "connect", "frame", "-lrb-", "-rrb-", "set", "feature", "-lrb-", "sift", "descriptor", "-rrb-", "calculate", "from", "frame", "-lrb-", "-rrb-", "set", "feature", "match", "frame", "give", "graph", "we", "run", "spectral", "clustering", "-lsb-", "von", "luxburg", "2007", "-rsb-", "take", "first", "eigenvector", "eigenvalue", "0.1", "remove", "connection", "between", "pair", "frame", "span", "different", "cluster", "effectively", "remove", "incorrect", "match", "-lrb-", "figure", "-rrb-", "since", "intuitively", "speak", "spectral", "clustering", "assign", "same", "cluster", "only", "frame", "well", "inter-connected", "good", "portal", "should", "exhibit", "good", "feature", "match", "well", "allow", "non-disorientating", "transition", "between", "video", "both", "more", "likely", "frame", "pair", "shoot", "from", "similar", "camera", "view", "i.e.", "frame", "pair", "only", "small", "displacement", "between", "match", "feature", "therefore", "we", "retain", "only", "best", "available", "portal", "between", "pair", "video", "clip", "end", "we", "enhance", "metric", "from", "equation", "favor", "small", "displacement", "define", "best", "portal", "frame", "pair", "-lrb-", "-rrb-", "maximize", "follow", "score", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "diagonal", "size", "frame", "-lrb-", "-rrb-", "set", "match", "feature", "matrix", "whose", "row", "correspond", "feature", "displacement", "vector", "Frobenius", "norm", "ratio", "standard", "deviation", "first", "second", "summand", "exclude", "Figure", "show", "example", "identify", "portal", "-lrb-", "see", "section", "detail", "we", "experimental", "setup", "-rrb-", "each", "portal", "we", "define", "support", "set", "set", "all", "frame", "from", "context", "be", "find", "match", "least", "one", "portal", "frame", "video", "portal", "include", "Videoscape", "we", "implement", "seven", "different", "transition", "technique", "which", "run", "gamut", "cut", "dissolve", "warp", "four", "3d", "reconstruction", "camera", "sweep", "section", "6.2", "we", "psychophysi", "cally", "assess", "which", "technique", "prefer", "different", "scene", "viewing", "condition", "cut", "jump", "directly", "between", "two", "portal", "frame", "dissolve", "linearly", "interpolate", "between", "two", "video", "over", "fix", "length", "warp", "3d", "reconstruction", "case", "exploit", "support", "set", "portal", "we", "begin", "employ", "off-the-shelf", "structurefrom-motion", "-lrb-", "sfm", "-rrb-", "technique", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "register", "all", "camera", "from", "each", "support", "set", "we", "also", "use", "off-the-shelf", "kltbased", "camera", "tracker", "-lsb-", "thorm?hlen", "2006", "-rsb-", "find", "camera", "pose", "video", "frame", "four", "second", "window", "around", "each", "portal", "-lrb-", "further", "detail", "include", "supplemental", "material", "-rrb-", "interpolate", "transform", "provide", "broad", "motion", "change", "between", "portal", "frame", "ontop", "individual", "video", "frame", "warped", "broad", "motion", "use", "-lrb-", "denser", "-rrb-", "klt", "feature", "point", "again", "as-similar-aspossible", "ml", "transform", "however", "some", "ghost", "still", "exist", "so", "temporally-smoothed", "optical", "flow", "field", "use", "correct", "error", "similar", "way", "Eisemann", "et", "al.", "-lsb-", "2008", "-rsb-", "all", "warp", "precompute", "once", "Videoscape", "construct", "four", "3d", "reconstruction", "transition", "use", "same", "structurefrom-motion", "video", "tracking", "result", "we", "perform", "automated", "clean-up", "remove", "isolate", "cluster", "point", "density", "estimation", "thresholding", "-lrb-", "i.e.", "find", "average", "radius", "k-nearest", "neighbor", "threshold", "-rrb-", "we", "register", "video", "tracking", "result", "SFM", "camera", "match", "screen-space", "feature", "point", "two", "further", "transition", "require", "geometry", "complete", "use", "Poisson", "reconstruction", "-lsb-", "Kazhdan", "et", "al.", "2006", "-rsb-", "additional", "background", "plane", "place", "beyond", "depth", "any", "geometry", "all", "camera", "view", "cover", "geometry", "we", "support", "full", "3d", "dynamic", "transition", "where", "two", "video", "project", "onto", "geometry", "finally", "we", "support", "full", "3d", "static", "transition", "where", "only", "portal", "frame", "project", "onto", "geometry", "useful", "when", "camera", "tracking", "inaccurate", "-lrb-", "due", "large", "dynamic", "object", "camera", "shake", "-rrb-", "typically", "provide", "view", "without", "ghost", "artifact", "conclusion", "suggest", "simply", "dissolve", "occluder", "background", "most", "case", "best", "method", "apply", "even", "when", "segmentation", "information", "dynamic", "object", "available", "key", "transition", "type", "show", "Figure", "however", "we", "stabilize", "before", "processing", "we", "jeopardize", "we", "vision-based", "matching", "reconstruction", "software", "stabilization", "break", "geometric", "assumption", "upon", "which", "we", "rely", "instead", "we", "pre-compute", "2d", "affine", "stabilization", "parameter", "-lrb-", "i.e.", "per-frame", "crop", "region", "-rrb-", "do", "apply", "they", "we", "input", "video", "we", "pass", "video", "unaltered", "we", "reconstruction", "pipeline", "we", "optionally", "apply", "pre-computed", "stabilization", "parameter", "real-time", "we", "renderer", "during", "transition", "we", "interpolate", "stabilization", "parameter", "across", "transition", "geometry-based", "transition", "we", "project", "original", "unstabilized", "video", "footage", "only", "stabilize", "virtual", "camera", "view", "we", "have", "develop", "prototype", "explorer", "application", "-lrb-", "figure", "-rrb-", "which", "exploit", "Videoscape", "datum", "structure", "allow", "seamless", "navigation", "through", "set", "video", "important", "aspect", "maintain", "visual", "link", "between", "graph", "-lrb-", "its", "embedding", "-rrb-", "video", "during", "animation", "help", "viewer", "from", "become", "lose", "we", "supplemental", "video", "demonstrate", "workflow", "interplay", "interactive", "exploration", "Mode", "watch", "video", "often", "immersive", "full-screen", "experience", "videoscape", "different", "-lrb-", "figure", "-rrb-", "workflow", "time", "progress", "portal", "near", "we", "notify", "viewer", "unobtrusive", "icon", "choose", "switch", "video", "opportunity", "move", "mouse", "thumbnail", "strip", "destination", "choice", "-lrb-", "neighbor", "graph", "node", "-rrb-", "smoothly", "appear", "ask", "what", "would", "you", "like", "see", "next", "here", "viewer", "can", "pause", "scrub", "through", "each", "thumbnail", "video", "scan", "contents", "future", "path", "thumbnail", "select", "we", "system", "generate", "appropriate", "transition", "from", "current", "scene", "view", "new", "video", "Audio", "cross-fade", "transition", "new", "video", "show", "new", "video", "take", "viewer", "choose", "destination", "view", "paradigm", "move", "between", "view", "scene", "applicable", "when", "other", "datum", "beyond", "video", "available", "e.g.", "when", "we", "can", "provide", "additional", "geographical", "context", "form", "we", "baseline", "experience", "choice", "lead", "dead", "end", "choice", "lead", "previously", "see", "view", "we", "add", "commonly", "understand", "road", "sign", "icon", "well", "should", "GPS", "orientation", "datum", "available", "we", "add", "togglable", "mini-map", "which", "display", "follow", "view", "frustum", "time", "from", "overhead", "hover", "over", "destination", "choice", "thumbnail", "show", "frustum", "real-world", "point", "mini-map", "update", "timeline", "accordingly", "overview", "mode", "any", "time", "mini-map", "can", "expand", "fill", "screen", "viewer", "present", "large", "overview", "Videoscape", "graph", "embed", "globe", "-lsb-", "Bell", "et", "al.", "2007", "-rsb-", "-lrb-", "figure", "top", "-rrb-", "second", "workflow", "we", "add", "eye", "icon", "map", "represent", "portal", "geographical", "location", "eye", "estimate", "from", "converge", "sensor", "datum", "so", "eye", "place", "approximately", "view", "scene", "videoscape", "can", "contain", "hundred", "portal", "we", "adaptively", "change", "density", "display", "eye", "so", "user", "overwhelm", "eye", "add", "map", "representative", "connectivity", "order", "so", "mostconnected", "portal", "always", "display", "when", "hover", "over", "eye", "we", "inlay", "image", "view", "constitute", "portal", "along", "cone", "show", "where", "view", "originate", "viewer", "can", "construct", "video", "tour", "path", "click", "eye", "sequence", "define", "path", "summarize", "strip", "video", "thumbnail", "appear", "right", "each", "thumbnail", "can", "scrub", "suitability", "entire", "planned", "tour", "can", "quickly", "assess", "we", "system", "can", "automatically", "generate", "tour", "path", "from", "specify", "start/end", "point", "third", "workflow", "fast", "geographical", "video", "browsing", "we", "draw", "real-world", "travel", "path", "onto", "map", "line", "when", "hover", "over", "line", "appropriate", "section", "video", "display", "along", "respective", "view", "cone", "here", "typically", "video", "show", "sideby-side", "map", "expose", "detail", "though", "viewer", "have", "full", "control", "over", "size", "video", "should", "prefer", "see", "more", "map", "-lrb-", "figure", "bottom", "-rrb-", "time", "progress", "portal", "identify", "highlight", "appropriate", "eye", "draw", "secondary", "view", "cone", "yellow", "show", "position", "alternative", "view", "click", "during", "time", "append", "view", "current", "tour", "path", "once", "path", "define", "either", "method", "large", "map", "return", "miniature", "size", "full-screen", "interactive", "mode", "play", "tour", "interplay", "between", "three", "workflow", "allow", "fast", "exploration", "large", "videoscape", "many", "video", "provide", "accessible", "non-linear", "interface", "content", "within", "collection", "video", "may", "otherwise", "difficult", "penetrate", "image/label-based", "search", "Mode", "we", "allow", "viewer", "search", "image", "define", "tour", "path", "search", "label", "image", "search", "image", "feature", "match", "against", "portal", "frame", "feature", "candidate", "portal", "frame", "find", "scrubbable", "video", "list", "appear", "show", "best", "matching", "candidate", "from", "path", "can", "select", "new", "video", "generate", "much", "same", "way", "before", "now", "return", "video", "bookend", "warp", "from", "submit", "image", "label", "search", "user", "provide", "key", "word", "match", "result", "present", "video", "list", "image", "search", "we", "perform", "three", "class", "experiment", "first", "class", "we", "evaluate", "each", "individual", "component", "construct", "videoscape", "-lrb-", "section", "6.1", "-rrb-", "here", "main", "objective", "gain", "insight", "performance", "comparison", "potential", "alternative", "second", "class", "experiment", "we", "psychophysically", "assess", "video-tovideo", "transition", "preference", "assess", "spatial", "awareness", "improvement", "through", "transition", "-lrb-", "section", "6.2", "-rrb-", "third", "class", "experiment", "we", "perform", "user", "study", "evaluate", "interface", "utility", "Videoscapes", "against", "exist", "system", "-lrb-", "section", "6.3", "-rrb-", "during", "project", "we", "capture", "various", "dataset", "demonstrate", "we", "method", "here", "we", "provide", "detailed", "analysis", "one", "dataset", "process", "use", "between", "all", "dataset", "virtually", "identical", "performance", "similar", "we", "analysis", "database", "comprise", "196", "video", "take", "several", "location", "London", "video", "include", "landmark", "big", "Ben", "London", "Eye", "St", "Paul?s", "Cathedral", "database", "also", "include", "general", "street", "footage", "between", "around", "landmark", "individual", "video", "feature", "variety", "motion", "include", "pan", "take", "view", "casual", "movement", "around", "location", "video", "vary", "location", "duration", "-lrb-", "typically", "between", "10", "seconds", "minute", "-rrb-", "time", "day", "foreground", "object", "viewpoint", "database", "video", "be", "capture", "asynchronously", "one", "camera", "-lrb-", "Sanyo", "fh1", "-rrb-", "resolution", "1920", "1080", "other", "database", "-lrb-", "South", "Bank", "Bicycles", "-rrb-", "be", "capture", "concurrently", "multiple", "heterogeneous", "camera", "vary", "frame", "rate", "where", "employ", "we", "sensor", "datum", "capture", "smartphone", "all", "video", "optional", "sensor", "datum", "could", "capture", "just", "one", "smartphone", "filter", "we", "frame", "sampling", "strategy", "-lrb-", "section", "4.1", "-rrb-", "reduce", "unnecessary", "duplication", "still", "slow", "rotate", "segment", "reduction", "number", "frame", "over", "regular", "sampling", "content", "dependent", "we", "datum", "set", "flow", "analysis", "pick", "approximately", "30", "fewer", "frame", "lead", "50", "reduction", "computation", "time", "subsequent", "stage", "compare", "sampling", "every", "50th", "frame", "-lrb-", "moderate", "trade-off", "between", "retain", "content", "number", "frame", "-rrb-", "random", "selection", "one", "scene", "from", "10", "video", "we", "compare", "number", "frame", "represent", "each", "scene", "naive", "improve", "strategy", "average", "scene", "overlap", "we", "judge", "visually", "equal", "flow-based", "method", "produce", "frame", "regular", "sampling", "produce", "7.5", "frame", "per", "scene", "indicate", "we", "pre-filtering", "stage", "extract", "frame", "more", "economically", "while", "maintain", "similar", "scene", "content", "sampling", "we", "first", "database", "approximately", "3,500", "frame", "be", "extract", "filter", "phase", "from", "start", "set", "approximately", "500,000", "Portal", "identification", "context", "refinement", "performance", "portal", "identification", "algorithm", "evaluate", "measure", "precision", "recall", "random", "subset", "we", "analysis", "database", "precision", "measure", "from", "all", "identify", "portal", "connect", "30", "randomly", "select", "video", "corresponding", "frame", "match", "be", "visually", "inspect", "portal", "be", "label", "correct", "when", "match", "frame", "represent", "same", "scene", "calculate", "recall", "435", "randomly", "select", "pair", "video", "be", "visually", "inspect", "see", "scene", "content", "overlap", "again", "ground", "truth", "portal", "be", "identify", "find", "when", "corresponding", "automatically", "identify", "portal", "Table", "prove", "importance", "each", "phase", "portal", "finding", "-lrb-", "threshold", "holistic", "phase", "fix", "2.2", "see", "section", "4.1", "-rrb-", "use", "only", "holistic", "matching", "high", "recall", "can", "reach", "precision", "rather", "low", "add", "feature", "matching", "lead", "drastic", "increase", "precision", "-lrb-", "holistic", "feature", "matching", "-rrb-", "finally", "all", "phase", "together", "yield", "precision", "98", "recall", "rate", "53", "possible", "achieve", "same", "precision", "feature", "matching", "-lrb-", "holistic", "feature", "matching", "-rrb-", "simply", "threshold", "number", "key", "correspondence", "reach", "100", "precision", "automatic", "method", "nearly", "impossible", "even", "analyze", "context", "information", "through", "graph-based", "refinement", "can", "completely", "rule", "out", "error", "rare", "case", "user", "can", "manually", "flag", "remain", "incorrect", "portal", "interactive", "viewer", "set", "196", "video", "all", "portal", "identification", "step", "take", "approximately", "four", "day", "one", "xeon", "x5560", "2.66", "ghz", "-lrb-", "use", "one", "core", "-rrb-", "use", "filter", "instead", "regular", "sampling", "save", "two", "day", "computation", "232", "portal", "be", "find", "except", "first", "phase", "specifically", "codebook", "generation", "off-line", "procedure", "could", "execute", "parallel", "average", "size", "portal", "support", "set", "20", "frame", "support", "set", "can", "augment", "neighbor", "support", "set", "frame", "include", "one", "neighborhood", "set", "increase", "average", "size", "45", "while", "two", "increase", "70", "however", "include", "all", "neighborhood", "recursively", "do", "produce", "complete", "reconstruction", "video", "database", "due", "vary", "video", "coverage", "instead", "graph", "linkage", "structure", "maintain", "global", "navigability", "we", "choose", "use", "support", "set", "extend", "two", "neighborhood", "good", "compromise", "between", "computation", "speed", "reconstruction", "extent", "we", "datum", "set", "reconstruction", "tracking", "all", "portal", "take", "two", "day", "run", "parallel", "eight", "xeon", "x5560", "2.66", "GHz", "core", "even", "though", "we", "use", "state-of-the-art", "multi-view", "3d", "reconstruction", "-lsb-", "Furukawa", "et", "al.", "2010", "-rsb-", "result", "geometry", "can", "poor", "quality", "various", "reason", "e.g.", "glass", "building", "thin", "building", "structure", "rotational", "symmetry", "simply", "database", "do", "provide", "sufficient", "baseline", "landmark", "we", "handle", "portal", "choose", "dissolve", "transition", "transition", "preference", "we", "want", "choose", "best", "transition", "technique", "from", "user", "perspective", "between", "two", "video", "under", "which", "circumstance", "one", "technique", "prefer", "over", "another", "we", "hypothesize", "only", "certain", "transition", "type", "appropriate", "certain", "scene", "we", "seven", "transition", "type", "-lrb-", "cut", "dissolve", "warp", "plane", "ambient", "point", "cloud", "static", "dynamic", "full", "3d", "reconstruction", "-rrb-", "we", "expect", "warp", "blend", "better", "when", "view", "change", "slight", "transition", "rely", "3d", "geometry", "better", "when", "view", "change", "considerable", "end", "we", "conduct", "user", "study", "which", "ask", "participant", "rank", "transition", "type", "preference", "we", "choose", "ten", "pair", "portal", "frame", "represent", "five", "different", "scene", "each", "scene", "one", "transition", "form", "slight", "view", "change", "-lrb-", "10", "average", "include", "zoom", "change", "-rrb-", "one", "transition", "form", "considerable", "view", "change", "-lrb-", "up", "55", "-rrb-", "target", "video", "always", "same", "both", "view", "change", "five", "scene", "be", "choose", "each", "display", "potentially", "difficult", "situation", "-lrb-", "scene", "dynamic", "object", "boundary", "scene", "many", "dynamic", "object", "view", "occlusion", "panning", "camera", "scene", "panning", "camera", "dynamic", "object", "scene", "fast", "move", "dynamic", "object", "shake", "camera/rolling", "shutter", "scene", "complicated", "foreground", "object", "move", "shake", "camera", "-rrb-", "all", "scene/transition", "type", "render", "-lrb-", "e.g.", "Figure", "-rrb-", "include", "supplementary", "material", "participant", "rank", "seven", "transition", "type", "each", "ten", "portal", "first", "scenario", "create", "video", "tour", "explain", "participant", "example", "show", "participant", "present", "each", "set", "video", "transition", "random", "order", "transition", "randomly", "place", "vertical", "video", "list", "21", "participant", "we", "experiment", "12", "be", "self-described", "expert", "experience", "graphic", "media", "production", "be", "amateur", "be", "novice", "average", "take", "52", "minute", "complete", "study", "provide", "text", "comment", "we", "perform", "multi-dimension", "scaling", "-lsb-", "Torgerson", "1958", "-rsb-", "place", "we", "transition", "type", "interval", "scale", "Figure", "show", "mean", "standard", "deviation", "across", "all", "scene", "individual", "result", "summarize", "Table", "detailed", "graph", "can", "find", "supplemental", "material", "result", "show", "overall", "preference", "static", "3d", "transition", "surprisingly", "3d", "transition", "where", "both", "video", "continue", "playing", "be", "prefer", "less", "look", "per", "scene", "result", "we", "hypothesize", "due", "ghost", "which", "stem", "from", "inaccurate", "camera", "track", "difficult", "shaky", "case", "many", "participant", "comment", "3d", "transition", "maintain", "important", "spatial", "relationship", "between", "landmark", "provide", "fluid", "camera", "movement", "warp", "significantly", "prefer", "against", "all", "full", "3d", "technique", "slight", "view", "change", "-lrb-", "0.05", "t-test", "-rrb-", "we", "believe", "cause", "screen", "area", "which", "lack", "projection", "because", "video", "have", "be", "pause", "during", "transition", "i.e.", "virtual", "camera", "still", "pan", "interpolate", "between", "video", "projection", "onto", "geometry", "do", "we", "supplemental", "material", "contain", "per-scene", "perceptual", "scale", "plot", "per-scene-class", "significance", "table", "towards", "automatically", "choose", "transition", "type", "outcome", "help", "develop", "rule", "select", "appropriate", "transition", "type", "however", "we", "experience", "system", "many", "portal", "slight", "view", "change", "actually", "similar", "scene", "-lrb-", "figure", "-rrb-", "do", "suffer", "shake", "so", "provide", "high-quality", "result", "when", "use", "dynamic", "3d", "transition", "we", "use", "static", "3d", "transition", "considerable", "view", "change", "result", "also", "show", "dissolve", "preferable", "cut", "we", "20", "participant", "be", "self-assessed", "familiar", "geographical", "area", "depict", "task", "-lrb-", "avg", "live", "year", "max", "10", "year", "min", "experiment", "follow", "-lrb-", "Figure", "-rrb-", "participant", "see", "overhead", "aerial", "imagery", "map", "mark", "ground-truth", "pin", "view", "direction", "pin", "mark", "realworld", "camera", "position", "video", "after", "seconds", "visible", "countdown", "begin", "-lrb-", "...", "map", "remove", "participant", "watch", "short", "clip", "video", "transition", "video", "video", "remove", "map", "reappear", "participant", "mark", "map", "location", "direction", "travel", "after", "transition", "video", "objectively", "we", "measure", "deviation", "from", "ground", "truth", "position", "direction", "marked", "red", "pin", "participant", "participant", "free", "replay", "video", "reposition", "pin/direction", "many", "time", "wish", "also", "free", "translate", "zoom", "map", "when", "place", "pin", "second", "video", "pin", "first", "video", "present", "map", "ground", "truth", "generate", "from", "gp", "coordinate", "hand-refined", "local", "knowledge", "known", "position", "camera", "shot", "we", "test", "two", "condition", "we", "experiment", "-rrb-", "cut", "transition", "without", "provide", "view", "direction", "-lrb-", "i.e.", "just", "location", "pin", "condition", "most", "akin", "other", "exist", "system", "particular", "multi-video", "variant", "Pongnumkul", "et", "al.", "-lsb-", "2008", "-rsb-", "-rrb-", "-rrb-", "static", "3d", "transition", "provide", "view", "direction", "-lrb-", "Videoscapes", "condition", "-rrb-", "each", "participant", "complete", "practice", "trial", "often", "wish", "follow", "randomly", "order", "trial", "each", "different", "scene", "which", "from", "each", "condition", "Performance", "measure", "five", "criterion", "which", "present", "Table", "along", "statistical", "significance", "overall", "both", "location", "direction", "error", "from", "ground", "truth", "time", "complete", "task", "be", "similar", "between", "Videoscapes", "cut/no", "direction", "condition", "however", "Videoscapes", "condition", "produce", "significantly", "fewer", "video", "replay", "significantly", "fewer", "location/view", "direction", "adjustment", "which", "we", "suggest", "indicator", "increase", "spatial", "awareness", "follow", "task", "each", "participant", "complete", "questionnaire", "q1", "which", "interface", "do", "you", "find", "easiest", "complete", "task", "Table", "summarize", "result", "q1", "participant", "who", "prefer", "we", "system", "task", "-lrb-", "17/20", "-rrb-", "35", "find", "much", "easier", "59", "find", "Easier", "find", "slightly", "easier", "q2", "participant", "who", "find", "we", "system", "provide", "greater", "spatial", "awareness", "sense", "orientation", "-lrb-", "19/20", "-rrb-", "58", "find", "much", "more", "26", "find", "more", "16", "find", "slightly", "more", "provide", "all", "question", "neutral", "response", "-lrb-", "both", "same", "-rrb-", "option", "Videoscapes", "condition", "require", "less", "video", "replay", "less", "location/direction", "adjustment", "same", "accuracy", "questionnaire", "show", "most", "participant", "prefer", "we", "system", "almost", "all", "save", "one", "think", "we", "system", "provide", "more", "spatial", "awareness", "correlate", "we", "quantitative", "datum", "demonstrate", "we", "system", "help", "maintain", "greater", "spatial", "awareness", "through", "transition", "evaluate", "we", "prototype", "interface", "meaningful", "way", "challenge", "exist", "system", "do", "provide", "comparative", "functionality", "yet", "we", "do", "wish", "provide", "trivial", "comparison", "equally", "evaluate", "we", "large", "system", "whole", "likely", "uninformative", "community", "would", "too", "specific", "we", "system", "we", "perform", "two", "different", "user", "study", "design", "provide", "quantitative", "qualitative", "feedback", "major", "component", "we", "system", "each", "study", "compare", "user", "performance/experience", "Videoscapes", "achieve", "exist", "alternative", "20", "participant", "spatial", "awareness", "experiment", "also", "perform", "both", "we", "interface", "experiment", "however", "we", "exploration", "session", "could", "think", "geographical", "tour", "summarization", "video", "database", "particularly", "case", "where", "user", "select", "end", "location", "leave", "path", "generate", "we", "system", "-lrb-", "instead", "interactively", "navigate", "-rrb-", "experiment", "each", "participant", "watch", "three", "video", "which", "have", "be", "automatically", "edit", "software", "-rrb-", "instantmovie", "from", "Adobe", "Premiere", "element", "7.0", "-rrb-", "intelligent", "fast", "forward", "Pongnumkul", "et", "al.", "-lsb-", "2008", "-rsb-", "-rrb-", "video", "tour", "mode", "Videoscapes", "-lrb-", "blend", "transition", "-rrb-", "each", "summarization", "video", "generate", "same", "input", "database", "participant", "video", "could", "replay", "video", "be", "present", "random", "order", "participant", "be", "ask", "concentrate", "way", "content", "present", "-lrb-", "style", "-rrb-", "any", "specific", "content", "participant", "complete", "questionnaire", "list", "below", "rank", "three", "style", "explicitly", "question", "be", "which", "style", "...", "do", "you", "most", "prefer", "q2", "...", "do", "you", "find", "most", "interesting", "q3", "...", "do", "you", "find", "provide", "best", "sense", "place", "q4", "...", "do", "you", "find", "most", "spatially", "confuse", "Q5", "...", "would", "you", "use", "most", "often", "you", "own", "video", "collection", "Q6", "...", "would", "you", "view", "most", "often", "online", "video", "collection", "Table", "summarize", "result", "ranking", "score", "-lrb-", "from", "-rrb-", "accumulate", "over", "all", "participant", "significance", "be", "compute", "kruskal-walli", "test", "pairwise", "mann-whitney", "u-test", "alpha", "0.05", "Videoscapes", "significantly", "most", "preferred", "summarization", "style", "provide", "best", "sense", "place", "least", "spatially", "confuse", "three", "video", "dataset", "exception", "be", "InstantMovie", "which", "contain", "two", "instance", "hard-to-remove", "overlaid", "theme", "graphic", "infrequent", "minor", "effect", "participant", "be", "explicitly", "ask", "ignore", "concentrate", "only", "content", "when", "consider", "answer", "Video", "browse", "comparison", "interface", "top", "we", "implementation", "-lsb-", "Pongnumkul", "et", "al.", "2008", "-rsb-", "adapt", "video", "database", "bottom", "iMovie", "11", "cut-out", "show", "scrub", "thumbnail", "expansion", "-lrb-", "which", "frequently", "make", "participant", "lose", "-rrb-", "we", "must", "take", "care", "extrapolate", "all", "dataset", "one", "example", "conclusive", "q5", "q6", "significantly", "suggest", "we", "system", "may", "prefer", "most", "often", "personal", "online", "video", "collection", "pongnumkul", "style", "also", "consistently", "prefer", "over", "InstantMovie", "style", "even", "though", "map", "ever", "show", "experiment", "suggest", "exploit", "geographical", "datum", "important", "addition", "video", "database", "summarization", "Video", "Browsing", "Experiment", "we", "final", "experiment", "attempt", "evaluate", "Videoscapes", "tool", "browse", "retrieve", "video", "contents", "participant", "be", "ask", "find", "five", "different", "video", "contents", "similar", "image", "query", "major", "landmark", "use", "three", "different", "interface", "-rrb-", "Apple", "iMovie", "11", "-rrb-", "we", "implementation", "multi-video", "version", "Pongnumkul", "et", "al.", "-lsb-", "2008", "-rsb-", "-rrb-", "videoscape", "-lrb-", "figure", "10", "-rrb-", "task", "four", "browse", "method", "within", "Videoscapes", "be", "also", "evaluate", "which", "include", "image", "search", "label", "search", "browse", "eye", "icon", "-lrb-", "second", "workflow", "section", "-rrb-", "geographical", "video", "browsing", "-lrb-", "third", "workflow", "section", "-rrb-", "experiment", "label", "database", "hold", "only", "objective", "label", "specific", "landmark", "before", "use", "each", "interface", "thoroughly", "explain", "satisfaction", "participant", "participant", "be", "give", "option", "use", "whichever", "method", "wish", "within", "each", "interface", "complete", "task", "each", "participant", "perform", "task", "each", "interface", "random", "order", "two", "main", "evaluation", "criterion", "t1", "average", "time", "take", "complete", "task", "-lrb-", "sec", "-rrb-", "t2", "average", "number", "occur", "write", "paper", "only", "support", "single", "video", "manual", "geotagging", "we", "automatically", "place", "thumbnail", "vary", "density", "zoom", "so", "visualization", "all", "interesting", "shot", "database", "possible", "Ours", "sig", "q1", "27", "40", "53", "0.000", "0.007", "q2", "30", "41", "49", "0.000", "0.121", "rence", "find", "same", "video", "more", "than", "once", "-lrb-", "i.e.", "error", "rate", "-rrb-", "Figure", "11", "show", "result", "indicate", "speed", "accuracy", "browsing", "significantly", "improve", "use", "Videoscapes", "over", "exist", "system", "we", "interface", "expose", "structure", "two", "fast", "way", "image", "label", "search", "geographically-placed", "visual", "grouping", "we", "eye", "icon", "participant", "also", "complete", "questionnaire", "follow", "task", "q1", "which", "interface", "do", "you", "most", "prefer", "complete", "task", "find", "content", "q2", "which", "interface", "do", "you", "think", "you", "would", "most", "prefer", "browse", "content", "generally", "result", "be", "compute", "before", "Table", "summarize", "result", "Videoscapes", "significantly", "prefer", "over", "both", "other", "system", "task", "significantly", "prefer", "over", "iMovie", "general", "case", "again", "we", "must", "generalize", "beyond", "experiment", "dataset", "response", "we", "interface", "promising", "when", "ask", "which", "interface", "component", "prefer", "from", "we", "prototype", "browse", "eye", "icon", "image", "search", "method", "be", "most", "prefer", "task", "general", "browsing", "video", "collection", "finally", "we", "ask", "participant", "whether", "would", "want", "use", "we", "interface", "browse", "personal", "online", "video", "collection", "result", "promising", "95", "respond", "would", "use", "least", "80", "respond", "sometimes", "often", "-lrb-", "Table", "-rrb-", "we", "now", "describe", "result", "show", "supplemental", "video", "generate", "video", "difficult", "represent", "print", "so", "we", "strongly", "encourage", "reader", "view", "we", "result", "motion", "first", "example", "demonstrate", "interactive", "exploration", "viewer", "first", "choose", "view", "lead", "towards", "Tate", "Modern", "transition", "cover", "400m", "move", "view", "stabilize", "hand-held", "shot", "river", "thame", "from", "Millennium", "Bridge", "all", "transition", "full", "3d", "static", "except", "Tate", "Modern", "transition", "which", "warp", "we", "second", "example", "map-based", "exploration", "session", "viewer", "demonstrate", "two", "overview", "workflow", "mode", "auto", "matic", "tour", "plot", "tour", "summary", "strip", "from", "here", "overview", "reduce", "mini-map", "video", "reveal", "behind", "result", "tour", "fast-forward", "through", "demonstrate", "we", "generate", "tour", "can", "cover", "large", "geographical", "area", "tour", "speed", "up", "dramatically", "portal", "transition", "less", "visible", "next", "we", "show", "interactive", "workflow", "mini-map", "viewer", "scrub", "portal", "choice", "thumbnail", "see", "path", "mini-map", "we", "show", "bicycle", "race", "example", "highlight", "slightly", "different", "use", "case", "we", "system", "here", "spectator", "video", "combine", "bicycle-mounted", "camera", "both", "previous", "tour", "example", "enforce", "temporal", "consistency", "we", "labeling", "system", "exploit", "videoscape", "structure", "computation", "provide", "instantaneous", "propagation", "through", "database", "we", "show", "viewer", "label", "museum", "provide", "review", "which", "instantly", "propagate", "similar", "video", "frame", "all", "other", "database", "video", "-lrb-", "only", "one", "which", "show", "-rrb-", "finally", "we", "last", "example", "generate", "provide", "two", "landmark", "image", "-lrb-", "house", "Parliament", "London", "Eye", "-rrb-", "visit", "during", "tour", "system", "match", "image", "against", "portal", "plan", "tour", "visit", "landmark", "tour", "show", "first", "search", "image", "warp", "stabilize", "video", "show", "series", "full", "3d", "dynamic", "transition", "-lrb-", "one", "particularly", "large", "view", "change", "-rrb-", "travel", "final", "landmark", "where", "warp", "second", "search", "image", "tour", "we", "do", "enforce", "temporal", "consistency", "allow", "video", "play", "backwards", "which", "increase", "number", "possible", "tour", "sparse", "dataset", "unstructured", "video", "collection", "present", "many", "challenge", "we", "believe", "we", "system", "rise", "many", "convincing", "result", "we", "result", "use", "dataset", "require", "only", "loose", "temporal", "coherence", "other", "dataset", "may", "require", "functionality", "more", "strongly", "video", "take", "during", "event", "we", "bicycle", "race", "example", "may", "require", "temporally", "consistent", "exploration", "Videoscape", "else", "e.g.", "position", "leader", "may", "suddenly", "change", "many", "other", "dataset", "do", "require", "temporal", "consistency", "novel", "experience", "may", "come", "from", "intentionally", "disable", "temporal", "consistency", "we", "system", "sufficiently", "general", "accommodate", "scenario", "design", "we", "propose", "method", "do", "model", "foreground", "object", "both", "portal", "identification", "3d", "reconstruction", "foreground", "object", "regard", "outlier", "match", "accordingly", "ignore", "can", "sometimes", "introduce", "distracting", "artifact", "some", "object", "warp", "vanish", "during", "transition", "example", "pedestrian", "may", "warp", "each", "other", "mitigate", "when", "use", "spatio-temporally", "coherent", "exploration", "-lrb-", "section", "-rrb-", "when", "temporally", "align", "video", "datum", "available", "foreground", "object", "could", "reliably", "segmented", "opportunity", "remove", "they", "before", "transition", "occur", "-lrb-", "dissolve", "against", "inpainting", "-rrb-", "dissolve", "new", "dynamic", "object", "second", "video", "after", "transition", "however", "video", "inpainting", "unreliable", "computationally", "expensive", "so", "we", "dissolve", "strategy", "support", "evidence", "from", "perceptual", "study", "we", "believe", "preferable", "currently", "quality", "we", "geometric", "reconstruction", "limit", "available", "view", "scene", "within", "video", "set", "increase", "size", "video", "collection", "would", "increase", "number", "view", "scene", "so", "help", "improve", "quality", "3d", "reconstruction", "course", "increase", "size", "datum", "set", "have", "its", "own", "drawback", "scope", "speed", "up", "we", "processing", "recent", "work", "-lsb-", "Frahm", "et", "al.", "2010b", "-rsb-", "have", "demonstrate", "speed", "improvement", "image", "couple", "more", "aggressive", "filter", "approach", "should", "enable", "much", "larger", "video", "set", "process", "similar", "amount", "time", "case", "we", "can", "still", "use", "full", "3d", "static", "transition", "we", "interpolation", "provide", "convincing", "camera", "motion", "style", "blend", "despite", "inaccurate", "camera", "track", "justified", "we", "participant", "prefer", "static", "3d", "transition", "over", "other", "transition", "type", "scenario", "importantly", "3d", "geometry", "still", "recover", "difficult", "case", "because", "support", "set", "portal", "provide", "sufficient", "context", "overview", "mode", "we", "system", "optionally", "use", "gp", "orientation", "information", "embed", "Videoscape", "map", "core", "we", "method", "we", "intentionally", "only", "use", "video", "frame", "maximum", "generality", "we", "incorporate", "gp", "orientation", "information", "filter", "phase", "we", "have", "yet", "extend", "matching", "phase", "however", "datum", "often", "unreliable", "city", "we", "allow", "large", "view", "change", "-lrb-", "e.g.", "zoom", "-rrb-", "integrate", "datum", "trivial", "express", "spatial", "information", "portal", "choice", "we", "interactive", "navigation", "mode", "challenge", "mini-map", "show", "frusta", "path", "when", "hover", "over", "portal", "thumbnail", "show", "where", "video", "choice", "move", "desire", "express", "within", "view", "camera", "we", "believe", "difficult", "problem", "portal", "image", "definition", "look", "similar", "so", "place", "they", "current", "view", "-lrb-", "equivalent", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "-rrb-", "tell", "user", "very", "little", "about", "what", "happen", "each", "candidate", "video", "path", "beyond", "portal", "equally", "what", "10", "video", "connect", "each", "move", "away", "different", "direction", "we", "solution", "give", "user", "fast", "access", "every", "frame", "every", "connected", "video", "through", "scrubbing", "naturally", "question", "arise", "whether", "we", "system", "could", "use", "community", "video", "database", "preliminary", "experiment", "we", "could", "find", "sufficient", "appropriate", "video", "location", "we", "system", "current", "community", "database", "signal-to-noise", "ratio", "too", "low", "stark", "contrast", "community", "photo", "database", "perhaps", "-rrb-", "currently", "online", "database", "do", "contain", "sufficient", "suitable", "video", "we", "system", "which", "we", "believe", "unlikely", "which", "would", "correct", "over", "time", "more", "video", "add", "-rrb-", "online", "database", "do", "contain", "video", "very", "difficult", "find", "current", "search", "base", "key-word", "wholevideo", "label", "association", "visual", "content", "geographical", "feature", "we", "work", "go", "some", "way", "ease", "browsing/search", "difficulty", "we", "have", "present", "system", "extract", "structure", "from", "casually", "capture", "unstructured", "video", "collection", "we", "build", "videoscape", "graph", "connect", "video", "through", "portal", "which", "show", "similar", "place", "event", "we", "introduce", "integrate", "set", "interface", "interact", "explore", "videoscape", "provide", "fast", "non-linear", "access", "whole", "video", "collection", "make", "easy", "observe", "liveliness", "dynamics", "environment", "event", "convey", "sense", "space", "time", "when", "navigate", "through", "video", "transition", "portal", "render", "spatially", "immersive", "way", "we", "have", "study", "user", "preference", "different", "transition", "type", "we", "use", "finding", "inform", "automatic", "transition", "selection", "system", "we", "have", "evaluate", "we", "prototype", "system", "database", "video", "which", "feature", "variety", "location", "time", "viewing", "condition", "through", "three", "further", "user", "study", "we", "have", "demonstrate", "we", "system", "provide", "benefit", "over", "exist", "system", "term", "spatial", "awareness", "video", "summarization", "video", "browsing", "we", "thank", "Min", "H.", "Kim", "he", "help", "psychophysical", "analysis", "Christian", "Kurz", "Thorsten", "Thorm?hlen", "track", "help", "Gunnar", "Thalin", "we", "custom", "deshaker", "build", "Malcolm", "Reynolds", "Maciej", "Gryka", "Chenglei", "Wu", "Ahmed", "Elhayek", "capture", "help", "Gabriel", "Brostow", "many", "other", "UCL", "MPI", "who", "discuss", "provide", "feedback", "all", "we", "participant", "Flickr", "user", "cogdog", "garryknight", "goincase", "ramoncutanda", "image", "Figure", "aerial", "imagery", "2012", "Microsoft", "NASA", "DigitalGlobe", "NAVTEQ", "Harris", "Corp", "Earthstar", "Geographics", "Google", "BlueSky", "we", "also", "thank", "engd", "veiv", "Centre", "UCL", "BBC", "EPSRC", "grant", "ep/i031170/1", "fund", "support", "GARWAL", "S.", "navely", "N.", "imon", "i.", "eitz", "zeliskus", "R.", "2009", "building", "Rome", "day", "D.", "arl", "aatz", "G.", "ser", "K.", "HEN", "D.", "RZESZCZUK", "R.", "ollefey", "M.", "2010", "handle", "urban", "location", "recognition", "2d", "homothetic", "problem", "eccv", "266", "279", "ALLAN", "L.", "ROSTOW", "G.", "uwein", "J.", "ollefey", "M.", "2010", "unstructured", "video-based", "rendering", "interactive", "exploration", "casually", "capture", "video", "ACM", "Trans", "SIGGRAPH", "-rrb-", "29", "87:1", "87:11", "ell", "D.", "UEHNEL", "F.", "AXWELL", "C.", "IM", "R.", "ASRAIE", "K.", "ASKINS", "T.", "OGAN", "P.", "OUGHLAN", "J.", "2007", "NASA", "World", "wind", "opensource", "gi", "mission", "operation", "IEEE", "Aerospace", "Conference", "surka", "G.", "RAY", "C.", "ANCE", "C.", "categorization", "bag", "keypoint", "atta", "R.", "OSHI", "D.", "J.", "ang", "J.", "Z.", "2008", "image", "retrieval", "idea", "influence", "trend", "new", "age", "ACM", "Comput", "ebevec", "P.", "E.", "aylor", "C.", "J.", "ALIK", "J.", "1996", "modeling", "render", "architecture", "from", "photograph", "hybrid", "geometryand", "image-based", "approach", "SIGGRAPH", "11", "20", "mytryk", "E.", "1984", "Focal", "Press", "M.", "P.", "C.", "arneb?ck", "G.", "2003", "two-frame", "motion", "estimation", "base", "polynomial", "expansion", "SCIA", "363", "370", "rahm", "j.-m.", "ollefey", "M.", "AZEBNIK", "S.", "ALLUP", "D.", "LIPP", "B.", "AGURAMA", "R.", "C.", "ach", "C.", "OHN", "SON", "T.", "2010", "fast", "robust", "large-scale", "mapping", "from", "video", "internet", "photo", "collection", "ISPRS", "Journal", "Photogrammetry", "Remote", "sense", "65", "538", "549", "urukawa", "Y.", "once", "J.", "2010", "Accurate", "dense", "robust", "multi-view", "stereopsis", "IEEE", "TPAMI", "32", "1362", "1376", "N.", "URLESS", "B.", "OPPE", "H.", "multi-view", "stereo", "community", "photo", "S.", "AUBOLD", "t.", "2010", "ambient", "point", "artley", "R.", "I.", "eath", "K.", "ELFAND", "N.", "VSJANIKOV", "M.", "ANJANEYA", "M.", "uiba", "L.", "J.", "2010", "image", "web", "computing", "exploit", "connectivity", "image", "collection", "IEEE", "CVPR", "3432", "3439", "H.", "2006", "ENNEDY", "L.", "AAMAN", "M.", "2008", "Generating", "diverse", "representative", "image", "search", "result", "landmark", "WWW", "297", "306", "ENNEDY", "L.", "AAMAN", "M.", "2009", "less", "talk", "more", "rock", "automate", "organization", "community-contributed", "collection", "concert", "video", "WWW", "311", "320", "azebnik", "S.", "chmid", "C.", "once", "J.", "2006", "beyond", "bag", "feature", "spatial", "pyramid", "match", "recognize", "natural", "scene", "category", "IEEE", "CVPR", "2169", "2178", "eung", "T.", "ALIK", "J.", "2001", "represent", "recognize", "visual", "appearance", "material", "use", "three-dimensional", "texton", "ijcv", "43", "29", "44", "X.", "C.", "ach", "C.", "AZEBNIK", "S.", "rahm", "j.-m", "2008", "modeling", "recognition", "landmark", "image", "collection", "use", "iconic", "scene", "graph", "eccv", "427", "440", "ippman", "a.", "1980", "movie-map", "application", "optical", "videodisc", "computer", "graphic", "Computer", "Graphics", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "14", "32", "42", "ipskus", "C.", "INZ", "C.", "EUMANN", "T.", "AGNOR", "M.", "2010", "high", "resolution", "image", "correspondence", "Video", "PostProduction", "visual", "Media", "Prod", "urdy", "N.", "J.", "RISWOLD", "W.", "G.", "2005", "system", "architecture", "ubiquitous", "video", "International", "Conference", "Mobile", "Systems", "application", "Services", "14", "orvan", "Y.", "O?S", "ULLIVAN", "C.", "2009", "handle", "occluder", "transition", "from", "panoramic", "image", "perceptual", "study", "ACM", "Trans", "Applied", "Perception", "15", "hilbin", "J.", "ivic", "J.", "isserman", "a.", "2011", "geometric", "latent", "dirichlet", "allocation", "matching", "graph", "large-scale", "image", "dataset", "ijcv", "95", "138", "153", "ongnumkul", "S.", "ang", "J.", "OHEN", "M.", "2008", "create", "map-based", "storyboard", "browse", "tour", "video", "ACM", "Symposium", "user", "Interface", "Software", "Technology", "13", "22", "aurer", "O.", "raundorfer", "F.", "ollefey", "M.", "2010", "OmniTour", "semi-automatic", "generation", "interactive", "virtual", "tour", "from", "omnidirectional", "video", "3DPVT", "J.", "2006", "ivic", "J.", "isserman", "a.", "2003", "Video", "Google", "text", "retrieval", "approach", "object", "match", "video", "iccv", "1470", "1477", "R.", "2008", "horm?hlen", "t.", "2006", "zuverl?ssige", "sch?tzung", "der", "Kamerabewegung", "aus", "einer", "Bildfolge", "phd", "thesis", "University", "Hannover", "Voodoo", "Camera", "Tracker", "can", "download", "from", "http://www.digilab.uni-hannover.de", "Wiley", "orgerson", "W.", "S.", "1958", "angorp", "P.", "HAURASIA", "G.", "AFFONT", "P.-Y.", "LEMING", "R.", "RETTAKIS", "G.", "2011", "perception", "visual", "artifact", "image-based", "rendering", "fa?ades", "Computer", "Graphics", "Forum", "-lrb-", "07", "-rrb-", "1241", "1250", "ea", "E.", "ULLONI", "A.", "RUIJFF", "E.", "EGENBRECHT", "H.", "CHMALSTIEG", "D.", "2010", "technique", "view", "transition", "multi-camera", "outdoor", "environment", "Graphics", "Interface", "193", "200", "EYAND", "T.", "eibe", "B.", "2011", "discover", "favorite", "view", "popular", "place", "iconoid", "shift", "AMIR", "A.", "R.", "HAH", "M.", "2010", "accurate", "image", "localization", "base", "Google", "map", "street", "view", "eccv", "255", "268" ],
  "content" : "We evaluate our system using three further user studies, which allows us to conclude that Videoscapes provides significant benefits over related methods. Our system leads to previously unseen ways of interactive spatio-temporal exploration of casually captured videos, and we demonstrate this on several video collections. While our system cannot provide directions from location A to B, as sparse video collections may not contain sufficient input, it does provide the spatial arrangement of landmarks contained within a video collection (distinct from the geolocations of video captures). Unlike tours through images, our system conveys a sense of place, dynamics and liveliness while still maintaining  seamless browsing with video transitions. We automatically identify portals from an appropriate subset of the video frames as there is often great redundancy in videos, and process the portals (and the corresponding video frames) to enable smooth transitions between videos. We exemplify the use of our system on databases of parts of London. Sets of videos captured on different days were processed into a Videoscape that can be interactively explored, and we demonstrate our interactive interfaces in our supplemental video. We employ robust key-point matching for portal identification (Section 4), an approach that has also been used in recent work on  content-based geolocation of images [Baatz et al. 2010; Zamir and Shah 2010; Li et al. 2008]. However, our setting is different since our graph models entire video collections covering many landmarks, and our filtering and matching technique are adapted specifically to our sparse video data. For instance, a naive application of [Frahm et al. 2010b] on our London video collection cannot yield a full 3D reconstruction of the depicted environment as the video data is sparse. As video coverage is sporadic, we reconstruct scene and camera geometry only for specific locations (i.e., at portals). As will be discussed in Section 4, they can all benefit from our analysis of global context in the graph structure. The system first builds a sparsely connected graph by performing feature-based matching, which is made incrementally denser via connectivity analysis. Our portal identification scheme also relies on key point matching followed by connectivity analysis based on the graph Laplacian. We capitalize on previous work in this area to render portals while navigating the Videoscape. As a consequence, the system operates in a spatially confined environment where a dense set of views can be easily captured. Our system improves upon this method by automatically identifying connections between many videos and joining them with visual transitions. The background is reconstructed from additional community photos of the scene, and the video cameras are calibrated w.r.t. The system is state of the art, but is tailored to spatially confined sets of videos that all see the same event at the same time from converging camera angles. In contrast, our system operates with a collection that shows a variety of general scenes filmed from a much less constrained set of camera positions at different times. Our system has both on-line and off-line components. The graph can be either directed or undirected, the difference being that an undirected graph allows videos to play backwards. If necessary, the graph can maintain temporal consistency by only allowing edges to portals that are forward in time. The graph can also include portals that join a single video at different times (a loop within a video). Along with the portal nodes, we also add nodes representing the start and end of each input video. This ensures that all connected video content is navigable. Our approach  is suitable for indoor and outdoor scenes. The online component provides interfaces to navigate the Videoscape by watching videos and rendering transitions between them at portals. A portal is a span of video frames in either video that shows the same physical location, possibly filmed from different viewpoints and at different times. In practice, we represent the portal by a single pair of portal frames from this span, one frame from each video, through which a visual transition to the other video can be rendered ( Figure 2 ). In addition to portals, we also identify all frames across all videos which broadly match these portal frames. This produces clusters of frames around visual targets, and enables 3D reconstruction of the portal geometry. After a portal and its supporting set have been identified, the portal geometry is reconstructed as a 3D model of the environment. First, we identify candidate portals by matching suitable frames between videos that contain similar content (Section 4.1). Out of these candidates, we select the most appropriate portals and deduce the support set for each. Filtering Naively matching all frames in the database against each other is computationally prohibitive. Ideally, the system would  select just enough frames per video such that all visual content were represented and all possible transitions were still found. Optical flow analysis [Farneb?ck 2003] provides a good indication of the camera motion and allows us to find appropriate video frames that are representative of the visual content. We analyze frame-to-frame flow, and pick one frame every time the cumulative flow in x (or y) exceeds 25% of the width (or height) of the video; that is, whenever the scene has moved 25% of a frame. With GPS and orientation sensor data provided, we further cull candidate frames that are unlikely to provide matches. However, even though we perform sensor fusion with a complementary filter, we still cull with respect to the sensor error as sensor data is often unreliable. This additional sensor filtering step allows us to process datasets 4? larger for the same computational cost. Holistic Matching and Feature Matching The holistic matching phase examines the global structural similarity of frames based on spatial pyramid matching [Lazebnik et al. 2006]. We use bag-ofvisual-word-type histograms of SIFT features [Csurka et al. 2004; Leung and Malik 2001] with a standard set of parameters (#pyramid levels = 3, codebook size = 200). The resulting matching score between each pair of frames is compared and pairs with scores lower than a threshold T H are discarded. The use of a holistic match before the subsequent feature matching has the advantage of reducing the overall time complexity, while not severely degrading matching results [Heath et al. 2010; Frahm et al. 2010a; Frahm et al. 2010b]. The output from the holistic matching phase is a set of candidate matches (i.e., pairs of frames), some of which may be incorrect. We improve results through feature matching, and match local frame context through the SIFT feature detector and descriptor. Context Refinement The output of the feature matching stage may still include false positive matches which are hard to remove using only the result of pairwise feature matching. Figure 3 shows an example of an incorrect match. In preliminary experiments, we observed that when simultaneously examining more than two pairs of frames, correct matches are more consistent with other correct matches than with incorrect matches. For example, when frame I 1 correctly matches frame I 2 , and frame I 2 and I 3 form another correct match, then it is very likely that I 1 also matches I 3 . We exploit this context information and perform a novel graph-based match refinement to prune false positives. We first build a graph representing all pairwise matches, where nodes are frames and edges connect matching frames. where I i and I j are connected frames, S(I) is the set of features (SIFT descriptors) calculated from frame I and M (I i , I j ) is the set of feature matches for frames I i and I j . Given this graph, we run spectral clustering [von Luxburg 2007], take the k first eigenvectors with eigenvalues > T I , T I = 0.1, and remove connections between pairs of frames that span different clusters. This effectively removes incorrect matches ( Figure 3 ), since, intuitively speaking, spectral clustering will assign to the same cluster only frames that are well inter-connected. A good portal should exhibit good feature matches as well as allow for a non-disorientating transition between videos ? both of these are more likely for frame pairs shot from similar camera views, i.e., frame pairs with only small displacements between matched features. Therefore, we retain only the best available portals between a pair of video clips. To this end, we enhance the metric from Equation 1 to favor such small displacements and define the best portal as the frame pair (I i , I j ) that maximizes the following score: M(I i ,I j ) F where D(?) is the diagonal size of a frame, M (?, ?) is the set of matching features, M is a matrix whose rows correspond to feature displacement vectors, ? F is the Frobenius norm, and ? is the ratio of the standard deviations of the first and the second summands excluding ? . Figure 4 shows examples of identified portals (see Section 6 for the details of our experimental setup). For each portal, we define the support set as the set of all frames from the context that were found to match to at least one of the portal frames. Videos with no portals are not included in the Videoscape. We implement seven different transition techniques which run this gamut: a cut, a dissolve, a warp and four 3D reconstruction camera sweeps. In Section 6.2, we psychophysi cally assess which techniques are preferred for different scenes and viewing conditions. The cut jumps directly between the two portal frames. The dissolve linearly interpolates between the two videos over a fixed length. The warp and the 3D reconstruction cases exploit the support set of the portal. We begin by employing an off-the-shelf structurefrom-motion (SFM) technique [Snavely et al. 2006] to register all cameras from each support set. We also use an off-the-shelf KLTbased camera tracker [Thorm?hlen 2006] to find camera poses for video frames in a four second window around each portal (further details are included in the supplemental material). Interpolating this transform provides the broad motion change between portal frames. Ontop of this, individual video frames are warped to the broad motion using the (denser) KLT feature points, again by an as-similar-aspossible MLS transform. However, some ghosting still exists, so a temporally-smoothed optical flow field is used to correct these errors in a similar way to Eisemann et al. [2008]. All warps are precomputed once the Videoscape is constructed. The four 3D reconstruction transitions use the same structurefrom-motion and video tracking results. We then perform an automated clean-up to remove isolated clusters of points by density estimation and thresholding (i.e., finding the average radius to the k-nearest neighbors and thresholding it). We register the video tracking result to the SFM cameras by matching screen-space feature points. Two further transitions require the geometry to be completed using Poisson reconstruction [Kazhdan et al. 2006] and an additional background plane placed beyond the depth of any geometry, such that all camera views are covered by geometry. With this, we support a full 3D ? dynamic transition, where the two videos are projected onto the geometry. Finally, we support a full 3D ? static transition, where only the portal frames are projected onto the geometry. This is useful when camera tracking is inaccurate (due to large dynamic objects or camera shake) as it typically provides a view without ghosting artifacts. Their conclusions suggest that simply dissolving occluders into the background is in most cases the best method to apply, even when segmentation information for dynamic objects is available. Key transition types are shown in Figure 5 . However, if we stabilize before processing, we jeopardize our vision-based matching and reconstruction as software stabilization breaks geometric assumptions upon which we rely. Instead, we pre-compute 2D affine stabilization parameters (i.e., a per-frame crop region) but do not apply them to our input videos ? we pass the videos unaltered to our reconstruction pipeline. Then, we optionally apply these pre-computed stabilization parameters in real-time in our renderer. During transitions, we interpolate the stabilization parameters across the transition. For geometry-based transitions, we project the original unstabilized video footage and only stabilize the virtual camera view. We have developed a prototype explorer application (Figures 6 & 7) which exploits the Videoscape data structure and allows seamless navigation through sets of videos. This important aspect maintains the visual link between the graph (and its embedding) and the videos during animations, and helps the viewer from becoming lost. Our supplemental video demonstrates these workflows and their interplay. Interactive Exploration Mode Watching videos is often an immersive full-screen experience, and a Videoscape is no different ( Figure 6 ). In this workflow, as time progresses and a portal is near, we notify the viewer with an unobtrusive icon. If they choose to switch videos at this opportunity by moving the mouse, a thumbnail strip of destination choices (neighboring graph nodes) smoothly appears asking ?what would you like to see next? Here, the viewer can pause and scrub through each thumbnail as video to scan the contents of future paths. With a thumbnail selected, our system generates an appropriate transition from the current scene view to the new video. Audio is cross-faded as the transition into the new video is shown, and then the new video takes the viewer to their chosen destination view. This paradigm of moving between views of scenes is applicable when no other data beyond video is available, e.g., when we cannot provide additional geographical context. This forms our baseline experience. If a choice leads to a dead end, or if a choice leads to the previously seen view, we add commonly understood road sign icons as well. Should GPS and orientation data be available, we add a togglable mini-map which displays and follows the view frustum in time from overhead. Hovering over a destination choice thumbnail shows the frustum and real-world point on the mini-map, and updates the timeline accordingly. Overview Modes At any time, the mini-map can be expanded to fill the screen, and the viewer is presented with a large overview of the Videoscape graph embedded into a globe [Bell et al. 2007] ( Figure 7 , top). In this second workflow, we add eye icons to the map to represent portals. The geographical location of the eye is estimated from converged sensor data, so that the eye is placed approximately at the viewed scene. As a Videoscape can contain hundreds of portals, we adaptively change the density of the displayed eyes so that the user is not overwhelmed. Eyes are added to the map in representative connectivity order, so that the mostconnected portals are always on display. When hovering over an eye, we inlay images of views that constitute the portal, along with cones showing where these views originated. The viewer can construct a video tour path by clicking eyes in sequence. The defined path is summarized in a strip of video thumbnails that appears to the right. As each thumbnail can be scrubbed, the suitability of the entire planned tour can be quickly assessed. Our system can automatically generate tour paths from specified start/end points. The third workflow is fast geographical video browsing. We draw real-world travelled paths onto the map as lines. When hovering over a line, the appropriate section of video is displayed along with the respective view cones. Here, typically the video is shown sideby-side with the map to expose detail, though the viewer has full control over the size of the video should they prefer to see more of the map ( Figure 7 , bottom). As time progresses, portals are identified by highlighting the appropriate eye and drawing secondary view cones in yellow to show the position of alternative views. Clicking during this time appends that view to the current tour path. Once a path is defined by either method, the large map then returns to miniature size and the full-screen interactive mode plays the tour. This interplay between the three workflows allows for fast exploration of large Videoscapes with many videos, and provides an accessible non-linear interface to content within a collection of videos that may otherwise be difficult to penetrate. Image/Label-based Search Mode We allow the viewer to search with images to define a tour path, and to search with labels. For image search, image features are matched against portal frame features, and candidate portal frames are found. A scrubbable video list appears showing the best matching candidates and from these a path can be selected. A new video is generated in much the same way as before, but now the returned video is bookended with warps from and to the submitted images. For label search, the user provides key words and matching results are presented in a video list as in the image search. We perform three classes of experiments: In the first class, we evaluate each individual component for constructing the Videoscape (Section 6.1). Here, the main objective is to gain an insight into the performance in comparison with potential alternatives. In the second class of experiments, we psychophysically assess video-tovideo transitions for preference, and assess spatial awareness improvement through transitions (Section 6.2). In the third class of experiments, we perform user studies to evaluate the interface and utility of Videoscapes against existing systems (Section 6.3). During the project, we captured various datasets to demonstrate our method. Here, we provide a detailed analysis of one of these datasets, but the processes used between all datasets are virtually identical and the performance is similar. Our analysis database comprises 196 videos taken at several locations in London. These videos include landmarks such as Big Ben, the London Eye, and St Paul?s Cathedral. The database also includes general street footage between and around landmarks. Individual videos feature a variety of motions, and include pans to take in a view or casual movement around a location. The videos vary in location, duration (typically between 10 seconds and 3 minutes),  time of day, foreground objects, and viewpoint. In this database, the videos were captured asynchronously with one camera (Sanyo FH1) at a resolution of 1920 ? 1080, but other databases (South Bank, Bicycles) were captured concurrently with multiple heterogeneous cameras and varying frame rates. Where employed, our sensor data was captured with smartphones, but all video and optional sensor data could be captured with just one smartphone. Filtering Our frame sampling strategy (Section 4.1) reduces unnecessary duplication in still and slow rotating segments. The reduction in the number of frames over regular sampling is content dependent, but in our data sets this flow analysis picks approximately 30% fewer frames, leading to a 50% reduction in computation time in subsequent stages compared to sampling every 50th frame (a moderate trade-off between retaining content and the number of frames). For a random selection of one scene from 10 videos, we compare the number of frames representing each scene for the naive and the improved strategy. On average, for scene overlaps that we judged to be visually equal, the flow-based method produces 5 frames, and the regular sampling produces 7.5 frames per scene. This indicates that our pre-filtering stage extracts frames more economically while maintaining a similar scene content sampling. In our first database, approximately 3,500 frames were extracted in the filtering phase from a starting set of approximately 500,000. Portal Identification and Context Refinement The performance of the portal identification algorithm was evaluated by measuring the precision and recall for a random subset of our analysis database. Precision was measured from all identified portals connecting to 30 randomly selected videos. The corresponding frame matches were visually inspected and portals were labeled as ?correct? when matching frames represented the same scene. To calculate recall, 435 randomly selected pairs of videos were visually inspected to see if their scene content overlapped. Again, ground truth portals were identified as ?found? when there was a corresponding automatically identified portal. Table 1 proves the importance of each phase of portal finding (the threshold for the holistic phase was fixed to T H = 2.2, see Section 4.1). Using only holistic matching, a high recall can be reached but precision is rather low. Adding feature matching leads to a drastic increase in precision (holistic & feature matching 1). Finally, all phases together yield a precision of 98% and a recall rate of 53%. It is possible to achieve the same precision with feature matching (holistic & feature matching 2) by simply thresholding the number of key correspondences. Reaching 100% precision with automatic methods is nearly impossible, even analyzing context information through graph-based refinement cannot completely rule out these errors. For these rare cases, the user can manually flag the remaining incorrect portals in the interactive viewer. On this set of 196 videos, all portal identification steps took approximately four days on one Xeon X5560 2.66GHz (using one core). Using filtering instead of regular sampling saves two days of computation. 232 portals were found. Except for the first phase, specifically the codebook generation, the off-line procedure could be executed in parallel. The average size of a portal support set is 20 frames. Support sets can be augmented by neighbors of the support set frames. Including one neighborhood set increased the average size to 45, while two increased it to 70. However, including all neighborhoods recursively does not produce a complete reconstruction of the video database due to varying video coverage. Instead, the graph linkage structure maintains global navigability. We choose to use support sets extended by two neighborhoods, as this was a good compromise between computation speed and reconstruction extent for our data set. Reconstruction and tracking for all portals took two days, running in parallel on eight Xeon X5560 2.66GHz cores. Even though we use state-of-the-art multi-view 3D reconstruction [Furukawa et al. 2010], the resulting geometry can be of poor quality for various reasons, e.g., glass buildings, thin building structures, rotational symmetry, or simply that the database does not provide a sufficient baseline for a landmark. We handle these portals by choosing a dissolve transition. Transition Preference We want to choose the best transition technique from a user perspective between two videos. Under which circumstances is one technique preferred over another? We hypothesize that only certain transition types are appropriate for certain scenes. Of our seven transition types (cut, dissolve, warp, plane, ambient point clouds, and static and dynamic full 3D reconstructions), we expect warps and blends to be better when the view change is slight, and transitions relying on 3D geometry to be better when the view change is considerable. To this end, we conducted a user study which asked participants to rank transition types by preference. We chose ten pairs of portal frames representing five different scenes. For each scene, one transition forms a slight view change (10 ? average; including zoom changes) and one transition forms a considerable view change (up to 55 ? ). The target video is always the same for both view changes. The five scenes were chosen as they each display a potentially difficult situation (Scene 1: dynamic objects at boundary; Scene 2: many dynamic objects with view occlusions and panning camera; Scene 3: panning cameras and dynamic objects; Scene 4: fast moving dynamic objects and shaking camera/rolling shutter; Scene 5: complicated foreground objects and moving, shaking camera). All scene/transition type renders (e.g., Figure 5 ) are included as supplementary material. Participants ranked the seven transition types for each of the ten portals. First, the scenario of creating a video tour is explained to the participant and an example is shown. Participants are then presented with each set of video transitions in a random order. The transitions are randomly placed into a vertical video list. Of the 21 participants in our experiment, 12 were self-described experts with experience in graphics and media production, 4 were amateurs, and 5 were novices. On average, it took 52 minutes to complete the study and provide text comments. We perform multi-dimension scaling [Torgerson 1958] to place our transition types on an interval scale. Figure 8 shows the mean and standard deviation across all scenes. Individual results are summarized in Table 2 ; detailed graphs can be found in the supplemental material. The results show that there is an overall preference for the static 3D transition. Surprisingly, 3D transitions where both videos continued playing were preferred less. Looking at the per- scene results, we hypothesize that this is due to ghosting which stems from inaccurate camera tracks in the difficult shaky cases. Many participants commented that the 3D transitions maintained important spatial relationships between landmarks and provided fluid camera movement. The warp is significantly preferred against all but the full 3D techniques for slight view changes (p < 0.05, t-test). We believe this is caused by screen areas which lack projection because the video has been paused during the transition, i.e., the virtual camera still pans as it interpolates between videos but the projection onto geometry does not. Our supplemental material contains per-scene perceptual scale plots and per-scene-class significance tables. Towards Automatically Choosing Transition Types This outcome helps develop rules for selecting appropriate transition types. However, in our experience with the system, many portals with slight view changes are actually similar to Scene 3 ( Figure 5 ) in that they do not suffer shake and so provide high-quality results when using the dynamic 3D transition. We use the static 3D transition for considerable view changes. The results also show that a dissolve is preferable to a cut. Our 20 participants were self-assessed as familiar with the geographical area depicted in the tasks (avg. lived there for 3 years, max. 10 years, min. The experiment is as follows ( Figure 9 ): 1 A participant sees an overhead aerial imagery map marked with a ground-truth pin and a view direction. The pin marks the realworld camera position of video 1. After 8 seconds, a visible countdown begins (3... 2 The map is removed and the participant watches a short clip of video 1 transitioning into video 2. 3 The video is removed, the map reappears, and the participant marks on the map the location and direction travelled to after the transition into video 2. Objectively, we measure the deviation from ground truth of the position and direction marked with the red pin by the participant. Participants are free to replay the video and reposition the pin/direction as many times as they wish, and are also free to translate and zoom the map. When placing the pin for the second video, the pin for the first video is present on the map. Ground truth is generated from GPS coordinates hand-refined with local knowledge and known positions of the camera shots. We test two conditions in our experiment: 1) cut transitions without providing view directions (i.e., just a location pin; the condition most akin to other existing systems, in particular to a multi-video variant of Pongnumkul et al. [2008]), and 2) static 3D transitions with provided view direction (the Videoscapes condition). Each participant completed 1 practice trial as often as they wished, followed by 8 randomly ordered trials each of a different scene, of which 4 are from each condition. Performance was measured with five criteria, which are presented in Table 3 along with statistical significance. Overall, both the location and direction error from ground truth and the time for completing the task were similar between the Videoscapes and the cut/no direction conditions. However, the Videoscapes condition produced significantly fewer video replays and significantly fewer location/view direction adjustments, which we suggest are indicators of increased spatial awareness. Following the task, each participant completed a questionnaire: Q1: ?With which interface did you find it easiest to complete the task? Table 4 summarizes the results. For Q1, of the participants who preferred our system for the task (17/20), 35% found it ?Much easier?, 59% found it ?Easier?, and 6% found it ?Slightly easier?. For Q2, of the participants who found our system provided the greater spatial awareness and sense of orientation (19/20), 58% found it ?Much more?, 26% found it ?More?, and 16% found it ?Slightly more? providing. In all questions, the neutral response (?Both same?) was an option. The Videoscapes condition required less video replays and less location/direction adjustments for the same accuracy. The questionnaire shows that most participants preferred our system, and almost all save one thought our system provided more spatial awareness. This correlates with our quantitative data, and demonstrates that our system helps maintain greater spatial awareness through transitions. Evaluating our prototype interface in a meaningful way is challenging: existing systems do not provide comparative functionality, yet we do not wish to provide a trivial comparison. Equally, evaluating our large system as a whole is likely uninformative to the community as it would be too specific to our system. As such, we performed two different user studies designed to provide quantitative and qualitative feedback for major components of our system. Each study compares user performance/experience with Videoscapes to that achieved with existing alternatives. The 20 participants in the spatial awareness experiment also performed both of our interface experiments. However, our exploration sessions could be thought of as a geographical tour or summarization of the video database, particularly for the case where the user selects start and end locations and leaves the path to be generated by our system (instead of interactively navigating). As such, in this experiment, each participant watches three videos which have been automatically edited by software: 1) An InstantMovie from Adobe Premiere Elements 7.0, 2) the intelligent fast forward of Pongnumkul et al. [2008], and 3) the video tour mode of Videoscapes (with blend transitions). Each ?summarization? video was generated with the same input database. For participants, videos could be replayed at will, and videos were presented in a random order. Participants were asked to concentrate on the way the content was presented (style), and not on any specific content. Participants completed the questionnaire listed below and ranked the three styles explicitly. The questions were: ?Which style... did you most prefer? ? Q2: ?... did you find most interesting? ? Q3: ?... did you find provided the best sense of place? ? Q4: ?... did you find most spatially confusing? ? Q5: ?... would you use most often in your own video collections? ? Q6: ?... would you view most often for online video collections? ?\n           Table 5 summarizes the results. Ranking scores (from 3 to 1) are accumulated over all participants. Significances were computed by the Kruskal-Wallis test and then by pairwise Mann-Whitney U-tests, with alpha at 0.05. Videoscapes is significantly the most preferred summarization style, provides the best sense of place, and is least spatially confusing of the three videos for the dataset\n          1 The exception being the InstantMovie, which contains two instances of hard-to-remove overlaid theme graphics and infrequent minor ?effects?. Participants were explicitly asked to ignore these and concentrate only on content when considering their answers. Video browsing comparison interfaces. Top: Our implementation of [Pongnumkul et al. 2008] adapted for video databases. Bottom: iMovie ?11. Cut-outs show scrubbing and thumbnail expansion (which frequently made participants lost). We must take care not to extrapolate this to all datasets as one example is not conclusive, but Q5 and Q6 significantly suggest that our system may be preferred most often for personal and online video collections. The Pongnumkul style is also consistently preferred over the InstantMovie style. Even though no map is ever shown, this experiment suggests that exploiting geographical data is an important addition to video database summarization. Video Browsing Experiment Our final experiment attempts to evaluate Videoscapes as a tool for browsing and retrieving video contents. Participants were asked to find five different videos with contents similar to an image query of a major landmark, using three different interfaces: 1) Apple iMovie ?11, 2) our implementation of a multi-video version of Pongnumkul et al. [2008] 2 , and 3) Videoscapes (Figures 7 & 10). For this task, four browsing methods within Videoscapes were also evaluated, which include image search, label search, browsing eye icons (the second workflow in Section 5), and geographical video browsing (the third workflow in Section 5). In this experiment, the label database held only objective labels of specific landmarks. Before use, each interface was thoroughly explained to the satisfaction of the participant, and participants were given the option to use whichever methods they wished within each interface for completing the task. Each participant performed the task in each interface in a random order. The two main evaluation criteria are T1: the average time taken to complete the task (sec.) and T2: the average number of occur-\n          2 As written, their paper only supports a single video and manual geotagging. We automatically place thumbnails and vary their density on zoom so that the visualization of all interesting shots in a database is possible. Ours 3 sig. Q1 27 40 53 < 0.000 0.007 Q2 30 41 49 < 0.000 0.121 rences of finding the same video more than once (i.e., error rate). Figure 11 shows the results. These indicate that the speed and accuracy of browsing is significantly improved by using Videoscapes over existing systems. Our interface exposes this structure in two fast ways: image or label search, and geographically-placed visual groupings with our eyes icons. Participants also completed a questionnaire following the task: Q1:?Which interface did you most prefer for completing the task of finding content? ? and Q2: ?Which interface do you think you would most prefer for browsing content generally? The results were computed as before, and Table 6 summarizes the results. Videoscapes is significantly preferred over both other systems in the task and significantly preferred over iMovie in the general case. Again we must not generalize beyond the experiment and dataset, but the responses to our interface are promising. When asked which interface components they preferred from our prototype, the browsing eye icons and image search methods were most preferred for the task and for the general browsing of video collections. Finally, we asked participants whether they would want to use our interface for browsing personal and online video collections. The results are promising, with 95% responding that they would use it, and at least 80% responding sometimes or often ( Table 7 ). We now describe results shown in the supplemental video. The generated videos are difficult to represent in print so we strongly encourage the reader to view our results in motion. The first example demonstrates interactive exploration. The viewer first chooses a view that leads towards the Tate Modern. This transition covers 400m, and moves the view to a stabilized hand-held shot of the River Thames from the Millennium Bridge. All transitions are full 3D ? static, except for the Tate Modern transition, which is a warp. Our second example is a map-based exploration session. The viewer demonstrates the two overview workflow modes, the auto matic tour plotting, and the tour summary strip. From here, the overview reduces to a mini-map, and the video is revealed behind. The resulting tour is then fast-forwarded through to demonstrate that our generated tours can cover large geographical areas. As the tour is sped up dramatically, the portal transitions are less visible. Next, we show the interactive workflow and mini-map. The viewer scrubs a portal choice thumbnails to see the path in the mini-map. We then show a bicycle race example, highlighting a slightly different use case for our system. Here, spectator videos are combined with bicycle-mounted cameras. Both this and the previous tour examples enforce temporal consistency. Our labeling system exploits the Videoscape structure and computation to provide instantaneous propagation through the database. We show a viewer labeling a museum and providing a review, which is then instantly propagated to similar video frames in all other database videos (only one of which is shown). Finally, our last example is generated by providing two landmark images (Houses of Parliament and the London Eye) to be visited during the tour. The system matches these images against portals, and plans a tour visiting the landmarks. The tour shows the first search image, then warps into stabilized video, showing a series of full 3D ? dynamic transitions (one with a particularly large view change) as it travels to the final landmark where it warps to the second search image. In this tour, we do not enforce temporal consistency and allow videos to play backwards, which increases the number of possible tours in sparse datasets. Unstructured video collections present many challenges, and we believe our system rises to many of these with convincing results. Our results use datasets that require only loose temporal coherence; other datasets may require this functionality more strongly. Videos taken during an event, such as our bicycle race example, may require a temporally consistent exploration of the Videoscape, else e.g., the position of the leader may suddenly change. Many other datasets do not require temporal consistency, and novel experiences may come from intentionally disabling temporal consistency. Our system is sufficiently general to accommodate these scenarios. By design, our proposed method does not model foreground objects. In both portal identification and 3D reconstruction, foreground objects are regarded as outliers in matching and accordingly are ignored. This can sometimes introduce distracting artifacts: some objects warp or vanish during the transition. For example, pedestrians may warp into each other. This is mitigated when using spatio-temporally coherent exploration (Section 5), when temporally aligned video data is available. If foreground objects could be reliably segmented, then there is an opportunity to remove them before a transition occurs (by dissolving against an inpainting), and then to dissolve in the new dynamic objects in the second video after the transition. However, video inpainting is unreliable and computationally expensive, and so, as our dissolve strategy is supported by evidence from perceptual studies, we believe it is preferable currently. The quality of our geometric reconstructions is limited by the available views of the scene within the video set. Increasing the size of the video collection would increase the number of views of a scene, and so help to improve the quality of 3D reconstruction. Of course, increasing the size of the data set has its own drawbacks. There is scope to speed up our processing, and recent work [Frahm et al.  2010b] has demonstrated speed improvements with images. Coupled with more aggressive filtering, this approach should enable a much larger video set to be processed in a similar amount of time. In these case, we can still use full 3D ? static transitions, as our interpolation provides convincing camera motion style blending despite inaccurate camera tracks. This is justified, as our participants preferred the static 3D transitions over other transition types in this scenario. Importantly, 3D geometry is still recovered in these difficult cases because the support set of the portal provides sufficient context. For the overview mode, our system optionally uses GPS and orientation information to embed the Videoscape into a map. For the core of our method, we intentionally only use the video frames for maximum generality. We incorporate GPS and orientation information into the filtering phase, but we have not yet extended this into the matching phase. However, as the data is often unreliable in cities, and as we allow large view changes (e.g., zooms), integrating this data is not trivial. Expressing spatial information for a portal choice in our interactive navigation mode is challenging. The mini-map shows frusta and paths when hovering over portal thumbnails to show to where the video choice will move, but there is a desire to express this within the view of the camera. We believe that this is a difficult problem. Portal images by definition look similar, so placing them into the current view (equivalent to [Snavely et al. 2006]) tells the user very little about what will happen in each candidate video path beyond the portal. Equally, what if there are 10+ videos connected, each moving away in different directions? Our solution is to give the user fast access to every frame in every connected video through scrubbing. Naturally, the question arises as to whether our system could be used on community video databases. In preliminary experiments, we could not find sufficient appropriate videos of a location for our system in current community databases as the signal-to-noise ratio was too low this is in stark contrast with community photo databases. Perhaps: 1) currently, online databases do not contain sufficient suitable videos for our system, which we believe unlikely, but which would be corrected over time as more videos are added; or 2) online databases do contain such videos, but they are very difficult to find as current searches are based on key-word or wholevideo label associations and not on visual content or geographical features. Our work goes some way to easing this browsing/search difficulty. We have presented a system to extract structure from casually captured unstructured video collections. We build a Videoscape graph that connects videos through portals which show similar places or events. We introduce an integrated set of interfaces to interact with and explore a Videoscape. These provide fast non-linear access to whole video collections and make it easy to observe the liveliness and dynamics of an environment or event, and to convey a sense of space and time. When navigating through videos, transitions at portals are rendered in a spatially immersive way. We have studied user preference for different transition types, and we use these findings  to inform an automatic transition selection system. We have evaluated our prototype system on a database of videos which features a variety of locations, times, and viewing conditions. Through three further user studies, we have demonstrated that our system provides benefits over existing systems in terms of spatial awareness, video summarization and video browsing. We thank Min H. Kim for his help with the psychophysical analysis; Christian Kurz and Thorsten Thorm?hlen for tracking help; Gunnar Thalin for our custom Deshaker build; Malcolm Reynolds, Maciej Gryka, Chenglei Wu, and Ahmed Elhayek for capture help; Gabriel Brostow and many others at UCL and MPI who discussed and provided feedback; all our participants; Flickr users cogdog, garryknight, goincase, and ramoncutanda for their images in Figure 1 ; aerial imagery c 2012 Microsoft, NASA, DigitalGlobe, NAVTEQ, Harris Corp, Earthstar Geographics, Google, and BlueSky. We also thank the EngD VEIV Centre at UCL, the BBC, and EPSRC grant EP/I031170/1 for funding support. A GARWAL , S., S NAVELY , N., S IMON , I., S EITZ , S ZELISKI , R. 2009. Building Rome in a day. A\n        , D., AND C ARL - B AATZ , G., K ?SER , K., C HEN , D., G RZESZCZUK , R., AND P OLLEFEYS , M. 2010. Handling urban location recognition as a 2D homothetic problem. ECCV, 266?279. B ALLAN , L., B ROSTOW , G., P UWEIN , J., AND P OLLEFEYS , M. 2010. Unstructured video-based rendering: Interactive exploration of casually captured videos. ACM Trans. SIGGRAPH) 29, 3, 87:1?87:11. B ELL , D., K UEHNEL , F., M AXWELL , C., K IM , R., K ASRAIE , K., G ASKINS , T., H OGAN , P., AND C OUGHLAN , J. 2007. NASA World Wind: Opensource GIS for mission operations. IEEE Aerospace Conference, 1?9. C SURKA , G., B RAY , C., D ANCE , C., AND F categorization with bags of keypoints. D ATTA , R., J OSHI , D., L I , J., AND W ANG , J. Z. 2008. Image retrieval: Ideas, influences, and trends of the new age. ACM Comput. D EBEVEC , P. E., T AYLOR , C. J., AND M ALIK , J. 1996. Modeling and rendering architecture from photographs: a hybrid geometryand image-based approach. SIGGRAPH, 11? 20. D MYTRYK , E. 1984. Focal Press. , M., B , P., , C., S , F ARNEB?CK , G. 2003. Two-frame motion estimation based on polynomial expansion. SCIA, 363?370. F RAHM , J.-M., P OLLEFEYS , M., L AZEBNIK , S., G ALLUP , D., C LIPP , B., R AGURAMA , R., W U , C., Z ACH , C., AND J OHN SON , T. 2010. Fast robust large-scale mapping from video and internet photo collections. ISPRS Journal of Photogrammetry and Remote Sensing 65, 538?549. F URUKAWA , Y., AND P ONCE , J. 2010. Accurate, dense, and robust multi-view stereopsis. IEEE TPAMI 32, 1362?1376. , N., C URLESS , B., H OPPE , H., AND Multi-view stereo for community photo 1?8. , S., H AUBOLD , , T. 2010. Ambient point\n        H ARTLEY , R. I., Z H EATH , K., G ELFAND , N., O VSJANIKOV , M., A ANJANEYA , M., AND G UIBAS , L. J. 2010. Image webs: computing and exploiting connectivity in image collections. IEEE CVPR, 3432?3439. H , H. 2006. K ENNEDY , L., AND N AAMAN , M. 2008. Generating diverse and representative image search results for landmarks. WWW, 297?306. K ENNEDY , L., AND N AAMAN , M. 2009. Less talk, more rock: automated organization of community-contributed collections of concert videos. WWW, 311?320. L AZEBNIK , S., S CHMID , C., AND P ONCE . , J. 2006. Beyond bags of features: spatial pyramid matching for recognizing natural scene categories. IEEE CVPR, 2169?2178. L EUNG , T., AND M ALIK , J. 2001. Representing and recognizing the visual appearance of materials using three-dimensional textons. IJCV 43, 29?44. L I , X., W U , C., Z ACH , C., L AZEBNIK , S., AND F RAHM , J.-M. 2008. Modeling and recognition of landmark image collections using iconic scene graphs. ECCV, 427?440. L IPPMAN , A. 1980. Movie-maps: An application of the optical videodisc to computer graphics. Computer Graphics (Proc. SIGGRAPH) 14, 3, 32?42. L IPSKI , C., L INZ , C., N EUMANN , T., AND M AGNOR , M. 2010. High Resolution Image Correspondences for Video PostProduction. Visual Media Prod. M C C URDY , N. J., AND G RISWOLD , W. G. 2005. A systems architecture for ubiquitous video. International Conference on Mobile Systems, Applications, and Services, 1?14. M ORVAN , Y., AND O?S ULLIVAN , C. 2009. Handling occluders in transitions from panoramic images: A perceptual study. ACM Trans. Applied Perception 6, 4, 1?15. P HILBIN , J., S IVIC , J., AND Z ISSERMAN , A. 2011. Geometric latent Dirichlet allocation on a matching graph for large-scale image datasets. IJCV 95, 2, 138?153. P ONGNUMKUL , S., W ANG , J., AND C OHEN , M. 2008. Creating map-based storyboards for browsing tour videos. ACM Symposium on User Interface Software and Technology, 13?22. S AURER , O., F RAUNDORFER , F., AND P OLLEFEYS , M. 2010. OmniTour: Semi-automatic generation of interactive virtual tours from omnidirectional video. 3DPVT, 1?8. , J. 2006. S IVIC , J., AND Z ISSERMAN , A. 2003. Video Google: A text retrieval approach to object matching in videos. ICCV, 1470?1477. S , R. 2008. T HORM?HLEN , T. 2006. Zuverl?ssige Sch?tzung der Kamerabewegung aus einer Bildfolge. PhD thesis, University of Hannover. ?Voodoo Camera Tracker? can be downloaded from http://www.digilab.uni-hannover.de . Wiley,\n        T ORGERSON , W. S. 1958. V ANGORP , P., C HAURASIA , G., L AFFONT , P.-Y., F LEMING , R., AND D RETTAKIS , G. 2011. Perception of visual artifacts in image-based rendering of fa?ades. Computer Graphics Forum 4 (07), 1241?1250. V EAS , E., M ULLONI , A., K RUIJFF , E., R EGENBRECHT , H., AND S CHMALSTIEG , D. 2010. Techniques for view transition in multi-camera outdoor environments. Graphics Interface, 193?200. W EYAND , T., AND L EIBE , B. 2011. Discovering favorite views of popular places with iconoid shift. Z AMIR , A. R., AND S HAH , M. 2010. Accurate image localization based on Google maps street view. ECCV, 255?268.",
  "resources" : [ ]
}