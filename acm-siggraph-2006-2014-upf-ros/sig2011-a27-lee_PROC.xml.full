{
  "uri" : "sig2011-a27-lee_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a27-lee_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "ShadowDraw: Real-Time User Guidance for Freehand Drawing",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Yong Jae-Lee",
      "name" : "Yong Jae",
      "surname" : "Lee"
    }, {
      "uri" : "http://drinventor/C. Lawrence-Zitnick",
      "name" : "C. Lawrence",
      "surname" : "Zitnick"
    }, {
      "uri" : "http://drinventor/Michael F.-Cohen",
      "name" : "Michael F.",
      "surname" : "Cohen"
    } ]
  },
  "bagOfWords" : [ "32f93ca8b4de51da0d34232d1e30c65b97b38b2b50462f378962683df596244e", "mit", "10.1145", "1964921.1964922", "name", "identification", "possible", "ShadowDraw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "Yong", "Jae", "Lee", "C.", "Lawrence", "Zitnick", "University", "Texas", "Austin", "Microsoft", "Research", "Figure", "result", "user", "study", "-lrb-", "top", "-rrb-", "freehand", "drawing", "object", "without", "use", "ShadowDraw", "-lrb-", "bottom", "-rrb-", "freehand", "drawing", "object", "use", "ShadowDraw", "Notice", "improve", "spacing", "proportion", "while", "maintain", "subject", "own", "unique", "style", "we", "present", "ShadowDraw", "system", "guide", "freeform", "drawing", "object", "user", "draw", "ShadowDraw", "dynamically", "update", "shadow", "image", "underlie", "user?s", "stroke", "shadow", "suggestive", "object", "contour", "guide", "user", "continue", "draw", "paradigm", "similar", "trace", "two", "major", "difference", "first", "we", "do", "provide", "single", "image", "from", "which", "user", "can", "trace", "rather", "ShadowDraw", "automatically", "blend", "relevant", "image", "from", "large", "database", "construct", "shadow", "second", "system", "dynamically", "adapt", "user?s", "drawing", "real-time", "produce", "suggestion", "accordingly", "ShadowDraw", "work", "efficiently", "match", "local", "edge", "patch", "between", "query", "construct", "from", "current", "drawing", "database", "image", "hash", "technique", "enforce", "both", "local", "global", "similarity", "provide", "sufficient", "speed", "interactive", "feedback", "shadow", "create", "aggregate", "edge", "map", "from", "best", "database", "match", "spatially", "weight", "match", "score", "we", "test", "we", "approach", "human", "subject", "show", "comparison", "between", "drawing", "be", "produce", "without", "system", "result", "show", "we", "system", "produce", "more", "realistically", "proportion", "line", "drawing", "cr", "category", "i.", "3.8", "-lsb-", "Computing", "Methodologies", "-rsb-", "computer", "graphics?applications", "keyword", "large", "scale", "image", "retrieval", "shape", "matching", "interactive", "drawing", "Links", "ACM", "Reference", "Format", "Lee", "Y.", "Zitnick", "C.", "Cohen", "M.", "2011", "Shadow", "draw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "ACM", "Trans", "graph", "30", "Article", "27", "-lrb-", "July", "2011", "-rrb-", "page", "dous", "10.1145", "1964921.1964922", "http://doi.acm.org/10.1145/1964921.1964922", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2011", "ACM", "0730-0301/2011", "07-art27", "10.00", "DOI", "10.1145", "1964921.1964922", "http://doi.acm.org/10.1145/1964921.1964922", "Michael", "F.", "Cohen", "Microsoft", "Research", "introduction", "ask", "draw", "face", "result", "most", "we", "-lrb-", "those", "little", "practice", "drawing", "-rrb-", "might", "look", "like", "one", "those", "upper", "row", "Figure", "create", "subject", "we", "user", "study", "use", "standard", "drawing", "interface", "similarly", "ask", "draw", "bicycle", "most", "we", "would", "have", "difficult", "time", "depict", "how", "frame", "wheel", "relate", "each", "other", "one", "solution", "search", "image", "thing", "we", "want", "draw", "either", "trace", "use", "some", "other", "way", "reference", "however", "aside", "from", "difficulty", "find", "photo", "what", "we", "want", "draw", "simply", "trace", "object", "edge", "eliminate", "much", "essence", "drawing", "i.e.", "very", "little", "freedom", "trace", "stroke", "conversely", "draw", "blank", "paper", "only", "image", "mind?s", "eye", "give", "drawer", "lot", "freedom", "freehand", "drawing", "can", "frustrate", "without", "significant", "training", "address", "we", "present", "ShadowDraw", "drawing", "interface", "automatically", "infer", "what", "you", "draw", "dynamically", "depict", "relevant", "shadow", "-lrb-", "figure", "-rrb-", "underneath", "drawing", "shadow", "may", "either", "use", "ignore", "drawer", "ShadowDraw", "preserve", "essence", "drawing", "i.e.", "freedom", "expressiveness", "same", "time", "use", "visual", "reference", "shadow", "guide", "drawer", "furthermore", "shadow", "from", "real", "image", "can", "enlighten", "artist", "gist", "many", "image", "simultaneously", "creation", "become", "mix", "both", "human", "intuition", "computer", "intelligence", "computer", "essence", "partner", "draw", "process", "provide", "guidance", "like", "teacher", "instead", "actually", "produce", "final", "artwork", "drawing", "bottom", "row", "Figure", "be", "draw", "same", "subject", "time", "use", "ShadowDraw", "Notice", "how", "user", "own", "creative", "style", "remain", "consistent", "between", "drawing", "while", "overall", "shape", "spacing", "more", "realistic", "ShadowDraw", "consist", "two", "main", "computational", "step", "plus", "user", "interface", "first", "offline", "step", "consist", "build", "database", "from", "collection", "30,000", "image", "collect", "from", "web", "each", "image", "convert", "edge", "drawing", "use", "long", "edge", "detector", "technique", "develop", "-lsb-", "Bhat", "et", "al.", "2009", "-rsb-", "store", "overlapping", "window", "each", "edge", "image", "analyze", "code", "store", "each", "window", "convert", "edge", "descriptor", "further", "code", "sketch", "distinct", "hash", "key", "use", "min-hash", "-lsb-", "Chum", "et", "al.", "2008", "-rsb-", "second", "online", "step", "user", "draw", "ShadowDraw", "analyze", "stroke", "use", "similar", "encode", "determine", "hash", "key", "overlap", "window", "fast", "matching", "database", "image", "top", "100", "matching", "database", "edge", "image", "further", "align", "drawing", "set", "spatially", "vary", "weight", "blend", "edge", "image", "shadow", "image", "user", "interface", "stroke", "overlay", "top", "evolve", "shadow", "image", "provide", "guidance", "future", "stroke", "we", "main", "contribution", "interactive", "drawing", "system", "dynamically", "adapt", "user?s", "draw", "provide", "real-time", "feedback", "number", "technical", "contribution", "make", "ShadowDraw", "unique", "although", "portion", "ShadowDraw", "follow", "basic", "framework", "content", "base", "image", "retrieval", "technique", "partial", "spatial", "matching", "ShadowDraw", "employ", "novel", "allow", "multiple", "match", "image", "base", "different", "sub-region", "image", "addition", "verification", "stage", "method", "determine", "blend", "weight", "unique", "work", "while", "have", "be", "previous", "work", "help", "user", "draw", "basic", "shape", "-lsb-", "Igarashi", "et", "al.", "1999", "Arvo", "Novins", "2000", "Igarashi", "Hughes", "2001", "-rsb-", "we", "knowledge", "we", "first", "develop", "interactive", "user", "interface", "assist", "freeform", "drawing", "we", "test", "we", "approach", "human", "subject", "show", "comparison", "between", "drawing", "be", "produce", "without", "system", "result", "show", "we", "system", "produce", "more", "realistically", "proportion", "line", "drawing", "particularly", "those", "who", "possess", "some", "skill", "lack", "expertise", "we", "purposely", "avoid", "paper", "make", "any", "claim", "system", "help", "produce", "more", "skilled", "drawer", "more", "artistic", "drawing", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "27:2", "Y.", "Lee", "et", "al.", "related", "work", "huge", "volume", "image", "video", "datum", "available", "web", "scientific", "database", "newspaper", "archive", "along", "recent", "advance", "efficient", "-lrb-", "approximate", "-rrb-", "nearest", "neighbor", "match", "scheme", "have", "open", "door", "number", "large", "scale", "match", "application", "general", "field", "content", "base", "image", "retrieval", "-lrb-", "cbir", "-rrb-", "use", "many", "different", "input", "modality", "search", "similar", "image", "database", "-lrb-", "see", "-lsb-", "Datta", "et", "al.", "2008", "-rsb-", "general", "survey", "field", "-rrb-", "here", "we", "briefly", "review", "related", "work", "large", "scale", "image", "retrieval", "technique", "especially", "those", "use", "line", "drawing", "query", "and/or", "aim", "construct", "new", "image", "drawing", "number", "system", "produce", "photographic-like", "result", "composit", "portion", "retrieve", "image", "sketch-tocollage", "system", "-lsb-", "Gavilan", "et", "al.", "2007", "-rsb-", "produce", "single", "composite", "collage", "match", "user", "provide", "color", "stroke", "database", "image", "segment", "out", "region", "interactively", "blend", "retrieve", "segment", "sketch2photo", "system", "-lsb-", "Chen", "et", "al.", "2009", "-rsb-", "produce", "composite", "image", "from", "user?s", "sketch", "scene", "text", "label", "annotated", "object", "candidate", "image", "region", "each", "object", "find", "web", "those", "produce", "best", "agreement", "put", "together", "form", "final", "composition", "PhotoSketch", "-lsb-", "Eitz", "et", "al.", "2009", "-rsb-", "progressively", "create", "image", "through", "sketch", "composit", "interface", "user", "interact", "system", "segment", "blend", "image", "retrieve", "from", "database", "1.5", "million", "image", "rather", "than", "start", "from", "blank", "page", "Scene", "Completion", "algorithm", "-lsb-", "Hays", "Efros", "2007", "-rsb-", "perform", "global", "scene", "match", "use", "query", "image", "which", "have", "hole", "system", "find", "best", "image", "complete", "scene", "object", "could", "have", "be", "miss", "region", "improve", "retrieval", "accuracy", "sketch-based", "system", "researcher", "have", "also", "design", "descriptor", "provide", "better", "match", "between", "human", "draw", "sketch", "natural", "image", "-lsb-", "Chalechale", "et", "al.", "2005", "Hu", "et", "al.", "2010", "-rsb-", "more", "general", "cbir", "effort", "include", "early", "SIGGRAPH", "work", "fast", "Multiresolution", "image", "query", "-lsb-", "Jacobs", "et", "al.", "1995", "-rsb-", "match", "user", "paint", "stroke", "underlie", "wavelet", "signature", "image", "database", "Blobworld", "approach", "-lsb-", "Carson", "et", "al.", "2002", "-rsb-", "query", "image", "region", "rather", "than", "entire", "image", "allow", "user", "specify", "which", "object", "image", "more", "relevant", "query", "-lsb-", "nister", "Stewenius", "2006", "-rsb-", "vocabulary", "tree", "create", "efficiently", "retrieve", "image", "from", "large", "database", "tree", "define", "hierarchical", "quantization", "local", "image", "feature", "provide", "multi-level", "scheme", "score", "match", "image", "-lsb-", "Chum", "et", "al.", "2007", "-rsb-", "idea", "query", "expansion", "text", "retrieval", "apply", "image", "domain", "where", "highest", "rank", "image", "from", "original", "query", "re-queried", "generate", "additional", "relevant", "image", "3d", "photorealistic", "virtual", "space", "create", "-lsb-", "Sivic", "et", "al.", "2008", "-rsb-", "allow", "user", "tour", "theme", "city", "street", "skyline", "similar", "image", "match", "stitch", "from", "large", "-lrb-", "few", "hundred", "thousand", "-rrb-", "image", "collection", "download", "from", "Flickr", "most", "recently", "mindfinder", "-lsb-", "Cao", "et", "al.", "2010", "-rsb-", "aim", "improve", "image", "retrieval", "allow", "user", "input", "text", "tag", "sketch", "color", "query", "system", "able", "retrieve", "image", "better", "match", "image", "user?s", "mind", "finally", "-lsb-", "Chaudhuri", "Koltun", "2010", "-rsb-", "data-driven", "suggestion", "make", "3d", "modeling", "system", "present", "suggestion", "match", "retrieve", "relevant", "shape", "database", "initial", "basic", "model", "ShadowDraw", "also", "leverage", "idea", "match", "large", "database", "image", "unlike", "previous", "method", "we", "end", "goal", "help", "user", "draw", "rather", "than", "perform", "image", "composition", "completion", "retrieval", "3d", "modeling", "furthermore", "ShadowDraw", "use", "only", "partial", "evolve", "draw", "query", "rather", "than", "other", "image", "and/or", "textual", "description", "we", "system", "require", "retrieval", "run", "real", "time", "we", "have", "see", "other", "system", "leverage", "image", "retrieval", "same", "kind", "application", "partial", "drawing", "input", "interactive", "draw", "interface", "Teddy", "-lsb-", "Igarashi", "et", "al.", "1999", "-rsb-", "fluid", "sketch", "-lsb-", "Arvo", "Novins", "2000", "-rsb-", "3d", "draw", "system", "-lsb-", "Igarashi", "Hughes", "2001", "-rsb-", "strive", "produce", "better", "drawing", "user", "method", "provide", "low-level", "information", "feedback", "form", "basic", "polygonal", "shape", "line", "curve", "more", "recently", "iCanDraw", "interface", "-lsb-", "Dixon", "et", "al.", "2010", "-rsb-", "provide", "step-by-step", "instruction", "corrective", "feedback", "guide", "user", "draw", "human", "face", "from", "reference", "image", "while", "similar", "motivation", "we", "approach", "provide", "guidance", "draw", "arbitrary", "high-level", "object", "use", "only", "example", "image", "recently", "-lsb-", "Cole", "et", "al.", "2008", "-rsb-", "author", "study", "artist", "line", "drawing", "3d", "shape", "analyze", "which", "line", "segment", "be", "be", "emphasize", "compute", "correlation", "between", "those", "segment", "contour", "produce", "use", "exist", "computer", "generate", "line", "drawing", "technique", "contour", "feature", "extractor", "while", "we", "work", "aim", "help", "user", "freeform", "drawing", "output", "we", "system", "can", "useful", "resource", "line", "work", "i.e.", "user", "drawing", "produce", "shadowdraw", "can", "use", "analyze", "contour", "more", "general", "object", "occur", "natural", "image", "approach", "shadowdraw", "include", "three", "main", "component", "-lrb-", "-rrb-", "construction", "inverted", "file", "structure", "-lsb-", "Witten", "et", "al.", "1999", "-rsb-", "index", "database", "image", "edge", "map", "-lrb-", "-rrb-", "query", "method", "give", "user", "stroke", "dynamically", "retrieve", "match", "image", "align", "they", "evolve", "drawing", "weight", "they", "base", "matching", "score", "-lrb-", "-rrb-", "user", "interface", "which", "display", "shadow", "weighted", "edge", "map", "beneath", "user?s", "draw", "help", "guide", "draw", "process", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "ShadowDraw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "27:3", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "example", "database", "image", "corresponding", "edge", "image", "3.1", "database", "creation", "image", "database", "should", "select", "so", "object", "depict", "well", "appearance", "pose", "likely", "similar", "those", "draw", "user", "ideally", "large", "database", "hand", "draw", "image", "would", "use", "database", "exceedingly", "hard", "construct", "instead", "we", "use", "set", "approximately", "30,000", "natural", "image", "collect", "from", "internet", "via", "approximately", "40", "categorical", "query", "t-shirt", "bicycle", "car", "etc.", "although", "image", "have", "many", "extraneous", "background", "object", "frame", "line", "etc.", "expectation", "average", "contain", "edge", "user", "may", "want", "draw", "we", "scale", "image", "fit", "300x300", "pixel", "resolution", "i.e.", "long", "side", "scale", "300", "pixel", "we", "process", "each", "image", "three", "stage", "add", "they", "inverted", "file", "structure", "first", "edge", "extract", "from", "image", "next", "local", "edge", "descriptor", "compute", "finally", "set", "concatenate", "hash", "call", "sketch", "compute", "from", "edge", "descriptor", "add", "database", "we", "store", "database", "inverted", "file", "other", "word", "index", "sketch", "value", "which", "turn", "point", "original", "image", "its", "edge", "Edge", "extraction", "give", "natural", "image", "we", "want", "find", "edge", "most", "likely", "draw", "user", "while", "ignore", "other", "perceptual", "study", "show", "long", "coherent", "edge", "salient", "human", "even", "faint", "-lsb-", "beaudot", "mullen", "2003", "Elder", "Goldberg", "2001", "-rsb-", "inspire", "work", "we", "use", "long", "edge", "detector", "describe", "-lsb-", "Bhat", "et", "al.", "2009", "-rsb-", "method", "locally", "normalize", "magnitude", "edge", "sum", "normalize", "magnitude", "weight", "local", "curvature", "along", "length", "edge", "result", "edge", "response", "related", "length", "edge", "its", "degree", "curvature", "rather", "than", "magnitude", "intensity", "gradient", "we", "store", "edge", "image", "use", "run", "length", "encoding", "average", "each", "compress", "edge", "image", "require", "5.3", "kb", "example", "show", "figure", "-lrb-", "d-f", "-rrb-", "Patch", "descriptor", "each", "edge", "image", "we", "determine", "edge", "position", "find", "maximum", "response", "perpendicular", "edge", "direction", "similar", "canny", "edge", "detection", "-lsb-", "canny", "1986", "-rsb-", "give", "image", "database", "corresponding", "edge", "orientation", "we", "compute", "set", "edge", "descriptor", "since", "goal", "match", "edge", "image", "incomplete", "evolve", "drawing", "we", "compute", "descriptor", "locally", "over", "60x60", "patch", "patch", "use", "compute", "neighbor", "descriptor", "overlap", "50", "result", "81", "descriptor", "over", "fix", "grid", "patch", "we", "encode", "each", "patch", "use", "BiCE", "descriptor", "-lsb-", "zitnick", "2010", "-rsb-", "which", "design", "encode", "histogram", "edge", "position", "orientation", "since", "edge", "draw", "user", "typically", "less", "dense", "than", "natural", "image", "we", "use", "low", "dimensional", "version", "BiCE", "we", "define", "three", "dimensional", "histogram", "discrete", "edge", "orientation", "18", "position", "perpendicular", "edge", "position", "tangent", "edge", "use", "notation", "-lsb-", "Zitnick", "2010", "-rsb-", "we", "set", "18", "bucket", "histogram", "binarize", "set", "top", "20", "one", "rest", "zero", "final", "descriptor", "have", "432", "binary", "bit", "other", "image", "patch", "descriptor", "sift", "-lsb-", "Lowe", "2004", "-rsb-", "Daisy", "-lsb-", "Winder", "et", "al.", "2009", "-rsb-", "rely", "relative", "strength", "edge", "magnitude", "provide", "discriminability", "show", "have", "reduce", "match", "performance", "compare", "bice", "-lsb-", "Zitnick", "2010", "-rsb-", "also", "applicable", "we", "scenario", "since", "relative", "edge", "magnitude", "which", "descriptor", "rely", "know", "user?s", "draw", "min-hash", "key", "feature", "BiCE", "descriptor", "its", "binary", "encoding", "can", "view", "set", "representation", "where", "one", "indicate", "edge", "presence", "make", "amenable", "min-hash", "which", "effective", "hash", "technique", "retrieval", "clustering", "-lsb-", "chum", "et", "al.", "2008", "Lee", "et", "al.", "2010", "Zitnick", "2010", "-rsb-", "min-hash", "have", "property", "probability", "two", "set", "have", "same", "hash", "value", "-lrb-", "i.e.", "collide", "-rrb-", "equal", "Jaccard", "similarity", "Jaccard", "similarity", "sim", "-lrb-", "-rrb-", "between", "two", "set", "cardinality", "intersection", "divide", "cardinality", "union", "-lrb-", "-rrb-", "sim", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "min-hash", "function", "randomly", "permute", "set", "index", "-lrb-", "i.e.", "one", "zero", "-rrb-", "all", "set", "-lrb-", "bice", "descriptor", "-rrb-", "permute", "use", "same", "min-hash", "function", "min-hash", "value", "give", "permuted", "set", "its", "smallest", "index", "contain", "value", "one", "after", "permutation", "single", "min-hash", "can", "non-discriminative", "introduce", "many", "false", "positive", "retrieval", "especially", "descriptor", "nonsparse", "increase", "precision", "we", "compute", "independent", "random", "min-hash", "function", "apply", "they", "all", "bice", "descriptor", "we", "concatenate", "result", "min-hash", "value", "each", "descriptor", "k-tuple", "call", "sketch", "probability", "two", "sketch", "collide", "thus", "reduce", "exponentially", "sim", "-lrb-", "-rrb-", "increase", "recall", "process", "repeat", "time", "use", "different", "set", "minhash", "function", "result", "sketch", "per", "descriptor", "maintain", "high", "recall", "while", "reduce", "false", "positive", "tradeoff", "must", "make", "between", "number", "sketch", "store", "each", "descriptor", "size", "sketch", "we", "experiment", "we", "store", "20", "sketch", "size", "each", "descriptor", "we", "store", "inverted", "file", "structure", "each", "min-hash", "sketch", "we", "allocate", "each", "unique", "sketch", "new", "entry", "structure", "we", "record", "image", "index", "patch", "location", "descriptor", "instance", "produce", "sketch", "3.2", "image", "match", "we", "hash", "scheme", "allow", "efficient", "image", "query", "since", "only", "image", "match", "sketch", "need", "consider", "section", "we", "describe", "real-time", "matching", "pipeline", "between", "edge", "image", "database", "user?s", "draw", "show", "Figure", "initially", "we", "use", "inverted", "file", "structure", "obtain", "set", "candidate", "match", "next", "each", "candidate", "match", "align", "user?s", "draw", "score", "two", "step", "match", "procedure", "necessary", "computational", "efficiency", "since", "only", "small", "subset", "database", "image", "need", "finely", "align", "weight", "we", "use", "score", "from", "alignment", "step", "compute", "set", "spatially", "vary", "weight", "each", "edge", "image", "output", "shadow", "image", "result", "from", "weighted", "average", "edge", "image", "finally", "we", "display", "shadow", "image", "user", "describe", "section", "candidate", "match", "we", "represent", "user?s", "draw", "set", "vectorize", "multi-segment", "stroke", "we", "create", "edge", "image", "from", "stroke", "draw", "line", "width", "one", "pixel", "between", "stroke", "point", "render", "line", "have", "same", "style", "edge", "extract", "from", "natural", "image", "database", "i.e.", "edge", "image", "use", "match", "do", "use", "stylized", "stroke", "see", "user", "describe", "section", "next", "we", "compute", "BiCE", "descriptor", "corresponding", "sketch", "same", "manner", "describe", "section", "3.1", "time", "use", "higher", "resolution", "grid", "18", "18", "324", "patch", "75", "overlap", "between", "neighbor", "patch", "we", "use", "higher", "resolution", "grid", "increase", "accuracy", "predict", "position", "increase", "invariance", "translation", "drawing", "we", "implementation", "user?s", "draw", "occupy", "area", "480", "480", "pixel", "result", "96", "96", "pixel", "patch", "24", "pixel", "offset", "between", "neighbor", "patch", "we", "compute", "descriptor", "sketch", "each", "324", "patch", "use", "inverse", "lookup", "table", "we", "match", "each", "sketch", "from", "user?s", "draw", "sketch", "store", "database", "match", "sketch", "cast", "one", "vote", "corresponding", "database", "image", "patch", "offset", "pair", "we", "aggregate", "match", "histogram", "store", "number", "match", "sketch", "each", "image", "each", "grid", "offset", "reduce", "size", "vote", "only", "store", "database", "patch", "offset", "within", "four", "patch", "grid", "point", "patch", "be", "consider", "user?s", "draw", "correspond", "relative", "shift", "less", "than", "96", "pixel", "between", "user?s", "draw", "database", "image", "result", "histogram", "have", "size", "where", "number", "image", "database", "after", "add", "all", "match", "each", "sketch", "histogram", "we", "find", "best", "matching", "offset", "each", "image", "add", "top", "100", "image", "candidate", "set", "discuss", "section", "3.1", "we", "compute", "20", "sketch", "each", "descriptor", "result", "maximum", "possible", "20", "vote", "per", "sketch", "histogram", "reduce", "bias", "from", "any", "single", "descriptor", "we", "limit", "contribution", "each", "descriptor", "four", "vote", "histogram", "give", "large", "database", "compute", "candidate", "set", "describe", "above", "can", "computationally", "expensive", "we", "take", "advantage", "fact", "user?s", "stroke", "change", "gradually", "over", "time", "increase", "performance", "each", "time", "step", "only", "vote", "result", "from", "sketch", "derive", "from", "patch", "have", "change", "update", "we", "accomplish", "subtract", "vote", "add", "from", "previous", "sketch", "from", "follow", "add", "vote", "from", "new", "sketch", "each", "time", "frame", "we", "also", "include", "candidate", "set", "any", "database", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "27:4", "Y.", "Lee", "et", "al.", "Figure", "outline", "online", "processing", "pipeline", "give", "user?s", "stroke", "sketch", "compute", "each", "sub-window", "match", "vote", "accumulate", "histogram", "top", "100", "match", "image", "align", "score", "weighted", "generate", "final", "shadow", "image", "Figure", "illustration", "spatial", "weight", "-lrb-", "-rrb-", "user?s", "view", "-lrb-", "-rrb-", "shadow", "image", "-lrb-", "-rrb-", "top", "two", "match", "corresponding", "spatial", "weight", "-lrb-", "top", "right", "-rrb-", "image", "contribute", "shadow", "image", "previous", "time", "frame", "image", "alignment", "candidate", "image", "set", "contain", "set", "image", "approximate", "offset", "define", "best", "matching", "offset", "describe", "above", "approximation", "arise", "from", "discretization", "offset", "grid", "patch", "we", "refine", "offset", "use", "1d", "variation", "Generalized", "Hough", "transform", "-lsb-", "Ballard", "1981", "-rsb-", "use", "standard", "Generalized", "Hough", "transform", "2d", "translation", "we", "create", "2d", "histogram", "over", "possible", "offset", "use", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "value", "pixel", "location", "-lrb-", "-rrb-", "similarly", "edge", "image", "we", "determine", "best", "offset", "find", "maximum", "value", "-lrb-", "-rrb-", "approach", "computationally", "expensive", "since", "we", "need", "sum", "over", "image", "every", "possible", "combination", "offset", "instead", "we", "compute", "dimension", "separately", "use", "two", "histogram", "-lrb-", "-rrb-", "sin", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "sin", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "similarly", "use", "cosine", "angle", "sine", "edge", "angle", "provide", "higher", "weight", "more", "informative", "vertical", "edge", "when", "determine", "horizontal", "offset", "similarly", "cosine", "horizontal", "edge", "we", "empirically", "find", "approach", "produce", "good", "result", "once", "histogram", "create", "slightly", "blur", "we", "determine", "final", "sub-pixel", "offset", "add", "quadratic", "interpolation", "result", "peak", "response", "additional", "accuracy", "two", "iteration", "run", "reduce", "computation", "we", "limit", "search", "range", "twice", "distance", "between", "grid", "point", "addition", "we", "compute", "equation", "-lrb-", "-rrb-", "reduce", "resolution", "image", "size", "160", "160", "we", "notate", "align", "edge", "image", "refine", "scale", "additional", "1d", "histogram", "may", "similarly", "compute", "across", "scale", "peak", "find", "image", "weighting", "we", "now", "have", "set", "candidate", "image", "align", "edge", "image", "align", "use", "offset", "we", "goal", "blend", "align", "edge", "image", "shadow", "image", "help", "guide", "user", "draw", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "ShadowDraw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "27:5", "Figure", "render", "pipeline", "user", "interface", "-lrb-", "-rrb-", "user?s", "render", "stroke", "-lrb-", "-rrb-", "weighting", "base", "pen", "position", "-lrb-", "-rrb-", "shadow", "image", "-lrb-", "-rrb-", "final", "rendering", "where", "blend", "weight", "image", "which", "we", "define", "below", "blend", "weight", "should", "high", "pixel", "where", "good", "match", "between", "drawing", "candidate?s", "align", "edge", "low", "pixel", "where", "we", "construct", "weight", "image", "from", "two", "term", "global", "match", "term", "spatially", "vary", "match", "term", "which", "normalize", "over", "all", "image", "candus", "date", "set", "weight", "use", "reduce", "visibility", "noisy", "shadow", "produce", "when", "drawing", "have", "just", "start", "all", "match", "score", "low", "illustration", "spatial", "weighting", "show", "Figure", "use", "spatial", "weighting", "result", "shadow", "composite", "multiple", "distinct", "edge", "image", "create", "appearance", "object", "do", "exist", "single", "database", "image", "we", "begin", "define", "spatially", "vary", "match", "term", "follow", "global", "match", "term", "we", "goal", "candidate", "image?s", "weight", "increase", "when", "its", "edge", "agree", "position", "orientation", "user?s", "stroke", "compute", "we", "first", "decompose", "each", "candidate", "edge", "image", "eight", "orient", "image", "similarly", "draw", "image", "eight", "orient", "image", "...", "each", "image", "capture", "only", "stroke", "parallel", "one", "eight", "evenly", "space", "orientation", "i.e.", "depict", "horizontal", "edge", "depict", "vertical", "edge", "remain", "six", "each", "capture", "one", "other", "orientation", "22.5", "interval", "edge?s", "orientation", "fall", "between", "two", "orientation", "its", "contribution", "linearly", "divide", "between", "two", "oriented", "edge", "image", "provide", "some", "invariance", "position", "we", "blur", "orient", "edge", "image", "gaussian", "kernel", "-lrb-", "-rrb-", "standard", "deviation", "1.25", "where", "relative", "distance", "between", "grid", "point", "also", "desirable", "image", "contain", "multiple", "edge", "near", "stroke", "receive", "same", "score", "contribution", "those", "single", "edge", "accomplish", "limit", "magnitude", "response", "from", "blur", "edge", "maximum", "response", "may", "result", "from", "single", "edge", "isolation", "we", "compute", "positive", "negative", "edge", "correlation", "image", "determine", "where", "edge", "image", "agree", "disagree", "user?s", "stroke", "we", "define", "positive", "correlation", "use", "product", "edge", "image", "same", "orientation", "define", "negative", "correlation", "use", "product", "orthogonally", "orient", "edge", "image", "1,8", "-lrb-", "+4", "-rrb-", "1,8", "we", "spatially", "vary", "match", "term", "simply", "gaussian", "blur", "version", "-lrb-", "-rrb-", "we", "add", "small", "offset", "ensure", "non-zero", "value", "reduce", "computation", "we", "compute", "image", "score", "weight", "reduce", "resolution", "image", "40", "40", "pixel", "next", "we", "define", "global", "match", "term", "equation", "-lrb-", "-rrb-", "use", "compute", "edge", "image?s", "blend", "weight", "aid", "computation", "we", "compute", "global", "match", "score", "each", "image", "use", "difference", "between", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "equation", "-lrb-", "-rrb-", "have", "high", "positive", "value", "when", "user?s", "stroke", "image?s", "edge", "agree", "position", "orientation", "however", "majority", "user?s", "stroke", "perpendicular", "image?s", "edge", "may", "negative", "finally", "we", "compute", "use", "nonlinear", "function", "average", "five", "highest", "score", "from", "candidate", "set", "10", "max", "we", "assign", "value", "0.5", "which", "mean", "value", "greater", "than", "zero", "only", "score", "greater", "than", "half", "average", "five", "highest", "score", "value", "favor", "image", "higher", "score", "set", "rate", "weight", "decay", "quadratic", "equation", "-lrb-", "-rrb-", "we", "set", "11", "11", "where", "correspond", "score", "would", "result", "from", "draw", "single", "stroke", "approximately", "250", "pixel", "use", "equation", "-lrb-", "11", "-rrb-", "increase", "user", "draw", "more", "stoke", "result", "greater", "visibility", "shadow", "summarize", "we", "compute", "shadow", "image", "real", "time", "compare", "user", "draw", "stroke", "candidate", "image", "extract", "from", "database", "we", "compute", "global", "spatially", "vary", "weight", "blend", "corresponding", "edge", "image", "create", "shadow", "image", "use", "draw", "interface", "we", "determine", "weight", "compare", "local", "orientation", "edge", "between", "develop", "drawing", "database", "edge", "image", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "27:6", "Y.", "Lee", "et", "al.", "Figure", "Video", "frame", "from", "four", "example", "draw", "session", "-lrb-", "top", "-rrb-", "final", "render", "visible", "user", "-lrb-", "bottom", "-rrb-", "shadow", "image", "last", "row", "demonstrate", "shadow?s", "robustness", "clutter", "user", "Interface", "ShadowDraw", "user", "interface", "appear", "first", "like", "standard", "drawing", "system", "blank", "paper-like", "surface", "we", "use", "WACOM", "Cintiq", "21ux", "screen/tablet", "user", "can", "draw", "erase", "stroke", "use", "stylus", "user", "see", "own", "drawing", "form", "pen", "stroke", "superimpose", "continuously", "update", "shadow", "see", "Figure", "draw", "area", "480", "480", "pixel", "line", "stroke", "render", "use", "dark", "blue", "marker", "style", "show", "Figure", "-lrb-", "-rrb-", "provide", "some", "color", "contrast", "we", "render", "shadow", "sepia", "tone", "further", "make", "shadow", "visible", "user", "while", "distract", "from", "user?s", "actual", "drawing", "we", "filter", "shadow", "image", "remove", "noisy", "faint", "edge", "12", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "where", "16", "multiply", "blur", "shadow", "image", "strengthen", "edge", "agree", "position", "weaken", "other", "use", "additionally", "suppress", "faint", "edge", "addition", "we", "apply", "small", "amount", "blur", "standard", "deviation", "1.25", "soften", "shadow", "see", "Figure", "-lrb-", "-rrb-", "finally", "we", "weight", "shadow", "have", "higher", "contrast", "near", "user?s", "cursor", "position", "13", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "gaussian", "weighted", "distance", "between", "standard", "deviation", "120", "pixel", "may", "set", "user", "control", "contrast", "shadow", "we", "render", "final", "image", "paper", "texture", "background", "show", "Figure", "-lrb-", "-rrb-", "result", "all", "we", "experiment", "run", "use", "database", "approximately", "30,000", "image", "we", "collect", "28,000", "image", "crawl", "image", "web", "search", "bear", "bike", "white", "background", "motorcycle", "honda", "we", "remove", "duplicate", "adjust", "resolution", "image", "intra-category", "object", "have", "similar", "scale", "we", "collect", "435", "face", "image", "from", "caltech", "101", "dataset", "-lsb-", "Fei-Fei", "et", "al.", "2004", "-rsb-", "since", "face", "image", "uniform", "scale", "user", "tend", "draw", "face", "different", "scale", "-lrb-", "full", "face", "vs.", "head", "shoulder", "-rrb-", "we", "add", "randomly", "scale", "image", "dataset", "total", "2,000", "face", "image", "we", "implement", "ShadowDraw", "modest", "system", "quadcore", "Intel", "i5", "CPU", "4gb", "RAM", "compress", "edge", "image", "from", "database", "store", "memory", "along", "inverse", "lookup", "table", "require", "850mb", "RAM", "we", "implement", "ShadowDraw", "use", "two", "thread", "provide", "smooth", "user", "interface", "foreground", "thread", "handle", "user", "input", "render", "user?s", "stroke", "background", "thread", "accept", "input", "user?s", "stroke", "compute", "shadow", "image", "average", "new", "shadow", "image", "compute", "every", "0.4", "0.9", "seconds", "depend", "number", "new", "stroke", "fast", "response", "critical", "create", "positive", "feedback", "loop", "which", "user", "obtain", "suggestion", "while", "still", "process", "draw", "stroke", "we", "choose", "parameter", "BiCE", "descriptor", "sketch", "size", "number", "sketch", "grid", "resolution", "through", "quantitative", "test", "several", "category", "face", "motorcycle", "butterfly", "we", "choose", "parameter", "associate", "fine", "alignment", "spatial", "weigh", "qualitatively", "examine", "several", "result", "Figure", "show", "several", "example", "draw", "session", "use", "ShadowDraw", "please", "refer", "accompany", "video", "see", "session", "action", "more", "result", "note", "drawing", "rate", "accelerate", "fit", "video", "user", "draw", "new", "stroke", "erase", "other", "shadow", "dynamically", "update", "best", "match", "user?s", "draw", "use", "we", "large", "database", "user", "can", "receive", "guidance", "variety", "object", "category", "include", "specific", "type", "object", "office", "chair", "folding", "chair", "rock", "chair", "last", "row", "Figure", "demonstrate", "score", "function?s", "robustness", "clutter", "even", "when", "many", "spurious", "stroke", "draw", "correct", "image", "give", "high", "weight", "shadow", "image", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "ShadowDraw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "27:7", "figure", "evaluation", "score", "drawing", "user", "poor", "average", "good", "group", "ShadowDraw", "achieve", "significantly", "better", "drawing", "result", "average", "group", "5.1", "user", "study", "we", "conduct", "user", "study", "assess", "effectiveness", "ShadowDraw", "untrained", "drawer", "first", "stage", "subject", "produce", "quick", "1-3", "minute", "drawing", "without", "ShadowDraw", "second", "stage", "separate", "set", "subject", "evaluate", "drawing", "draw", "we", "conduct", "experiment", "16", "subject", "eight", "woman", "eight", "man", "each", "subject", "ask", "draw", "five", "object", "Shoe", "Bicycle", "Butterfly", "Face", "rabbit", "without", "ShadowDraw", "total", "10", "drawing", "we", "randomly", "permute", "sequence", "object", "present", "each", "user", "shadow", "appear", "every", "other", "drawing", "thus", "half", "object", "be", "draw", "first", "without", "shadow", "other", "half", "draw", "first", "shadow", "we", "use", "rabbit", "control", "variable", "ensure", "fairness", "experiment", "be", "rabbit", "image", "we", "database", "please", "see", "supplementary", "video", "example", "drawing", "subject", "before", "begin", "study", "we", "explain", "UI", "functionality", "pen", "-lrb-", "drawing", "erase", "-rrb-", "white", "canvas", "next", "button", "proceed", "next", "object", "once", "current", "drawing", "complete", "we", "explain", "user", "shadow", "would", "appear", "every", "other", "drawing", "he/she", "free", "use", "they", "guidance", "completely", "ignore", "they", "we", "also", "explain", "shadow", "could", "remove", "tap", "region", "outside", "canvas", "-lrb-", "would", "reappear", "user", "start", "draw", "again", "-rrb-", "warm-up", "exercise", "acquaint", "they", "interface", "include", "draw", "circle", "t-shirt", "each", "user", "give", "30", "minute", "complete", "all", "10", "drawing", "we", "record", "sequence", "keystroke", "-lrb-", "both", "draw", "erase", "-rrb-", "time", "spend", "each", "drawing", "average", "user", "complete", "task", "about", "20", "minute", "spend", "more", "time", "object", "which", "require", "more", "detail", "face", "less", "those", "shoe", "we", "also", "ask", "subject", "fill", "out", "short", "questionnaire", "end", "study", "evaluation", "eight", "additional", "subject", "-lrb-", "who", "do", "participate", "draw", "experiment", "-rrb-", "evaluate", "drawing", "we", "display", "each", "draw", "pair", "-lrb-", "produce", "without", "ShadowDraw", "-rrb-", "sideby-side", "we", "ask", "each", "evaluator", "choose", "what", "she", "perceive", "better", "draw", "she", "could", "decide", "she", "give", "option", "choose", "tie", "repeat", "all", "draw", "pair", "we", "randomize", "order", "placement", "drawing", "so", "drawing", "produce", "shadowdraw", "do", "always", "appear", "leave", "right", "assess", "user?s", "draw", "ability", "we", "next", "ask", "each", "evaluator", "rate", "each", "user?s", "collection", "drawing", "produce", "without", "ShadowDraw", "scale", "where", "poor", "average", "excellent", "we", "group", "user", "three", "group", "-lrb-", "poor", "average", "good", "-rrb-", "base", "drawing", "ability", "score", "average", "eight", "evaluator", "rating", "should", "note", "none", "subject", "rate", "close", "excellent", "all", "rating", "average", "below", "analysis", "remark", "we", "first", "present", "we", "finding", "draw", "score", "per", "group", "all", "object", "category", "figure", "show", "result", "we", "compute", "average", "score", "per", "group", "-lrb-", "i.e.", "average", "individual", "user", "vote", "group", "-rrb-", "poor", "group", "have", "three", "user", "score", "-lsb-", "-rrb-", "average", "group", "have", "seven", "user", "score", "-lsb-", "-rrb-", "good", "group", "have", "six", "user", "score", "above", "overall", "ShadowDraw", "achieve", "significantly", "higher", "score", "draw", "result", "average", "group", "inconclusive", "result", "other", "two", "group", "-lrb-", "see", "Figure", "-rrb-", "Figure", "show", "per", "category", "breakdown", "score", "average", "group", "ShadowDraw", "particularly", "helpful", "draw", "structurally", "complex", "object", "like", "bicycle", "we", "see", "noticeable", "improvement", "face", "butterfly", "well", "rabbit", "category", "use", "control", "variable", "so", "tie", "result", "expect", "shoe", "show", "some", "decline", "shadowdraw", "perhaps", "because", "variability", "shoe", "appearance", "produce", "higher", "amount", "noise", "shadow", "image", "lack", "higher", "score", "good", "group", "confirm", "we", "intuition", "people", "who", "can", "draw", "quite", "well", "produce", "equally", "good", "drawing", "without", "ShadowDraw", "interesting", "phenomenon", "insignificant", "difference", "score", "poor", "group", "upon", "closer", "inspection", "drawing", "we", "find", "user", "be", "extremely", "poor", "drawer", "-lrb-", "i.e.", "aspect", "ratio", "basic", "shape", "drawing", "be", "far", "off", "from", "those", "object", "be", "intend", "draw", "-rrb-", "thus", "system", "have", "chance", "properly", "match", "retrieve", "relevant", "database", "image", "also", "explain", "why", "we", "achieve", "significant", "improvement", "over", "baseline", "average", "group", "user", "group", "able", "draw", "basic", "shape", "rough", "proportion", "object", "correctly", "have", "difficulty", "apply", "exact", "proportion", "detail", "essential", "produce", "compelling", "drawing", "which", "precisely", "where", "ShadowDraw", "can", "help", "several", "remedy", "allow", "even", "poorest", "drawer", "benefit", "from", "we", "system", "most", "obvious", "give", "they", "more", "practice", "learn", "capability", "system", "test", "hypothesis", "we", "ask", "each", "user", "draw", "another", "face", "after", "complete", "previous", "task", "we", "allow", "user", "explore", "suggest", "user", "draw", "more", "oval", "vs.", "round", "face", "outline", "get", "more", "relevant", "shadow", "Figure", "show", "some", "example", "user", "drawing", "face", "before", "after", "practice", "noticeable", "change", "towards", "realistic", "proportion", "drawing", "those", "poor", "skill", "-lrb-", "left", "-rrb-", "good", "skill", "-lrb-", "right", "-rrb-", "Notice", "how", "subject?s", "personal", "style", "maintain", "between", "drawing", "more", "proficient", "drawer", "simply", "trace", "shadow", "although", "most", "would", "agree", "poorer", "result", "have", "be", "improve", "become", "matter", "taste", "more", "skilled", "drawing", "whether", "new", "drawing", "better", "truly", "assess", "overall", "aesthetic", "improvement", "result", "beyond", "scope", "paper", "Figure", "show", "some", "more", "example", "user", "drawing", "without", "ShadowDraw", "each", "column", "show", "drawing", "produce", "same", "user", "one", "can", "clearly", "notice", "significant", "change", "towards", "more", "realism", "drawing", "especially", "term", "proportion", "different", "part", "object", "include", "important", "feature", "structure", "layout", "object", "-lrb-", "see", "bicycle", "-rrb-", "overall", "shape", "-lrb-", "see", "butterfly", "-rrb-", "user", "satisfaction", "when", "ask", "questionnaire", "how", "would", "you", "compare", "you", "drawing", "result", "ShadowDraw", "vs.", "those", "without", "ShadowDraw", "average", "user", "give", "score", "4.0", "range", "where", "much", "worse", "difference", "much", "better", "when", "ask", "how", "would", "you", "rate", "you", "satisfaction", "draw", "ShadowDraw", "vs.", "without", "ShadowDraw", "average", "user", "give", "score", "3.9", "some", "positive", "comment", "from", "open", "end", "question", "include", "fun", "helpful", "become", "dependent", "very", "quickly", "help", "draw", "faster", "than", "without", "ShadowDraw", "great", "product", "already", "love", "get", "have", "one", "really", "help", "I", "relax", "!!?", "have", "background", "art", "ShadowDraw", "make", "draw", "lot", "fun", "when", "shadow", "provide", "be", "couple", "comment", "indicate", "ShadowDraw", "sometimes", "distracting", "...", "occasionally", "get", "confuse", "about", "my", "line", "vs.", "shadow", "line", "we", "can", "create", "button", "UI", "label", "Shadow", "on/off", "so", "user", "can", "choose", "view", "hide", "shadow", "overall", "user", "appear", "enjoy", "use", "ShadowDraw", "produce", "better", "drawing", "-lrb-", "self-rated", "-rrb-", "than", "could", "achieve", "without", "ShadowDraw", "essence", "what", "define", "ShadowDraw", "do", "produce", "final", "artwork", "rather", "guide", "user", "when", "user", "want", "help", "make", "draw", "experience", "fun", "final", "drawing", "become", "visually", "please", "one?s", "self", "Figure", "per", "category", "evaluation", "score", "drawing", "user", "average", "group", "ShadowDraw", "improve", "draw", "result", "most", "category", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "27:8", "Y.", "Lee", "et", "al.", "Figure", "example", "face", "drawing", "produce", "without", "ShadowDraw", "after", "training", "ShadowDraw", "each", "pair", "draw", "same", "subject", "conclusion", "discussion", "we", "have", "present", "system", "guide", "user", "draw", "dynamically", "produce", "shadow", "derive", "from", "thousand", "image", "we", "have", "demonstrate", "we", "method", "can", "retrieve", "relevant", "image", "real", "time", "base", "incomplete", "evolve", "sketch", "user", "we", "report", "user", "study", "show", "improve", "realism", "many", "user", "drawing", "more", "importantly", "we", "learn", "number", "unexpected", "thing", "from", "study", "perhaps", "most", "interesting", "finding", "ShadowDraw", "most", "effective", "those", "have", "modicum", "draw", "skill", "we", "can", "surmise", "those", "little", "skill", "be", "unable", "produce", "initial", "drawing", "sufficient", "retrieval", "relevant", "image", "production", "shadow", "those", "already", "possess", "draw", "skill", "may", "have", "be", "distract", "shadow", "number", "way", "we", "hope", "alleviate", "both", "issue", "address", "need", "wider", "range", "user", "more", "novice", "drawer", "be", "able", "specify", "category", "class", "would", "trim", "database", "search", "would", "likely", "lead", "more", "relevant", "shadow", "even", "when", "initial", "drawing", "have", "little", "resemblance", "class", "more", "expert", "user", "interface", "provide", "more", "control", "over", "shadow", "strength", "can", "easily", "add", "also", "ability", "draw", "negative", "stroke", "discourage", "shadow", "contain", "those", "stroke", "would", "again", "provide", "finer", "control", "over", "shadow", "guidance", "we", "also", "want", "explore", "trade-off", "between", "image", "database", "size", "shadow", "efficacy", "we", "have", "show", "ability", "handle", "many", "thousand", "image", "over", "wide", "variety", "category", "while", "more", "flexibility", "would", "attain", "larger", "database", "expert", "problem", "encounter", "novice", "drawer", "might", "accentuate", "more", "work", "require", "understand", "trade-off", "also", "many", "more", "subtle", "issue", "tension", "between", "guidance", "freedom", "how", "draw", "express", "object?s", "essence", "versus", "get", "all", "proportion", "right", "perhaps", "require", "explore", "new", "score", "method", "allow", "nonrigid", "transformation", "matching", "shadow", "generation", "step", "we", "be", "encourage", "see", "user", "personal", "drawing", "style", "be", "maintain", "when", "use", "ShadowDraw", "say", "more", "work", "require", "make", "system", "truly", "aid", "improve", "overall", "aesthetic", "result", "require", "more", "nuanced", "understanding", "relationship", "between", "aesthetics", "realism", "drawing", "one", "could", "assemble", "large", "database", "artistic", "drawing", "we", "may", "able", "use", "some", "technology", "we", "report", "begin", "learn", "relationship", "leave", "future", "work", "conclusion", "we", "have", "find", "area", "investigation", "very", "exciting", "look", "forward", "extend", "work", "many", "direction", "we", "hope", "paper", "have", "express", "both", "scope", "specific", "work", "report", "well", "many", "possible", "related", "avenue", "future", "exploration", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011", "ShadowDraw", "Real-Time", "user", "guidance", "Freehand", "Drawing", "27:9", "acknowledgement", "author", "would", "like", "thank", "Ce", "Liu", "many", "insightful", "discussion", "help", "shape", "work", "paper", "we", "would", "also", "like", "thank", "anonymous", "reviewer", "many", "helpful", "comment", "finally", "thanks", "all", "participant", "who", "be", "willing", "draw", "part", "user", "study", "hopefully", "ShadowDraw", "make", "they", "feel", "more", "comfortable", "pick", "up", "pen", "draw", "future", "reference", "rvo", "J.", "OVINS", "K.", "2000", "fluid", "sketch", "continuous", "recognition", "morphing", "simple", "hand-drawn", "shape", "ACM", "UIST", "allard", "D.", "1981", "generalize", "hough", "transform", "detect", "arbitray", "shape", "Pattern", "recognition", "vol", "13", "111", "122", "eaudot", "W.", "ullen", "K.", "2003", "how", "long", "range", "contour", "integration", "human", "color", "vision", "visual", "neuroscience", "vol", "15", "51", "64", "hat", "P.", "ITNICK", "C.", "L.", "OHEN", "M.", "URLESS", "B.", "2009", "Gradientshop", "gradient-domain", "optimization", "framework", "image", "video", "filter", "TOG", "anny", "J.", "1986", "computational", "approach", "edge", "detection", "tpamus", "vol", "679", "698", "ao", "Y.", "ang", "H.", "ang", "C.", "Z.", "HANG", "L.", "hang", "L.", "2010", "Mindfinder", "Finding", "image", "sketch", "ACM", "Multimedia", "International", "Conference", "arson", "C.", "ELONGIE", "S.", "REENSPAN", "H.", "ALIK", "J.", "2002", "Blobworld", "image", "segmentation", "use", "expectationmaximization", "its", "application", "image", "querying", "tpamus", "vol", "24", "1026", "1038", "halechale", "a.", "aghdy", "G.", "ertin", "a.", "2005", "sketch-based", "image", "matching", "use", "angular", "partitioning", "IEEE", "Trans", "Systems", "Man", "Cybernetics", "haudhurus", "S.", "OLTUN", "V.", "2010", "data-driven", "suggestion", "creativity", "support", "3d", "modeling", "ACM", "SIGGRAPH", "ASIA", "hen", "T.", "HENG", "M.-M.", "P.", "hamir", "a.", "S.M.", "2009", "Sketch2photo", "Internet", "image", "montage", "ACM", "SIGGRAPH", "ASIA", "hum", "O.", "hilbin", "J.", "ivic", "J.", "sard", "M.", "isser", "man", "a.", "2007", "total", "recall", "Automatic", "query", "expansion", "generative", "feature", "model", "object", "retrieval", "cvpr", "hum", "O.", "hilbin", "J.", "isserman", "a.", "2008", "near", "duplicate", "image", "detection", "min-hash", "tf-idf", "weighting", "bmvc", "ole", "F.", "OLOVINSKIY", "a.", "impaecher", "a.", "arro", "H.", "S.", "inkelstein", "a.", "unkhouser", "T.", "usinkiewicz", "S.", "2008", "where", "do", "people", "draw", "line", "SIGGRAPH", "atta", "R.", "OSHI", "D.", "J.", "ang", "J.", "Z.", "2008", "image", "retrieval", "idea", "influence", "trend", "new", "age", "ACM", "Computing", "survey", "vol", "40", "60", "ixon", "D.", "rasad", "M.", "AMMOND", "T.", "2010", "icandraw", "use", "sketch", "recognition", "corrective", "feedback", "assist", "user", "draw", "human", "face", "ACM", "CHI", "itz", "M.", "ILDEBRAND", "K.", "OUBEKEUR", "T.", "LEXA", "M.", "2009", "Photosketch", "sketch", "base", "image", "query", "compositing", "system", "ACM", "SIGGRAPH", "talk", "program", "lder", "J.", "OLDBERG", "R.", "2001", "image", "editing", "contour", "domain", "tpamus", "vol", "23", "291", "296", "ei", "eus", "L.", "ergus", "R.", "erona", "P.", "2004", "Learning", "generative", "visual", "model", "from", "few", "training", "example", "incremental", "bayesian", "approach", "test", "101", "object", "category", "Workshop", "Generative-Model", "base", "Vision", "CVPR", "avilan", "D.", "AITO", "S.", "AKAJIMA", "M.", "2007", "sketch-tocollage", "ACM", "SIGGRAPH", "Posters", "ay", "J.", "fro", "a.", "A.", "2007", "scene", "completion", "use", "million", "photograph", "ACM", "SIGGRAPH", "R.", "ARNARD", "M.", "ollomosse", "J.", "2010", "gradient", "field", "descriptor", "sketch", "base", "retrieval", "localization", "icip", "garashi", "T.", "UGHES", "J.", "F.", "2001", "suggestive", "interface", "3d", "drawing", "ACM", "UIST", "garashi", "T.", "atsuoka", "S.", "anaka", "H.", "1999", "Teddy", "sketch", "interface", "3d", "freeform", "design", "ACM", "SIGGRAPH", "ACOBS", "C.", "E.", "inkelstein", "a.", "ALESIN", "D.", "H.", "1995", "fast", "multiresolution", "image", "query", "SIGGRAPH", "ee", "D.", "C.", "Q.", "sard", "M.", "2010", "partition", "min-hash", "partial", "duplicate", "image", "discovery", "eccv", "owe", "D.", "G.", "2004", "distinctive", "image", "feature", "from", "scale-invariant", "keypoint", "ijcv", "ister", "D.", "TEWENIUS", "H.", "2006", "scalable", "recognition", "vocabulary", "tree", "cvpr", "ivic", "J.", "ANEVA", "B.", "orralba", "a.", "VIDAN", "S.", "reeman", "W.", "2008", "create", "explore", "large", "photorealistic", "virtual", "space", "Workshop", "Internet", "Vision", "CVPR", "INDER", "S.", "UA", "G.", "ROWN", "M.", "2009", "pick", "best", "daisy", "cvpr", "ITTEN", "I.", "H.", "offat", "a.", "ell", "t.", "1999", "manage", "Gigabytes", "Compressing", "indexing", "document", "image", "Morgan", "Kaufmann", "itnick", "C.", "L.", "2010", "binary", "coherent", "edge", "descriptor", "eccv", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "27", "publication", "date", "July", "2011" ],
  "content" : "\n  \n    32f93ca8b4de51da0d34232d1e30c65b97b38b2b50462f378962683df596244e\n    mit\n    10.1145/1964921.1964922\n    Name identification was not possible. \n  \n  \n    \n      \n        ShadowDraw: Real-Time User Guidance for Freehand Drawing\n      \n      Yong Jae Lee C. Lawrence Zitnick University of Texas at Austin Microsoft Research\n      \n        \n        Figure 1: Results of the user study: (top) Freehand drawings of objects without using ShadowDraw, (bottom) freehand drawings of objects using ShadowDraw. Notice the improved spacing and proportions while maintaining the subjects? own unique styles.\n      \n      We present ShadowDraw, a system for guiding the freeform drawing of objects. As the user draws, ShadowDraw dynamically updates a shadow image underlying the user?s strokes. The shadows are suggestive of object contours that guide the user as they continue drawing. This paradigm is similar to tracing, with two major differences. First, we do not provide a single image from which the user can trace; rather ShadowDraw automatically blends relevant images from a large database to construct the shadows. Second, the system dynamically adapts to the user?s drawings in real-time and produces suggestions accordingly. ShadowDraw works by efficiently matching local edge patches between the query, constructed from the current drawing, and a database of images. A hashing technique enforces both local and global similarity and provides sufficient speed for interactive feedback. Shadows are created by aggregating the edge maps from the best database matches, spatially weighted by their match scores. We test our approach with human subjects and show comparisons between the drawings that were produced with and without the system. The results show that our system produces more realistically proportioned line drawings. CR Categories: I.3.8 [Computing Methodologies]: Computer Graphics?Applications; Keywords: large scale image retrieval, shape matching, interactive drawing\n      Links:\n    \n    \n      \n        ACM Reference Format\n        Lee, Y., Zitnick, C., Cohen, M. 2011. Shadow Draw: Real-Time User Guidance for Freehand Drawing. ACM Trans. Graph. 30, 4, Article 27 (July 2011), 9 pages. DOI = 10.1145/1964921.1964922 http://doi.acm.org/10.1145/1964921.1964922.\n      \n      \n        Copyright Notice\n        Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2011 ACM 0730-0301/2011/07-ART27 $10.00 DOI 10.1145/1964921.1964922 http://doi.acm.org/10.1145/1964921.1964922\n        Michael F. Cohen Microsoft Research\n      \n      \n        1 Introduction\n      \n      If asked to draw a face, the result for most of us (those with little practice in drawing) might look like one of those in the upper row of Figure 1 , created by subjects in our user study using a standard drawing interface. Similarly, if asked to draw a bicycle, most of us would have a difficult time depicting how the frame and wheels relate to each other. One solution is to search for an image of the thing we want to draw, and to either trace it or to use it in some other way as a reference. However, aside from the difficulty of finding a photo of what we want to draw, simply tracing object edges eliminates much of the essence of drawing, i.e., there is very little freedom in tracing strokes. Conversely, drawing on a blank paper with only the image in the mind?s eye gives the drawer a lot of freedom, but freehand drawing can be frustrating without significant training. To address this, we present ShadowDraw, a drawing interface that automatically infers what you are drawing and then dynamically depicts relevant shadows (Figures 5 and 6) underneath the drawing. These shadows may be either used or ignored by the drawer. ShadowDraw preserves the essence of drawing, i.e., freedom and expressiveness, and at the same time uses visual references, shadows, to guide the drawer. Furthermore, shadows from real images can enlighten the artist with the gist of many images simultaneously. The creation becomes a mix of both human intuition and computer intelligence. The computer, in essence, is a partner in the drawing process, providing guidance like a teacher, instead of actually producing the final artwork. The drawings in the bottom row of Figure 1 were drawn by the same subjects, this time using ShadowDraw. Notice how the users? own creative styles remain consistent between the drawings, while the overall shapes and spacing are more realistic. ShadowDraw consists of two main computational steps plus the user interface. The first offline step consists of building a database from a collection of 30,000 images collected from the Web. Each image is converted to an edge drawing using the long edge detector technique developed by [Bhat et al. 2009] and stored. Overlapping windows in each edge image are analyzed, coded, and stored. Each window is converted to edge descriptors, and further coded as sketches with distinct hash keys using min-hash [Chum et al. 2008]. In the second online step, as the user draws, ShadowDraw analyzes the strokes using a similar encoding to determine hash keys for overlapping windows for fast matching with the database of images. The top 100 matching database edge images are further aligned to the drawing. A set of spatially varying weights blend the edge images into a shadow image. In the user interface, the strokes are overlaid on top of an evolving shadow image that provides guidance for future strokes. Our main contribution is an interactive drawing system that dynamically adapts to the user?s drawing and provides real-time feedback. A number of technical contributions make ShadowDraw unique. Although portions of ShadowDraw follow the basic framework of content based image retrieval, the technique of partial spatial matching that ShadowDraw employs is novel, in that it allows for multiple matching images based on different sub-regions of the image. In addition, the verification stage and methods for determining the blending weights are unique to this work. While there have been previous works that helps users draw basic shapes [Igarashi et al. 1999; Arvo and Novins 2000; Igarashi and Hughes 2001], to our knowledge, we are the first to develop an interactive user interface to assist freeform drawing. We test our approach with human subjects and show comparisons between the drawings that were produced with and without the system. The results show that our system produces more realistically proportioned line drawings, particularly for those who possess some skill but lack expertise. We purposely avoid in this paper, making any claims that the system helps produce more skilled drawers or more artistic drawings.\n      ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n      27:2\n      ?\n      Y. Lee et al.\n      \n        2 Related Work\n        The huge volume of image and video data available on the Web, scientific databases, and newspaper archives, along with recent advances in efficient (approximate) nearest neighbor matching schemes have opened the door for a number of large scale matching applications. The general field of content based image retrieval (CBIR) uses many different inputs modalities to search for similar images in a database (see [Datta et al. 2008] for a general survey of this field). Here, we briefly review related work in large scale image retrieval techniques, especially those that use line drawings for the query and/or are aimed at constructing new images and drawings. There are a number of systems that produce photographic-like results by compositing portions of retrieved images. The Sketch-toCollage system [Gavilan et al. 2007] produces a single composite collage by matching user provided color strokes to a database of images, segmenting out regions and interactively blending the retrieved segments. The Sketch2Photo system [Chen et al. 2009] produces a composite image from the user?s sketch of a scene with text label annotated objects. Candidate image regions for each object are found on the Web and those that produce the best agreement are put together to form the final composition. PhotoSketch [Eitz et al. 2009] progressively creates images through a sketching and compositing interface. The user interacts with the system to segment and blend the images retrieved from a database of 1.5 million images. Rather than starting from a blank page, the Scene Completion algorithm [Hays and Efros 2007] performs a global scene match using a query image, which has ?holes?. The system finds the best images for completing the scene with objects that could have been in the missing regions. To improve retrieval accuracy of these sketch-based systems, researchers have also designed descriptors that provide better matches between human drawn sketches and natural images [Chalechale et al. 2005; Hu et al. 2010]. More general CBIR efforts include the early SIGGRAPH work, Fast Multiresolution Image Querying [Jacobs et al. 1995], that matches user paint strokes to underlying wavelet signatures of images in the database. The ?Blobworld? approach [Carson et al.  2002] queries image regions rather than the entire image, to allow the user to specify which objects in the image are more relevant to the query. In [Nister and Stewenius 2006], a vocabulary tree is created to efficiently retrieve images from a large database. The tree defines a hierarchical quantization of the local image features and provides a multi-level scheme to score matching images. In [Chum et al. 2007], the idea of ?query expansion? in text retrieval is applied to the image domain, where the highest ranked images from the original query are re-queried to generate additional relevant images. A 3D photorealistic virtual space is created in [Sivic et al. 2008] to allow users to tour themes, such as city streets or skylines. Similar images are matched and stitched from a large (few hundred thousand) image collection downloaded from Flickr. Most recently, MindFinder [Cao et al. 2010] aims to improve image retrieval by allowing the user to input a text tag, a sketch, and a color as a query. The system is able to retrieve images that better match the image in the user?s mind. Finally, in [Chaudhuri and Koltun 2010], data-driven suggestions are made for 3D modeling. The system presents suggestions by matching and retrieving relevant shapes in the database to the initial basic model. ShadowDraw also leverages the idea of matching to a large database of images. Unlike previous methods, our end goal is to help the user draw rather than to perform an image composition, completion, retrieval, or 3D modeling. Furthermore, ShadowDraw uses only a partial and evolving drawing for the query rather than other images and/or textual descriptions. Our system requires the retrieval to run in real time. We have seen no other system that leverages image retrieval for the same kind of application or with partial drawings as the input. There are interactive drawing interfaces such as Teddy [Igarashi et al. 1999], Fluid Sketches [Arvo and Novins 2000], and the 3D drawing system of [Igarashi and Hughes 2001] that strive to produce better drawings for the user. These methods provide low-level information feedback in the form of basic polygonal shapes, lines, and curves. More recently, the iCanDraw interface [Dixon et al. 2010] provides step-by-step instructions and corrective feedback to guide a user to draw a human face from a reference image. While similar in motivation, our approach provides guidance for drawing arbitrary high-level objects using only example images. Recently, in [Cole et al. 2008], the authors studied artists? line drawings of 3D shapes to analyze which line segments were being emphasized, and to compute correlations between those segments and the contours produced using existing computer generated line drawing techniques and contour feature extractors. While our work aims to help users in freeform drawing, the outputs of our system can be a useful resource for this line of work, i.e., the user drawings produced with ShadowDraw can be used for analyzing the contours of more general objects occurring in natural images.\n      \n      \n        3 Approach\n        ShadowDraw includes three main components: (1) the construction of an inverted file structure [Witten et al. 1999] that indexes a database of images and their edge maps; (2) a query method that, given user strokes, dynamically retrieves matching images, aligns them to the evolving drawing and weights them based on a matching score; and (3) the user interface, which displays a shadow of weighted edge maps beneath the user?s drawing to help guide the drawing process.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n        ShadowDraw: Real-Time User Guidance for Freehand Drawing\n        ?\n        27:3\n        \n          \n          \n          \n        \n        (a) (b) (c)\n        \n          \n          \n          \n        \n        (d) (e) (f)\n        \n          Figure 2: Example database images and their corresponding edge images.\n        \n        \n          3.1 Database Creation\n          The images in the database should be selected so that the objects depicted, as well as their appearance and pose, are likely to be similar to those drawn by the user. Ideally, a large database of hand drawn images would be used, but such a database is exceedingly hard to construct. Instead, we use a set of approximately 30,000 natural images collected from the internet via approximately 40 categorical queries such as ?t-shirt?, ?bicycle?, ?car?, etc. Although such images have many extraneous backgrounds, objects, framing lines, etc., the expectation is that, on average, they will contain edges a user may want to draw. We scale the images to fit a 300x300 pixel resolution, i.e., the long side is scaled to 300 pixels. We then process each image in three stages and add them to an inverted file structure. First, edges are extracted from the image. Next, local edge descriptors are computed. Finally, sets of concatenated hashes called sketches are computed from the edge descriptors and added to the database. We store the database as an inverted file, in other words, indexed by sketch value, which in turn points to the original image and its edges.  Edge extraction Given a natural image, we want to find the edges most likely to be drawn by a user while ignoring others. Perceptual studies show that long coherent edges are salient to humans even if faint [Beaudot and Mullen 2003; Elder and Goldberg 2001]. Inspired by these works, we use the long edge detector described in [Bhat et al. 2009]. The method locally normalizes the magnitudes of the edges, and then sums the normalized magnitudes, weighted by local curvature, along the length of the edge. The result is an edge response that is related to the length of the edge and its degree of curvature, rather than the magnitude of the intensity gradients. We store the edge images, E, using run length encoding. On average, each compressed edge image requires 5.3KB. Examples are shown in Figure 2 (d-f). Patch descriptors For each edge image, we determine the edge positions by finding maxima in the responses perpendicular to the edge direction, similar to Canny edge detection [Canny 1986]. Given an image I in the database with corresponding edges E and orientations ?, we compute a set of edge descriptors d i ? D. Since the goal is to match an edge image E to incomplete and evolving drawings, we compute the descriptors locally over 60x60 patches. The patches used to compute neighboring descriptors overlap by 50%, resulting in 81 descriptors over the 9?9 fixed grid of patches. We encode each patch using the BiCE descriptor [Zitnick 2010], which is designed to encode the histogram of edge positions and orientations. Since the edges drawn by a user are typically less dense than in natural images, we use a low dimensional version of BiCE. We define a three dimensional histogram, with 4 discrete edge orientations, 18 positions perpendicular to the edge, and 6 positions tangent to the edge. Using the notation of [Zitnick 2010], we set n x = 18, n y = 6, n ? = 4, and n l = 1. The buckets of the histogram are binarized by setting the top 20% to one and the rest to zero. The final descriptor, d i , has 432 binary bits. Other image patch descriptors, such as SIFT [Lowe 2004] and Daisy [Winder et al. 2009], rely on relative strength of the edge magnitudes to provide discriminability, and are shown to have reduced matching performance compared to BiCE in [Zitnick 2010]. They are also not applicable to our scenario, since the relative edge magnitudes on which these descriptors rely are not known for the user?s drawing. Min-hash A key feature of the BiCE descriptor is its binary encoding: it can be viewed as a set representation where the ones indicate edge presence. This makes it amenable to min-hash, which is an effective hashing technique for retrieval and clustering [Chum et al. 2008; Lee et al. 2010; Zitnick 2010]. Min-hash has the property that the probability of two sets having the same hash value (i.e., ?colliding?) is equal to their Jaccard similarity. The Jaccard similarity sim(d i , d j ) between two sets, d i and d j , is the cardinality of their intersection divided by the cardinality of their union:\n          \n            1\n            #(d i ? d j ) sim(d i , d j ) = . #(d i ? d j )\n          \n          A min-hash function randomly permutes the set indices (i.e., the ones and zeros). All sets (BiCE descriptors) are permuted using the same min-hash function. The min-hash value for a given permuted set is its smallest index containing a value of one after the permutation. A single min-hash can be non-discriminative and introduce many false positive retrievals, especially if the descriptor is nonsparse. To increase precision, we compute k independent random min-hash functions, and apply them to all BiCE descriptors. We concatenate the resulting k min-hash values for each descriptor into k-tuples called sketches. The probability of two sketches colliding is thus reduced exponentially to sim(d i , d j ) k . To increase recall, this process is repeated n times using n different sets of k minhash functions, resulting in n sketches per descriptor. To maintain high recall while reducing false positives, tradeoffs must be made between the number of sketches stored for each descriptor and the size of the sketch, k. In our experiments, we store n = 20 sketches of size k = 3 for each descriptor. We store an inverted file structure for each of the n min-hash sketches. We allocate each unique sketch as a new entry in the structure. We record the image index and patch location of the descriptor instance that produced the sketch.\n        \n        \n          3.2 Image Matching\n          Our hashing scheme allows for efficient image queries, since only images with matching sketches need to be considered. In this section, we describe the real-time matching pipeline between the edge images in the database and the user?s drawing, as shown in Figure 3. Initially, we use the inverted file structure to obtain a set of candidate matches. Next, each candidate match is aligned with  the user?s drawing and scored. This two step matching procedure is necessary for computational efficiency, since only a small subset of the database images need to be finely aligned and weighted. We use the scores from the alignment step to compute a set of spatially varying weights for each edge image. The output is a shadow image resulting from the weighted average of the edge images. Finally, we display the shadow image to the user as described in Section 4. Candidate matches We represent the user?s drawing as a set of vectorized multi-segment strokes. We create an edge image E  ? from these strokes by drawing lines with a width of one pixel between the stroke points. The rendered lines have the same style as the edges extracted from the natural images in the database, i.e., the edge image E  ? used for matching does not use the stylized strokes that are seen by the user described in Section 4. Next, we compute BiCE descriptors and their corresponding sketches in the same manner as described in Section 3.1, this time using a higher resolution grid of 18 ? 18 = 324 patches with 75% overlap between neighboring patches. We use a higher resolution grid to increase the accuracy of the predicted position, and to increase invariance to translations in the drawing. In our implementation, the user?s drawing occupies an area of 480 ? 480 pixels, resulting in 96 ? 96 pixel patches with 24 pixel offsets between neighboring patches. We compute descriptors and sketches for each of the 324 patches. Using the inverse lookup table, we match each sketch from the user?s drawing to the sketches stored in the database. A matching sketch casts one vote for the corresponding database image and patch offset pair. We aggregate the matches in a histogram H storing the number of matching sketches for each image at each grid offset. To reduce the size of H, votes are only stored if the database patch offset is within four patch grid points of the patch being considered in the user?s drawing. This corresponds to relative shifts of less than 96 pixels between the user?s drawing and the database images. The resulting histogram has size m ? 9 ? 9, where m is the number of images in the database. After adding all the matches for each sketch to the histogram, we find the best matching offset for each image, and add the top 100 images to the candidate set C. As discussed in Section 3.1, we compute n = 20 sketches for each descriptor, resulting in a maximum possible 20 votes per sketch in the histogram. To reduce the bias from any single descriptor, we limit the contribution of each descriptor to four votes in the histogram. Given a large database, computing the candidate set as described above can be computationally expensive. We take advantage of the fact that the user?s strokes change gradually over time to increase performance. At each time step, only votes resulting from sketches derived from patches that have changed are updated. We accomplish this by subtracting the votes added from the previous sketches from H, followed by adding in the votes from the new sketches. At each time frame, we also include in the candidate set any database\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n          27:4\n          ?\n          Y. Lee et al.\n          \n            \n            Figure 3: An outline of the online processing pipeline. Given a user?s strokes, sketches are computed for each sub-window, and matching votes are accumulated in a histogram. The top 100 matching images are aligned, scored, and weighted to generate the final shadow image.\n          \n          \n            \n            Figure 4:\n          \n          Illustration of spatial weights: (a) user?s view, (b) shadow image, (c, d) top two matches and corresponding spatial weights (top right).\n          image that contributed to the shadow image in the previous time frame.  Image alignment The candidate image set C contains a set of images with approximate offsets d x and d y defined by the best matching offset as described above. The approximation arises from the discretization of the offsets in the grid of patches. We refine these offsets using a 1D variation of the Generalized Hough transform [Ballard 1981]. Using the standard Generalized Hough transform for 2D translations, we create a 2D histogram T over possible offsets x and y using:\n          \n            2\n            T (x, y) = E(p  ? x , p y )E(p x + d x + x, p y + d y + y), p\n          \n          where E(p  ? x , p y ) is the value of E  ? at pixel p in location (p x , p y ), and similarly for the edge image E. We determine the best offset by finding the maximum value of T (x, y). This approach is computationally expensive since we need to sum over the image for every possible combination of x and y offsets. Instead we compute the x and y dimensions separately using two histograms:\n          \n            3\n            T x (x) = sin( ?(p  ? x , p y )) E(p  ? x , p y )\n          \n          p sin(?(p x + d x + x, p y + d y ))E(p x + d x + x, p y + d y ),\n          and similarly for T y using the cosine of the angles. The sine of the edge angles provides higher weights to the more informative vertical edges when determining the horizontal offsets, and similarly for T y and cosine with horizontal edges. We empirically found this approach to produce good results. Once the histograms T x and T y are created, they are slightly blurred with ? h = 2. We determine  the final sub-pixel offsets d x and d y by adding a quadratic interpolation of the resulting peak response in T x and T y to d x and d y . For additional accuracy, two iterations are run. To reduce computation, we limit the search range of x and y to twice the distance between the grid points. In addition, we compute Equation (3) on reduced resolution images of size 160 ? 160. We notate the aligned edge image as E i . To refine the scale, an additional 1D histogram T s may be similarly computed across scales and the peak found. Image weighting We now have a set of candidate images, C, and their aligned edge images E i , aligned using offsets d i . Our goal is to blend these aligned edge images into a shadow image, S, that will help guide the user as they draw:\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n          ShadowDraw: Real-Time User Guidance for Freehand Drawing\n          ?\n          27:5\n          \n            \n            Figure 5: Rendering pipeline for user interface: (a) user?s rendered strokes, (b) weighting based on pen position, (c) shadow image, and (d) final rendering.\n          \n          \n            4\n            S = W i E i , i\n          \n          where W i is the blending weight image, which we define below. The blending weight should be high for pixels where there is a good match between the drawing and the candidate?s aligned edges and low for pixels where there is not. We construct the weight image from two terms, a global match term v i and a spatially varying match term, V i , which are normalized over all images in the candi-\n          \n            5\n            date set: v i V i W i = ? .\n          \n          \n            5\n            j v j V j\n          \n          The weight ? is used to reduce the visibility of noisy shadows produced when the drawing has just started and all the match scores are low. An illustration of the spatial weighting is shown in Figure 4 . The use of a spatial weighting results in shadows that are a composite of multiple distinct edge images, creating the appearance of an object that does not exist in a single database image. We begin by defining the spatially varying match term V i followed by the global match term v i and ?. Our goal is for a candidate image?s weights to increase when its edges agree in position and orientation with the user?s strokes. To compute V i , we first decompose each candidate edge image into eight oriented images, ? t , and similarly, the drawing image into eight oriented images, ?  ? t , for t = 1, . . . , 8. Each image captures only strokes parallel to one of eight evenly spaced orientations; i.e., ? 1 depicts horizontal edges, ? 5 depicts vertical edges, and the remaining six each capture one other orientation at 22.5 ? intervals. If an edge?s orientation falls in between two orientations, its contribution is linearly divided between the two oriented edge images. To provide some invariance to position, we blur the oriented edge images with a Gaussian kernel G(? s ) with a standard deviation ? s = 1.25?, where ? is the relative distance between the grid points. It is also desirable that images that contain multiple edges near a stroke receive the same score contribution as those with a single edge. This is accomplished by limiting the magnitude of the response from the blurred edges to the maximum response that may result from a single edge in isolation. We then compute positive and negative edge correlation images, ? + and ? ? , to determine where the edge image agrees and disagrees with the user?s strokes. We define positive correlation using the product of the edge images with the same orientations, and define negative correlation using the product of orthogonally oriented edge images:\n          \n            6\n            ? + = ? t ? ?  ? t t=1,8\n          \n          \n            7\n            ? ? = ? t ? ?  ? (t+4)%8 t=1,8\n          \n          Our spatially varying match term V i is simply a Gaussian blurred version of ? + ,\n          \n            8\n            V i = G(? + i , 4?)\n          \n          We add a small offset to V i to ensure non-zero values. To reduce computation, we compute the image scores and weights on reduced resolution images of 40 ? 40 pixels. Next, we define the global match term v i in Equation (5) that is used to compute the edge image?s blending weights W i . To aid in the computation of v i , we compute a global match score h i for each image using the difference between ? + and ? ? ,\n          \n            9\n            h i = ? i + (p) ? ? i ? (p) p\n          \n          Equation (9) has a high positive value when the user?s strokes and the image?s edges agree in position and orientation. However, if a majority of the user?s strokes are perpendicular to the image?s edges, h i may be negative. Finally, we compute v i using a nonlinear function of h i and the average h ? of the five highest scores from the candidate set,\n          \n            10\n            h i ? ?h ? ? v i = max 0, h ? ? ?h ? .\n          \n          We assign a value of 0.5 to ?, which means the value of v i is greater than zero only if the score is greater than half the average of the five highest scores. A value of ? = 2 favors images with higher scores and sets the rate of weight decay as quadratic. In Equation (5), we set ? to\n          \n            11\n            ? = i v i\n          \n          \n            11\n            + i v i\n          \n          where to corresponds to the score that would result from drawing a single stroke of approximately 250 pixels. Using Equation (11), ? increases as the user draws more stokes, resulting in greater visibility of the shadows.  To summarize, we compute a shadow image in real time by comparing the user drawn strokes to the candidate images extracted from the database. We compute global and spatially varying weights to blend the corresponding edge images to create the shadow image used in the drawing interface. We determine these weights by comparing local orientations of edges between the developing drawing and the database edge images.\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n          27:6\n          ?\n          Y. Lee et al.\n          \n            \n            Figure 6: Video frames from four example drawing sessions: (top) final rendering visible to user, (bottom) shadow image. The last row demonstrates the shadow?s robustness to clutter.\n          \n        \n      \n      \n        4 User Interface\n        The ShadowDraw user interface appears at first like a standard drawing system on a blank paper-like surface. We used a WACOM Cintiq 21ux screen/tablet. The user can draw or erase strokes using a stylus. The user sees their own drawing formed with pen strokes superimposed on a continuously updated shadow S, see Figure 5 . The drawing area is 480 ? 480 pixels, and the line strokes are rendered using a dark blue marker style, as shown in Figure 5(a) . To provide some color contrast, we render the shadow in a sepia tone. To further make the shadow visible to the user, while not distracting from the user?s actual drawing, we filter the shadow image to remove noisy and faint edges,\n        \n          12\n          S  ? = S ? (G(S, ? r ) ? ?)\n        \n        where ? r = 16. Multiplying by a blurred shadow image strengthens edges that agree in position and weakens others. ? = 1 is used to additionally suppress faint edges. In addition, we apply a small amount of blur with a standard deviation of 1.25 to soften the shadows, see Figure 5(c) . Finally, we weight the shadow S to have higher contrast near the user?s cursor position p c :\n        \n          13\n          S (p) = ? S(p)  ? + (1 ? ?)? S(p)  ?\n        \n        where ? is a Gaussian weighted distance between p and p c with a standard deviation of 120 pixels. ? may be set by the user and controls the contrast of the shadows. We render the final image on a paper textured background as shown in Figure 5(d) .\n      \n      \n        5 Results\n        All our experiments are run using a database of approximately 30,000 images. We collected 28,000 images by crawling image web searches such as ?bear?, ?bike white background?, and ?motorcycle honda?. We removed duplicates and adjusted the resolution of the images such that intra-category objects have similar scales. We collected 435 face images from the Caltech 101 dataset [Fei-Fei et al. 2004]. Since the face images are of uniform scale, and users tend to draw faces at different scales, (full face vs. head and shoulders) we added randomly scaled images to the dataset for a total of 2,000 face images. We implemented ShadowDraw on a modest system with a quadcore Intel i5 CPU with 4GB RAM. The compressed edge images from the database are stored in memory along with the inverse lookup table requiring 850MB of RAM. We implemented ShadowDraw using two threads to provide a smooth user interface. The foreground thread handles user input and renders the user?s strokes. The background thread accepts as input the user?s strokes and computes a shadow image. On average, a new shadow image is computed every 0.4 to 0.9 seconds depending on the number of new strokes. A fast response is critical in creating a positive feedback loop in which the user obtains suggestions while still in the process of drawing a stroke. We chose the parameters for the BiCE descriptor, sketch sizes, number of sketches, and grid resolutions through quantitative tests on several categories, such as faces, motorcycles, and butterflies. We chose the parameters associated with fine alignment and spatial weighing by qualitatively examining several results. Figure 6 shows several example drawing sessions using ShadowDraw. Please refer to the accompanying video to see the sessions in action and for more results. Note the drawing rate is accelerated to fit in the video. As the user draws new strokes and erases others, the shadows dynamically update to best match the user?s drawing. Using our large database, the user can receive guidance for a variety of object categories, including specific types of objects such as office chairs, folding chairs, or rocking chairs. The last row of Figure 6 demonstrates the scoring function?s robustness to clutter. Even when many spurious strokes are drawn, the correct images are given high weight in the shadow image.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n        ShadowDraw: Real-Time User Guidance for Freehand Drawing\n        ?\n        27:7\n        \n          \n          Figure 7: Evaluation scores of drawings by users in the ?poor?, ?average?, and ?good? groups. ShadowDraw achieves significantly better drawing results for the ?average? group.\n        \n        \n          5.1 User Studies\n          We conducted a user study to assess the effectiveness of ShadowDraw on untrained drawers. In the first stage, subjects produced quick 1-3 minute drawings with and without ShadowDraw. In a second stage, a separate set of subjects evaluated the drawings.  Drawing We conducted the experiment with 16 subjects, eight women and eight men. Each subject was asked to draw five objects, Shoe, Bicycle, Butterfly, Face, and Rabbit, with and without ShadowDraw, for a total of 10 drawings. We randomly permuted the sequence of objects presented to each user. The shadows appeared on every other drawing; thus half the objects were drawn first without shadows and the other half drawn first with shadows. We used Rabbit as a control variable to ensure fairness of the experiments: there were no rabbit images in our database. Please see the supplementary video for example drawings by the subjects. Before beginning the study, we explained the UI: the functionality of the pen (drawing and erasing), the white canvas, and the ?start? and ?next? button to proceed to the next object once the current drawing was completed. We explained to the user that shadows would appear on every other drawing, and that he/she was free to use them for guidance or to completely ignore them. We also explained that the shadows could be removed by tapping on a region outside of the canvas (and would reappear as the user started drawing again). Warm-up exercises to acquaint them with the interface included drawing a circle and a t-shirt. Each user was given 30 minutes to complete all 10 drawings. We recorded the sequence of keystrokes (both drawing and erasing), and time spent on each drawing. On average, the users completed the task in about 20 minutes, spending more time on objects which require more detail such as Faces and less on those such as Shoes. We also asked subjects to fill out a short questionnaire at the end of the study. Evaluation Eight additional subjects (who did not participate in the drawing experiment) evaluated the drawings. We displayed each drawing pair (produced with and without ShadowDraw) sideby-side. We asked each evaluator to choose what she perceived to be the ?better drawing?. If she could not decide, she was given the option of choosing ?tie?. This was repeated for all drawing pairs. We randomized the order and placement of the drawings, so that the drawings produced with ShadowDraw did not always appear left or right. To assess the user?s drawing abilities, we next asked each evaluator to rate each user?s collection of drawings produced without ShadowDraw on a scale of 1 to 5, where 1 is poor, 3 is average, and 5 is excellent. We grouped the users into three groups (poor, average, and good) based on their drawing ability score, by averaging the eight evaluator ratings. It should be noted that none of the subjects was rated as close to excellent, as all ratings averaged below 4. Analysis and Remarks We first present our findings on the drawing scores per group for all object categories. Figure 7 shows the results. We compute an average score per group (i.e., by averaging the individual user votes in the group). The ?poor? group has three users, with scores in [1, 2); the ?average? group has seven users, with scores in [2, 3); and the ?good? group has six users, with scores above 3. Overall, ShadowDraw achieves significantly higher scored drawing results for the ?average? group, and inconclusive results in the other two groups (see Figure 7 ). Figure 8 shows the per category breakdown of scores for the ?average? group. ShadowDraw was particularly helpful for drawing structurally complex objects like bicycles. We see noticeable improvement for faces and butterflies as well. The rabbit category was used as a control variable, so the ?tie? result was expected. Shoes showed some decline with ShadowDraw perhaps because the variability in shoe appearance produced a higher amount of noise in the shadow image. The lack of higher scores in the ?good? group confirms our intuition that people who can draw quite well will produce equally good drawings with or without ShadowDraw. The interesting phenomenon is the insignificant difference in the scores for the ?poor? group. Upon closer inspection of their drawings, we found that these users were extremely poor drawers (i.e., the aspect ratios and basic shapes of their drawings were far off from those of the objects they were intending to draw) and thus the system had no chance to properly match and retrieve relevant database images. This also explains why we achieve significant improvement over the baseline for the ?average? group. The users in this group are able to draw the basic shapes and rough proportions of the objects correctly, but have difficulty applying exact proportions and details essential for producing compelling drawings, which is precisely where ShadowDraw can help. There are several remedies to allow even the poorest drawer to benefit from our system. The most obvious is to give them more practice to learn the capabilities of the system. To test this hypothesis, we asked each user to draw another face after they completed the previous task. We allowed the user to explore and suggested that the user draw a more ?oval? vs. ?round? face outline to get more relevant shadows. Figure 9 shows some examples of the users? drawings of faces before and after such practice. There is a noticeable change towards realistic proportions in the drawings for those with poor skill (left) and good skill (right). Notice how the subject?s personal style is maintained between drawings, and that the more proficient drawers are not simply tracing the shadows. Although most would agree the poorer results have been improved, it becomes a matter of taste for the more skilled drawings whether the new drawings are better. Truly assessing the overall aesthetic improvement in the results is beyond the scope of this paper. Figure 1 shows some more examples of the users? drawings with and without ShadowDraw. Each column shows the drawings produced by the same user. One can clearly notice a significant change towards more realism in the drawings, especially in terms of the proportions of the different parts of an object and including important features such as the structure and layout of the object (see the bicycles), and overall shape (see the butterflies). User Satisfaction When asked in the questionnaire, ?How would you compare your drawing results with ShadowDraw vs. those without ShadowDraw?,? on average, the users gave a score of 4.0 in a range where 1 is ?much worse?, 3 is ?no difference?, and 5 is ?much better?. When asked, ?How would you rate your satisfaction of drawing with ShadowDraw vs. without ShadowDraw?,? on average, the users gave a score of 3.9. Some positive comments from an open ended question included: ? ?Fun and helpful, I became dependent on it very quickly.? ? ?Helps in drawing faster than without ShadowDraw? ? ?This is a great product and I already love it got to have one. Really helps me relax!!? ? ?Having no background in art, ShadowDraw made drawing a lot of fun when a shadow was provided.? There were a couple of comments indicating that ShadowDraw was sometimes distracting, such as ?...I occasionally got confused about my lines vs. shadow lines.? For this, we can create a button on the UI labeled as ?Shadow on/off?, so that the user can choose to view or hide the shadows. Overall, the users appeared to enjoy using ShadowDraw and produced better drawings (as self-rated) than they could achieve without ShadowDraw. This is the essence of what defines ShadowDraw. It does not produce the final artwork; rather, it guides the user when the user wants the help. This makes the drawing experience fun, and with that, the final drawing becomes visually pleasing to one?s self.\n          \n            Figure 8: Per category evaluation scores of drawings by users in the ?average? group. ShadowDraw improves drawing results for most categories.\n          \n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n          27:8\n          ?\n          Y. Lee et al.\n          \n            \n            Figure 9: Example face drawings produced without ShadowDraw and after training with ShadowDraw. Each pair was drawn by the same subject.\n          \n        \n      \n      \n        6 Conclusion and Discussion\n        We have presented a system that guides a user to draw by dynamically producing shadows derived from thousands of images. We have demonstrated that our method can retrieve relevant images in real time based on incomplete and evolving sketches by the user.  We reported on a user study that showed improved realism in many users? drawings, but more importantly, we learned a number of unexpected things from the study. Perhaps the most interesting finding was that ShadowDraw was most effective for those that had a modicum of drawing skill. We can surmise that those with little skill were unable to produce initial drawings sufficient for the retrieval of relevant images for the production of shadows. Those already possessing drawing skill may have been distracted by the shadows. There are a number of ways we hope to alleviate both issues to address the needs of a wider range of users. For more novice drawers, being able to specify the category class would trim the database search and would likely lead to more relevant shadows even when the initial drawing has little resemblance to the class. For more expert users, an interface providing more control over shadow strength can be easily added. Also, an ability to draw negative strokes to discourage shadows containing those strokes would again provide finer control over the shadow guidance. We also want to explore the trade-offs between image database size and the shadows? efficacy. We have shown the ability to handle many thousands of images over a wide variety of categories. While more flexibility would be attained by a larger database for experts, the problems encountered by the novice drawers might be accentuated. More work is required to understand these trade-offs. There are also many more subtle issues such as the tension between guidance and freedom in how a drawing expresses an object?s essence versus getting all the proportions right. This will perhaps require exploring new scoring methods that allow for nonrigid transformations in the matching and shadow generation steps. We were encouraged to see that the users? personal drawing styles were maintained when using ShadowDraw. That said, more work is required to make the system truly aid in improving the overall aesthetic result. This will require a more nuanced understanding of the relationship between aesthetics and realism in drawings. If one could assemble a large database of artistic drawings, we may be able to use some of the technology we report to begin to learn such a relationship, but this is left for future work. In conclusion, we have found this area of investigation very exciting and look forward to extending the work in many directions. We hope this paper has expressed both the scope of the specific work reported on, as well as the many possible related avenues for future exploration.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n        ShadowDraw: Real-Time User Guidance for Freehand Drawing\n        ?\n        27:9\n      \n      \n        Acknowledgements\n        The authors would like to thank Ce Liu for many insightful discussions and help in shaping the work in this paper. We would also like to thank the anonymous reviewers for their many helpful comments. Finally, thanks to all the participants who were willing to draw as part of the user studies. Hopefully ShadowDraw will make them feel more comfortable picking up a pen to draw in the future.\n      \n      \n        References\n        \n          A RVO , J., AND N OVINS , K. 2000. Fluid sketches: Continuous recognition and morphing of simple hand-drawn shapes. ACM UIST.\n          B ALLARD , D. 1981. Generalizing the hough transform to detect arbitray shapes. In Pattern Recognition, vol. 13, 111?122.\n          B EAUDOT , W., AND M ULLEN , K. 2003. How long range is contour integration in human color vision. In Visual Neuroscience, vol. 15, 51?64.\n          B HAT , P., Z ITNICK , C. L., C OHEN , M., AND C URLESS , B. 2009. Gradientshop: A gradient-domain optimization framework for image and video filtering. TOG.\n          C ANNY , J. 1986. A computational approach to edge detection. In TPAMI, vol. 8, 679?698.\n          C AO , Y., W ANG , H., W ANG , C., L I , Z., Z HANG , L., AND Z HANG , L. 2010. Mindfinder: Finding images by sketching. In ACM Multimedia International Conference.\n          C ARSON , C., B ELONGIE , S., G REENSPAN , H., AND M ALIK , J. 2002. Blobworld: Image segmentation using expectationmaximization and its application to image querying. In TPAMI, vol. 24, 1026?1038.\n          C HALECHALE , A., N AGHDY , G., AND M ERTINS , A. 2005. Sketch-based image matching using angular partitioning. IEEE Trans. Systems, Man, and Cybernetics.\n          C HAUDHURI , S., AND K OLTUN , V. 2010. Data-driven suggestions for creativity support in 3d modeling. ACM SIGGRAPH ASIA.\n          C HEN , T., C HENG , M.-M., T AN , P., S HAMIR , A., AND H U , S.M. 2009. Sketch2photo: Internet image montage. ACM SIGGRAPH ASIA.\n          C HUM , O., P HILBIN , J., S IVIC , J., I SARD , M., AND Z ISSER MAN , A. 2007. Total recall: Automatic query expansion with a generative feature model for object retrieval. In CVPR.\n          C HUM , O., P HILBIN , J., AND Z ISSERMAN , A. 2008. Near duplicate image detection: min-hash and tf-idf weighting. In BMVC.\n          C OLE , F., G OLOVINSKIY , A., L IMPAECHER , A., B ARROS , H. S., F INKELSTEIN , A., F UNKHOUSER , T., AND R USINKIEWICZ , S. 2008. Where do people draw lines? SIGGRAPH.\n          D ATTA , R., J OSHI , D., L I , J., AND W ANG , J. Z. 2008. Image retrieval: Ideas, influences, and trends of the new age. In ACM Computing Surveys, vol. 40, 1?60.\n          D IXON , D., P RASAD , M., AND H AMMOND , T. 2010. icandraw: Using sketch recognition and corrective feedback to assist a user in drawing human faces. ACM CHI.\n          E ITZ , M., H ILDEBRAND , K., B OUBEKEUR , T., AND A LEXA , M. 2009. Photosketch: A sketch based image query and compositing system. ACM SIGGRAPH Talk Program.\n          E LDER , J., AND G OLDBERG , R. 2001. Image editing in the contour domain. In TPAMI, vol. 23, 291?296.\n          F EI -F EI , L., F ERGUS , R., AND P ERONA , P. 2004. Learning generative visual models from few training examples: an incremental bayesian approach tested on 101 object categories. In Workshop on Generative-Model Based Vision, CVPR.\n          G AVILAN , D., S AITO , S., AND N AKAJIMA , M. 2007. Sketch-tocollage. ACM SIGGRAPH Posters.\n          H AYS , J., AND E FROS , A. A. 2007. Scene completion using millions of photographs. ACM SIGGRAPH.\n          H U , R., B ARNARD , M., AND C OLLOMOSSE , J. 2010. Gradient field descriptor for sketch based retrieval and localization. ICIP.\n          I GARASHI , T., AND H UGHES , J. F. 2001. A suggestive interface for 3d drawing. ACM UIST.\n          I GARASHI , T., M ATSUOKA , S., AND T ANAKA , H. 1999. Teddy: A sketching interface for 3d freeform design. ACM SIGGRAPH.\n          J ACOBS , C. E., F INKELSTEIN , A., AND S ALESIN , D. H. 1995. Fast multiresolution image querying. In SIGGRAPH.\n          L EE , D. C., K E , Q., AND I SARD , M. 2010. Partition min-hash for partial duplicate image discovery. In ECCV.\n          L OWE , D. G. 2004. Distinctive image features from scale-invariant keypoints. IJCV.\n          N ISTER , D., AND S TEWENIUS , H. 2006. Scalable recognition with a vocabulary tree. In CVPR.\n          S IVIC , J., K ANEVA , B., T ORRALBA , A., A VIDAN , S., AND F REEMAN , W. 2008. Creating and exploring a large photorealistic virtual space. In Workshop on Internet Vision, CVPR.\n          W INDER , S., H UA , G., AND B ROWN , M. 2009. Picking the best daisy. In CVPR.\n          W ITTEN , I. H., M OFFAT , A., AND B ELL , T. 1999. Managing Gigabytes: Compressing and Indexing Documents and Images. Morgan Kaufmann.\n          Z ITNICK , C. L. 2010. Binary coherent edge descriptors. In ECCV.\n        \n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 27, Publication date: July 2011.\n      \n    \n  ",
  "resources" : [ ]
}