{
  "uri" : "sig2012a-a157-rivers_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012a/a157-rivers_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Sculpting by Numbers",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Alec R.-Rivers",
      "name" : "Alec R.",
      "surname" : "Rivers"
    }, {
      "uri" : "http://drinventor/Andrew-Adams",
      "name" : "Andrew",
      "surname" : "Adams"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "most", "people", "find", "challenge", "sculpt", "carve", "manually", "form", "precise", "shape", "we", "argue", "usually", "because", "lack", "manual", "dexterity", "average", "person", "able", "perform", "very", "precise", "manipulation", "rather", "because", "lack", "precise", "3d", "information", "can", "figure", "out", "what", "need", "do", "modify", "work", "progress", "order", "reach", "goal", "shape", "analogy", "can", "make", "task", "reproduce", "2d", "painting", "when", "give", "outline", "need", "only", "fill", "child?s", "color", "book", "paint-by-numbers", "kit", "even", "unskilled", "user", "can", "accurately", "reproduce", "complex", "painting", "challenge", "lie", "place", "paint", "canvas", "know", "where", "place", "motivate", "observation", "we", "present", "sculpt", "number", "method", "provide", "analogous", "guidance", "creation", "3d", "object", "which", "assist", "user", "make", "object", "precisely", "match", "shape", "target", "3d", "model", "we", "employ", "spatially-augmented", "reality", "approach", "-lrb-", "see", "e.g.", "raskar", "et", "al.", "-lsb-", "1998", "-rsb-", "Bimber", "Raskar", "-lsb-", "2005", "-rsb-", "overview", "spatially-augmented", "reality", "-rrb-", "which", "visual", "feedback", "illustrate", "discrepancy", "between", "work", "progress", "target", "3d", "shape", "approach", "first", "propose", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "approach", "projector-camera", "pair", "use", "scan", "object", "be", "create", "use", "structured", "light", "scan", "shape", "compare", "target", "3d", "model", "projector", "annotate", "object", "color", "indicate", "how", "object", "ought", "change", "match", "target", "user", "follow", "guidance", "adjust", "object", "rescan", "when", "necessary", "bring", "object", "closer", "target", "shape", "over", "time", "we", "propose", "method", "provide", "guidance", "illustrate", "depth", "disparity", "between", "current", "work", "target", "similar", "approach", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "well", "additional", "form", "guidance", "which", "we", "call", "edge", "guidance", "which", "aid", "reproduce", "high-frequency", "surface", "detail", "we", "demonstrate", "application", "reproduce", "exist", "physical", "object", "different", "scale", "use", "different", "material", "we", "further", "show", "how", "scene", "can", "repeatedly", "deform", "match", "sequence", "target", "3d", "model", "purpose", "stop-motion", "animation", "start", "from", "depth", "video", "record", "Kinect", "we", "produce", "claymation", "physically", "correct", "dynamics", "finally", "we", "present", "result", "user", "testing", "novice", "sculptor", "advent", "digital", "fabrication", "technology", "3d", "printer", "laser", "cutter", "CNC", "machine", "have", "make", "possible", "automatically", "produce", "2d", "3d", "object", "main", "tool", "use", "3d", "digital", "fabrication", "3d", "printer", "5-axis", "cnc", "machine", "Devices", "both", "kind", "typically", "expensive", "though", "recent", "effort", "DIY", "community", "have", "make", "low-priced", "entry-level", "tool", "available", "-lsb-", "Hokanson", "Reilly", "Kelly", "MakerBot", "Industries", "Drumm", "2011", "Sells", "et", "al.", "2009", "-rsb-", "3d", "printing", "cnc", "method", "highly", "accurate", "suffer", "from", "fabrication", "artifact", "limit", "size", "material", "object", "can", "produce", "multi-material", "fabrication", "device", "particularly", "challenging", "recent", "work", "computer", "graphic", "have", "therefore", "focus", "software", "technique", "enable", "alternative", "method", "fabrication", "we", "technique", "fall", "category", "two", "recent", "work", "propose", "method", "generate", "model", "slide", "planar", "slice", "approximate", "target", "3d", "model", "-lsb-", "Hildebrand", "et", "al.", "2012", "McCrae", "et", "al.", "2011", "-rsb-", "Lau", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "method", "convert", "3d", "furniture", "model", "fabricatable", "planar", "part", "other", "incorporate", "fabrication", "consideration", "directly", "modeling", "process", "so", "fabrication", "straightforward", "design", "Kilian", "et", "al.", "-lsb-", "2008", "-rsb-", "propose", "method", "design", "shape", "can", "produce", "folding", "single", "sheet", "material", "while", "Mori", "Igarashi", "-lsb-", "2007", "-rsb-", "propose", "interface", "design", "stuff", "animal", "we", "approach", "aim", "address", "limitation", "both", "manual", "craft", "current", "digital", "fabrication", "technology", "combine", "element", "each", "hybrid", "approach", "hybrid", "human-computer", "interface", "have", "be", "use", "before", "especially", "medical", "application", "example", "combine", "human", "surgeon?s", "ability", "plan", "motion", "react", "change", "condition", "robotic", "arm?s", "precision", "-lsb-", "kragic", "et", "al.", "2005", "Mako", "Surgical", "-rsb-", "interestingly", "approach", "typically", "human", "do", "planning", "machine", "do", "execution", "while", "we", "approach", "reverse", "Rivers", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "hybrid", "approach", "digital", "fabrication", "2d", "shape", "which", "user", "coarsely", "position", "tool", "can", "automatically", "adjust", "its", "own", "position", "keep", "exactly", "digital", "plan", "we", "approach", "share", "philosophical", "goal", "enable", "digitallyguided", "manual", "fabrication", "apply", "3d", "shape", "work", "just", "guide", "user", "without", "automatic", "error", "correction", "hybrid", "manual-automatic", "method", "have", "also", "be", "use", "2d", "design", "painter", "have", "use", "photograph", "camera", "obscura", "reference", "-lsb-", "Coke", "1964", "Pennsylvania", "Academy", "fine", "art", "et", "al.", "1994", "-rsb-", "Flagg", "Rehg", "-lsb-", "Flagg", "Rehg", "2006", "-rsb-", "present", "ProjectorGuided", "Painting", "system", "digitally", "guide", "artist", "engage", "reproduce", "2d", "painting", "support", "artist", "build", "painting", "out", "layer", "include", "show", "how", "orient", "brush", "certain", "stage", "explicitly", "help", "task", "mix", "paint", "achieve", "desire", "color", "Dixon", "et", "al.", "-lsb-", "2010", "-rsb-", "propose", "hybrid", "method", "task", "sketch", "human", "face", "from", "reference", "photograph", "which", "user", "guide", "automatic", "feedback", "guidance", "include", "high-level", "sketch", "principle", "each", "part", "face", "well", "specific", "feedback", "how", "adjust", "freehand", "drawing", "more", "closely", "match", "shape", "reference", "image", "shadowdraw", "-lsb-", "Lee", "et", "al.", "2011", "-rsb-", "propose", "more", "open-ended", "system", "which", "user", "allow", "draw", "freely", "while", "shadow", "image", "suggestive", "contour", "update", "display", "behind", "sketch", "illustrate", "possible", "sketch", "completion", "while", "we", "interested", "challenge", "create", "physical", "sculpture", "from", "virtual", "model", "complementary", "challenge", "create", "virtual", "model", "use", "physical", "object", "input", "method", "have", "also", "be", "explore", "Sheng", "et", "al.", "-lsb-", "2006", "-rsb-", "present", "system", "which", "user", "whose", "finger", "be", "be", "track", "could", "manipulate", "deformable", "physical", "prop", "perform", "sculpt", "operation", "virtual", "3d", "model", "goal", "provide", "more", "natural", "hands-on", "modeling", "experience", "we", "approach", "guide", "sculpting", "involve", "scanning", "work", "progress", "project", "guidance", "annotate", "physical", "object", "3d", "scanning", "have", "be", "active", "area", "research", "many", "year", "variety", "optical", "method", "be", "propose", "see", "Chen", "et", "al.", "-lsb-", "2000", "-rsb-", "survey", "one", "oldest", "most", "robust", "method", "involve", "project", "sequence", "Gray", "code", "imaging", "result", "retrieve", "depth", "information", "-lsb-", "Inokuchi", "et", "al.", "1984", "Caspi", "et", "al.", "1998", "Rusinkiewicz", "et", "al.", "2002", "-rsb-", "method", "simple", "implement", "robust", "able", "capture", "full", "depth", "field", "quickly" ],
  "content" : "Most people find it challenging to sculpt, carve or manually form a precise shape. We argue that this is usually not because they lack manual dexterity ? the average person is able to perform very precise manipulations ? but rather because they lack precise 3D information, and cannot figure out what needs to be done to modify a work in progress in order to reach a goal shape. An analogy can be made to the task of reproducing a 2D painting: when given outlines that need only be filled in, as in a child?s coloring book or a paint-by-numbers kit, even an unskilled user can accurately reproduce a complex painting; the challenge lies not in placing paint on the canvas but in knowing where to place it. Motivated by this observation, we present Sculpting by Numbers, a method to provide analogous guidance for the creation of 3D objects, which assists a user in making an object that precisely matches the shape of a target 3D model. We employ a spatially-augmented reality approach (see e.g. Raskar et al. [1998] or Bimber and Raskar [2005] for an overview of spatially-augmented reality), in which visual feedback illustrates the discrepancy between a work in progress and a target 3D shape. This approach was first proposed by Skeels and Rehg [2007]. In this approach, a projector-camera pair is used to scan the object being created using structured light. The scanned shape is compared with the target 3D model, and the projector then annotates the object with colors that indicate how the object ought to be changed to match the target. The user follows this guidance to adjust the object and rescans when necessary, bringing the object closer to the target shape over time. Our proposed method provides guidance that illustrates depth disparities between the current work and the target, similar to the approach of Skeels and Rehg [2007], as well as an additional form of guidance, which we call edge guidance, which aids in reproducing high-frequency surface details. We demonstrate an application to reproducing an existing physical object at a different scale or using different materials. We further show how a scene can be repeatedly deformed to match a sequence of target 3D models for the purpose of stop-motion animation. Starting from a depth video recorded with a Kinect, we produce claymation with physically correct dynamics. Finally, we present results of user testing with novice sculptors. The advent of digital fabrication technologies, such as 3D printers, laser cutters, and CNC machines, has made it possible to automatically produce 2D and 3D objects. The main tools used in 3D digital fabrication are 3D printers and 5-axis CNC machines. Devices of both kinds are typically expensive, though recent efforts in the DIY community have made low-priced entry-level tools available [Hokanson and Reilly ; Kelly ; MakerBot Industries ; Drumm 2011 ; Sells et al. 2009]. 3D printing and CNC methods are highly accurate, but suffer from fabrication artifacts, and are limited in the size and materials of the object they can produce. Multi-material fabrication with these devices is particularly challenging. Recent work in computer graphics has therefore focused on software techniques that enable alternative methods for fabrication, and our technique falls into this category. Two recent works propose methods to generate models of sliding planar slices that approximate target 3D models [Hildebrand et al. 2012; McCrae et al. 2011]. Lau et al. [2011] propose a method to convert 3D furniture models into fabricatable planar parts. Others incorporate fabrication considerations directly into the modeling process, so that fabrication is straightforward by design: Kilian et al. [2008] propose a method for designing shapes that can be produced by folding a single sheet of material, while Mori and Igarashi [2007] propose an interface for designing stuffed animals. Our approach aims to address the limitations of both manual crafting and current digital fabrication technologies by combining elements of each in a hybrid approach. Hybrid human-computer interfaces have been used before, especially in medical applications, for example to combine a human surgeon?s ability to plan motions and react to changing conditions with a robotic arm?s precision [Kragic et al. 2005; Mako Surgical ]. Interestingly, in these approaches, it is typically the human that does the planning and the machine that does the execution, while in our approach it is the reverse. Rivers et al. [2012] propose a hybrid approach to digital fabrication of 2D shapes in which a user coarsely positions a tool that can then automatically adjust its own position to keep it exactly on a digital plan; our approach shares the philosophical goal of enabling digitallyguided manual fabrication, but applies to 3D shapes, and works by just guiding a user without automatic error correction. Hybrid manual-automatic methods have also been used for 2D design. Painters have used photographs or a camera obscura as references [Coke 1964; Pennsylvania Academy of the Fine Arts et al. 1994]. Flagg and Rehg [Flagg and Rehg 2006] presented ProjectorGuided Painting, a system for digitally guiding an artist engaged in reproducing a 2D painting. They support the artist in building a painting out of layers, including showing how to orient the brush at certain stages, and explicitly help with the task of mixing paint to achieve a desired color. Dixon et al. [2010] proposed a hybrid method for the task of sketching a human face from a reference photograph in which the user is guided by automatic feedback. The guidance included high-level sketching principles for each part of the face as well as specific feedback on how to adjust the freehand drawings to more closely match the shape of the reference image. ShadowDraw [Lee et al. 2011] proposed a more open-ended system in which a user is allowed to draw freely while a ?shadow image? of suggestive contours is updated and displayed behind the sketch to illustrate possible sketch completions. While we are interested in the challenge of creating physical sculptures from a virtual model, the complementary challenge of creating virtual models using physical objects as an input method has also been explored. Sheng et al. [2006] presented a system in which a user whose fingers were being tracked could manipulate deformable physical props to perform sculpting operations on a virtual 3D model, with the goal of providing a more natural, hands-on modeling experience. Our approach to guided sculpting involves scanning the work in progress and projecting guidance that annotates the physical object. 3D scanning has been an active area of research for many years, with a variety of optical methods being proposed; see Chen et al. [2000] for a survey. One of the oldest and most robust methods involves projecting a sequence of Gray codes and imaging the result to retrieve depth information [Inokuchi et al. 1984; Caspi et al. 1998; Rusinkiewicz et al. 2002]. This method is simple to implement, robust, and able to capture a full depth field quickly.",
  "resources" : [ ]
}