{
  "uri" : "sig2013-a58-vangorp_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a58-vangorp_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Perception of Perspective Distortions in Image-Based Rendering",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Peter-Vangorp",
      "name" : "Peter",
      "surname" : "Vangorp"
    }, {
      "uri" : "http://drinventor/Christian-Richardt",
      "name" : "Christian",
      "surname" : "Richardt"
    }, {
      "uri" : "http://drinventor/Emily A.-Cooper",
      "name" : "Emily A.",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Gaurav-Chaurasia",
      "name" : "Gaurav",
      "surname" : "Chaurasia"
    }, {
      "uri" : "http://drinventor/Martin S.-Banks",
      "name" : "Martin S.",
      "surname" : "Banks"
    }, {
      "uri" : "http://drinventor/George-Drettakis",
      "name" : "George",
      "surname" : "Drettakis"
    } ]
  },
  "bagOfWords" : [ "image-based", "rendering", "-lrb-", "ibr", "-rrb-", "provide", "realistic", "3d", "imagery", "few", "photograph", "input", "-lsb-", "Shum", "et", "al.", "2006", "-rsb-", "thus", "avoid", "manual", "time-consuming", "content", "creation", "pipeline", "traditional", "computer", "graphic", "recent", "street-level", "navigation", "system", "-lrb-", "e.g.", "Google", "Maps", "Street", "ViewTM", "-lsb-", "Vincent", "2007", "-rsb-", "Bing", "Maps", "StreetsideTM", "Mappy", "Urban", "DiveTM", "-rrb-", "use", "simple", "form", "ibr", "consist", "panorama", "piecewise", "planar", "approximation", "ground", "building", "fa", "ade", "despite", "simplicity", "system", "create", "reasonably", "compelling", "3d", "experience", "we", "refer", "street-level", "ibr", "however", "result", "image", "only", "correct", "when", "view", "from", "where", "original", "photograph", "be", "take", "when", "move", "away", "from", "point", "distortion", "can", "become", "quite", "large", "show", "fig.", "-lrb-", "-rrb-", "because", "system", "typically", "restrict", "viewer", "near", "one", "capture", "point", "two", "term", "important", "understand", "image", "create", "street-level", "ibr", "user", "perception", "those", "image", "image", "distortion", "refer", "retinal", "image", "-lrb-", "picture", "viewing", "-rrb-", "same", "image", "create", "when", "view", "original", "3d", "scene", "distortion", "can", "occur", "because", "display", "image", "distorted", "and/or", "because", "viewer", "position", "center", "projection", "-lrb-", "cop", "-rrb-", "perceptual", "outcome", "refer", "viewer?s", "perception", "derive", "from", "retinal", "image", "key", "insight", "we", "work", "make", "link", "between", "distortion", "street-level", "ibr", "what", "study", "human", "vision", "tell", "we", "about", "result", "perceptual", "outcome", "vision", "science", "literature", "provide", "two", "useful", "hypothesis", "concern", "perception", "picture", "scene", "hypothesis", "retinal", "hypothesis", "scene", "hypothesis", "state", "viewer", "compensate", "incorrect", "view", "position", "so", "perceptual", "outcome", "much", "closer", "original", "3d", "scene", "than", "dictate", "distorted", "retinal", "image", "understand", "note", "viewer", "must", "position", "picture?s", "center", "projection", "retinal", "image", "faithful", "copy", "image", "would", "create", "view", "original", "3d", "scene", "retinal", "hypothesis", "other", "hand", "state", "viewer", "do", "compensate", "incorrect", "position", "rather", "perceptual", "outcome", "dictate", "distorted", "retinal", "image", "Vision", "science", "literature", "review", "Sec", "show", "each", "hypothesis", "valid", "different", "context", "situation", "more", "complex", "street-level", "ibr", "system", "image", "first", "capture", "photograph", "fa", "ade", "from", "give", "position", "image", "project", "onto", "simplify", "geometry", "like", "3d", "polygon", "3d", "polygon", "turn", "view", "from", "new", "camera", "project", "2d", "form", "final", "image", "distortion", "occur", "because", "new", "camera", "have", "different", "cop", "than", "one", "associate", "original", "photograph", "situation", "have", "be", "study", "extensively", "vision", "science", "similar", "case", "arise", "picture", "within", "picture", "-lsb-", "Pirenne", "1970", "-rsb-", "when", "photograph", "within", "photograph", "slant", "two", "cop", "one", "photograph", "3d", "scene", "-lrb-", "cop", "usually", "central", "surface", "normal", "-rrb-", "one", "slant", "photograph", "within", "scene", "-lrb-", "cop", "central", "surface", "normal", "-rrb-", "viewer", "can", "compensate", "incorrect", "position", "respect", "first", "cop", "generally", "unable", "compensate", "incorrect", "position", "respect", "second", "cop", "thus", "compensation", "occur", "picture", "create", "one", "camera", "content", "create", "second", "camera", "street-level", "ibr", "create", "both", "scenario", "we", "use", "concept", "from", "vision", "science", "understand", "how", "image", "distortion", "create", "street-level", "ibr", "system", "perceive", "thereby", "determine", "how", "best", "create", "imagery", "without", "objectionable", "perceptual", "distortion", "we", "make", "three", "primary", "contribution", "we", "modify", "retinal", "hypothesis", "extend", "street-level", "ibr", "quantitative", "model", "emerge", "from", "modification", "use", "perspective", "information", "generate", "predict", "3d", "percept", "two", "perceptual", "experiment", "we", "determine", "how", "well", "extend", "retinal", "scene", "hypothesis", "predict", "perceptual", "outcome", "we", "find", "outcome", "fall", "in-between", "two", "prediction", "those", "outcome", "very", "consistent", "viewer", "rating", "how", "acceptable", "different", "view", "we", "develop", "predictive", "model", "perceptual", "distortion", "streetlevel", "ibr", "use", "present", "guideline", "acceptability", "novel", "view", "capture", "camera", "density", "we", "also", "perform", "validation", "study", "we", "prediction", "illustrate", "use", "guide", "user", "street-level", "ibr", "so", "image", "novel", "view", "acceptable", "during", "navigation", "perception", "artifact", "image-based", "rendering", "texture", "mapping", "have", "be", "widely", "use", "represent", "surface", "color", "also", "shape", "detail", "too", "small", "model", "geometrically", "example", "projective", "texture", "mapping", "urban", "visualization", "-lsb-", "Debevec", "et", "al.", "1998", "-rsb-", "case", "shape", "detail", "do", "change", "when", "simulated", "-lrb-", "virtual", "-rrb-", "viewpoint", "change", "would", "when", "change", "viewpoint", "original", "scene", "consequence", "image", "become", "distorted", "view-dependent", "texture", "mapping", "-lsb-", "Debevec", "et", "al.", "1998", "-rsb-", "unstructured", "lumigraph", "render", "-lsb-", "Buehler", "et", "al.", "2001", "-rsb-", "provide", "way", "choose", "among", "blend", "between", "multiple", "texture", "be", "capture", "from", "different", "viewpoint", "practice", "texture", "viewpoint", "sample", "densely", "so", "novel", "simulated", "viewpoint", "can", "still", "create", "image", "distortion", "angle", "distortion", "blend", "artifact", "quite", "objectionable", "previous", "work", "perception", "image", "distortion", "ibr", "have", "focus", "transition", "artifact", "occur", "when", "simulated", "viewpoint", "move", "between", "input", "viewpoint", "occlusion", "boundary", "particularly", "synchronization", "camera", "motion", "key", "determinant", "perceive", "quality", "transition", "-lsb-", "Morvan", "O?Sullivan", "2009", "-rsb-", "transition", "more", "acceptable", "when", "exact", "edge", "correspondence", "coherent", "motion", "homogeneous", "region", "provide", "-lsb-", "Stich", "et", "al.", "2011", "-rsb-", "image", "simulated", "novel", "viewpoint", "more", "acceptable", "when", "transition", "faster", "fewer", "transition", "artifact", "distance", "viewpoint", "use", "texture", "small", "-lsb-", "Vangorp", "et", "al.", "2011", "-rsb-", "geometric", "distortion", "due", "view", "distance", "inconsistent", "field", "view", "image", "have", "be", "study", "context", "traditional", "render", "rather", "than", "ibr", "confirm", "observer", "make", "most", "accurate", "geometric", "estimate", "scene", "when", "close", "consistent", "viewpoint", "-lsb-", "Steinicke", "et", "al.", "2011", "-rsb-", "despite", "advance", "we", "understanding", "previous", "work", "have", "provide", "explanation", "why", "some", "image", "distortion", "acceptable", "while", "other", "we", "claim", "study", "human", "picture", "perception", "provide", "datum", "concept", "prove", "useful", "better", "implementation", "street-level", "ibr", "system", "Vision", "Science", "Literature", "Picture", "Perception", "Pictures", "which", "define", "2d", "representation", "3d", "scene", "base", "perspective", "projection", "can", "yield", "compelling", "impression", "3d", "structure", "estimate", "3d", "structure", "human", "use", "variety", "depth", "cue", "potentially", "available", "picture", "cue", "base", "perspective", "lighting", "atmospheric", "effect", "triangulation", "here", "we", "focus", "perspective-based", "cue", "because", "most", "affect", "image", "distortion", "occur", "street-level", "ibr", "perspective-based", "cue", "can", "use", "determine", "3d", "layout", "scene", "up", "scale", "factor", "instance", "when", "photograph", "take", "slant", "rectangle", "objectively", "parallel", "edge", "rectangle", "image", "non-parallel", "one", "extend", "line", "until", "intersect", "vanish", "point", "angle", "between", "line", "from", "viewer", "vanish", "point", "line", "from", "viewer", "rectangle", "specify", "slant", "tilt", "rectangle", "-lsb-", "Sedgwick", "1991", "-rsb-", "when", "perspectively", "correct", "picture", "view", "from", "cop", "people", "quite", "accurate", "recover", "3d", "geometry", "original", "scene", "include", "slant", "surface", "scene", "-lsb-", "Smith", "Smith", "1961", "Cooper", "et", "al.", "2012", "-rsb-", "viewer?s", "eye", "offset", "from", "cop", "perspective-based", "cue", "longer", "specify", "original", "3d", "scene", "instead", "specify", "different", "distorted", "scene", "Research", "picture", "perception", "have", "be", "focus", "how", "those", "distortion", "affect", "perception", "3d", "shape", "put", "forward", "two", "hypothesis", "define", "Sec", "scene", "retinal", "hypothesis", "some", "situation", "experimental", "evidence", "favor", "scene", "hypothesis", "example", "when", "viewer", "leave", "right", "cop", "view", "picture", "its", "frame", "both", "eye", "compensate", "incorrect", "viewing", "position", "perceive", "3d", "structure", "reasonably", "accurately", "-lsb-", "Rosinski", "et", "al.", "1980", "Vishwanath", "et", "al.", "2005", "-rsb-", "some", "have", "claim", "compensation", "base", "use", "familiar", "shape", "cube", "-lsb-", "Perkins", "1972", "Yang", "Kubovy", "1999", "-rsb-", "while", "other", "have", "claim", "base", "measurement", "orientation", "surface", "picture", "-lsb-", "Wallach", "Marshall", "1986", "Vishwanath", "et", "al.", "2005", "-rsb-", "other", "situation", "experimental", "evidence", "favor", "retinal", "hypothesis", "even", "small", "off-axis", "displacement", "-lsb-", "bank", "et", "al.", "2009", "-rsb-", "when", "slant", "picture", "object", "nearly", "perpendicular", "picture", "surface", "little", "compensation", "off-axis", "viewing", "occur", "-lsb-", "Goldstein", "1987", "Todorovi", "2008", "-rsb-", "when", "viewer", "too", "close", "too", "far", "from", "picture", "do", "compensate", "induce", "image", "distortion", "therefore", "perceive", "3d", "structure", "incorrectly", "-lsb-", "Adams", "1972", "Cooper", "et", "al.", "2012", "Lumsden", "1983", "Todorovi", "2009", "-rsb-", "thus", "scene", "retinal", "hypothesis", "account", "best", "perceptual", "outcome", "different", "situation", "we", "goal", "study", "how", "well", "hypothesis", "predict", "perceptual", "distortion", "street-level", "ibr", "street-level", "image-based", "Viewing", "another", "key", "goal", "use", "we", "finding", "provide", "practical", "guideline", "application", "setting", "we", "focus", "simplify", "image-based", "setup", "which", "akin", "exist", "visualization", "street-level", "imagery", "e.g.", "Google", "Maps", "Street", "ViewTM", "Bing", "Maps", "StreetsideTM", "Mappy", "Urban", "DiveTM", "while", "exact", "detail", "system", "always", "available", "use", "panoramic", "image", "capture", "discrete", "point", "along", "path", "render", "use", "equivalent", "view-dependent", "texture", "mapping", "-lsb-", "Debevec", "et", "al.", "1998", "-rsb-", "onto", "single", "planar", "proxy", "each", "fa", "ade", "similar", "technique", "most", "system", "transition", "between", "viewpoint", "occur", "fast", "blur", "mix", "between", "viewpoint", "possibly", "blend", "approach", "-lrb-", "e.g.", "akin", "Buehler", "et", "al.", "-lsb-", "2001", "-rsb-", "-rrb-", "we", "analysis", "also", "apply", "other", "type", "streetlevel", "ibr", "Microsoft", "PhotosynthTM", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "Street", "Slide", "-lsb-", "Kopf", "et", "al.", "2010", "-rsb-", "even", "multi-perspective", "image", "-lsb-", "Yu", "et", "al.", "2010", "-rsb-", "long", "corner", "fa", "ade", "only", "deform", "perspective", "projection", "image", "deformation", "apply", "align", "stitch", "image", "panorama", "paper", "we", "assume", "fa", "ade", "capture", "similar", "manner", "use", "panorama", "-lrb-", "wide-angle", "image", "-rrb-", "discrete", "point", "along", "path", "reproject", "onto", "planar", "proxy", "fa", "ade", "ground" ],
  "content" : "Image-based rendering (IBR) provides realistic 3D imagery with a few photographs as input [Shum et al. 2006], thus avoiding the manual and time-consuming content creation pipeline of traditional computer graphics. Recent street-level navigation systems (e.g., Google Maps Street ViewTM [Vincent 2007], Bing Maps StreetsideTM, or Mappy Urban DiveTM) use a simple form of IBR consisting of a panorama and piecewise planar approximations of the ground and building fa  ?ades. Despite their simplicity, such systems create reasonably compelling 3D experiences: we will refer to these as street-level IBR. However, the resulting images are only correct when viewed from where the original photographs were taken; when moving away from this point, distortions can become quite large, as shown in Fig. 1(b) . Because of this, such systems typically restrict viewers to be near one of the capture points. Two terms are important for understanding the images created in street-level IBR and users? perceptions of those images. Image distortion refers to retinal images (in picture viewing) that are not the same as the images created when viewing the original 3D scenes. Such distortions can occur because the displayed image is distorted and/or because the viewer is not positioned at the center of projection (COP). Perceptual outcome refers to the viewer?s perception derived from the retinal image. A key insight in our work is to make the link between distortions in street-level IBR and what studies of human vision tell us about the resulting perceptual outcomes. The vision science literature provides two useful hypotheses concerning the perception of pictures: the scene hypothesis and the retinal hypothesis. The scene hypothesis states that viewers compensate for incorrect viewing position, so the perceptual outcome is much closer to the original 3D scene than dictated by the distorted retinal image. To understand this, note that the viewer must be positioned at the picture?s center of projection for the retinal image to be a faithful copy of the image that would be created by viewing the original 3D scene. The retinal hypothesis, on the other hand, states that viewers do not compensate for incorrect position; rather the perceptual outcome is dictated by the distorted retinal image. Vision science literature, reviewed in Sec. 2, shows that each hypothesis is valid, but for different contexts. The situation is more complex in street-level IBR. In these systems, an image is first captured by photographing a fa  ?ade from a given position. This image is then projected onto simplified geometry, like a 3D polygon. The 3D polygon is in turn viewed from a new camera, and projected into 2D to form the final image. Distortions occur because the new camera has a different COP than the one associated with the original photograph. Such situations have not been studied extensively in vision science, but a similar case arises in pictures within pictures [Pirenne 1970]. When a photograph within the photograph is slanted, there are two COPs, one for the photographed 3D scene (that COP is usually on the central surface normal) and one for the slanted photograph within the scene (that COP is not on the central surface normal). Viewers can compensate for their incorrect position with respect to the first COP, but are generally unable to compensate for incorrect position with respect to the second COP. Thus, compensation occurs for pictures created with one camera and not for content created by a second camera. Street-level IBR creates both of these scenarios. We will use concepts from vision science to understand how image distortions created in street-level IBR systems are perceived and thereby to determine how best to create such imagery without objectionable perceptual distortions. We make three primary contributions: ? We modify the retinal hypothesis to extend it to street-level IBR. The quantitative model that emerges from this modification uses perspective information to generate predicted 3D percepts. ? In two perceptual experiments, we determine how well the extended retinal and scene hypotheses predict perceptual outcomes. We find that outcomes fall in-between the two predictions and that those outcomes are very consistent with viewers? ratings of how acceptable different views are. ? We develop a predictive model for perceptual distortions in streetlevel IBR, and use it to present guidelines for acceptability of novel views and capture camera density. We also perform a validation study of our predictions, and illustrate their use to guide users of street-level IBR so that images of novel views are acceptable during navigation. Perception of Artifacts in Image-based Rendering Texture mapping has been widely used to represent surface color and also shape details that are too small to model geometrically. An example is projective texture mapping for urban visualizations [Debevec et al. 1998]. In such cases, the shape details do not change when the simulated (or virtual) viewpoint changes as they would when changing viewpoint in the original scene; as a consequence, the image becomes distorted. View-dependent texture mapping [Debevec et al. 1998] and unstructured Lumigraph rendering [Buehler et al. 2001] provide ways to choose among or blend between multiple textures that were captured from different viewpoints. In practice, the texture viewpoints are not sampled densely, so novel simulated viewpoints can still create image distortions ? angle distortions and blending artifacts ? that are quite objectionable. Previous work on the perception of image distortions in IBR has focused on the transition artifacts that occur when the simulated viewpoint moves between input viewpoints. Occlusion boundaries, particularly their synchronization with camera motion, are key determinants of the perceived quality of transitions [Morvan and O?Sullivan 2009]. Transitions are more acceptable when exact edge correspondences, coherent motion and homogeneous regions are provided [Stich et al. 2011]. Images for simulated  novel viewpoints are more acceptable when the transitions are faster, there are fewer transition artifacts, and the distance to the viewpoint of the used texture is small [Vangorp et al. 2011]. Geometric distortions due to viewing distances inconsistent with the field of view of the image have been studied in the context of traditional rendering rather than IBR, confirming that observers make the most accurate geometric estimates of the scene when they are close to the consistent viewpoint [Steinicke et al. 2011]. Despite these advances in our understanding, previous work has not provided an explanation of why some image distortions are acceptable, while others are not. We claim that studies of human picture perception provide data and concepts that prove to be useful for a better implementation of street-level IBR systems. Vision Science Literature on Picture Perception Pictures, which are defined as 2D representations of 3D scenes based on perspective projection, can yield compelling impressions of 3D structure. To estimate 3D structure, humans use a variety of depth cues that are potentially available in pictures: cues based on perspective, lighting, atmospheric effects, and triangulation. Here we focus on perspective-based cues because they are most affected by the image distortions that occur in street-level IBR. Perspective-based cues can be used to determine the 3D layout of a scene up to a scale factor. For instance, when a photograph is taken of a slanted rectangle, the objectively parallel edges of the rectangle are imaged as non-parallel. If one extends the lines until they intersect at the vanishing point, the angle between a line from the viewer to the vanishing point and a line from the viewer to the rectangle specifies the slant and tilt of the rectangle [Sedgwick 1991]. When a perspectively correct picture is viewed from the COP, people are quite accurate at recovering the 3D geometry of the original scene, including the slants of surfaces in that scene [Smith and Smith 1961, Cooper et al. 2012]. If the viewer?s eye is offset from the COP, perspective-based cues no longer specify the original 3D scene; instead, they specify a different, distorted scene. Research in picture perception has been focused on how those distortions affect the perception of 3D shape, and puts forward the two hypotheses defined in Sec. 1: the scene and retinal hypotheses. In some situations, the experimental evidence favors the scene hypothesis. For example, when viewers are left or right of the COP and view the picture and its frame with both eyes, they compensate for their incorrect viewing position and perceive 3D structure reasonably accurately [Rosinski et al. 1980, Vishwanath et al. 2005]. Some have claimed that this compensation is based on the use of familiar shapes, such as cubes [Perkins 1972, Yang and Kubovy 1999] while others have claimed that it is based on measurement of the orientation of the surface of the picture [Wallach and Marshall 1986, Vishwanath et al. 2005]. In other situations, the experimental evidence favors the retinal hypothesis, even for small off-axis displacements [Banks et al. 2009]. When the slant of a pictured object is nearly perpendicular to the picture surface, little compensation for off-axis viewing occurs [Goldstein 1987, Todorovi? 2008]. When viewers are too close to or too far from the picture, they do not compensate for the induced image distortions and therefore perceive 3D structure incorrectly [Adams 1972, Cooper et al. 2012, Lumsden 1983, Todorovi? 2009]. Thus the scene and retinal hypotheses account best for perceptual outcomes in different situations. Our goal is to study how well these hypotheses predict perceptual distortions in street-level IBR. Street-level Image-based Viewing Another key goal is to use our findings to provide practical guidelines in an application setting. We will focus on a simplified image-based setup which is akin to existing visualizations of street-level imagery, e.g., Google Maps Street ViewTM, Bing Maps StreetsideTM and Mappy Urban DiveTM. While exact details of these systems are not always available, they use panoramic images captured at discrete points along a path, and rendered using the equivalent of view-dependent texture mapping [Debevec et al. 1998] onto a single planar proxy for each fa  ?ade, or a similar technique. In most of these systems, transitions between viewpoints occur as a fast blurred mix between the viewpoints, possibly with a blending approach (e.g., akin to Buehler et al. [2001]). Our analysis also applies to other types of streetlevel IBR such as Microsoft PhotosynthTM [Snavely et al. 2006], Street Slide [Kopf et al. 2010], and even multi-perspective images [Yu et al. 2010] as long as corners in fa  ?ades are only deformed by perspective projections and not by the image deformations applied to align and stitch images into panoramas. In this paper, we assume fa  ?ades are captured in a similar manner: using panoramas (or wide-angle images) at discrete points along a path and reprojected onto a planar proxy for the fa  ?ade and ground.",
  "resources" : [ ]
}