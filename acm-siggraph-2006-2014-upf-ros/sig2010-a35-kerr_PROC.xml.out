{
  "uri" : "sig2010-a35-kerr_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2010/a35-kerr_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Toward Evaluating Material Design Interface Paradigms for Novice Users",
    "published" : "2010",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/William B.-Kerr",
      "name" : "William B.",
      "surname" : "Kerr"
    }, {
      "uri" : "http://drinventor/Fabio-Pellacini",
      "name" : "Fabio",
      "surname" : "Pellacini"
    } ]
  },
  "bagOfWords" : [ "fig.", "show", "example", "output", "from", "we", "subject", "use", "each", "interface", "first", "six", "match", "trial", "subject", "ask", "match", "target", "brdf", "closely", "possible", "second", "three", "open", "trial", "subject", "use", "own", "creativity", "design", "brdf", "base", "suggestive", "image", "twenty", "subject", "spend", "roughly", "one", "hour", "each", "three", "interface", "within", "boundary", "we", "study", "performance", "scale", "well", "material", "complexity", "degree", "freedom", "however", "some", "user", "do", "poorly", "when", "multiple", "brdf", "interact", "spatially-varying", "way", "within", "boundary", "we", "study", "introduction", "color", "roughly", "double", "time", "take", "reach", "goal", "reduce", "quality", "result", "similar", "factor", "error", "space", "any", "user", "study", "we", "observation", "only", "strictly", "apply", "within", "boundary", "test", "case", "additionally", "we", "believe", "principle", "use", "design", "we", "study", "can", "employ", "evaluate", "additional", "material", "design", "task", "move", "toward", "comprehensive", "evaluation", "material", "design", "interface", "Analytic", "BRDF", "model", "physical", "slider", "Perceptual", "Sliders", "Pellacini", "et", "al.", "-lsb-", "2000", "-rsb-", "develop", "perceptual", "parameterization", "Ward", "BRDF", "model", "through", "psychophysical", "experiment", "since", "perceptual", "parameterization", "exist", "all", "brdf", "use", "study", "we", "develop", "perceptually-inspired", "parameterization", "base", "-lsb-", "Pellacini", "et", "al.", "2000", "-rsb-", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "Sec", "image", "Navigation", "other", "interface", "we", "exclude", "interface", "type", "because", "unclear", "how", "extend", "support", "texture", "variation", "robustly", "brdf", "can", "also", "define", "arbitrary", "curve", "over", "angular", "parameterization", "use", "-lsb-", "Lawrence", "et", "al.", "2006", "-rsb-", "three", "editing", "task", "normally", "perform", "spatially-varying", "brdf", "change", "spatial", "pattern", "-lrb-", "texture", "painting", "synthesis", "-rrb-", "select", "region", "similar", "appearance", "-lrb-", "e.g.", "use", "-lsb-", "Pellacini", "Lawrence", "2007", "-rsb-", "-rrb-", "alter", "brdf", "select", "region", "we", "represent", "spatially-varying", "brdf", "linear", "combination", "basis", "brdf", "spatially-varying", "weight", "-lsb-", "Lawrence", "et", "al.", "2006", "-rsb-", "where", "user", "edit", "parameter", "basis", "brdf", "we", "choose", "model", "since", "fit", "measure", "datum", "well", "appearance", "design", "study", "we", "follow", "approach", "we", "use", "-lsb-", "Kerr", "Pellacini", "2009", "-rsb-", "evaluation", "lighting", "design", "interface", "apply", "material", "editing", "domain", "we", "seek", "evaluate", "relative", "effectiveness", "different", "interface", "paradigm", "material", "design", "context", "design", "realistic", "material", "focus", "novice", "user", "specifically", "-lrb-", "-rrb-", "we", "want", "measure", "how", "efficiently", "user", "can", "perform", "specific", "material", "adjustment", "-lrb-", "-rrb-", "we", "want", "understand", "which", "interface", "paradigm", "provide", "better", "artistic", "exploration", "possible", "material", "variation", "Novice", "Users", "all", "subject", "rate", "experience", "level", "material", "design", "either", "scale", "from", "can", "consider", "novice", "reduce", "Complexity", "one", "hand", "we", "want", "achieve", "complex-enough", "material", "editing", "task", "ensure", "meaningful", "measurement", "other", "we", "want", "avoid", "bias", "datum", "ensure", "subject", "can", "successfully", "complete", "require", "task", "without", "incur", "too", "much", "fatigue", "work", "novice", "make", "triage", "even", "more", "necessary", "we", "simplify", "material", "editing", "task", "focus", "editing", "parameter", "analytic", "brdf", "while", "we", "include", "different", "brdf", "model", "we", "do", "ask", "subject", "select", "between", "different", "model", "during", "trial", "Materials", "we", "design", "task", "subject", "edit", "material", "represent", "isotropic", "Ward", "-lsb-", "1992", "-rsb-", "cook-torrance", "-lsb-", "1981", "-rsb-", "brdf", "-lrb-", "sec", "we", "investigate", "three", "variation", "model", "first", "we", "use", "achromatic", "material", "half", "trial", "color", "other", "half", "Third", "determine", "whether", "presence", "spatial", "variation", "affect", "design", "task", "we", "investigate", "editing", "spatially-varying", "brdf", "represent", "linear", "combination", "two", "basis", "brdf", "spatially-varying", "weight", "where", "user", "edit", "parameter", "basis", "brdf", "we", "choose", "model", "since", "fit", "measure", "datum", "well", "-lsb-", "Lawrence", "et", "al.", "2006", "-rsb-", "example", "each", "material", "type", "can", "find", "fig.", "natural", "illumination", "consider", "ideal", "material", "perception", "when", "only", "single", "image", "available", "-lsb-", "Dror", "et", "al.", "2001", "Fleming", "et", "al.", "2001", "-rsb-", "we", "use", "tone", "mapping", "equation", "image", "-lrb-", "intensity", "exposure", "-rrb-", "1/gamma", "gamma", "2.2", "subject", "have", "control", "over", "exposure", "gamma", "geometry", "we", "image", "consist", "sphere", "float", "space", "we", "use", "sphere", "avoid", "occlusion", "artifact", "glossy", "reflection", "cause", "compute", "direct", "illumination", "only", "we", "determine", "sphere", "shape", "best", "we", "purpose", "give", "we", "render", "limitation", "interface", "we", "compare", "three", "user", "interface", "physical", "slider", "perceptual", "slider", "image", "navigation", "implementation", "detail", "interface", "present", "Sec", "we", "ask", "subject", "perform", "two", "type", "material", "design", "task", "match", "trial", "allow", "we", "quantitatively", "measure", "user", "performance", "while", "provide", "clear", "goal", "subject", "who", "have", "never", "experienced", "material", "design", "before", "trial", "allow", "we", "observe", "how", "user", "artistically", "explore", "space", "possible", "material", "configuration", "more", "natural", "harder", "measure", "task", "we", "ask", "subject", "complete", "number", "trial", "during", "which", "all", "action", "record", "further", "analysis", "each", "subject", "perform", "all", "trial", "use", "all", "interface", "preparatory", "study", "open", "matching", "goal", "use", "final", "study", "be", "test", "ensure", "time", "limit", "be", "appropriate", "task", "could", "complete", "we", "perform", "six", "matching", "trial", "three", "open", "trial", "progressively", "increase", "number", "degree", "freedom", "material", "model", "start", "configuration", "goal", "configuration", "time", "limit", "each", "trial", "summarize", "fig.", "Fig.", "match", "trial", "goal", "configuration", "be", "take", "from", "parametric", "fit", "present", "-lsb-", "Ngan", "et", "al.", "2005", "-rsb-", "measure", "material", "-lsb-", "Matusik", "et", "al.", "2003", "-rsb-", "grayscale", "trial", "diffuse", "specular", "coefficient", "metallic", "blue", "white", "bball", "be", "desaturate", "grayscale", "trial", "goal", "model", "hand", "after", "rendering", "acrylic", "violet", "since", "fit", "unavailable", "Color", "trial", "use", "fit", "blue", "bball", "ch-ball-green-metallic", "respectively", "trial", "use", "fit", "white-bball", "metallic", "gold", "weight", "texture", "we", "vary", "match", "trial", "material", "complexity", "observe", "possible", "change", "user", "workflow", "interface", "effectiveness", "under", "condition", "open", "trial", "we", "select", "target", "image", "differ", "from", "workspace", "lighting", "environment", "vary", "content", "encourage", "free-form", "artistic", "exploration", "we", "choose", "one", "grayscale", "two", "color", "material", "goal", "object", "vary", "material", "property", "object", "same", "goal", "image", "share", "some", "material", "property", "keep", "objective", "from", "be", "completely", "unspecified", "same", "initial", "goal", "material", "configuration", "use", "all", "subject", "all", "interface", "each", "trial", "have", "fixed", "time", "limit", "subject", "can", "end", "trial", "sooner", "satisfy", "current", "result", "end", "each", "match", "trial", "subject", "rate", "accuracy", "matching", "scale", "questionnaire", "after", "perform", "all", "trial", "all", "interface", "subject", "complete", "questionnaire", "where", "rate", "each", "interface", "scale", "follow", "category", "-lrb-", "-rrb-", "natural", "way", "think", "about", "material", "editing", "-lrb-", "-rrb-", "preference", "match", "trial", "-lrb-", "-rrb-", "preference", "open", "trial", "-lrb-", "-rrb-", "overall", "preference", "subject", "also", "strictly", "rank", "interface", "each", "category", "immediately", "after", "finish", "trial", "each", "single", "interface", "subject", "ask", "leave", "free-form", "comment", "aspect", "each", "interface", "reproducibility", "we", "include", "copy", "questionnaire", "additional", "material", "procedure", "all", "subject", "have", "normal", "corrected-to-normal", "vision", "subject", "edit", "material", "about", "hour", "each", "ensure", "good", "statistical", "significance", "we", "test", "while", "keep", "fatigue", "low", "subject", "complete", "study", "three", "60-minute", "session", "one", "each", "interface", "we", "randomize", "order", "interface", "each", "subject", "before", "each", "session", "subject", "complete", "training", "phase", "become", "familiar", "specific", "interface", "we", "train", "each", "subject", "individually", "allow", "question", "accommodate", "each", "subject?s", "learn", "need", "instructor", "verify", "subject", "use", "each", "part", "interface", "answer", "subject", "question", "before", "proceed", "experiment", "subject", "use", "interface", "until", "he", "she", "feel", "comfortable", "once", "trial", "begin", "all", "user", "interface", "action", "record", "study", "conduct", "controlled", "lighting", "environment", "negligible", "ambient", "lighting", "simulate", "typical", "working", "condition", "artist", "maximize", "visibility", "screen", "we", "use", "24-inch", "Dell", "2407WFPb", "lcd", "display", "1280", "800", "resolution", "distance", "approximately", "foot", "from", "subject", "-lrb-", "monitor", "native", "resolution", "1900", "1200", "-rrb-", "all", "render", "image", "256x256", "pixel", "screen", "cover", "area", "square", "inch", "we", "use", "Intel", "2.8", "Ghz", "Core2", "Quad", "Q9550", "PC", "gb", "RAM", "NVidia", "GeForce", "9800", "gt", "graphic", "card", "section", "we", "discuss", "we", "implementation", "user", "interface", "include", "study", "reproducibility", "we", "include", "video", "supplemental", "material", "show", "each", "interface", "detail", "rendering", "we", "use", "real-time", "render", "algorithm", "-lsb-", "BenArtzi", "et", "al.", "2006", "-rsb-", "preview", "brdf", "edit", "under", "direct", "natural", "illumination", "we", "implementation", "render", "45f", "p", "256", "256", "pixel", "image", "sphere", "use", "study", "we", "consider", "add", "global", "illumination", "-lsb-", "Ben-Artzi", "et", "al.", "2008", "-rsb-", "decide", "potential", "artifact", "result", "from", "approximate", "brdf", "lower", "frequency", "might", "effect", "we", "measurement", "algorithm", "we", "use", "doesn?t", "allow", "roughness", "fresnel", "term", "CookTorrance", "BRDF", "model", "simultaneously", "modify", "take", "approximately", "0.6", "seconds", "switch", "between", "parameter", "we", "implementation", "brdf", "model", "we", "implementation", "we", "parameterize", "isotropic", "Ward", "BRDF", "where", "diffuse", "albedo", "energy", "specular", "component", "surface", "roughness", "angle", "between", "surface", "normal", "incoming", "outgoing", "halfangle", "respectively", "we", "parameterize", "Cook-Torrance", "BRDF", "ct", "follow", "-lsb-", "Ngan", "et", "al.", "2005", "-rsb-", "-lrb-", "-rrb-", "-lrb-", "cos", "-rrb-", "-lrb-", "tan", "-rrb-", "cos", "cos", "cos", "cos", "cos", "min", "cos", "cos", "where", "diffuse", "albedo", "energy", "specular", "component", "surface", "roughness", "fresnel", "reflectance", "direction", "orthogonal", "surface", "angle", "between", "outgoing", "half-vector", "direction", "some", "trial", "use", "brdf", "ww", "Ward", "lobe", "define", "where", "cos", "cos", "other", "select", "trial", "we", "use", "spatially-varying", "material", "Universal", "Interface", "Features", "all", "interface", "use", "same", "screen", "layout", "consist", "workspace", "window", "goal", "window", "rating", "button", "-lrb-", "fig.", "-rrb-", "undo", "key", "allow", "user", "walk", "back", "through", "unlimited", "number", "edit", "compensate", "fact", "material", "create", "use", "system", "may", "conserve", "energy", "warning", "indicator", "appear", "upper", "right", "corner", "user?s", "image", "when", "brdf", "energy", "conserving", "physical", "slider", "we", "use", "slider", "interface", "means", "which", "user", "set", "parameter", "brdf", "model", "e.g.", "diffuse", "albedo", "specular", "energy", "roughness", "fresnel", "term", "each", "user", "control", "parameter", "list", "slider", "bar", "next", "parameter", "can", "change", "click", "anywhere", "bar", "gradual", "change", "can", "see", "drag", "slider", "continuously", "across", "bar", "would", "ignore", "common", "color", "editing", "practice", "artificially", "handicap", "interface", "we", "use", "CIELAB", "luminance", "-lrb-", "-rrb-", "achromatic", "intensity", "saturation", "hue", "chromaticity", "-lsb-", "Fairchild", "1998", "-rsb-", "Perceptual", "Sliders", "perceptual", "parameterization", "differ", "from", "physical", "one", "both", "effect", "scale", "furthermore", "since", "all", "perceptual", "parameterization", "derive", "from", "achromatic", "datum", "we", "follow", "-lsb-", "Wills", "et", "al.", "2009", "Pellacini", "et", "al.", "2000", "-rsb-", "derive", "parameter", "grayscale", "diffuse", "specular", "component", "add", "hue", "saturation", "they", "we", "use", "same", "saturation", "hue", "control", "physical", "slider", "Slider", "control", "work", "same", "way", "physical", "slider", "modify", "perceptually-inspired", "parameter", "determine", "correct", "scaling", "each", "parameter", "axis", "brdf", "model?s", "configuration", "space", "we", "use", "image-based", "brdf", "difference", "metric", "from", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "since", "psychophysical", "datum", "have", "be", "publish", "range", "brdf", "parameter", "we", "investigate", "we", "scale", "we", "perceptually-inspired", "parameter", "equal", "step", "parameter", "yield", "step", "accord", "distance", "metric", "we", "include", "comparison", "metric", "we", "parameterization", "supplemental", "document", "all", "parameterization", "follow", "represent", "achromatically", "accord", "CIELAB", "luminance", "range", "-lsb-", "-rsb-", "Ward", "brdf", "we", "use", "parameterization", "from", "-lsb-", "Pellacini", "et", "al.", "2000", "-rsb-", "modify", "parameter", "where", "diffuse", "luminance", "gloss", "contrast", "gloss", "distinctness", "we", "raise", "power", "1/4", "because", "more", "closely", "match", "scale", "accord", "equation", "-lrb-", "-rrb-", "which", "valid", "larger", "range", "than", "original", "experiment", "cover", "-lsb-", "Pellacini", "et", "al.", "2000", "-rsb-", "trial", "use", "textured", "Ward", "simply", "have", "two", "instance", "perceptually", "inspire", "Ward", "parameter", "Cook-Torrance", "brdf", "we", "use", "follow", "parameterization", "where", "diffuse", "luminance", "gloss", "contrast", "gloss", "distinctness", "gloss", "sheen", "0.02", "minimum", "allow", "value", "when", "-lsb-", "-rsb-", "Cook-Torrance", "parameter", "similar", "Ward", "counterpart", "add", "set", "contrast", "specular", "component", "grazing", "angle", "while", "preserve", "its", "contrast", "non-grazing", "angle", "Ward", "brdf", "two", "lobe", "we", "use", "follow", "parameterization", "s1", "s2", "where", "diffuse", "luminance", "gloss", "contrast", "lobe", "blend", "parameter", "overall", "gloss", "distinctness", "haze", "parameter", "max", "maximum", "possible", "value", "normalization", "image", "Navigation", "we", "base", "we", "implementation", "image", "navigation", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "interface", "consist", "series", "tab", "reveal", "different", "image", "array", "some", "tab", "show", "variation", "material", "model", "parameter", "along", "two", "axis", "while", "other", "serve", "color", "picker", "diffuse", "specualr", "coefficient", "parameter", "image", "space", "accord", "image", "difference", "metric", "equation", "-lrb-", "-rrb-", "spacing", "size", "determine", "user-controlled", "slider", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "we", "limit", "interface", "display", "only", "two", "parameter", "simultaneously", "ensure", "thumbnail", "large", "enough", "perform", "accurate", "selection", "fig.", "show", "what", "we", "two-parameter", "layout", "look", "like", "use", "image", "navigation", "we", "implement", "system", "which", "all", "model", "parameter", "can", "assign", "either", "horizontal", "vertical", "axis", "from", "current", "configuration", "two", "step", "either", "direction", "either", "parameter", "axis", "show", "result", "five", "five", "image", "array", "25", "image", "represent", "different", "combination", "two", "parameter", "we", "also", "give", "user", "preset", "configuration", "helpful", "combination", "parameter", "reduce", "confusion", "-lrb-", "e.g.", "diffuse", "versus", "specular", "brightness", "diffuse", "color", "picker", "-rrb-", "since", "we", "perceptuallyinspired", "parameterization", "scale", "similarly", "difference", "metric", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "we", "space", "image", "equal", "step", "parameter", "space", "may", "cause", "space", "display", "2d", "image", "scale", "differently", "horizontal", "than", "vertical", "error", "space", "even", "though", "uniform", "parameter", "space", "we", "don?t", "find", "problem", "we", "do", "allow", "slider", "determine", "size", "step", "because", "real-time", "feedback", "we", "feel", "would", "like", "take", "perceptual", "slider", "interface", "simply", "give", "multiple", "preview", "time", "give", "button", "increase", "decrease", "step", "size", "log", "scale", "we", "keep", "image", "navigation", "perceptual", "slider", "implementation", "respective", "interface", "metaphor", "while", "give", "image", "navigation", "power", "make", "small", "large", "edit", "rendering", "time", "thumbnail", "depend", "material", "configuration", "normally", "0.25", "seconds", "exception", "array", "where", "both", "gloss", "distinctness", "sheen", "vary", "simultaneously", "where", "2.5", "seconds", "-lrb-", "see", "previous", "section", "-rrb-", "we", "account", "we", "analysis", "we", "present", "analysis", "we", "datum", "two", "part", "first", "we", "analyse", "output", "render", "system", "subject", "proceed", "through", "each", "trial", "second", "we", "analyse", "feedback", "from", "user", "end", "each", "trial", "questionnaire", "unless", "state", "otherwise", "test", "statistical", "significance", "compute", "repeat", "measure", "analysis", "variance", "-lrb-", "anova", "-rrb-", "-lsb-", "Stevens", "1996", "-rsb-", "handle", "within-subject", "factor", "create", "correlation", "which", "invalidate", "assumption", "independence", "standard", "one-way", "ANOVA", "value", "below", "0.1", "indicate", "90", "confidence", "two", "population", "mean", "differ", "give", "measure", "sample", "all", "figure", "error", "bar", "represent", "standard", "error", "Time", "Completion", "we", "investigate", "work", "speed", "user", "each", "interface", "generally", "subject", "able", "complete", "each", "trial", "within", "allotted", "time", "limit", "one", "more", "interface", "fig.", "we", "show", "mean", "time", "completion", "each", "match", "trial", "over", "all", "subject", "Time", "completion", "image", "navigation", "almost", "always", "significantly", "higher", "than", "either", "physical", "perceptual", "slider", "match", "trial", "-lrb-", "0.051", "-rrb-", "except", "trial", "we", "believe", "trial", "differ", "because", "many", "subject", "run", "out", "time", "give", "up", "early", "image", "navigation", "reduce", "its", "mean", "time", "result", "match", "lower", "quality", "we", "conclude", "image", "navigation", "must", "slower", "work", "trial", "we", "reach", "limit", "subject", "patience", "time", "completion", "physical", "perceptual", "slider", "show", "significant", "difference", "trial", "2-6", "physical", "slider", "average", "20", "seconds", "faster", "than", "perceptual", "slider", "trial", "-lrb-", "0.053", "-rrb-", "only", "statistically", "significant", "difference", "-lrb-", "0.1", "-rrb-", "be", "between", "perceptual", "slider", "-lrb-", "69.0", "-rrb-", "image", "navigation", "-lrb-", "107.5", "-rrb-", "trial", "-lrb-", "0.039", "-rrb-", "physical", "slider", "-lrb-", "113.5", ",201.9", "-rrb-", "image", "navigation", "-lrb-", "150.9", ",179.3", "-rrb-", "trial", "-lrb-", "0.080", "-rrb-", "-lrb-", "0.048", "-rrb-", "respectively", "trial", "grayscale", "trial", "color", "use", "same", "brdf", "model", "do", "trial", "average", "factor", "time", "completion", "between", "grayscale", "trial", "color", "trial", "1.886", "match", "error", "evaluate", "user", "performance", "match", "trial", "we", "compute", "error", "between", "subject?s", "brdf", "goal", "brdf", "use", "image-based", "difference", "metric", "equation", "-lrb-", "-rrb-", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "metric", "have", "be", "show", "capture", "perceive", "difference", "brdf", "fig.", "show", "error", "over", "time", "one", "subject", "perform", "same", "trial", "all", "interface", "when", "subject", "successful", "error", "decrease", "toward", "correct", "solution", "converge", "some", "low", "error", "value", "accompany", "supplemental", "material", "paper", "we", "include", "error", "graph", "all", "subject", "all", "trial", "together", "render", "image", "material", "configuration", "fixed", "time", "interval", "summarize", "overall", "performance", "each", "interface", "we", "analyse", "final", "image", "error", "each", "match", "trial", "average", "over", "all", "subject", "-lrb-", "fig.", "-rrb-", "both", "physical", "perceptual", "slider", "outperform", "image", "navigation", "all", "trial", "-lrb-", "0.064", "-rrb-", "except", "trial", "where", "image", "navigation", "have", "roughly", "same", "error", "perceptual", "slider", "error", "trial", "especially", "high", "image", "navigation", "could", "part", "due", "render", "limitation", "specific", "image", "navigation", "Cook-Torrance", "brdf", "-lrb-", "sec", "we", "conclude", "failure", "case", "reasonably", "alignment", "rest", "datum", "when", "take", "account", "surprisingly", "significant", "difference", "error", "between", "perceptual", "physical", "slider", "except", "trial", "where", "physical", "slider", "outperform", "perceptual", "slider", "-lrb-", "0.064", "-rrb-", "trial", "again", "exhibit", "difference", "we", "can", "identify", "conclusively", "other", "CookTorrance", "trial", "do", "show", "difference", "between", "physical", "perceptual", "slider", "nor", "do", "other", "grayscale", "trial", "time", "completion", "we", "compare", "grayscale", "trial", "color", "trial", "average", "factor", "error", "between", "grayscale", "trial", "color", "trial", "2.167", "convergence", "illustrate", "convergence", "behavior", "different", "interface", "we", "average", "image", "error", "across", "all", "subject", "over", "time", "fig.", "average", "statistically", "valid", "we", "find", "give", "revealing", "visual", "summary", "overall", "behavior", "can", "see", "graph", "physical", "perceptual", "slider", "tend", "converge", "more", "quickly", "than", "image", "navigation", "lower", "error", "we", "also", "see", "convergence", "behavior", "physical", "perceptual", "slider", "similar", "though", "trial", "seem", "show", "better", "convergence", "phy", "ical", "slider", "finally", "we", "note", "trial", "show", "particularly", "poor", "convergence", "image", "navigation", "trial", "spatiallyvarying", "brdf", "many", "subject", "give", "up", "run", "out", "time", "use", "image", "navigation", "we", "also", "see", "slower", "poorer", "convergence", "slider", "interface", "trial", "subjective", "image", "Quality", "end", "each", "trial", "subject", "rate", "work", "scale", "be", "worst", "be", "best", "match", "trial", "rate", "term", "how", "close", "workspace", "goal", "image", "match", "open", "trial", "rate", "term", "how", "satisfied", "subject", "result", "fig.", "show", "average", "rating", "each", "trial", "subjective", "image", "quality", "correlate", "computed", "error", "final", "image", "linear", "correlation", "coefficient", "0.5895", "match", "trial", "subject", "average", "rate", "work", "better", "when", "use", "slider", "than", "image", "navigation", "all", "trial", "-lrb-", "0.058", "-rrb-", "only", "do", "subject", "perform", "objectively", "better", "use", "slider", "interface", "compare", "image", "navigation", "measure", "error", "perceive", "themselves", "do", "better", "well", "rating", "slider", "interface", "compare", "one", "another", "contain", "significant", "difference", "except", "trial", "-lrb-", "0.042", "-rrb-", "computed", "error", "open", "trial", "significant", "difference", "image", "rating", "between", "any", "three", "interface", "except", "physical", "slider", "have", "slightly", "higher", "average", "rating", "than", "image", "navigation", "trial", "-lrb-", "0.015", "-rrb-", "Interface", "Rankings", "rating", "subject", "rate", "rank", "each", "interface", "category", "where", "rating", "can", "have", "tie", "ranking", "force", "choice", "-lrb-", "see", "Sec", "average", "rating", "stack", "fre", "quency", "ranking", "show", "Fig.", "evaluate", "statistical", "significance", "rank", "we", "use", "Friedman", "test", "-lsb-", "Friedman", "1937", "-rsb-", "nonparametric", "test", "take", "account", "within-subject", "effect", "low", "p-value", "indicate", "high", "confidence", "subject", "have", "make", "significant", "distinction", "between", "two", "interface", "all", "category", "except", "preference", "open", "trial", "slider", "interface", "outrank", "image", "navigation", "-lrb-", "0.074", "perceptual", "vs.", "image", "navigation", "natural", "category", "0.002", "otherwise", "-rrb-", "we", "find", "statistical", "difference", "between", "rank", "two", "slider", "interface", "roughly", "half", "subject", "rank", "physical", "slider", "higher", "than", "perceptual", "overall", "preference", "vice", "versa", "we", "find", "similar", "trend", "interface", "rating", "slider", "interface", "average", "roughly", "equivalent", "all", "category", "when", "compare", "image", "navigation", "slider", "interface", "except", "open", "trial", "preference", "image", "navigation", "rate", "much", "lower", "all", "category", "-lrb-", "0.002", "-rrb-", "complexity", "we", "have", "show", "interface", "use", "perform", "material", "editing", "influence", "performance", "difficulty", "seem", "scale", "linearly", "average", "number", "user", "controlled", "parameter", "material", "model", "regression", "average", "time", "completion", "suggest", "linear", "relationship", "-lrb-", "0.919", "-rrb-", "error", "have", "similar", "trend", "physical", "-lrb-", "0.996", "-rrb-", "perceptual", "-lrb-", "0.880", "-rrb-", "slider", "much", "image", "navigation", "-lrb-", "0.499", "-rrb-", "unclear", "we", "increase", "complexity", "further", "novice", "would", "still", "able", "accomplish", "task", "trial", "appear", "indicate", "point", "which", "many", "subject", "give", "up", "we", "datum", "do", "indicate", "any", "significant", "trend", "material", "editing", "between", "use", "Ward", "CookTorrance", "BRDF", "model", "Cook-Torrance", "BRDF", "trial", "appear", "more", "challenging", "than", "Ward", "BRDF", "trial", "Cook-Torrance", "BRDF", "trial", "appear", "less", "challenging", "than", "Ward", "BRDF", "trial", "we", "make", "claim", "usefulness", "one", "model", "over", "another", "we", "study", "design", "give", "subject", "choice", "between", "two", "we", "now", "discuss", "common", "trend", "way", "we", "subject", "use", "different", "interface", "edit", "material", "fig.", "we", "show", "work", "do", "two", "different", "subject", "correspond", "error", "graph", "can", "find", "fig.", "image", "error", "graph", "from", "all", "subject", "all", "trial", "can", "find", "supplemental", "material", "well", "select", "video", "workflow", "blocking", "refinement", "subject", "do", "fix", "each", "parameter", "value", "independently", "permanently", "make", "rough", "adjustment", "move", "configuration", "good", "local", "space", "hierarchically", "refine", "smaller", "smaller", "space", "until", "precise", "configuration", "reach", "mean", "parameter", "revisit", "change", "many", "time", "during", "course", "editing", "session", "behavior", "universal", "across", "all", "subject", "inability", "configure", "image", "Navigation", "we", "notice", "majority", "time", "spend", "when", "use", "image", "navigation", "spend", "change", "configuration", "material", "subject", "appear", "have", "trouble", "set", "up", "2d", "navigation", "array", "image", "we", "observe", "most", "we", "subject", "be", "confuse", "despite", "have", "preset", "configuration", "subject", "comment", "feel", "limit", "layout", "because", "could", "find", "combination", "need", "find", "match", "bit", "confine", "tool", "feel", "like", "could", "control", "my", "work", "much", "-lsb-", "image", "navigation", "-rsb-", "way", "you", "know", "what", "change", "clear", "how", "exactly", "get", "slider", "approach", "part", "little", "easier", "when", "use", "either", "physical", "perceptual", "slider", "subject", "make", "change", "far", "more", "often", "can", "see", "Sec", "lead", "faster", "better", "convergence", "goal", "additionally", "only", "do", "image", "navigation", "yield", "change", "less", "often", "those", "change", "be", "undo", "more", "often", "undo", "use", "roughly", "twice", "often", "image", "navigation", "than", "slider", "interface", "-lrb-", "0.051", "-rrb-", "physical", "perceptual", "slider", "share", "roughly", "same", "undo", "usage", "image", "Navigation", "Sliders", "when", "use", "image", "navigation", "almost", "all", "subject", "display", "behavior", "use", "only", "one", "axis", "time", "effectively", "reduce", "slider", "interface", "discrete", "configuration", "visible", "time", "while", "most", "occurrence", "behavior", "be", "interleave", "use", "2d", "array", "color", "picker", "some", "subject", "would", "go", "entire", "trial", "use", "only", "technique", "lead", "we", "believe", "many", "situation", "where", "user", "think", "independent", "parameter", "space", "Sliders", "Equalized", "Interactivity", "universally", "subject", "rarely", "snap", "slider", "particular", "value", "suggest", "optimal", "workflow", "novice", "smoothly", "vary", "appearance", "until", "image", "look", "like", "what", "look", "do", "seem", "less", "confusing", "than", "see", "several", "image", "side-byside", "we", "investigate", "behavior", "disable", "ability", "drag", "slider", "interface", "leave", "only", "option", "click", "specific", "location", "slider", "run", "five", "additional", "subject", "through", "otherwise", "unchanged", "study", "situation", "all", "subject", "essentially", "mimic", "drag", "action", "repeatedly", "clike", "small", "interval", "along", "bar", "average", "number", "click", "physical", "perceptual", "slider", "roughly", "equal", "except", "trial", "where", "perceptual", "slider", "average", "roughly", "1/3", "more", "click", "than", "physical", "slider", "-lrb-", "0.034", "-rrb-", "we", "believe", "interactivity", "nullify", "difference", "between", "two", "interface", "novice", "prefer", "nudge", "control", "until", "image", "look", "right", "rather", "than", "purposefully", "set", "value", "Material", "Properties", "after", "use", "each", "interface", "subject", "be", "ask", "what", "think", "most", "least", "difficult", "aspect", "design", "process", "question", "open", "interpretation", "we", "do", "get", "several", "comment", "about", "specific", "parameter", "property", "material", "model", "we", "categorize", "comment", "adjustment", "color", "relative", "diffuse", "specular", "intensity", "highlight", "shaping", "-lrb-", "specular", "roughness", "fresnel", "effect", "-rrb-", "number", "time", "each", "category", "be", "mention", "-lrb-", "sum", "all", "three", "questionnaire", "per", "subject", "-rrb-", "list", "below", "color", "23", "most", "difficult", "least", "difficult", "relative", "intensity", "most", "difficult", "least", "difficult", "highlight", "shaping", "most", "difficult", "13", "least", "difficult", "we", "draw", "two", "piece", "information", "from", "datum", "first", "because", "color", "mention", "most", "often", "user", "must", "feel", "important", "factor", "overall", "material", "appearance", "second", "majority", "subject", "feel", "color", "most", "difficult", "part", "design", "process", "surprising", "give", "most", "work", "develop", "perceptual", "parameterization", "material", "have", "be", "do", "grayscale", "exploration", "open", "trial", "subject", "perform", "exploratory", "task", "require", "less", "fine", "tuning", "many", "subject", "comment", "directly", "be", "explore", "wide", "space", "rather", "than", "refining", "example", "one", "subject", "comment", "open", "trial", "have", "I", "look", "all", "over", "place", "cool", "option", "where", "matching", "tend", "make", "smaller", "change", "another", "comment", "my", "workflow", "completely", "random", "experimental", "when", "do", "open", "trial", "we", "observe", "performance", "image", "navigation", "interface", "compare", "slider", "improve", "greatly", "from", "match", "open", "trial", "user", "explain", "comment", "use", "-lsb-", "image", "navigation", "-rsb-", "much", "like", "other", "one", "match", "open", "trial", "lot", "easier", "see", "something", "good", "here", "-lsb-", "image", "navigation", "-rsb-", "matching", "very", "difficult", "have", "try", "many", "different", "thing", "open", "trial", "be", "enjoyable", "could", "pick", "from", "option", "-lsb-", "preset", "-rsb-", "button", "bring", "up", "-lsb-", "image", "navigation", "-rsb-", "open", "one", "be", "easier", "because", "get", "better", "view", "what", "want", "we", "conclude", "give", "its", "problem", "precise", "adjustment", "image", "navigation", "must", "better", "pure", "exploration", "lack", "need", "control", "complete", "solution", "material", "design", "otherwise", "would", "able", "compete", "so", "closely", "slider", "open", "trial", "we", "now", "discuss", "result", "we", "study", "we", "remind", "reader", "strictly", "speak", "we", "observation", "only", "apply", "within", "boundary", "test", "case", "all", "user", "study", "same", "time", "we", "belief", "trend", "observe", "study", "should", "apply", "other", "similar", "appearance", "design", "task", "novice", "novice", "can", "edit", "Materials", "we", "have", "find", "novice", "capable", "design", "editing", "realistic", "material", "when", "interface", "support", "they", "novice", "can", "perform", "relatively", "complex", "task", "efficient", "way", "physical", "perceptual", "slider", "we", "find", "subject", "can", "perform", "material", "editing", "equivalently", "well", "whether", "use", "physical", "parameter", "perceptually-inspired", "parameter", "provide", "we", "implementation", "additionally", "subject", "pool", "split", "half", "which", "prefer", "we", "conclude", "interactivity", "more", "important", "than", "whatever", "advantage", "perceptually-inspired", "parameter", "we", "give", "we", "subject", "yield", "image", "Navigation", "vs.", "Sliders", "we", "most", "prominent", "result", "poor", "performance", "image", "navigation", "interface", "compare", "individual", "parameter", "adjustment", "via", "slider", "because", "image", "navigation", "can", "show", "enough", "parameter", "combination", "simultaneously", "due", "limited", "screen", "real", "estate", "perhaps", "parameterbased", "organization", "use", "-lsb-", "Ngan", "et", "al.", "2006", "-rsb-", "optimal", "so", "optimal", "layout", "remain", "undiscovered", "Material", "Complexity", "we", "find", "color", "significant", "challenge", "material", "editing", "take", "almost", "twice", "long", "subject", "match", "colored", "material", "than", "grayscale", "error", "significantly", "higher", "subject", "also", "tell", "we", "color", "often", "most", "diffi", "cult", "part", "design", "process", "we", "believe", "need", "investigation", "method", "perceptual", "color", "manipulation", "material", "under", "color", "lighting", "note", "editing", "color", "material", "editing", "very", "different", "from", "set", "color", "image", "editing", "we", "find", "difficulty", "trial", "measure", "time", "completion", "error", "linear", "number", "material", "parameter", "give", "user", "slider", "interface", "when", "editing", "material", "more", "than", "one", "lobe", "subject", "could", "accomplish", "task", "give", "enough", "time", "we", "find", "spatially-varying", "material", "more", "challenging", "than", "other", "type", "material", "study", "result", "higher", "final", "error", "finally", "we", "discover", "significant", "difference", "between", "editing", "Ward", "Cook-Torrance", "brdf", "common", "Workflow", "we", "subject", "exhibit", "common", "workflow", "pattern", "we", "notice", "subject", "generally", "employ", "block-and-refine", "workflow", "move", "from", "large", "edit", "small", "edit", "slider", "interface", "subject", "do", "set", "parameter", "directly", "prefer", "smoothly", "change", "they", "until", "look", "right", "interactivity", "important", "reduce", "effect", "parameterization", "type", "exploration", "advantage", "slider", "interface", "over", "image", "navigation", "less", "obvious", "open", "trial", "take", "account", "control", "problem", "image", "navigation", "imply", "navigation", "better", "metaphor", "support", "exploration", "broad", "material", "variation", "Limitations", "main", "limitation", "work", "scope", "material", "editing", "task", "we", "investigate", "first", "we", "have", "only", "study", "subset", "possible", "brdf", "model", "second", "we", "do", "explicitly", "investigate", "whether", "novice", "can", "effectively", "pick", "material", "model", "from", "list", "available", "option", "Third", "we", "do", "investigate", "creation", "spatial", "pattern", "although", "we", "believe", "task", "well", "beyond", "capability", "novice", "user", "fourth", "we", "forego", "study", "interface", "painting", "because", "material", "representation", "restriction", "paper", "present", "first", "step", "toward", "quantitatively", "evaluate", "use", "effectiveness", "user", "interface", "material", "design", "focus", "novice", "user", "we", "find", "novice", "can", "edit", "material", "equally", "well", "slider", "control", "either", "physical", "perceptually-inspired", "parameter", "long", "interactivity", "available", "image", "navigation", "can", "help", "user", "find", "important", "material", "configuration", "when", "artistically", "explore", "possibility", "perform", "slower", "less", "accuracy", "when", "precise", "adjustment", "necessary", "novice", "tend", "work", "similarly", "one", "another", "make", "large", "edit", "first", "systematically", "readjust", "each", "parameter", "smaller", "step", "until", "converge", "final", "solution", "we", "only", "study", "small", "subset", "interface", "model", "material", "design", "development", "method", "compare", "interface", "operate", "different", "material", "model", "would", "useful", "we", "would", "like", "thank", "Jonathan", "Denning", "Lori", "Lorigo", "help", "prepare", "paper", "work", "support", "nsf", "-lrb-", "cns-070820", "ccf-0746117", "-rrb-", "Intel", "Sloan", "Foundation" ],
  "content" : "Fig. 1 shows examples of output from our subjects using each of these interfaces. First, in six matching trials, subjects are asked to match a target BRDF as closely as possible. Second, in three open trials, subjects use their own creativity to design a BRDF based on a suggestive image. Twenty subjects spend roughly one hour with each of the three interfaces. Within the boundaries of our study, performance scales well with material complexity in degrees of freedom. However, some users do poorly when multiple BRDFs interact in a spatially-varying way. Within the boundaries of our study, the introduction of color roughly doubles the time it takes to reach a goal, and reduces the quality of the result by a similar factor in the error space. As with any user study, our observations only strictly apply within the boundary of the tested cases. Additionally, we believe that the principles used to design our study can be employed to evaluate additional material design tasks in moving toward a comprehensive evaluation of material design interfaces. Analytic BRDF Models. Physical Sliders. Perceptual Sliders. Pellacini et al. [2000] develop a perceptual parameterization of the Ward BRDF model through psychophysical experiments. Since no perceptual parameterizations exist for all BRDFs used in this study, we develop perceptually-inspired parameterizations based on [Pellacini et al. 2000] and [Ngan et al. 2006] in Sec. Image Navigation. Other Interfaces. We exclude this interface type because it is unclear how to extend it to support texture variations robustly. BRDFs can also be defined by arbitrary curves over an angular parameterization, as used in [Lawrence et al. 2006]. Three editing tasks are normally performed on spatially-varying BRDFs: changing the spatial patterns (by texture painting or synthesis), selecting regions of similar appearance (e.g. using [Pellacini and Lawrence 2007]) and altering the BRDFs of the selected regions. We represent spatially-varying BRDFs as linear combinations of basis BRDFs with spatially-varying weights [Lawrence et al. 2006], where users edit the parameters of the basis BRDFs. We choose this model since it fits measured data well. Appearance Design Study. We follow the approach we used in [Kerr and Pellacini 2009] for the evaluation of lighting design interfaces and apply it to the material editing domain. We seek to evaluate the relative effectiveness of different interface paradigms for material design in the context of designing realistic materials with a focus on novice users. Specifically, (1) we want to measure how efficiently these users can perform specific material adjustments and (2) we want to understand which interface paradigms provide better artistic exploration of possible material variations. Novice Users. All subjects rated their experience level with material design as either 1 or 2 on a scale from 1 to 5, and can be considered novices. Reducing Complexity. On the one hand, we want to achieve complex-enough material editing tasks to ensure meaningful measurements. On the other, we want to avoid bias in the data by ensuring subjects can successfully complete the required tasks without incurring too much fatigue. Working with novices makes this triage even more necessary. We simplify the material editing task by focusing on editing the parameters of analytic BRDFs, and while we include different BRDF models, we do not ask subjects to select between different  models during trials. Materials. In our design tasks, subjects edit materials represented as isotropic Ward [1992] and Cook-Torrance [1981] BRDFs (Sec. We investigate three variations of these models. First, we use achromatic materials for half of the trials and color for the other half. Third, to determine whether the presence of spatial variation affects the design tasks, we investigate the editing of spatially-varying BRDFs represented as linear combinations of two basis BRDFs with spatially-varying weights, where users edit the parameters of the basis BRDFs. We choose this model since it fits measured data well [Lawrence et al. 2006]. Examples of each material type can be found in Fig. 2 . Natural illumination is considered ideal for material perception when only a single image is available [Dror et al. 2001; Fleming et al. 2001]. We use the tone mapping equation Image = (Intensity ? 2 exposure ) 1/gamma with a gamma of 2.2. Subjects have no control over exposure or gamma. The geometry in our images consists of a sphere floating in space. We use a sphere to avoid occlusion artifacts in glossy reflections caused by computing direct illumination only. We determine that a sphere shape is the best for our purposes given our rendering limitations. Interfaces. We compare three user interfaces: physical sliders, perceptual sliders, and image navigation. Implementation details of these interfaces are presented in Sec. We ask subjects to perform two types of material design tasks. Matching trials allow us to quantitatively measure users? performance, while providing a clear goal for subjects who have never experienced material design before. These trials allow us to observe how users artistically explore the space of possible material configurations, a more natural but harder to measure task. We ask subjects to complete a number of trials, during which all actions are recorded for further analysis. Each subject performs all trials using all interfaces. Preparatory Studies. The open and matching goals used in the final study were tested to ensure that time limits were appropriate and that the tasks could be completed. We perform six matching trials and three open trials with a progressively increasing number of degrees of freedom in the material model. Starting configuration, goal configuration, and time limit for each trial are summarized in Fig. 2 and Fig. 3 . For matching trials, goal configurations were taken from parametric fits presented in [Ngan et al. 2005] to measured materials in [Matusik et al. 2003]. For grayscale trials 1 and 2, the diffuse and specular coefficients of ?metallic blue? and ?white bball? were desaturated. For grayscale trial 3, the goal was modeled by hand after a rendering of ?acrylic violet?, since a fit was unavailable. Color trials 4 and 5 use fits for ?blue bball? and ?ch-ball-green-metallic? respectively. Trial 6 uses fits for ?white-bball? and ?metallic gold? weighted by a texture. We vary matching trials in material complexity to observe possible changes in users? workflow and interfaces? effectiveness under these conditions. For open trials, we select target images that differ from the workspace lighting environment and vary in content to encourage free-form artistic exploration. We choose one grayscale and two colored material goals with objects of varying material properties. Objects in the same goal image share some material properties to keep the objective from being completely unspecified. The same initial and goal material configurations are used for all subjects and all interfaces. Each trial has a fixed time limit, and subjects can end the trial sooner if satisfied with the current result. At the end of each matching trial, subjects rate the accuracy of the matching on a scale of 1 to 5. Questionnaire. After performing all trials with all interfaces, subjects complete a questionnaire where they rate each interface on a scale of 1 to 5 in the following categories: (1) natural way to think about material editing, (2) preference in matching trials, (3) preference in open trials, and (4) overall preference. Subjects also strictly rank interfaces in each of these categories. Immediately after finishing trials for each single interface, subjects are asked to leave free-form comments on aspects of each interface. For reproducibility, we include copies of the questionnaires as additional material. Procedure. All subjects had normal or corrected-to-normal vision. Subjects edit materials for about 3 hours each to ensure good statistical significance of our tests, while keeping fatigue low. Subjects complete the study in three 60-minutes sessions, one for each interface. We randomize the order of the interfaces for each subject. Before each session, subjects complete a training phase to become familiar with the specific interface. We train each subject individually to allow questions, accommodating each subject?s learning needs. The instructor verifies that the subject uses each part of the interface, and answers the subjects? questions. Before proceeding to the experiment, the subject uses the interface until he or she feels comfortable. Once trials begin, all user interface actions are recorded. The study is conducted in a controlled lighting environment with negligible ambient lighting, to simulate typical working conditions of artists and maximize visibility of the screen. We use a 24-inch Dell 2407WFPb LCD display at 1280?800 resolution at a distance of approximately 1 foot from the subject (monitor native resolution: 1900 ? 1200). All rendered images are 256x256 pixels on screen covering an area of 4 square inches. We used an Intel 2.8 Ghz Core2 Quad Q9550 PC with 4 GB of RAM and an NVidia GeForce 9800 GT graphics card. In this section we discuss our implementations of the user interfaces included in the study. For reproducibility, we include a video as  supplemental material that shows each interface in detail. Rendering. We use the real-time rendering algorithm of [BenArtzi et al. 2006] to preview BRDF edits under direct natural illumination. Our implementation renders 45f ps on the 256 x 256 pixel images of a sphere used in the study. We considered adding global illumination as in [Ben-Artzi et al. 2008], but decided that the potential artifacts resulting from approximating the BRDF at a lower frequency might effect our measurements. The algorithm we use doesn?t allow the roughness and Fresnel terms of the CookTorrance BRDF model to be simultaneously modified. It takes approximately 0.6 seconds to switch between these parameters in our implementation. BRDF models. In our implementation, we parameterize the isotropic Ward BRDF ? w as where ? d is the diffuse albedo, ? s is the energy of the specular component, ? is the surface roughness and ? i , ? o , ? h are the angles between the surface normal and the incoming, outgoing and halfangle respectively. We parameterize the Cook-Torrance BRDF ? ct following [Ngan et al. 2005] as with F = F 0 + (1 ? F 0 )(1 ? cos ? b ) 5 , e ?(tan ? h /m) 2 D = , m 2 cos 4 ? h 2 cos ? h cos ? i 2 cos ? h cos ? o G = min 1, , , cos ? b cos ? b where ? d is the diffuse albedo, ? s is the energy of the specular component, ? is the surface roughness, F 0 is the Fresnel reflectance for a direction orthogonal to the surface, and ? b is the angle between the outgoing and half-vector direction. Some trials use BRDFs ? ww with 2 Ward lobes defined by where ? = 1/ cos ? i cos ? o . In other selected trials, we use a spatially-varying material. Universal Interface Features. All interfaces use the same screen layout consisting of a workspace window, a goal window, and rating buttons ( Fig. 4 ). An undo key allows the user to walk back through an unlimited number of edits. To compensate for the fact that materials created using this system may not conserve energy, a warning indicator appears in the upper right corner of the user?s image when the BRDF is not energy conserving. Physical Sliders. We use a slider interface as the means by which a user sets the parameters of the BRDF model, e.g. the diffuse albedo ? d , specular energy ? s , roughness ?, m and Fresnel term F 0 . Each user controlled parameter is listed with a slider bar next to it. The parameter can be changed by clicking anywhere on the bar, and gradual changes can be seen by dragging the slider continuously across the bar. This would ignore common color editing practices, artificially handicapping the interface. We use CIELAB luminance (L) for achromatic intensity, and saturation and hue for chromaticity [Fairchild 1998]. Perceptual Sliders. Perceptual parameterizations differ from physical ones in both effect and scale. Furthermore, since all perceptual parameterizations are derived from achromatic data, we follow [Wills et al. 2009; Pellacini et al. 2000] and derive parameters for grayscale diffuse and specular components, and then add hue and saturation to them. We use the same saturation and hue controls as with physical sliders. Slider controls work the same way as with physical sliders, but modify the perceptually-inspired parameters. To determine the correct scaling of each parameter axis in BRDF model?s configuration space, we use the image-based BRDF difference metric from [Ngan et al. 2006] since psychophysical data has not been published for the range of BRDF parameters we investigate. We scale our perceptually-inspired parameters such that equal steps of the parameter yield steps according in the distance metric. We include a comparison of this metric to our parameterizations as a supplemental document. For all parameterizations that follow, ? d and ? s are represented achromatically according to CIELAB luminance in the range [0, 1]. For Ward BRDFs, we use the parameterization from [Pellacini et al. 2000] with a modified d parameter: L = ? d where L is the diffuse luminance, c is the gloss contrast, and d is the gloss distinctness. We raise ? to a power of 1/4 because it more closely matches scaling according to equation (4) which is valid for a larger range of ? than the original experiment covered in [Pellacini et al. 2000]. The trials using textured Ward simply have two instances of the perceptually inspired Ward parameters. For Cook-Torrance BRDFs, we use the following parameterization: L = ? d where L is the diffuse luminance, c is the gloss contrast, d is the gloss distinctness, s is the gloss sheen and = 0.02 is the minimum  allowed value of F 0 when s ? [0, 1]. Cook-Torrance parameters L, c, and d are similar to their Ward counterparts; with added s to set the contrast of the specular component at grazing angles while preserving its contrast at non-grazing angles. For Ward BRDFs with two lobes, we use the following parameterization: L = ? d c = 3 ? s1 + ? s2 + ? d /2 ? 3 ? d /2 where L is the diffuse luminance, c is the gloss contrast, b is a lobe blending parameter, d is the overall gloss distinctness, h is a haze parameter and ? max is the maximum possible ? value for normalization. Image Navigation. We base our implementation of image navigation on [Ngan et al. 2006]. Their interface consists of a series of tabs that reveal different image arrays. Some tabs show variations of material model parameters along two axes, while others serve as color pickers for the diffuse and specualr coefficient parameters. Images are spaced according to the image difference metric in equation (4), and the spacing size is determined by a user-controlled slider. As in [Ngan et al. 2006], we limit the interface to display only two parameters simultaneously to ensure that thumbnails are large enough to perform accurate selection. Fig. 5 shows what our two-parameter layout looks like using image navigation. We implement a system by which all model parameters can be assigned to either a horizontal or vertical axis. From the current configuration, two steps in either direction for either parameter axes are shown. This results in a five by five image array of 25 images representing different combinations of two parameters. We also give the user preset configurations that are helpful combinations of parameters to reduce confusion (e.g. diffuse versus specular brightness or a diffuse color picker). Since our perceptuallyinspired parameterizations scale similarly to the difference metric in [Ngan et al. 2006], we space images by equal steps in that parameter space. This may cause the space displayed in a 2D image to be scaled differently on the horizontal than the vertical in error space even though they are uniform in parameter space, but we don?t find it to be a problem. We do not allow a slider to determine the size of these steps, because with real-time feedback we feel this would be like taking the perceptual sliders interface and simply giving multiple previews at a time. By giving buttons that increase and decrease the step sizes on a log scale, we keep image navigation and perceptual sliders implementations to their respective interface metaphors, while giving image navigation the power to make small and large edits. Rendering time for the thumbnails depends on the material configuration, but is normally 0.25 seconds with the exception of arrays where both gloss distinctness and sheen vary simultaneously where it is 2.5 seconds (see previous section). We account for this in our analysis. We present an analysis of our data in two parts. First, we analyse the output of the rendering system as subjects proceed through each trial. Second, we analyse the feedback from users at the end of each trial and in the questionnaires. Unless stated otherwise, tests for statistical significance are computed with repeated measures analysis of variance (ANOVA) [Stevens 1996]. This handles within-subject factors that create correlations which invalidate the assumption of independence in standard one-way ANOVA. A p value below 0.1 indicates a 90% confidence that the two population means differ given the measure of the sample. In all figures, error bars represent standard error. Time to Completion. We investigate the work speed of users with each interface. Generally, subjects are able to complete each trial within the allotted time limit with one or more interfaces. In Fig. 6 , we show the mean time to completion for each matching trial over all subjects. Time to completion for image navigation is almost always significantly higher than either physical or perceptual sliders on matching trials (p ? 0.051), excepting trial 6. We believe trial 6 differs because many subjects ran out of time or gave up early with image navigation, reducing its mean time and resulting in matches of lower quality. We conclude that image navigation must be slower to work with on trial 6, and that we are reaching the limit of subjects? patience. The time to completion for physical and perceptual sliders shows no significant difference on trials 2-6, but physical sliders average 20 seconds faster than perceptual sliders on trial 1 (p = 0.053). The only statistically significant differences (p < 0.1) were between perceptual sliders (69.0s) and image navigation (107.5s) on trial 7 (p = 0.039), and physical sliders (113.5s,201.9s) and image navigation (150.9s,179.3s) on trials 8 (p = 0.080) and 9 (p = 0.048) respectively. Trial 1 in grayscale and trial 4 in color use the same BRDF model, as do trials 2 and 5. The average factor of time to completion between grayscale trials and color trials is 1.886. Matching Error. To evaluate user performance in matching trials, we compute the error between the subject?s BRDF and the goal BRDF using the image-based difference metric in equation (4) [Ngan et al. 2006]. This metric has been shown to capture perceived differences in BRDFs. Fig. 7 shows the error over time for one subject performing the same trial with all interfaces. When subjects are successful, error decreases toward the correct solution, converging on some low error value. In the accompanying supplemental material for this paper we include error graphs for all subjects on all trials together with rendered images of their material configurations at fixed time intervals. To summarize the overall performance of each interface we analyse the final image error for each matching trial averaged over all subjects ( Fig. 6 ). Both physical and perceptual sliders outperform image navigation on all trials (p < 0.064) except for trial 4, where image navigation has roughly the same error as perceptual sliders. The error on trial 2 is especially high for image navigation. This could in part be due to the rendering limitation specific to image navigation on Cook-Torrance BRDFs (Sec. We conclude that these failure cases are reasonably in alignment with the rest of the data when taking this into account. Surprisingly, there is no significant difference in errors between perceptual and physical sliders except on trial 2 where physical sliders outperform perceptual sliders (p = 0.064). This trial again exhibits a difference that we cannot identify conclusively. The other CookTorrance trial does not show such a difference between physical and perceptual sliders, nor do the other grayscale trials. As with the time to completion, we compare grayscale trials 1 and 2 to color trials 4 and 5. The average factor in error between grayscale trials and color trials is 2.167. Convergence. To illustrate the convergence behavior of different interfaces we average the image error across all subjects over time in Fig. 7 . This average is not statistically valid, but we find that it gives a revealing visual summary of overall behavior. As can be seen in the graphs, physical and perceptual sliders tend to converge more quickly than image navigation, and with lower error. We also see that convergence behavior of physical and perceptual sliders are similar, though trial 2 seems to show better convergence with phys- ical sliders. Finally, we note that trials 2 and 6 show particularly poor convergence for image navigation. In trial 6 with a spatiallyvarying BRDF, many subjects give up or run out of time using image navigation. We also see slower and poorer convergence with the slider interfaces on this trial. Subjective Image Quality. At the end of each trial, subjects rate their work on a scale of 1 to 5, with 1 being the worst and 5 being the best. Matching trials are rated in terms of how close the workspace and goal images match. Open trials are rated in terms of how satisfied the subject is with their result. Fig. 8 shows the average ratings for each trial. This subjective image quality correlates with the computed error of the final image with a linear correlation coefficient of ?0.5895. In matching trials, subjects on average rate their work better when using sliders than with image navigation on all trials (p ? 0.058). Not only do subjects perform objectively better using slider interfaces compared to image navigation as measured by error, they perceive themselves as doing better as well. Ratings for the slider interfaces compared to one another contain no significant differences, except on trial 2 (p = 0.042), as with the computed error. In open trials, there is no significant difference in the image ratings between any of the three interfaces, except for physical sliders having a slightly higher average rating than image navigation on trial 7 (p = 0.015). Interface Rankings and Ratings. Subjects rate and rank each interface in 4 categories where ratings can have ties, but rankings are forced choice (see Sec. Average ratings and stacked fre- quencies of rankings are shown in Fig. 9 . For evaluating statistical significance of ranks we use the Friedman test [Friedman 1937], a nonparametric test that takes into account within-subject effects. A low p-value indicates high confidence that subjects have made a significant distinction between two interfaces. In all categories except preference on open trials, slider interfaces outrank image navigation (p = 0.074 on perceptual vs. image navigation in the natural category, p ? 0.002 otherwise). We find no statistical difference between ranks for the two slider interfaces. Roughly half of subjects rank physical sliders higher than perceptual in overall preference, and vice versa. We find similar trends in the interface ratings. The slider interfaces average to roughly equivalent in all categories. When comparing image navigation to slider interfaces, except for open trial preference, image navigation is rated much lower in all categories (p ? 0.002). Complexity. We have shown that the interface used to perform material editing influences performance. Difficulty seems to scale linearly on average with the number of user controlled parameters in the material model. Regression on average time to completion suggests a linear relationship (r 2 ? 0.919). Error has a similar trend with physical (r 2 = 0.996) and perceptual (r 2 = 0.880) sliders, but not as much with image navigation (r 2 = 0.499). It is unclear if we increased complexity further, that novices would still be able to accomplish the task. Trial 6 appears to indicate that there is a point at which many subjects will give up. Our data does not indicate any significant trends in material editing between using the Ward or CookTorrance BRDF model. The Cook-Torrance BRDF in trial 2 appears to be more challenging than the Ward BRDF in trial 1, but the Cook-Torrance BRDF in trial 5 appears to be less challenging than the Ward BRDF in trial 4. We make no claims as to the usefulness of one model over another, as our study is not designed to give subjects a choice between the two. We now discuss common trends in the way our subjects use the different interfaces to edit materials. In Fig. 1 we show work done by two different subjects. Corresponding error graphs can be found in Fig. 7 . Images and error graphs from all subjects and all trials can be found in the supplemental material, as well as selected videos of workflow. Blocking and Refinement. Subjects do not fix each parameter value independently and permanently. They make rough adjustments to move the configuration into a good local space and hierarchically refine into smaller and smaller spaces until the precise configuration is reached. This means that parameters are revisited and changed many times during the course of an editing session. Such behavior is universal across all subjects. Inability to Configure Image Navigation. We notice that the majority of the time spent when using image navigation is not spent changing the configuration of the material. Subjects appear to have trouble setting up the 2D navigation array of images. We observed that most of our subjects were confused by this, despite having preset configurations. Subjects comment ?I felt limited by the layout because I could not find the combination I needed to find a match. I was a bit confined by the tools and felt like I could not control my work as much;? and ?[with image navigation] in a way, you know what to change, but not clear how exactly to get there. In the slider approach, that part was a little easier. ? When using either physical or perceptual sliders, subjects made changes far more often. As can be seen in Sec. 6, this led to faster and better convergence on a goal. Additionally, not only did image navigation yield changes less often, those changes were undone more often. Undo is used roughly twice as often with image navigation than with the slider interfaces (p ? 0.051). Physical and perceptual sliders share roughly the same undo usage. Image Navigation as Sliders. When using image navigation, almost all subjects displayed behavior of using only one axis at a time, effectively reducing it to a slider interface with 5 discrete configurations visible at a time. While most occurrences of this behavior were interleaved with use of the 2D array or the color pickers, some subjects would go entire trials using only this technique. This leads us to believe that there are many situations where users think in independent parameter space. Sliders Equalized by Interactivity. Universally, subjects rarely snap sliders to a particular value. This suggests that the optimal workflow for novices is to smoothly vary appearance until the image looks like what they are looking for. Doing this seems to be less confusing than seeing several images side-byside. We investigate this behavior by disabling the ability to drag in the slider interfaces, leaving only the option to click a specific location on the slider, and running five additional subjects through the otherwise unchanged study. In this situation, all subjects essentially mimicked a dragging action by repeatedly cliking at small intervals along the bar. The average number of clicks for physical and perceptual sliders was roughly equal, except on trial 3 where perceptual sliders averaged roughly 1/3 more clicks than physical sliders (p = 0.034). We believe that interactivity nullifies the differences between these two interfaces and that novices prefer nudging controls until an image looks right, rather than purposefully setting values. Material Properties. After using each interface, subjects were asked what they thought the most and least difficult aspect of the design process was. This question was open for interpretation, but we did get several comments about specific parameters and properties of the material models. We categorize these comments into the adjustment of color, relative diffuse and specular intensities, and highlight shaping (specular roughness and fresnel effects). The number of times each of these categories were mentioned (sum of all three questionnaires per subject) are listed below: 1. Color: 23 most difficult, 8 least difficult 2. Relative intensities: 2 most difficult, 7 least difficult 3. Highlight shaping: 5 most difficult, 13 least difficult We draw two pieces of information from this data. First, because color is mentioned most often, users must feel it is an important factor in the overall material appearance. Second, a majority of subjects felt that color was the most difficult part of the design process. This is surprising given that most work in developing perceptual parameterizations of materials has been done in grayscale. Exploration. In open trials subjects perform an exploratory task that requires less fine tuning. Many subjects commented directly that they were exploring in a wide space rather than refining. For example, one subject commented ?the open trials had me looking all over the place for cool options, where the matching I tended to make smaller changes. ? Another commented ?my workflow was completely random and experimental when doing open trials. ? We observe that the performance of the image navigation interface compared to sliders improves greatly from matching to open trials. Users explain in comments, ?I used [image navigation] much like the other ones for matching, but for open trials it was a lot easier to see something good here;? ?[with image navigation] matching was very difficult. I had to try many different things. The open trials were enjoyable. I could pick from the options the [preset] buttons brought up;? and ?[with image navigation] the open ones were easier because I got a better view of what I wanted. ? We conclude that given its problems with precise adjustment, image navigation must be better at pure exploration, but lacks the needed control for a complete solution to material design. Otherwise, it would not be able to compete so closely with sliders in these open trials. We now discuss the results of our study. We remind the reader that strictly speaking, our observations only apply within the boundary of the tested cases, as with all user studies. At the same time, it is our belief that the trends observed in this study should apply to other similar appearance design tasks for novices. Novices Can Edit Materials. We have found that novices are capable of designing and editing realistic materials. When an interface supports them, novices can perform relatively complex tasks in an efficient way. Physical and Perceptual Sliders. We found that subjects can perform material editing equivalently well whether they use physical parameters or the perceptually-inspired parameters provided by our implementation. Additionally, the subject pool is split in half as to which is preferred. We conclude that interactivity is more important than whatever advantages the perceptually-inspired parameters we gave our subjects yield. Image Navigation vs. Sliders. Our most prominent result is the poor performance of the image navigation interface compared to individual parameter adjustment via sliders. This is because image navigation cannot show enough parameter combinations simultaneously due to the limited screen real estate. Perhaps the parameterbased organization used in [Ngan et al. 2006] is not optimal, but if so, the optimal layout remains undiscovered. Material Complexity. We find color to be a significant challenge in material editing. It takes almost twice as long for subjects to match colored materials than grayscale, and the error is significantly higher. Subjects also tell us that color is often the most diffi cult part of the design process. We believe that there is need for an investigation of methods for perceptual color manipulation of materials under colored lighting. Note that editing color in material editing is very different from setting color in image editing. We find the difficulty of trials, measured by time to completion and error, to be linear in the number of material parameters given to the users for the slider interfaces. When editing materials with more than one lobe, subjects could accomplish the task given enough time. We found spatially-varying materials to be more challenging than the other types of materials studied, resulting in higher final error. Finally, we discovered no significant difference between editing Ward or Cook-Torrance BRDFs. Common Workflow. Our subjects exhibit common workflow patterns. We notice that subjects generally employ a block-and-refine workflow, moving from large edits to small edits. In slider interfaces, subjects do not set parameters directly, but prefer to smoothly change them until they look right. This interactivity is important, and reduces the effect of the parameterization type. Exploration. The advantage of slider interfaces over image navigation is less obvious in open trials. Taking into account the control problems of image navigation, this implies that navigation is a better metaphor to support exploration of broad material variations. Limitations. The main limitation of this work is the scope of material editing tasks we investigate. First, we have only studied a subset of possible BRDF models. Second, we did not explicitly investigate whether novices can effectively pick a material model from a list of available options. Third, we did not investigate the creation of spatial patterns, although we believe that this task is well beyond the capability of novice users. Fourth, we forego the study of interfaces such as painting because of material representation restrictions. This paper presents a first step toward quantitatively evaluating the use and effectiveness of user interfaces for material design, with a focus on novice users. We find that novices can edit materials equally well with sliders that control either physical or perceptually-inspired parameters as long as interactivity is available. Image navigation can help users find important material configurations when artistically exploring possibilities, but performs slower and with less accuracy when precise adjustments are necessary. Novices tend to work similarly to one another, making large edits first and then systematically readjusting each parameter by smaller steps until converging on a final solution. We only study a small subset of interfaces and models for material design, and the development of a method to compare interfaces that operate on different material models would be useful. We would like to thank Jonathan Denning and Lori Lorigo for their help in preparing this paper. This work was supported by NSF (CNS-070820, CCF-0746117), Intel, and the Sloan Foundation.",
  "resources" : [ ]
}