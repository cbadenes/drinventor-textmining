{
  "uri" : "sig2014-a145-templin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a145-templin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Krzysztof-Templin",
      "name" : "Krzysztof",
      "surname" : "Templin"
    }, {
      "uri" : "http://drinventor/Piotr-Didyk",
      "name" : "Piotr",
      "surname" : "Didyk"
    }, {
      "uri" : "http://drinventor/Karol-Myszkowski",
      "name" : "Karol",
      "surname" : "Myszkowski"
    }, {
      "uri" : "http://drinventor/Mohamed-Hefeeda",
      "name" : "Mohamed",
      "surname" : "Hefeeda"
    }, {
      "uri" : "http://drinventor/Hans-Peter-Seidel",
      "name" : "Hans-Peter",
      "surname" : "Seidel"
    }, {
      "uri" : "http://drinventor/Wojciech-Matusik",
      "name" : "Wojciech",
      "surname" : "Matusik"
    } ]
  },
  "bagOfWords" : [ "lead", "simple", "model", "describe", "vergence", "adaptation", "curve", "give", "initial", "target", "disparity", "impact", "optimization", "visual", "quality", "s3d", "content", "demonstrate", "separate", "experiment", "summary", "we", "make", "follow", "contribution", "measurement", "vergence", "response", "rapid", "disparity", "change", "define", "initial", "target", "disparity", "derivation", "evaluation", "model", "relate", "disparity", "change", "vergence", "adaptation", "curve", "along", "average", "observer", "parameter", "interactive", "tool", "visualization", "minimization", "adaptation", "time", "tonic", "-rrb-", "mechanism", "responsible", "precise", "verge", "target", "well", "further", "tracking", "slower", "depth", "change", "Semmlow", "et", "al.", "-lsb-", "1986", "-rsb-", "find", "less", "dynamic", "depth", "change", "ramp", "velocity", "below", "deg/s", "only", "sustained", "mechanism", "active", "above", "deg/s", "transient", "mechanism", "dominate", "otherwise", "both", "mechanism", "active", "thus", "both", "accommodation", "vergence", "response", "can", "improve", "manipulation", "convergence", "local", "image", "defocus", "respectively", "-lsb-", "Ukai", "Kato", "2002", "Zwicker", "et", "al.", "2006", "-rsb-", "out-of-screen", "effect", "even", "beyond", "range", "use", "cinematography", "object", "interest", "typically", "move", "steadily", "off", "screen", "case", "so", "viewer", "can", "adapt", "its", "extreme", "position", "-lsb-", "Zilly", "et", "al.", "2011", "-rsb-", "Vergence", "Measurements", "large", "body", "research", "measurement", "vergence", "dynamics", "response", "pulse", "step", "ramp", "sinusoidal", "disparity", "change", "most", "experiment", "use", "physical", "target", "passively-shifted", "screen", "-lsb-", "erkelen", "et", "al.", "1989", "Hung", "et", "al.", "1994", "-rsb-", "wide", "range", "disparity", "35", "deg", "have", "be", "consider", "-lsb-", "Erkelens", "et", "al.", "1989", "-rsb-", "typical", "range", "below", "10", "deg", "relatively", "large", "step", "amplitude", "typically", "larger", "than", "deg", "-lsb-", "Hung", "2001", "-rsb-", "assume", "disparity", "range", "correspond", "approximately", "comfort", "zone", "desktop", "viewing", "condition", "give", "Shibata", "et", "al.", "-lsb-", "2011", "Fig.", "23", "-rsb-", "also", "initial", "disparity", "magnitude", "important", "we", "measurement", "both", "convergence", "divergence", "case", "s3d", "content", "processing", "problem", "scene", "transition", "challenge", "context", "stereoscopic", "content", "since", "scene", "transition", "often", "create", "large", "temporal", "disparity", "discontinuity", "lead", "visual", "discomfort", "contrast", "Bernhard", "et", "al.", "active", "approach", "we", "propose", "cut", "optimization", "process", "keep", "disparity", "constant", "during", "vergence", "adaptation", "Heinzle", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "computational", "camera", "rig", "which", "enable", "intuitive", "control", "over", "camera", "parameter", "we", "approach", "we", "take", "those", "two", "factor", "account", "author", "also", "suggest", "can", "use", "optimize", "stereoscopic", "parameter", "section", "we", "experimentally", "derive", "evaluate", "model", "eye", "vergence", "response", "step-like", "change", "disparity", "we", "also", "estimate", "model", "parameter", "average", "observer", "collect", "datum", "useful", "number", "application", "show", "Sec", "participant", "sixteen", "subject", "-lrb-", "-rrb-", "take", "part", "we", "experiment", "all", "have", "normal", "corrected-to-normal", "vision", "all", "pass", "test", "stereo-blindness", "equipment", "stimulus", "be", "present", "use", "Nvidia", "3D", "Vision", "kit", "Acer", "gd235hz", "23.6-inch", "screen", "native", "resolution", "1920", "1080", "tracker", "record", "1000", "sample", "per", "second", "-lrb-", "500", "per", "eye", "-rrb-", "allow", "fine-scale", "analysis", "vergence", "response", "spatial", "accuracy", "accord", "eye-tracker", "manufacturer", "up", "0.25", "0.5", "deg", "stimulus", "stimulus", "we", "experiment", "low-pass", "filter", "white-noise", "patch", "change", "its", "disparity", "discrete", "step", "over", "time", "patch", "present", "centrally", "screen", "neutral", "grey", "background", "subtend", "ca.", "11", "degree", "visual", "angle", "single", "trial", "consist", "sequence", "disparity", "...", "choose", "from", "fix", "set", "order", "disparity", "randomize", "avoid", "learning", "effect", "only", "eulerian", "path", "be", "use", "i.", "every", "possible", "transition", "appear", "exactly", "once", "Task", "each", "session", "experiment", "start", "calibration", "procedure", "describe", "eye", "tracker", "manual", "next", "every", "participant", "have", "perform", "trial", "task", "simply", "observe", "patch", "participant", "be", "encourage", "take", "break", "whenever", "feel", "tired", "after", "each", "break", "eye", "tracker", "re-calibrated", "entire", "session", "take", "approximately", "40", "minute", "datum", "analysis", "after", "each", "session", "binary", "output", "eye", "tracker", "convert", "plain-text", "version", "use", "converter", "tool", "provide", "manufacturer", "time", "stimulus", "onset", "be", "mark", "output", "file", "timestamp", "functionality", "provide", "tracker?s", "API", "which", "enable", "easy", "synchronization", "gaze", "datum", "stimulus", "each", "transition", "we", "extract", "1-second", "segment", "follow", "smooth", "use", "small", "box", "filter", "convert", "vergence", "value", "miss", "unreliable", "sample", "-lrb-", "due", "e.", "Data", "transition", "one", "type", "group", "curve", "fit", "average", "base", "premise", "we", "believe", "precision", "sufficient", "we", "purpose", "one", "subject", "-lrb-", "s7", "-rrb-", "perform", "30", "trial", "30", "60", "90", "px", "cut-off", "frequency", "low-pass", "filter", "20", "cpd", "give", "30", "1260", "transition", "measure", "result", "present", "fig.", "discussion", "signal", "converge", "quickly", "give", "relatively", "smooth", "datum", "after", "ca.", "repetition", "little", "could", "gain", "after", "ca.", "10", "repetition", "obtain", "datum", "point", "can", "model", "almost", "perfectly", "use", "two", "plane", "mean", "error", "close", "standard", "deviation", "can", "light", "finding", "we", "decide", "limit", "disparity", "value", "use", "main", "experiment", "30", "90", "px", "number", "repetition", "10", "aim", "main", "experiment", "twofold", "confirm", "vergence", "time", "can", "well", "model", "use", "two", "plane", "suggest", "pilot", "experiment", "so", "estimate", "parameter", "average-observer", "model", "useful", "practical", "application", "experiment", "16", "subject", "perform", "10", "trial", "-lrb-", "except", "subject", "s6", "s9", "s10", "whom", "-rrb-", "cut-off", "frequency", "10", "range", "disparity", "subject", "s9", "reduce", "2/3", "due", "report", "problem", "fusion", "result", "present", "fig.", "discussion", "average", "standard", "deviation", "error", "after", "fitting", "plane", "obtain", "datum", "equal", "36", "ms.", "indicate", "very", "good", "fit", "justify", "we", "assumption", "vergence", "adaptation", "time", "can", "model", "use", "plane", "expect", "we", "measurement", "show", "give", "initial", "disparity", "direction", "step", "larger", "magnitude", "lead", "longer", "vergence", "adaptation", "time", "give", "initial", "disparity", "-lrb-", "fig.", "right", "abscissa", "-rrb-", "step", "magnitude", "-lrb-", "one", "yellow", "one", "green", "line", "per", "magnitude", "-rrb-", "step", "towards", "screen", "generally", "faster", "right", "graph", "yellow", "line", "-lrb-", "convergent", "step", "-rrb-", "have", "lower", "time", "than", "corresponding", "green", "line", "-lrb-", "divergent", "step", "-rrb-", "left", "reverse", "we", "hypothesize", "related", "accommodation-vergence", "coupling", "which", "attract", "vergence", "towards", "screen", "plane", "where", "a/v", "conflict", "disappear", "additionally", "give", "step", "magnitude", "direction", "-lrb-", "fig.", "either", "one", "yellow", "one", "green", "line", "-rrb-", "decrease", "initial", "disparity", "convergent", "step", "get", "slower", "whereas", "divergent", "step", "get", "faster", "effect", "could", "convincingly", "explain", "amount", "a/v", "conflict", "which", "increase", "disparity", "magnitude", "negative", "initial", "disparity", "divergent", "step", "work", "towards", "resolve", "conflict", "whereas", "convergent", "step", "work", "towards", "increase", "positive", "initial", "disparity", "role", "reverse", "larger", "magnitude", "initial", "disparity", "more", "stress", "put", "visual", "system", "demand", "resolve", "-lrb-", "increase", "-rrb-", "conflict", "higher", "thus", "larger", "discrepancy", "between", "convergent", "divergent", "step", "effect", "should", "take", "account", "while", "optimize", "stereoscopic", "content", "simple", "minimization", "disparity", "difference", "necessary", "lead", "shorter", "adaptation", "time", "another", "interesting", "finding", "fix", "target", "disparity", "adaptation", "time", "convergent", "step", "hardly", "dependent", "step", "magnitude", "phenomenon", "first", "unintuitive", "could", "explain", "a/v", "coupling", "well", "larger", "step", "magnitude", "which", "should", "intuitively", "contribute", "longer", "adaptation", "time", "may", "offset", "vary", "initial", "stress", "exert", "a/v", "conflict", "visual", "system", "we", "experiment", "we", "consider", "only", "computer", "display", "observe", "relatively", "short", "distance", "one", "hand", "larger", "view", "distance", "depth", "field", "increase", "thereby", "reduce", "importance", "a/v", "coupling", "hypothesize", "cause", "observe", "variation", "vergence", "adaptation", "time", "answer", "question", "similar", "effect", "initial", "disparity", "adaptation", "time", "can", "observe", "other", "viewing", "condition", "e.", "cinema", "require", "further", "investigation", "obtain", "model", "derive", "use", "simple", "stimulus", "-lrb-", "flat", "whitenoise", "pattern", "-rrb-", "six", "3d", "photograph", "take", "LG", "Optimus", "3d", "p725", "smartphone", "be", "use", "-lrb-", "see", "Fig.", "-rrb-", "disparity", "picture", "be", "estimate", "use", "sift", "flow", "algorithm", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "single", "trial", "6.5", "minute", "random", "sequence", "compose", "three", "photograph", "from", "one", "group", "show", "previously", "single", "appearance", "picture", "last", "between", "1.25", "2.5", "-lrb-", "choose", "randomly", "-rrb-", "be", "break", "between", "appearance", "task", "simply", "observe", "picture", "participant", "be", "ask", "perform", "one", "trial", "each", "group", "first", "automatic", "step", "segment", "where", "saccade", "occur", "time", "cut", "within", "first", "100", "m", "after", "cut", "be", "discard", "initial", "disparity", "estimate", "use", "disparity", "map", "fixation", "coordinate", "just", "before", "cut", "-lrb-", "initial", "fixation", "-rrb-", "target", "disparity", "choose", "use", "follow", "heuristic", "whenever", "duration", "first", "fixation", "shorter", "than", "300", "m", "second", "fixation", "use", "otherwise", "initial", "fixation", "assume", "also", "target", "fixation", "second", "manual", "step", "all", "segment", "be", "briefly", "review", "correct", "filter", "target", "fixation", "error", "false", "negative", "be", "case", "when", "saccade", "near", "cut", "small", "enough", "change", "significantly", "vergence", "response", "false", "positive", "be", "non-typical", "case", "include", "limit", "eye-tracker", "error", "clearly", "incorrect", "vergence", "response", "indicate", "lack", "fusion", "segment", "unusually", "large", "saccade-to-fixation", "ratio", "erratic", "saccade", "indicate", "partial", "fixation", "etc.", "end", "718", "out", "3028", "segment", "be", "discard", "we", "provide", "all", "annotated", "segment", "along", "custom", "viewer/editor", "additional", "material", "encourage", "reader", "inspect", "datum", "we", "use", "evaluation", "end", "segment", "same", "initial/target", "disparity", "be", "group", "group", "more", "member", "be", "average", "compare", "against", "model", "prediction", "respective", "subject", "result", "experiment", "present", "Fig.", "discussion", "although", "we", "prediction", "slightly", "overestimate", "time", "transition", "photograph", "we", "model", "correlate", "well", "actual", "time", "indicate", "relatively", "low", "standard", "deviation", "error", "study", "prove", "we", "model", "good", "predictor", "-lrb-", "up", "additive", "constant", "-rrb-", "transition", "time", "natural", "image", "we", "hypothesize", "improve", "performance", "due", "presence", "higher-order", "cue", "absent", "white-noise", "stimulus", "where", "sole", "depth", "cue", "binocular", "disparity", "section", "we", "propose", "set", "tool", "aid", "production", "stereoscopic", "content", "utilize", "we", "model", "minimize", "vergence", "adaptation", "time", "we", "also", "analyze", "impact", "minimization", "visual", "quality", "one", "propose", "tool", "use", "object-recognition", "experiment", "transition", "Time", "visualization", "straightforward", "application", "model", "visualization", "tool", "provide", "stereographer", "vfx", "artist", "interactive", "analysis", "transition", "time", "therefore", "fixation", "point", "both", "sequence", "need", "precisely", "determine", "datum", "can", "obtain", "from", "various", "source", "e.", "possible", "use", "eye-tracker", "datum", "do", "require", "many", "subject", "have", "be", "show", "eye", "scan-paths", "form", "highly", "repetitive", "pattern", "between", "different", "spectator", "same", "video", "sequence", "-lsb-", "Wang", "et", "al.", "2012", "-rsb-", "moreover", "skilled", "director", "capable", "precisely", "guide", "predict", "viewer", "attention", "prediction", "further", "facilitate", "tendency", "increase", "object", "motion", "modern", "movie", "-lsb-", "cut", "et", "al.", "2011", "-rsb-", "fact", "typical", "2d-movie", "cut", "trigger", "saccade", "towards", "screen", "center", "-lsb-", "Mital", "et", "al.", "2011", "Wang", "et", "al.", "2012", "-rsb-", "thus", "information", "about", "fixation", "point", "we", "method", "can", "very", "reliably", "provide", "director", "besides", "Carmi", "Itti", "-lsb-", "2006", "-rsb-", "observe", "saccade", "immediately", "after", "cut", "drive", "mostly", "bottom-up", "factor", "can", "predict", "relatively", "well", "exist", "saliency", "model", "once", "fixation", "point", "before", "after", "cut", "know", "corresponding", "disparity", "value", "need", "determine", "once", "fixation", "point", "along", "disparity", "value", "know", "transition", "time", "can", "directly", "calculate", "from", "model", "since", "compute", "model", "prediction", "inexpensive", "can", "use", "provide", "real-time", "preview", "transition", "time", "camera", "parameter", "Optimization", "apart", "from", "predict", "transition", "time", "visualize", "they", "editing", "purpose", "one", "can", "automate", "process", "stereoscopic", "content", "preparation", "optimization", "problem", "cut", "can", "define", "we", "model", "can", "serve", "core", "cost", "function", "discuss", "Sec", "stereoscopic", "content", "can", "optimize", "manipulate", "various", "parameter", "can", "change", "entire", "sequence", "-lrb-", "wide", "range", "manipulation", "can", "use", "adjust", "stereoscopic", "content", "range", "from", "very", "simple", "one", "like", "change", "camera", "separation", "convergence", "-lrb-", "i.", "plane", "zero", "parallax", "-rrb-", "more", "complicated", "one", "depth", "remapping", "Cut", "Positioning", "two", "sequence", "between", "which", "cut", "occur", "overlap", "time", "also", "possible", "find", "best", "moment", "cut", "end", "we", "optimize", "only", "stereoscopic", "parameter", "also", "position", "cut", "can", "perform", "efficiently", "simply", "iterate", "over", "all", "possible", "cut", "position", "addition", "all", "horizontal", "shift", "left/right", "view", "optimal", "cut", "can", "choose", "automatically", "can", "show", "editor", "suggestion", "design", "tool", "perform", "task", "show", "fig.", "right", "supplemental", "video", "follow", "experiment", "we", "focus", "time", "necessary", "recognize", "3d", "arrangement", "object", "after", "cut", "we", "assume", "shorter", "recognition", "time", "indicator", "higher", "quality", "we", "measure", "time", "need", "recognize", "object", "arrangement", "show", "time", "closely", "match", "we", "model", "stimulus", "we", "use", "two", "shot", "correspond", "cut", "3d", "version", "big", "Buck", "Bunny", "animation", "we", "modify", "they", "place", "two", "small", "darkgray", "circle", "between", "eye", "character", "approximately", "same", "disparity", "character", "-lrb-", "see", "Fig.", "leave", "inset", "-rrb-", "two", "3d", "arrangement", "circle", "each", "shot", "be", "consider", "one", "upper", "one", "lower", "circle", "closer", "observer", "convergence", "shot", "modify", "so", "average", "disparity", "circle", "equal", "before", "after", "cut", "seven", "pair", "disparity", "step", "be", "use", "75", "105/90", "60", "90", "30", "30", "90/60", "30/30", "30", "60/90", "60", "30/90", "75", "90/105", "px", "each", "initial", "disparity", "both", "convergent", "divergent", "step", "possible", "which", "prevent", "anticipatory", "eye", "movement", "subject", "single", "trial", "each", "procedure", "have", "follow", "structure", "First", "first", "shot", "show", "s.", "Next", "second", "shot", "show", "between", "0.1", "1.5", "-lrb-", "control", "quest", "-rrb-", "arrangement", "circle", "choose", "randomly", "every", "trial", "after", "screen", "blank", "subject", "ask", "indicate", "arrangement", "same", "both", "shot", "same", "circle", "-lrb-", "i.", "upper", "lower", "-rrb-", "closer", "observer", "both", "before", "after", "cut", "subject", "have", "press", "key", "key", "otherwise", "task", "definition", "ensure", "subject", "actually", "perform", "vergence", "transition", "all", "14", "procedure", "be", "perform", "parallel", "randomly", "interleave", "session", "experiment", "last", "20", "min", "-lrb-", "average", "standard", "deviation", "quest", "instance", "73", "m", "-rrb-", "subject", "s3", "s11", "s12", "s15", "s16", "take", "part", "experiment", "s11", "participate", "three", "session", "s16", "two", "remain", "three", "one", "session", "result", "datum", "obtain", "use", "above", "procedure", "fit", "two", "plane", "minimize", "rmse", "plane", "obtain", "from", "all", "subject", "be", "average", "-lrb-", "first", "within", "subject", "between", "subject", "-rrb-", "compare", "average", "model", "result", "present", "fig.", "corrective", "constant", "shift", "83", "m", "minimize", "rmse", "yield", "low", "prediction", "error", "42", "ms.", "correlation", "imply", "optimize", "camera", "convergence", "use", "we", "model", "instead", "disparity", "distance", "cost", "function", "produce", "cut", "shorter", "recognition", "time", "similar", "improvement", "can", "expect", "when", "optimize", "other", "camera", "parameter", "cut", "position", "illustrate", "practical", "importance", "we", "model", "s3d", "game", "film", "we", "propose", "new", "model", "which", "predict", "time", "human", "observer", "need", "adapt", "vergence", "rapid", "disparity", "change", "we", "first", "present", "measurement", "transition", "time", "simple", "stimulus", "demonstrate", "time", "valid", "also", "complex", "scene", "experiment", "reveal", "interesting", "fact", "about", "viewer", "behavior", "during", "rmse", "scene", "cut", "which", "provide", "valuable", "knowledge", "stereoscopic", "content", "creator", "additionally", "we", "propose", "set", "tool", "editing", "stereoscopic", "content", "minimize", "vergence", "adaptation", "time", "after", "cut", "finally", "we", "demonstrate", "impact", "minimize", "adaptation", "time", "visual", "quality", "s3d", "content", "measure", "subject?s", "performance", "3d", "object", "recognition", "task", "work", "partially", "support", "nsf", "iis1111415", "nsf", "iis-1116296", "model", "adaptation", "effect", "vergence", "accommodation", "after", "exposure", "simulated", "virtual", "reality", "stimulus", "ophthalmic", "physiol", "20", "242", "51", "rkelen", "C.", "J.", "DER", "teen", "J.", "teinman", "R.", "M.", "OLLEWIJN", "H.", "1989", "ocular", "vergence", "under", "natural", "condition", "gaze-shift", "between", "real", "target", "differ", "distance", "direction", "Royal", "inke", "R.", "1989", "principle", "mental", "imagery", "MIT", "Press", "einzle", "S.", "REISEN", "P.", "ALLUP", "D.", "HEN", "C.", "aner", "D.", "molic", "a.", "urg", "a.", "atusik", "W.", "ross", "M.", "H.", "2011", "computational", "stereo", "camera", "system", "programmable", "control", "loop", "ACM", "Trans", "30", "94", "OFFMAN", "D.", "IRSHICK", "a.", "keley", "K.", "ANKS", "M.", "2008", "vergence-accommodation", "conflict", "hinder", "visual", "performance", "cause", "visual", "fatigue", "J.", "Vision", "30", "ung", "G.", "K.", "IUFFREDA", "K.", "J.", "emmlow", "J.", "L.", "j.-l", "1994", "vergence", "eye", "movement", "under", "natural", "viewing", "condition", "ophthalmol", "35", "3486", "92", "ung", "G.", "K.", "1992", "adaptation", "model", "accommodation", "vergence", "ophthalmic", "physiol", "12", "319", "26", "ung", "G.", "K.", "1998", "Dynamic", "model", "vergence", "eye", "movement", "system", "simulation", "use", "Matlab/Simulink", "computer", "method", "program", "Biomedicine", "55", "59", "68", "ung", "G.", "K.", "2001", "model", "oculomotor", "control", "World", "Scientific", "Publishing", "Singapore", "OPPAL", "S.", "J.", "ITNICK", "C.", "L.", "OHEN", "M.", "ang", "S.", "B.", "ESSLER", "B.", "olburn", "a.", "2011", "viewer-centric", "editor", "3d", "movie", "IEEE", "Comput", "31", "20", "RISHNAN", "V.", "arazian", "F.", "TARK", "L.", "1973", "analysis", "latency", "prediction", "fusional", "vergence", "system", "J.", "Optometry", "Arch", "academy", "Optometry", "50", "933", "RISHNAN", "V.", "arazian", "F.", "TARK", "L.", "1977", "Dynamic", "measure", "vergence", "accommodation", "American", "Journal", "Optometrics", "physiological", "optics", "54", "470", "ambooij", "M.", "IJ", "SSELSTEIJN", "W.", "ortuin", "m.", "eyn", "derickx", "i.", "2009", "visual", "discomfort", "visual", "fatigue", "stereoscopic", "display", "review", "J.", "Imaging", "Sci", "ambooij", "M.", "IJ", "SSELSTEIJN", "W.", "eynderickx", "i.", "2011", "visual", "discomfort", "3d", "tv", "Assessment", "method", "modeling", "display", "32", "209", "18", "visual", "image", "safety", "ang", "m.", "ornung", "a.", "ang", "O.", "oulako", "S.", "molic", "a.", "ross", "M.", "2010", "nonlinear", "disparity", "mapping", "stereoscopic", "3d", "ACM", "Trans", "29", "75", "iu", "C.", "uen", "J.", "orralba", "a.", "2011", "sift", "flow", "dense", "correspondence", "across", "scene", "its", "application", "pattern", "analysis", "machine", "Intelligence", "IEEE", "transaction", "33", "978", "94", "eester", "L.", "IJ", "SSELSTEIJN", "W.", "EUNTIENS", "P.", "2004", "survey", "perceptual", "evaluation", "requirement", "threedimensional", "tv", "circuit", "Systems", "Video", "Technology", "IEEE", "transaction", "14", "381", "91", "endiburu", "B.", "2009", "3d", "movie", "make", "Stereoscopic", "Digital", "Cinema", "from", "Script", "Screen", "Focal", "Press", "ital", "P.", "mith", "T.", "ILL", "R.", "ENDERSON", "J.", "2011", "clustering", "gaze", "during", "dynamic", "scene", "viewing", "predict", "motion", "cognitive", "computation", "24", "kuyama", "F.", "1998", "human", "visual", "accommodation", "vergence", "eye", "movement", "while", "view", "stereoscopic", "display", "actual", "target", "Society", "vol", "skam", "t.", "ornung", "a.", "owle", "H.", "ITCHELL", "K.", "ross", "M.", "H.", "2011", "oscam-optimized", "stereoscopic", "camera", "control", "interactive", "3d", "ACM", "Trans", "30", "189", "wen", "C.", "2013", "invite", "talk", "2nd", "Toronto", "International", "Stereoscopic", "3D", "Conference", "ushton", "S.", "K.", "iddell", "P.", "M.", "1999", "Applied", "Ergonomics", "30", "69", "78", "chor", "C.", "M.", "1979", "relationship", "between", "fusional", "vergence", "eye", "movement", "fixation", "disparity", "19", "12", "1359", "67", "chor", "C.", "M.", "1992", "relationship", "between", "fusional", "vergence", "eye", "movement", "fixation", "disparity", "Optometry", "Vision", "Science", "69", "258", "69", "chor", "C.", "1999", "influence", "interaction", "between", "accommodation", "convergence", "lag", "accommodation", "ophthalmic", "physiol", "19", "134", "50", "emmlow", "J.", "ETZEL", "P.", "1979", "Dynamic", "contribution", "component", "binocular", "vergence", "josa", "69", "639", "45", "emmlow", "J.", "UNG", "G.", "IUFFREDA", "K.", "1986", "quantitative", "assessment", "disparity", "vergence", "component", "ophthalmol", "27", "558", "64", "hiba", "T.", "IM", "J.", "OFFMAN", "D.", "M.", "ANKS", "M.", "S.", "2011", "zone", "comfort", "predict", "visual", "discomfort", "stereo", "display", "J.", "Vision", "11", "11", "AM", "W.", "J.", "peranza", "F.", "ZQUEZ", "C.", "ENAUD", "R.", "ur", "N.", "2012", "visual", "comfort", "stereoscopic", "object", "move", "horizontal", "mid-sagittal", "plane", "SPIE", "A.", "J.", "Woods", "N.", "S.", "Holliman", "G.", "E.", "Favalora", "Eds", "8288:13", "KAI", "K.", "ATO", "Y.", "2002", "use", "video", "refraction", "measure", "dynamic", "property", "near", "triad", "observer", "3-d", "display", "ophthalmic", "physiol", "22", "385", "ang", "H.", "X.", "reeman", "J.", "ERRIAM", "E.", "P.", "ASSON", "U.", "eeger", "D.", "J.", "2012", "temporal", "eye", "movement", "strategy", "during", "naturalistic", "viewing", "J.", "Vision", "12", "16", "ATSON", "A.", "B.", "ELLI", "D.", "G.", "1983", "quest", "bayesian", "adaptive", "psychometric", "method", "perception", "psychophysic", "33", "113", "20", "ANO", "S.", "MOTO", "M.", "ITSUHASHI", "T.", "2004", "two", "factor", "visual", "fatigue", "cause", "stereoscopic", "hdtv", "image", "display", "25", "-lrb-", "Nov.", "-rrb-", "141", "50", "illy", "F.", "LUGER", "J.", "AUFF", "P.", "2011", "production", "rule", "stereo", "acquisition", "IEEE", "99", "590", "606", "wicker", "m.", "atusik", "W.", "URAND", "F.", "fister", "H.", "orline", "c.", "2006", "antialiase", "automultiscopic", "3d", "display", "egsr", "73", "82" ],
  "content" : "This leads to a simple model describing the vergence adaptation curve, given the initial and target disparities. Impact of the optimization on the visual quality of S3D content is demonstrated in a separate experiment. In summary, we make the following contributions: ? measurements of vergence response to rapid disparity changes defined by initial and target disparities; ? derivation and evaluation of a model relating disparity change to vergence adaptation curve, along with average observer parameters; ? interactive tool for visualization and minimization of adaptation time. tonic) mechanism is responsible for the precise verging on the target, as well as further tracking of slower depth changes. Semmlow et al. [1986] found that for less dynamic depth changes, with the ramp velocity below 2 deg/s, only the sustained mechanism is active, above 9 deg/s the transient mechanism dominates, and otherwise both mechanisms are active. Thus, both accommodation and vergence response can be improved by manipulation of convergence and local image defocus, respectively [Ukai and Kato 2002; Zwicker et al. 2006]. Out-of-screen effects even beyond this range are used in cinematography, but the object of interest typically moves steadily off the screen in such cases, so that the viewer can adapt to its extreme position [Zilly et al. 2011]. Vergence Measurements There is a large body of research on measurements of vergence dynamics in response to pulse, step, ramp, and sinusoidal disparity changes. Most experiments used physical targets or passively-shifted screens [Erkelens et al. 1989; Hung et al. 1994]. A wide range of disparities ?35 deg have been considered [Erkelens et al. 1989], but a typical range was below ?10 deg with relatively large step amplitudes, typically larger than 2 deg [Hung 2001]. The assumed disparity range corresponds approximately to the comfort zone in desktop viewing conditions given by Shibata et al. [2011, Fig. 23]. Also, the initial disparity magnitude is important in our measurements, both for the convergence and divergence case. S3D Content Processing The problem of scene transitions is challenging in the context of stereoscopic content, since scene transitions often create large temporal disparity discontinuities leading to visual discomfort. In contrast to Bernhard et al.?s active approach, we propose a cut optimization process that keeps the disparities constant during the vergence adaptation. Heinzle et al. [2011] proposed a computational camera rig, which enables intuitive control over camera parameters. In our approach, we take those two factors into account. The authors also suggest that it can be used for optimizing stereoscopic parameters. In this section, we experimentally derive and evaluate a model of eye vergence response to step-like changes in disparity. We also estimate model parameters for an average observer. The collected data is useful in a number of applications, as shown in Sec. Participants Sixteen subjects (8 F, 8 M) took part in our experiment. All had normal or corrected-to-normal vision, and all passed a test for stereo-blindness. Equipment Stimuli were presented using an Nvidia 3D Vision 2 kit and an Acer GD235HZ 23.6-inch screen with native resolution of 1920 ? 1080. The tracker records 1000 samples per second (500 per eye), allowing for fine-scale analysis of the vergence response. The spatial accuracy according to the eye-tracker manufacturer is up to 0.25?0.5 deg. Stimulus The stimulus in our experiment was a low-pass filtered white-noise patch changing its disparity in discrete steps over time. The patch was presented centrally on the screen, on a neutral grey background, and it subtended ca. 11 degrees of visual angle. A single trial consisted of a sequence of disparities d 1 , d 2 , . . . , d n , chosen from a fixed set D. The ordering of the disparities was randomized to avoid learning effect, but only Eulerian paths were used, i. , d 1 = d n , and every possible transition appeared exactly once. Task Each session of the experiment started with a calibration procedure, as described in the eye tracker manual. Next, every participant had to perform m trials, and the task was to simply observe the patch. The participants were encouraged to take breaks whenever they felt tired, and after each break the eye tracker was re-calibrated. The entire session took approximately 40 minutes. Data Analysis After each session, binary output of the eye tracker was converted to a plain-text version using the converter tool provided by the manufacturer. The times of stimulus onsets were marked in the output files with timestamps ? a functionality provided by the tracker?s API, which enabled easy synchronization of the gaze data with stimuli. For each transition, we extracted the 1-second segment following it, smoothed using a small box filter, and converted it to vergence values. Missing or unreliable samples (due to, e. Data for transitions of one type was grouped, and a curve was fitted to the average. Based on these premises, we believe the precision was sufficient for our purposes. In it, one subject (S7) performed m = 30 trials, with d i = 0, ?30, ?60, ?90 px, and the cut-off frequency of the low-pass filter f = 20 cpd. This gave 30 ? 7 ? 6 = 1260 transitions measured. The results are presented in Fig. 2 . Discussion The signal converged quickly, giving relatively smooth data after ca. 5 repetitions, and little could be gained after ca. 10 repetitions. The obtained data points can be modeled almost perfectly using two planes, with mean error close to 0, and standard deviation of ca. In light of these findings we decided to limit the disparity values used in the main experiment to d i = ?30, ?90 px, and the number of repetitions m to 10. The aim of the main experiment was twofold: to confirm that vergence times can be well modeled using two planes, as suggested by the pilot experiment, and, if so, to estimate parameters of the average-observer model, useful in practical applications. In this experiment n = 16 subjects performed m = 10 trials (except subjects S6, S9, and S10 for whom m = 5), with the cut-off frequency f = 10. The range of disparities for subject S9 was reduced to 2/3, due to reported problems with fusion. The results are presented in Fig. 3 . Discussion The average standard deviation of error after fitting the planes to the obtained data equals 36 ms. This indicates a very good fit, and justifies our assumption that the vergence adaptation time can be modeled using planes. As expected, our measurements show that given the initial disparity and direction, steps with larger magnitude lead to longer vergence adaptation times. Given the initial disparity ( Fig. 3 , right, abscissae) and step magnitude (one yellow and one green line per magnitude), steps towards the screen are generally faster: To the right of the graph, yellow lines (convergent steps) have lower times than the corresponding green lines (divergent steps). To the left, this is reversed. We hypothesize that it is related to accommodation-vergence coupling, which attracts vergence towards the screen plane, where the A/V conflict disappears. Additionally, given the step magnitude and direction ( Fig. 3 , either one yellow or one green line), with decreasing initial disparity, convergent steps get slower whereas divergent steps get faster. This effect could be convincingly explained by the amount of A/V conflict which increases with disparity magnitude. At negative initial disparities, divergent steps work towards resolving the conflict, whereas convergent steps work towards increasing it. With positive initial disparities the roles are reversed. The larger the magnitude of the initial disparity, the more stress is put on the visual system, and the demand to resolve (or not to increase) the conflict is higher. Thus, the larger discrepancy between convergent and divergent steps. These effects should be taken into account while optimizing stereoscopic content, as simple minimization of disparity difference will not necessary lead to shorter adaptation times. Another interesting finding is that with fixed target disparity, adaptation times for convergent steps are hardly dependent on the step magnitude. This phenomenon, at first unintuitive, could be explained by the A/V coupling as well: Larger step magnitudes, which should intuitively contribute to longer adaptation times, may be offset by varying initial stress exerted by the A/V conflict on the visual system. In our experiment we considered only a computer display observed at a relatively short distance. On the one hand, at larger viewing distances the depth of field increases, thereby reducing the importance of the A/V coupling, the hypothesized cause of the observed variation in vergence adaptation time. Answering the question, if similar effect of initial disparity on the adaptation time can be observed in other viewing conditions, e. , in cinema, requires further investigation. The obtained model was derived using simple stimuli (flat whitenoise patterns). Six 3D photographs taken with an LG Optimus 3D P725 smartphone were used (see Fig. 4 ). The disparities in the picture were estimated using the SIFT flow algorithm [Liu et al. 2011]. In a single trial a 6.5minute random sequence composed of the three photographs from one of the groups was shown. As previously, a single appearance of a picture lasted between 1.25 s and 2.5 s (chosen randomly), and there were no breaks between appearances. The task was to simply observe the pictures, and the participants were asked to perform one trial for each group. In the first, automatic step segments where a saccade occurred at the time of the cut, or within the first 100 ms after the cut, were discarded. Then, initial disparity was estimated using the disparity map and the fixation coordinates just before the cut (initial fixation). The target disparity was chosen using the following heuristic: whenever the duration of the first fixation was shorter than 300 ms, the second fixation was used; otherwise, the initial fixation was assumed to be also the target fixation. In the second, manual step, all segments were briefly reviewed to correct filtering and target fixation errors. The false negatives were the cases when the saccade near the cut was small enough not to change significantly the vergence response. The false positives were the non-typical cases, including, but not limited to, eye-tracker errors, clearly incorrect vergence response indicating lack of fusion, segments with unusually large saccade-to-fixation ratio, erratic saccades indicating partial fixations, etc. In the end, 718 out of 3028 segments were discarded. We provide all annotated segments along with a custom viewer/editor as additional materials, and encourage the readers to inspect the data we used in this evaluation. In the end, segments with the same initial/target disparities were grouped; groups with 5 or more members were averaged and compared against the model prediction for the respective subject. The results of the experiment are presented in Fig. 5 . Discussion Although our prediction slightly overestimated the time of transition for photographs, our model correlated well with the actual time, as indicated by relatively low standard deviation of the error. The study proves that our model is a good predictor (up to an additive constant) of transition time for natural images. We hypothesize that improved performance was due to the presence of higher-order cues, absent in white-noise stimuli, where the sole depth cue was binocular disparity. In this section, we propose a set of tools for aiding in the production of stereoscopic content, that utilizes our model to minimize vergence adaptation times. We also analyze the impact of the minimization on visual quality in one of the proposed tools using an object-recognition experiment. Transition Time Visualization A straightforward application of the model is a visualization tool providing stereographers and VFX artists with an interactive analysis of transition times. Therefore, the fixation points in both sequences need to be precisely determined. Such data can be obtained from various sources, e. , it is possible to use eye-tracker data. This does not require many subjects, as it  has been shown that eye scan-paths form highly repetitive patterns between different spectators for the same video sequences [Wang et al. 2012]. Moreover, skilled directors are capable of precisely guiding and predicting viewers? attention. Such prediction is further facilitated by the tendency of increasing object motion in modern movies [Cutting et al. 2011] and by the fact that typical 2D-movie cuts trigger saccades towards the screen center [Mital et al. 2011; Wang et al. 2012]. Thus, the information about fixation points for our methods can be very reliably provided by the directors. Besides, Carmi and Itti [2006] observed that the saccades immediately after the cut are driven mostly by the bottom-up factors and can be predicted relatively well by existing saliency models. Once the fixation points before and after the cut are known, the corresponding disparity values need to be determined. Once the fixation points along with disparity values are known, transition times can be directly calculated from the model. Since computing model predictions is inexpensive, it can be used to provide real-time preview of transition times. Camera Parameters Optimization Apart from predicting transition times and visualizing them for editing purposes, one can automate the process of stereoscopic content preparation. An optimization problem for cuts can be defined, and our model can serve as the core of the cost function. As discussed in Sec. 2, stereoscopic content can be optimized by manipulating various parameters. These can be changed for the entire sequence (e. There is a wide range of manipulations that can be used to adjust stereoscopic content. They range from very simple ones, like changing camera separation and convergence (i. , the plane of zero parallax), to more complicated ones, such as depth remapping. Cut Positioning If the two sequences between which the cut occurs overlap in time, it is also possible to find the best moment for the cut. To this end, we optimize not only stereoscopic parameters, but also the position of the cut. This can be performed efficiently by simply iterating over all possible cut positions, in addition to all horizontal shifts of the left/right views. The optimal cut can be chosen automatically or can be shown to the editor as a suggestion. The design of a tool performing these tasks is shown in Fig. 1 , right, and in the supplemental video. In the following experiment, we focus on the time necessary to recognize the 3D arrangement of objects after a cut. We assume shorter recognition times to be an indicator of higher quality. We measured the time needed to recognize object arrangement, and showed that this time closely matches our model. As stimuli, we used two shots corresponding to a cut in the 3D version of the Big Buck Bunny animation. We modified them by placing two small darkgray circles between the eyes of the character, with approximately the same disparity as the character (see Fig. 6 , left, inset). Two 3D arrangements of circles for each shot were considered: one with the upper, and one with the lower circle closer to the observer. The convergence in the shots was modified so that the average disparity of the circles was equal to d i before and d t after the cut. Seven pairs of disparity steps were used: ?75 ? ?105/90, ?60 ? ?90/ ? 30, ?30 ? ?90/60, 0 ? ?30/30, 30 ? ?60/90, 60 ? 30/90, and 75 ? ?90/105 px. For each initial disparity, both a convergent and a divergent step was possible, which prevented anticipatory eye movements in subjects. A single trial of each procedure had the following structure: First, the first shot was shown for 2 s. Next, the second shot was shown for a period between 0.1 and 1.5 s (controlled by QUEST). The arrangement of the circles was chosen randomly in every trial. After the screen was blanked, the subject was asked to indicate if the arrangement was the same in both shots: If the same circle (i. , upper or lower) was closer to the observer both before and after the cut, the subject had to press the Y key, and the N key otherwise. Such a task definition ensured that the subject actually performed the vergence transition d i ? d t . All 14 procedures were performed in parallel, randomly interleaved. A session of the experiment lasted 20 min (average standard deviation in a QUEST instance 73 ms). Subjects S3, S11, S12, S15, and S16 took part in the experiment. S11 participated in three sessions, S16 in two, and the remaining three in one session. Results The data obtained using the above procedure was fitted with two planes minimizing the RMSE. The planes obtained from all subjects were averaged (first within subjects, then between subjects), and compared to their average model. The results are presented in Fig. 6 . A corrective constant shift of 83 ms minimizes the RMSE, and yields a low prediction error of 42 ms. This correlation implies that optimizing camera convergence using our model instead of disparity distance as the cost function will produce cuts with shorter recognition times. Similar improvement can be expected when optimizing other camera parameters or cut positions. This illustrates the practical importance of our model for S3D games and films. We proposed a new model which predicts the time a human observer needs to adapt vergence to rapid disparity changes. We first presented measurements of transition times for simple stimuli, and demonstrated that these times are valid also for complex scenes. The experiment revealed interesting facts about viewer behavior during RMSE\n        scene cuts, which provides valuable knowledge for stereoscopic content creators. Additionally, we proposed a set of tools for the editing of stereoscopic content to minimize the vergence adaptation time after cuts. Finally, we demonstrated the impact of minimizing adaptation times on the visual quality of S3D content as measured by a subject?s performance in the 3D object recognition task. This work was partially supported by NSF IIS1111415 and NSF IIS-1116296. Modelling adaptation effects in vergence and accommodation after exposure to a simulated virtual reality stimulus. Ophthalmic Physiol. 20, 3, 242?51. E RKELENS , C. J., V AN DER S TEEN , J., S TEINMAN , R. M., AND C OLLEWIJN , H. 1989. Ocular vergence under natural conditions. Gaze-shifts between real targets differing in distance and direction. of the Royal. F INKE , R. 1989. Principles of Mental Imagery. MIT Press. H EINZLE , S., G REISEN , P., G ALLUP , D., C HEN , C., S ANER , D., S MOLIC , A., B URG , A., M ATUSIK , W., AND G ROSS , M. H. 2011. Computational stereo camera system with programmable control loop. ACM Trans. 30, 4, 94. H OFFMAN , D., G IRSHICK , A., A KELEY , K., AND B ANKS , M. 2008. Vergence-accommodation conflicts hinder visual performance and cause visual fatigue. J. Vision 8, 3, 1?30. H UNG , G. K., C IUFFREDA , K. J., S EMMLOW , J. L., AND H OR , J.-L. 1994. Vergence eye movements under natural viewing conditions. Ophthalmol. 35, 3486?92. H UNG , G. K. 1992. Adaptation model of accommodation and vergence. Ophthalmic Physiol. 12, 3, 319?26. H UNG , G. K. 1998. Dynamic model of the vergence eye movement system: Simulations using Matlab/Simulink. Computer Methods and Programs in Biomedicine 55, 1, 59?68. H UNG , G. K. 2001. Models of oculomotor control. World Scientific Publishing, Singapore. K OPPAL , S. J., Z ITNICK , C. L., C OHEN , M., K ANG , S. B., R ESSLER , B., AND C OLBURN , A. 2011. A viewer-centric editor for 3d movies. IEEE Comput. 31, 1, 20. K RISHNAN , V., F ARAZIAN , F., AND S TARK , L. 1973. An analysis of latencies and prediction in the fusional vergence system. J. Optometry and Arch. Academy of Optometry 50, 933?9. K RISHNAN , V., F ARAZIAN , F., AND S TARK , L. 1977. Dynamic measures of vergence accommodation. American Journal of Optometrics and Physiological Optics 54, 470?3. L AMBOOIJ , M., IJ SSELSTEIJN , W., F ORTUIN , M., AND H EYN DERICKX , I. 2009. Visual discomfort and visual fatigue of stereoscopic displays: A review. J. Imaging Sci. L AMBOOIJ , M., IJ SSELSTEIJN , W., AND H EYNDERICKX , I. 2011. Visual discomfort of 3D TV: Assessment methods and modeling. Displays 32, 4, 209?18. Visual Image Safety. L ANG , M., H ORNUNG , A., W ANG , O., P OULAKOS , S., S MOLIC , A., AND G ROSS , M. 2010. Nonlinear disparity mapping for stereoscopic 3D. ACM Trans. 29, 4, 75. L IU , C., Y UEN , J., AND T ORRALBA , A. 2011. Sift flow: Dense correspondence across scenes and its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on 33, 5, 978?94. M EESTERS , L., IJ SSELSTEIJN , W., AND S EUNTIENS , P. 2004. A survey of perceptual evaluations and requirements of threedimensional tv. Circuits and Systems for Video Technology, IEEE Transactions on 14, 3, 381?91. M ENDIBURU , B. 2009. 3D Movie Making: Stereoscopic Digital Cinema from Script to Screen. Focal Press. M ITAL , P., S MITH , T., H ILL , R., AND H ENDERSON , J. 2011. Clustering of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation 3, 1, 5?24. O KUYAMA , F. 1998. Human visual accommodation and vergence eye movement while viewing stereoscopic display and actual target. Society, vol. O SKAM , T., H ORNUNG , A., B OWLES , H., M ITCHELL , K., AND G ROSS , M. H. 2011. Oscam-optimized stereoscopic camera control for interactive 3d. ACM Trans. 30, 6, 189. O WENS , C., 2013. Invited talk. 2nd Toronto International Stereoscopic 3D Conference. R USHTON , S. K., AND R IDDELL , P. M. 1999. Applied Ergonomics 30, 1, 69?78. S CHOR , C. M. 1979. The relationship between fusional vergence eye movements and fixation disparity. 19, 12, 1359?67. S CHOR , C. M. 1992. The relationship between fusional vergence eye movements and fixation disparity. Optometry and Vision Science 69, 4, 258?69. S CHOR , C. 1999. The influence of interactions between accommodation and convergence on the lag of accommodation. Ophthalmic Physiol. 19, 2, 134?50. S EMMLOW , J., AND W ETZEL , P. 1979. Dynamic contributions of the components of binocular vergence. JOSA 69, 639?45. S EMMLOW , J., H UNG , G., AND C IUFFREDA , K. 1986. Quantitative assessment of disparity vergence components. Ophthalmol. 27, 558?64. S HIBATA , T., K IM , J., H OFFMAN , D. M., AND B ANKS , M. S. 2011. The zone of comfort: Predicting visual discomfort with stereo displays. J. Vision 11, 8, 11. T AM , W. J., S PERANZA , F., V ?ZQUEZ , C., R ENAUD , R., AND H UR , N. 2012. Visual comfort: stereoscopic objects moving in the horizontal and mid-sagittal planes. SPIE, A. J. Woods, N. S. Holliman, and G. E. Favalora, Eds. , 8288:13. U KAI , K., AND K ATO , Y. 2002. The use of video refraction to measure the dynamic properties of the near triad in observers of a 3-d display. Ophthalmic Physiol. 22, 5, 385?8. W ANG , H. X., F REEMAN , J., M ERRIAM , E. P., H ASSON , U., AND H EEGER , D. J. 2012. Temporal eye movement strategies during naturalistic viewing. J. Vision 12, 1, 16. W ATSON , A. B., AND P ELLI , D. G. 1983. QUEST: a Bayesian adaptive psychometric method. Perception and Psychophysics 33, 2, 113?20. Y ANO , S., E MOTO , M., AND M ITSUHASHI , T. 2004. Two factors in visual fatigue caused by stereoscopic HDTV images. Displays 25, 4 (Nov.), 141?50. Z ILLY , F., K LUGER , J., AND K AUFF , P. 2011. Production rules for stereo acquisition. IEEE 99, 4, 590?606. Z WICKER , M., M ATUSIK , W., D URAND , F., P FISTER , H., AND F ORLINES , C. 2006. Antialiasing for automultiscopic 3D displays. EGSR, 73?82.",
  "resources" : [ ]
}