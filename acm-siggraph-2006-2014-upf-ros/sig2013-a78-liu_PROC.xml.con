{
  "uri" : "sig2013-a78-liu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a78-liu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Bundled Camera Paths for Video Stabilization",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shuaicheng-Liu",
      "name" : "Shuaicheng",
      "surname" : "Liu"
    }, {
      "uri" : "http://drinventor/Lu-Yuan",
      "name" : "Lu",
      "surname" : "Yuan"
    }, {
      "uri" : "http://drinventor/Ping-Tan",
      "name" : "Ping",
      "surname" : "Tan"
    }, {
      "uri" : "http://drinventor/Jian Sun-null",
      "name" : "Jian Sun",
      "surname" : null
    } ]
  },
  "bagOfWords" : [ "we", "run", "we", "method", "Intel", "i7", "3.2", "GHZ", "Quad-Core", "machine", "8g", "RAM", "we", "extract", "400-600", "surf", "feature", "-lsb-", "Bay", "et", "al.", "2008", "-rsb-", "per", "frame", "motion", "estimation", "we", "always", "divide", "video", "frame", "16", "16", "cell", "video", "1280", "720", "resolution", "we", "unoptimized", "system", "take", "392", "millisecond", "process", "frame", "-lrb-", "around", "2.5", "fp", "-rrb-", "specifically", "we", "spend", "300m", "50m", "12m", "30m", "extract", "feature", "estimate", "motion", "optimize", "camera", "path", "render", "final", "result", "all", "original", "result", "video", "provide", "we", "webpage", "we", "first", "verify", "effectiveness", "different", "component", "propose", "approach", "global", "path", "vs.", "bundle", "path", "example", "Figure", "result", "accord", "global", "path", "have", "remain", "jitters", "some", "image", "region", "because", "parallax", "make", "global", "homography", "motion", "model", "invalid", "therefore", "some", "image", "region", "can", "stabilize", "very", "well", "we", "bundle", "path", "can", "handle", "kind", "typical", "situation", "please", "refer", "we", "accompany", "video", "visual", "comparison", "spatially-variant", "homography", "vs.", "Homography", "Mixture", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "homography", "mixture", "model", "roll", "shutter", "correction", "divide", "video", "frame", "1d", "array", "horizontal", "block", "use", "gaussian", "mixture", "homography", "each", "block", "model", "beyond", "single", "2d", "transformation", "able", "partially", "handle", "parallax", "http://www.ece.nus.edu.sg/stfpage/eletp/projects/stabilization/stabili", "zationsig13.html", "compare", "we", "2d", "mesh-based", "spatially-variant", "homography", "model", "have", "two", "limitation", "-rrb-", "do", "address", "horizontal", "depth", "variation", "-rrb-", "use", "weaker", "feature", "point", "-lrb-", "which", "apply", "lower", "threshold", "level", "feature", "detection", "-rrb-", "simple", "gaussian", "mixture", "regularization", "weaker", "feature", "point", "may", "result", "larger", "fitting", "error", "ability", "use", "simple", "gaussian", "smoothing", "limit", "Figure", "show", "comparison", "two", "model", "example", "scene", "have", "horizontal", "depth", "variation", "sky", "region", "lack", "feature", "point", "Figure", "-lrb-", "-rrb-", "result", "use", "YouTube", "Stabilizer", "-lrb-", "integrate", "Homography", "Mixture", "feature", "-rrb-", "we", "can", "observe", "severe", "geometrical", "distortion", "further", "verify", "we", "observation", "we", "replace", "we", "spatially-variant", "model", "homography", "mixture", "model", "-lrb-", "we", "implementation", "-rrb-", "we", "framework", "generate", "result", "Figure", "-lrb-", "-rrb-", "where", "we", "observe", "similar", "distortion", "comparison", "we", "warping-based", "motion", "estimation", "can", "fundamentally", "handle", "depth", "variation", "-lrb-", "limit", "vertical", "direction", "-rrb-", "we", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "do", "suffer", "from", "distortion", "please", "also", "see", "comparison", "accompany", "video", "Rolling", "Shutter", "Handling", "Figure", "compare", "we", "method", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "two", "example", "video", "from", "paper", "we", "model", "account", "frame", "distortion", "skew", "-lrb-", "leave", "example", "-rrb-", "local", "wobble", "-lrb-", "right", "example", "-rrb-", "more", "example", "include", "supplementary", "video", "which", "show", "we", "achieve", "similar", "result", "correct", "rolling", "shutter", "distortion", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "quantitatively", "evaluate", "measure", "result", "from", "different", "aspect", "we", "define", "three", "objective", "metric", "crop", "distortion", "we", "first", "two", "metric", "measure", "crop", "ratio", "global", "distortion", "we", "first", "fit", "global", "homography", "each", "frame", "between", "input", "video", "output", "video", "we", "compute", "crop", "ratio", "distortion", "each", "frame", "crop", "ratio", "can", "directly", "compute", "from", "scale", "component", "homography", "one", "global", "crop", "ratio", "whole", "sequence", "each", "frame", "provide", "estimation", "we", "average", "estimation", "all", "frame", "final", "metric", "distortion", "compute", "define", "section", "4.1", "because", "any", "distortion", "single", "frame", "destroy", "perfection", "whole", "result", "we", "choose", "minimum", "across", "whole", "sequence", "final", "metric", "worst-case", "metric", "allow", "we", "easily", "see", "whether", "whole", "result", "video", "completely", "successful", "good", "result", "both", "metric", "should", "close", "stability", "third", "metric", "measure", "stability", "result", "Designing", "good", "metric", "non-trivial", "because", "hard", "compare", "two", "different", "video", "we", "suggest", "empirically", "good", "metric", "use", "frequency", "analysis", "estimate", "2d", "motion", "from", "video", "we", "basic", "assumption", "more", "energy", "contain", "low", "frequency", "part", "motion", "more", "stable", "video", "computationally", "we", "estimate", "we", "bundle", "camera", "path", "approximate", "true", "motion", "-lrb-", "optical", "flow", "-rrb-", "video", "we", "do", "smooth", "out", "anything", "after", "estimation", "we", "extract", "translation", "rotation", "component", "from", "each", "path", "each", "component", "1d", "temporal", "signal", "finally", "we", "evaluate", "energy", "percentage", "low", "frequency", "component", "-lrb-", "expect", "dc", "component", "-rrb-", "1d", "signal", "measure", "stability", "specifically", "we", "take", "few", "lowest", "-lrb-", "empirically", "set", "from", "2nd", "6th", "-rrb-", "frequency", "calculate", "energy", "percentage", "over", "full", "frequency", "-lrb-", "exclude", "dc", "component", "-rrb-", "similar", "distortion", "we", "take", "smallest", "measurement", "among", "translation", "rotation", "final", "metric", "good", "result", "metric", "should", "approach", "here", "well", "purpose", "comparison", "test", "whether", "we", "result", "comparable", "-lrb-", "better", "than", "-rrb-", "previous", "successful", "result", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "Grundmann", "et", "al.", "2011", "-rsb-", "we", "collect", "eleven", "test", "video", "from", "papers", "-lrb-", "thumbnail", "Figure", "10", "-rrb-", "compare", "we", "result", "publish", "result", "-lrb-", "all", "from", "author", "project", "webpage", "-rrb-", "overall", "all", "method", "generate", "similar", "stability", "both", "subjectively", "quantitatively", "-lrb-", "Figure", "10", "-rrb-", "example", "while", "we", "result", "slightly", "better", "some", "video", "term", "crop", "ratio", "distortion", "video", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3d", "stabilization", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "achieve", "best", "stability", "distortion", "score", "suggest", "3d", "method", "first", "choice", "-lrb-", "term", "stability", "distortion", "error", "-rrb-", "when", "3d", "motion", "can", "successfully", "estimate", "although", "we", "result", "slightly", "worse", "stability", "visual", "difference", "quite", "small", "-lrb-", "please", "verify", "from", "supplementary", "video", "-rrb-", "furthermore", "aggressive", "smoothing", "3d", "method", "sometimes", "lead", "output", "fov", "too", "small", "demonstrate", "crop", "score", "we", "method", "manage", "provide", "good", "trade-off", "video", "-lrb-", "5-9", "-rrb-", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "-lsb-", "Goldstein", "Fattal", "2012", "-rsb-", "we", "method", "achieve", "similar", "stability", "while", "we", "method", "slightly", "better", "crop", "distortion", "video", "-lrb-", "10-11", "-rrb-", "we", "method", "outperform", "l1optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "stability", "-lrb-", "slightly", "-rrb-", "crop", "ratio", "distortion", "score", "Figure", "11", "highlight", "most", "challenging", "video", "-lrb-", "10", "-rrb-", "dataset", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "refer", "example", "failure", "case", "because", "single", "subspace", "can", "account", "feature", "trajectory", "both", "face", "background", "result", "have", "visible", "distortion", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "produce", "better", "result", "example", "video", "result", "we", "still", "observe", "large", "temporal", "distortion", "background", "region", "-lrb-", "see", "we", "accompany", "video", "-rrb-", "comparison", "we", "method", "can", "successfully", "handle", "example", "-lrb-", "achieve", "best", "term", "all", "three", "metric", "-rrb-", "because", "warping-based", "motion", "model", "can", "represent", "complicated", "motion", "due", "publicly", "available", "implementation", "previous", "work", "we", "compare", "we", "system", "two", "well-known", "commercial", "system", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "after", "Effects", "CS6", "YouTube", "Stabilizer", "base", "combination", "norm", "path", "optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "homography", "mixture", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "Warp", "Stabilizer", "Adobe", "after", "Effects", "largely", "base", "subspace", "stabilization", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "we", "understand", "commercial", "product", "often", "different", "from", "give", "research", "system", "we", "believe", "two", "system", "represent", "essential", "element", "research", "conduct", "field", "comparison", "make", "sense", "examine", "strength", "weakness", "robustness", "-lrb-", "various", "video", "use", "set", "fix", "parameter", "-rrb-", "we", "system", "Dataset", "we", "assemble", "comprehensive", "dataset", "174", "short", "video", "-lrb-", "10", "60", "seconds", "-rrb-", "from", "previous", "publication", "internet", "we", "own", "capture", "know", "strength", "weakness", "method", "different", "situation", "we", "roughly", "divide", "we", "datum", "category", "base", "camera", "motion", "scene", "type", "-lrb-", "-rrb-", "simple", "-lrb-", "II", "-rrb-", "quick", "rotation", "-lrb-", "III", "-rrb-", "zoom", "-lrb-", "iv", "-rrb-", "large", "parallax", "-lrb-", "-rrb-", "driving", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "running", "better", "measure", "stability", "background", "motion", "-lrb-", "cause", "camera", "shake", "-rrb-", "we", "use", "manual", "foreground", "mask", "exclude", "foreground", "motion", "YouTube", "Stabilizer", "parameter-free", "online", "tool", "Warp", "Stabilizer", "interactive", "system", "user", "might", "carefully", "tune", "few", "parameter", "here", "we", "wish", "examine", "its", "robustness", "automatic", "tool", "fix", "its", "parameter", "we", "use", "example", "video", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "decide", "best", "parameter", "finally", "we", "choose", "default", "parameter", "-lrb-", "smoothness", "50", "smooth", "Motion", "Subspace", "Warp", "-rrb-", "produce", "result", "quantitative", "comparison", "each", "category", "we", "compute", "average", "metric", "standard", "deviation", "three", "system", "-lrb-", "figure", "12", "-lrb-", "-rrb-", "-rrb-", "we", "discuss", "result", "regard", "each", "system", "detail", "below", "all", "three", "system", "perform", "well", "category", "-lrb-", "-rrb-", "simple", "since", "category", "contain", "video", "relatively", "smooth", "camera", "motion", "mild", "depth", "variation", "though", "we", "method", "have", "minor", "advantage", "user", "can", "safely", "choose", "any", "three", "get", "desire", "result", "among", "remain", "category", "we", "want", "highlight", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "three", "system", "achieve", "similar", "stability", "while", "we", "system", "clearly", "better", "term", "distortion", "we", "show", "two", "example", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "visual", "comparison", "we", "system", "YouTube", "Stabilizer", "example", "show", "limitation", "1d", "array", "homography", "mixture", "can", "model", "depth", "change", "horizontal", "direction", "Warp", "Stabilizer", "also", "generate", "some", "shearing/skewing", "artifact", "some", "video", "frame", "though", "principle", "3d", "method", "should", "able", "handle", "parallax", "Figure", "12", "-lrb-", "-rrb-", "show", "example", "-lrb-", "please", "note", "shearing", "bookshelf", "-rrb-", "probably", "due", "subspace", "analysis", "failure", "cause", "occlusion", "we", "method", "succeed", "all", "example", "comparison", "category", "clearly", "demonstrate", "advantage", "we", "warping-based", "motion", "model", "deal", "large", "parallax", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "which", "challenging", "case", "method", "require", "long", "feature", "tracking", "Warp", "Stabilizer", "often", "generate", "significant", "crop", "Figure", "12", "-lrb-", "-rrb-", "example", "alleviate", "problem", "we", "try", "interactively", "tune", "its", "smoothing", "parameter", "when", "apply", "weaker", "smoothing", "however", "we", "find", "its", "result", "become", "shaky", "comparison", "we", "method", "generate", "stable", "result", "much", "less", "crop", "category", "-lrb-", "v?vii", "-rrb-", "three", "system", "generate", "similar", "stability", "level", "-lrb-", "Warp", "Stabilizer", "slightly", "better", "category", "vii", "-rrb-", "while", "we", "sy", "tem", "consistently", "better", "respect", "either", "crop", "ratio", "distortion", "control", "we", "notice", "we", "method", "generate", "relatively", "smaller", "standard", "deviation", "three", "metric", "all", "category", "suggest", "we", "method", "generate", "more", "consistent", "result", "from", "various", "input", "user", "study", "we", "further", "conduct", "user", "study", "40", "participant", "evaluate", "compare", "we", "method", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "AfterEffects", "CS6", "every", "participant", "require", "evaluate", "result", "28", "different", "input", "video", "-lrb-", "randomly", "sample", "from", "we", "dataset", "-rrb-", "which", "video", "each", "category", "mention", "above", "-lrb-", "video", "prepare", "way", "two", "they", "compare", "we", "result", "YouTube", "Stabilizer", "other", "two", "Warp", "Stabilizer", "-rrb-", "user", "study", "we", "use", "scheme", "forced", "two-alternative", "choice", "every", "participant", "ask", "pick", "better", "one", "between", "result", "we", "method", "YouTube", "Stabilizer", "between", "result", "we", "method", "Warp", "Stabilizer", "video", "display", "subject", "random", "order", "subject", "unaware", "video", "category", "neither", "do", "know", "which", "technique", "use", "produce", "stabilize", "result", "Figure", "13", "-lrb-", "-rrb-", "show", "interface", "user", "study", "original", "video", "display", "top", "two", "stabilize", "one", "show", "side-by-side", "below", "user", "can", "simultaneously", "play", "input", "video", "both", "two", "result", "better", "examine", "difference", "video", "can", "play", "back", "forth", "pause", "certain", "frame", "help", "user", "carefully", "make", "decision", "user", "can", "also", "play", "each", "video", "individually", "examine", "quality", "without", "other", "distraction", "we", "ask", "user", "disregard", "difference", "aspect", "ratio", "sharpness", "since", "each", "one", "may", "undergo", "different", "video", "codec", "further", "post-processing", "which", "make", "uniform", "treatment", "difficult", "user", "study", "result", "show", "13", "-lrb-", "-rrb-", "each", "category", "we", "show", "average", "percentage", "user", "preference", "general", "majority", "all", "user", "show", "significant", "preference", "towards", "we", "result", "when", "compare", "any", "other", "two", "system", "respectively", "particular", "participant", "prefer", "overall", "quality", "we", "result", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "over", "YouTube", "Stabilizer", "-lrb-", "72", "vs.", "28", "-rrb-", "Warp", "Stabilizer", "-lrb-", "69", "vs.", "31", "-rrb-", "result", "consistent", "we", "metric", "evaluation", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "user", "show", "strong", "bias", "preference", "toward", "we", "result", "over", "Warp", "Stabilizer", "-lrb-", "93", "vs.", "rotation", "83", "vs.", "17", "zoom", "-rrb-", "possibly", "due", "significant", "crop", "result", "Warp", "Stabilizer", "category", "-lrb-", "v?vii", "-rrb-", "more", "participant", "prefer", "we", "result", "other", "two", "system", "although", "three", "system", "generate", "similar", "stability", "level", "accord", "we", "stability", "metric", "likely", "because", "superior", "distortion", "crop", "control", "we", "method", "category", "-lrb-", "-rrb-", "simple", "user", "express", "similar", "preference", "toward", "three", "result", "after", "user", "study", "we", "also", "ask", "all", "participant", "articulate", "criterion", "feedback", "we", "conclude", "main", "criterion", "unacceptable", "video", "-rrb-", "video", "get", "smaller", "field", "view", "even", "contain", "frame", "visible", "empty", "-lrb-", "black", "-rrb-", "area", "-rrb-", "video", "present", "structure", "distortion", "individual", "frame", "-rrb-", "motion", "some", "video", "frame", "vibrate", "oscillate", "-rrb-", "scene", "transition", "look", "abrupt", "smooth", "video", "from", "criterion", "we", "propose", "metric", "can", "partially", "relate", "human", "preference", "both", "quantitative", "evaluation", "user", "study", "result", "consistently", "indicate", "we", "system", "perform", "better", "than", "other", "two", "system", "we", "find", "when", "3d", "reconstruction", "successful", "3d", "method", "often", "generate", "best", "result", "however", "we", "system", "more", "robust", "we", "do", "require", "feature", "tracking", "produce", "comparable", "only", "slightly", "worse", "result", "interesting", "note", "we", "adaptive", "path", "optimization", "can", "also", "apply", "path", "smoothing", "3d", "method", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "-rsb-", "which", "often", "use", "low-pass", "filter", "-lrb-", "gaussian", "smoothing", "-rrb-", "curve", "fitting", "path", "planning", "comparison", "we", "adaptive", "camera", "path", "smoothing", "technique", "can", "automatically", "adjust", "smoothness", "strength", "consider", "discontinuity", "distortion", "we", "show", "example", "video", "we", "project", "webpage", "case", "where", "warping-based", "motion", "model", "fail", "handle", "severe", "occlusion", "dis-occlusion", "especially", "when", "combine", "roll", "shutter", "effect", "Figure", "14", "show", "two", "example", "we", "warping-based", "motion", "model", "choose", "large", "enforce", "strong", "coherence", "between", "grid", "cell", "way", "we", "can", "minimize", "geometrical", "distortion", "same", "time", "we", "sacrifice", "motion", "accuracy", "eventually", "stability", "result", "general", "we", "find", "geometrical", "distortion", "more", "disruptive", "than", "some", "slight", "remain", "jitters", "we", "path", "optimization", "do", "strictly", "follow", "cinematography", "rule", "which", "may", "desirable", "certain", "application", "we", "discontinuity-preservation", "optimization", "produce", "visually", "please", "result", "most", "example", "necessary", "we", "could", "apply", "strategy", "-lsb-", "gleicher", "Liu", "2007", "-rsb-", "post-process", "solve", "problem", "we", "also", "do", "deal", "motion", "blur", "sometimes", "stabilize", "result", "contain", "visible", "blur", "artifact", "problem", "can", "address", "recent", "work", "-lsb-", "Cho", "et", "al.", "2012", "-rsb-", "we", "first", "verify", "effectiveness", "different", "component", "propose", "approach", "global", "path", "vs.", "bundle", "path", "example", "Figure", "result", "accord", "global", "path", "have", "remain", "jitters", "some", "image", "region", "because", "parallax", "make", "global", "homography", "motion", "model", "invalid", "therefore", "some", "image", "region", "can", "stabilize", "very", "well", "we", "bundle", "path", "can", "handle", "kind", "typical", "situation", "please", "refer", "we", "accompany", "video", "visual", "comparison", "spatially-variant", "homography", "vs.", "Homography", "Mixture", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "homography", "mixture", "model", "roll", "shutter", "correction", "divide", "video", "frame", "1d", "array", "horizontal", "block", "use", "gaussian", "mixture", "homography", "each", "block", "model", "beyond", "single", "2d", "transformation", "able", "partially", "handle", "parallax", "http://www.ece.nus.edu.sg/stfpage/eletp/projects/stabilization/stabili", "zationsig13.html", "compare", "we", "2d", "mesh-based", "spatially-variant", "homography", "model", "have", "two", "limitation", "-rrb-", "do", "address", "horizontal", "depth", "variation", "-rrb-", "use", "weaker", "feature", "point", "-lrb-", "which", "apply", "lower", "threshold", "level", "feature", "detection", "-rrb-", "simple", "gaussian", "mixture", "regularization", "weaker", "feature", "point", "may", "result", "larger", "fitting", "error", "ability", "use", "simple", "gaussian", "smoothing", "limit", "Figure", "show", "comparison", "two", "model", "example", "scene", "have", "horizontal", "depth", "variation", "sky", "region", "lack", "feature", "point", "Figure", "-lrb-", "-rrb-", "result", "use", "YouTube", "Stabilizer", "-lrb-", "integrate", "Homography", "Mixture", "feature", "-rrb-", "we", "can", "observe", "severe", "geometrical", "distortion", "further", "verify", "we", "observation", "we", "replace", "we", "spatially-variant", "model", "homography", "mixture", "model", "-lrb-", "we", "implementation", "-rrb-", "we", "framework", "generate", "result", "Figure", "-lrb-", "-rrb-", "where", "we", "observe", "similar", "distortion", "comparison", "we", "warping-based", "motion", "estimation", "can", "fundamentally", "handle", "depth", "variation", "-lrb-", "limit", "vertical", "direction", "-rrb-", "we", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "do", "suffer", "from", "distortion", "please", "also", "see", "comparison", "accompany", "video", "Rolling", "Shutter", "Handling", "Figure", "compare", "we", "method", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "two", "example", "video", "from", "paper", "we", "model", "account", "frame", "distortion", "skew", "-lrb-", "leave", "example", "-rrb-", "local", "wobble", "-lrb-", "right", "example", "-rrb-", "more", "example", "include", "supplementary", "video", "which", "show", "we", "achieve", "similar", "result", "correct", "rolling", "shutter", "distortion", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "quantitatively", "evaluate", "measure", "result", "from", "different", "aspect", "we", "define", "three", "objective", "metric", "crop", "distortion", "we", "first", "two", "metric", "measure", "crop", "ratio", "global", "distortion", "we", "first", "fit", "global", "homography", "each", "frame", "between", "input", "video", "output", "video", "we", "compute", "crop", "ratio", "distortion", "each", "frame", "crop", "ratio", "can", "directly", "compute", "from", "scale", "component", "homography", "one", "global", "crop", "ratio", "whole", "sequence", "each", "frame", "provide", "estimation", "we", "average", "estimation", "all", "frame", "final", "metric", "distortion", "compute", "define", "section", "4.1", "because", "any", "distortion", "single", "frame", "destroy", "perfection", "whole", "result", "we", "choose", "minimum", "across", "whole", "sequence", "final", "metric", "worst-case", "metric", "allow", "we", "easily", "see", "whether", "whole", "result", "video", "completely", "successful", "good", "result", "both", "metric", "should", "close", "stability", "third", "metric", "measure", "stability", "result", "Designing", "good", "metric", "non-trivial", "because", "hard", "compare", "two", "different", "video", "we", "suggest", "empirically", "good", "metric", "use", "frequency", "analysis", "estimate", "2d", "motion", "from", "video", "we", "basic", "assumption", "more", "energy", "contain", "low", "frequency", "part", "motion", "more", "stable", "video", "computationally", "we", "estimate", "we", "bundle", "camera", "path", "approximate", "true", "motion", "-lrb-", "optical", "flow", "-rrb-", "video", "we", "do", "smooth", "out", "anything", "after", "estimation", "we", "extract", "translation", "rotation", "component", "from", "each", "path", "each", "component", "1d", "temporal", "signal", "finally", "we", "evaluate", "energy", "percentage", "low", "frequency", "component", "-lrb-", "expect", "dc", "component", "-rrb-", "1d", "signal", "measure", "stability", "specifically", "we", "take", "few", "lowest", "-lrb-", "empirically", "set", "from", "2nd", "6th", "-rrb-", "frequency", "calculate", "energy", "percentage", "over", "full", "frequency", "-lrb-", "exclude", "dc", "component", "-rrb-", "similar", "distortion", "we", "take", "smallest", "measurement", "among", "translation", "rotation", "final", "metric", "good", "result", "metric", "should", "approach", "here", "well", "purpose", "comparison", "test", "whether", "we", "result", "comparable", "-lrb-", "better", "than", "-rrb-", "previous", "successful", "result", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "Grundmann", "et", "al.", "2011", "-rsb-", "we", "collect", "eleven", "test", "video", "from", "papers", "-lrb-", "thumbnail", "Figure", "10", "-rrb-", "compare", "we", "result", "publish", "result", "-lrb-", "all", "from", "author", "project", "webpage", "-rrb-", "overall", "all", "method", "generate", "similar", "stability", "both", "subjectively", "quantitatively", "-lrb-", "Figure", "10", "-rrb-", "example", "while", "we", "result", "slightly", "better", "some", "video", "term", "crop", "ratio", "distortion", "video", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3d", "stabilization", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "achieve", "best", "stability", "distortion", "score", "suggest", "3d", "method", "first", "choice", "-lrb-", "term", "stability", "distortion", "error", "-rrb-", "when", "3d", "motion", "can", "successfully", "estimate", "although", "we", "result", "slightly", "worse", "stability", "visual", "difference", "quite", "small", "-lrb-", "please", "verify", "from", "supplementary", "video", "-rrb-", "furthermore", "aggressive", "smoothing", "3d", "method", "sometimes", "lead", "output", "fov", "too", "small", "demonstrate", "crop", "score", "we", "method", "manage", "provide", "good", "trade-off", "video", "-lrb-", "5-9", "-rrb-", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "-lsb-", "Goldstein", "Fattal", "2012", "-rsb-", "we", "method", "achieve", "similar", "stability", "while", "we", "method", "slightly", "better", "crop", "distortion", "video", "-lrb-", "10-11", "-rrb-", "we", "method", "outperform", "l1optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "stability", "-lrb-", "slightly", "-rrb-", "crop", "ratio", "distortion", "score", "Figure", "11", "highlight", "most", "challenging", "video", "-lrb-", "10", "-rrb-", "dataset", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "refer", "example", "failure", "case", "because", "single", "subspace", "can", "account", "feature", "trajectory", "both", "face", "background", "result", "have", "visible", "distortion", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "produce", "better", "result", "example", "video", "result", "we", "still", "observe", "large", "temporal", "distortion", "background", "region", "-lrb-", "see", "we", "accompany", "video", "-rrb-", "comparison", "we", "method", "can", "successfully", "handle", "example", "-lrb-", "achieve", "best", "term", "all", "three", "metric", "-rrb-", "because", "warping-based", "motion", "model", "can", "represent", "complicated", "motion", "due", "publicly", "available", "implementation", "previous", "work", "we", "compare", "we", "system", "two", "well-known", "commercial", "system", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "after", "Effects", "CS6", "YouTube", "Stabilizer", "base", "combination", "norm", "path", "optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "homography", "mixture", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "Warp", "Stabilizer", "Adobe", "after", "Effects", "largely", "base", "subspace", "stabilization", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "we", "understand", "commercial", "product", "often", "different", "from", "give", "research", "system", "we", "believe", "two", "system", "represent", "essential", "element", "research", "conduct", "field", "comparison", "make", "sense", "examine", "strength", "weakness", "robustness", "-lrb-", "various", "video", "use", "set", "fix", "parameter", "-rrb-", "we", "system", "Dataset", "we", "assemble", "comprehensive", "dataset", "174", "short", "video", "-lrb-", "10", "60", "seconds", "-rrb-", "from", "previous", "publication", "internet", "we", "own", "capture", "know", "strength", "weakness", "method", "different", "situation", "we", "roughly", "divide", "we", "datum", "category", "base", "camera", "motion", "scene", "type", "-lrb-", "-rrb-", "simple", "-lrb-", "II", "-rrb-", "quick", "rotation", "-lrb-", "III", "-rrb-", "zoom", "-lrb-", "iv", "-rrb-", "large", "parallax", "-lrb-", "-rrb-", "driving", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "running", "better", "measure", "stability", "background", "motion", "-lrb-", "cause", "camera", "shake", "-rrb-", "we", "use", "manual", "foreground", "mask", "exclude", "foreground", "motion", "YouTube", "Stabilizer", "parameter-free", "online", "tool", "Warp", "Stabilizer", "interactive", "system", "user", "might", "carefully", "tune", "few", "parameter", "here", "we", "wish", "examine", "its", "robustness", "automatic", "tool", "fix", "its", "parameter", "we", "use", "example", "video", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "decide", "best", "parameter", "finally", "we", "choose", "default", "parameter", "-lrb-", "smoothness", "50", "smooth", "Motion", "Subspace", "Warp", "-rrb-", "produce", "result", "quantitative", "comparison", "each", "category", "we", "compute", "average", "metric", "standard", "deviation", "three", "system", "-lrb-", "figure", "12", "-lrb-", "-rrb-", "-rrb-", "we", "discuss", "result", "regard", "each", "system", "detail", "below", "all", "three", "system", "perform", "well", "category", "-lrb-", "-rrb-", "simple", "since", "category", "contain", "video", "relatively", "smooth", "camera", "motion", "mild", "depth", "variation", "though", "we", "method", "have", "minor", "advantage", "user", "can", "safely", "choose", "any", "three", "get", "desire", "result", "among", "remain", "category", "we", "want", "highlight", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "three", "system", "achieve", "similar", "stability", "while", "we", "system", "clearly", "better", "term", "distortion", "we", "show", "two", "example", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "visual", "comparison", "we", "system", "YouTube", "Stabilizer", "example", "show", "limitation", "1d", "array", "homography", "mixture", "can", "model", "depth", "change", "horizontal", "direction", "Warp", "Stabilizer", "also", "generate", "some", "shearing/skewing", "artifact", "some", "video", "frame", "though", "principle", "3d", "method", "should", "able", "handle", "parallax", "Figure", "12", "-lrb-", "-rrb-", "show", "example", "-lrb-", "please", "note", "shearing", "bookshelf", "-rrb-", "probably", "due", "subspace", "analysis", "failure", "cause", "occlusion", "we", "method", "succeed", "all", "example", "comparison", "category", "clearly", "demonstrate", "advantage", "we", "warping-based", "motion", "model", "deal", "large", "parallax", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "which", "challenging", "case", "method", "require", "long", "feature", "tracking", "Warp", "Stabilizer", "often", "generate", "significant", "crop", "Figure", "12", "-lrb-", "-rrb-", "example", "alleviate", "problem", "we", "try", "interactively", "tune", "its", "smoothing", "parameter", "when", "apply", "weaker", "smoothing", "however", "we", "find", "its", "result", "become", "shaky", "comparison", "we", "method", "generate", "stable", "result", "much", "less", "crop", "category", "-lrb-", "v?vii", "-rrb-", "three", "system", "generate", "similar", "stability", "level", "-lrb-", "Warp", "Stabilizer", "slightly", "better", "category", "vii", "-rrb-", "while", "we", "sy", "tem", "consistently", "better", "respect", "either", "crop", "ratio", "distortion", "control", "we", "notice", "we", "method", "generate", "relatively", "smaller", "standard", "deviation", "three", "metric", "all", "category", "suggest", "we", "method", "generate", "more", "consistent", "result", "from", "various", "input", "user", "study", "we", "further", "conduct", "user", "study", "40", "participant", "evaluate", "compare", "we", "method", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "AfterEffects", "CS6", "every", "participant", "require", "evaluate", "result", "28", "different", "input", "video", "-lrb-", "randomly", "sample", "from", "we", "dataset", "-rrb-", "which", "video", "each", "category", "mention", "above", "-lrb-", "video", "prepare", "way", "two", "they", "compare", "we", "result", "YouTube", "Stabilizer", "other", "two", "Warp", "Stabilizer", "-rrb-", "user", "study", "we", "use", "scheme", "forced", "two-alternative", "choice", "every", "participant", "ask", "pick", "better", "one", "between", "result", "we", "method", "YouTube", "Stabilizer", "between", "result", "we", "method", "Warp", "Stabilizer", "video", "display", "subject", "random", "order", "subject", "unaware", "video", "category", "neither", "do", "know", "which", "technique", "use", "produce", "stabilize", "result", "Figure", "13", "-lrb-", "-rrb-", "show", "interface", "user", "study", "original", "video", "display", "top", "two", "stabilize", "one", "show", "side-by-side", "below", "user", "can", "simultaneously", "play", "input", "video", "both", "two", "result", "better", "examine", "difference", "video", "can", "play", "back", "forth", "pause", "certain", "frame", "help", "user", "carefully", "make", "decision", "user", "can", "also", "play", "each", "video", "individually", "examine", "quality", "without", "other", "distraction", "we", "ask", "user", "disregard", "difference", "aspect", "ratio", "sharpness", "since", "each", "one", "may", "undergo", "different", "video", "codec", "further", "post-processing", "which", "make", "uniform", "treatment", "difficult", "user", "study", "result", "show", "13", "-lrb-", "-rrb-", "each", "category", "we", "show", "average", "percentage", "user", "preference", "general", "majority", "all", "user", "show", "significant", "preference", "towards", "we", "result", "when", "compare", "any", "other", "two", "system", "respectively", "particular", "participant", "prefer", "overall", "quality", "we", "result", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "over", "YouTube", "Stabilizer", "-lrb-", "72", "vs.", "28", "-rrb-", "Warp", "Stabilizer", "-lrb-", "69", "vs.", "31", "-rrb-", "result", "consistent", "we", "metric", "evaluation", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "user", "show", "strong", "bias", "preference", "toward", "we", "result", "over", "Warp", "Stabilizer", "-lrb-", "93", "vs.", "rotation", "83", "vs.", "17", "zoom", "-rrb-", "possibly", "due", "significant", "crop", "result", "Warp", "Stabilizer", "category", "-lrb-", "v?vii", "-rrb-", "more", "participant", "prefer", "we", "result", "other", "two", "system", "although", "three", "system", "generate", "similar", "stability", "level", "accord", "we", "stability", "metric", "likely", "because", "superior", "distortion", "crop", "control", "we", "method", "category", "-lrb-", "-rrb-", "simple", "user", "express", "similar", "preference", "toward", "three", "result", "after", "user", "study", "we", "also", "ask", "all", "participant", "articulate", "criterion", "feedback", "we", "conclude", "main", "criterion", "unacceptable", "video", "-rrb-", "video", "get", "smaller", "field", "view", "even", "contain", "frame", "visible", "empty", "-lrb-", "black", "-rrb-", "area", "-rrb-", "video", "present", "structure", "distortion", "individual", "frame", "-rrb-", "motion", "some", "video", "frame", "vibrate", "oscillate", "-rrb-", "scene", "transition", "look", "abrupt", "smooth", "video", "from", "criterion", "we", "propose", "metric", "can", "partially", "relate", "human", "preference", "both", "quantitative", "evaluation", "user", "study", "result", "consistently", "indicate", "we", "system", "perform", "better", "than", "other", "two", "system", "we", "find", "when", "3d", "reconstruction", "successful", "3d", "method", "often", "generate", "best", "result", "however", "we", "system", "more", "robust", "we", "do", "require", "feature", "tracking", "produce", "comparable", "only", "slightly", "worse", "result", "interesting", "note", "we", "adaptive", "path", "optimization", "can", "also", "apply", "path", "smoothing", "3d", "method", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "-rsb-", "which", "often", "use", "low-pass", "filter", "-lrb-", "gaussian", "smoothing", "-rrb-", "curve", "fitting", "path", "planning", "comparison", "we", "adaptive", "camera", "path", "smoothing", "technique", "can", "automatically", "adjust", "smoothness", "strength", "consider", "discontinuity", "distortion", "we", "show", "example", "video", "we", "project", "webpage", "case", "where", "warping-based", "motion", "model", "fail", "handle", "severe", "occlusion", "dis-occlusion", "especially", "when", "combine", "roll", "shutter", "effect", "Figure", "14", "show", "two", "example", "we", "warping-based", "motion", "model", "choose", "large", "enforce", "strong", "coherence", "between", "grid", "cell", "way", "we", "can", "minimize", "geometrical", "distortion", "same", "time", "we", "sacrifice", "motion", "accuracy", "eventually", "stability", "result", "general", "we", "find", "geometrical", "distortion", "more", "disruptive", "than", "some", "slight", "remain", "jitters", "we", "path", "optimization", "do", "strictly", "follow", "cinematography", "rule", "which", "may", "desirable", "certain", "application", "we", "discontinuity-preservation", "optimization", "produce", "visually", "please", "result", "most", "example", "necessary", "we", "could", "apply", "strategy", "-lsb-", "gleicher", "Liu", "2007", "-rsb-", "post-process", "solve", "problem", "we", "also", "do", "deal", "motion", "blur", "sometimes", "stabilize", "result", "contain", "visible", "blur", "artifact", "problem", "can", "address", "recent", "work", "-lsb-", "Cho", "et", "al.", "2012", "-rsb-", "we", "have", "present", "new", "2d", "video", "stabilization", "method", "bundle", "camera", "path", "model", "propose", "method", "can", "simultaneously", "generate", "comparable", "result", "3d", "method", "while", "keep", "merit", "2d", "method", "use", "image", "warping", "technique", "motion", "representation", "interesting", "finding", "paper", "future", "we", "would", "extend", "kind", "representation", "other", "video-based", "application" ],
  "content" : "We run our method on an Intel i7 3.2GHZ Quad-Core machine with 8G RAM. We extract 400-600 SURF features [Bay et al. 2008] per frame. For motion estimation, we always divide the video frame to 16 ? 16 cells. For a video of 1280 ? 720 resolution, our unoptimized system takes 392 milliseconds to process a frame (around 2.5fps). Specifically, we spend 300ms, 50ms, 12ms and 30ms to extract features, estimate motion, optimize camera paths and render the final result. All original and result videos are provided on our webpage 1 . We first verify the effectiveness of different components of the proposed approach. A Global Path vs. Bundled Paths For the example in Figure 1 , the result according to a global path has remaining jitters in some image regions. This is because the parallax makes the global homography motion model invalid, therefore some image regions cannot be stabilized very well. But our bundled paths can handle this kind of typical situation. Please refer to our accompanying video for a visual comparison. Spatially-variant Homographies vs. Homography Mixture Grundmann et al. [2012] proposed a homography mixture model for rolling shutter correction. They divide a video frame into a 1D array of horizontal blocks, and use a Gaussian mixture of homographies for each block. This model is beyond a single 2D transformation and able to partially handle parallax. 1 http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Stabilization/Stabili- zationSig13.html Compared with our 2D mesh-based, spatially-variant homographies, this model has two limitations: 1) it does not address horizontal depth variation; 2) it uses weaker feature points (which apply lower threshold level for feature detection) and a simple Gaussian mixture for the regularization. Weaker feature points may result in larger fitting errors and the ability to use simple Gaussian smoothing is limited. Figure 8 shows a comparison of these two models. In this example, the scene has horizontal depth variation and the sky region lacks feature points. Figure 8 (a) is the result of using YouTube Stabilizer (integrated Homography Mixture feature). We can observe severe geometrical distortions. To further verify our observation, we replace our spatially-variant model with the homography mixture model (our implementation) in our framework and generate the result in Figure 8 (d), where we observe similar distortion. In comparison, our warping-based motion estimation can fundamentally handle depth variation (not limited to vertical direction). Our result ( Figure 8 (c)) does not suffer from such distortion. Please also see the comparison in the accompanying video. Rolling Shutter Handling Figure 9 compares our methods with [Grundmann et al. 2012] on two example videos from their paper. Our model accounts for frame distortions such as skew (left example) and local wobble (right example). More examples are included in the supplementary video, which shows we achieve similar results on correcting rolling shutter distortion as [Grundmann et al. 2012]. To quantitatively evaluate and measure the result from different aspects, we define three objective metrics. Cropping and distortion Our first two metrics measure cropping ratio and global distortion. We first fit a global homography at each frame between input video and output video. We then compute the cropping ratio and distortion for each frame. The cropping ratio can be directly computed from the scale component of the homography. There is one global cropping ratio for the whole sequence, and each frame provides an estimation. We average these estimations at all frames as the final metric. The distortion is computed as defined in Section 4.1. Because any distortion in a single frame will destroy the perfection of the whole result, we choose their minimum across the whole sequence as the final metric. This ?worst-case? metric  allows us to easily see whether the whole result video is completely successful. For a good result, both metrics should be close to 1. Stability The third metric measures the stability of the result. Designing a good metric is non-trivial because it is hard to compare two different videos. We suggest an empirically good metric using frequency analysis on estimated 2D motion from a video. Our basic assumption is that the more energy is contained in the low frequency part of the motion, the more stable a video is. Computationally, we estimate our bundled camera paths to approximate the true motion (optical flow) in a video. We do not smooth out anything after the estimation. Then, we extract translation and rotation components from each path. Each component is a 1D temporal signal. Finally, we evaluate the energy percentage of the low frequency components (expect for DC component) in these 1D signals to measure the stability. Specifically, we take a few of the lowest (empirically set as from the 2nd to the 6th) frequencies and calculate the energy percentage over full frequencies (excluded by the DC component). Similar to the distortion, we take the smallest measurement among the translation and rotation as the final metric. For a good result, the metric should approach 1 here as well. The purpose of this comparison is to test whether our results are comparable with (if not better than) previous ?successful? results in [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012; Grundmann et al. 2011]. We collect eleven test videos from these papers (thumbnails in Figure 10 ), and compare our results with their published results (all from authors? project webpages). Overall, all methods generate similar stability both subjectively and quantitatively ( Figure 10 ) on these examples, while our results are slightly better on some videos in terms of cropping ratio and distortion. For video (2)-(4), 3D stabilization [Liu et al. 2009] achieves the best stability and distortion scores. It suggests that 3D methods are the first choice (in term of stability and distortion error), when the 3D motion can be successfully estimated. Although our results are slightly worse in stability, the visual difference is quite small (please verify from the supplementary video). Furthermore, the aggressive smoothing in 3D methods sometimes leads to an output FOV that is too small as demonstrated by the cropping score. Our method manages to provide a good trade-off. For video (5-9), [Liu et al. 2011], [Goldstein and Fattal 2012], and our method achieve similar stability, while our method is slightly better in cropping and distortion. For video (10-11) 2 , our method outperforms the L1optimization [Grundmann et al. 2011] in stability (slightly), cropping ratio, and distortion scores. Figure 11 highlights the most challenging video (10) in this dataset. Liu et al. [2011] refer this example as a failure case because a single subspace cannot account for the feature trajectories on both the face and the background. Their results have visible distortion. [Grundmann et al. 2011] produced better result on this example. But in the video result, we still observe large temporal distortion on the background region. (See our accompanying video.) In comparison, our method can successfully handle this example (achieve best in terms of all three metrics) because the warping-based motion model can represent this complicated motion. Due to no publicly available implementation of previous works, we compare our system with two well-known commercial systems ? YouTube Stabilizer and ?Warp Stabilizer? in Adobe After Effects CS6. The YouTube Stabilizer is based on the combination of the L 1 -norm path optimization [Grundmann et al. 2011] and homography mixtures [Grundmann et al. 2012]. The ?Warp Stabilizer? in Adobe After Effects is largely based on subspace stabilization [Liu et al. 2011]. We understand that commercial products are often different from a given research system. But we believe these two systems represent the essential elements of research conducted in this field, and the comparison makes sense for examining strengths or weaknesses and robustness (for various videos using a set of fixed parameters) of our system. Dataset We assemble a comprehensive dataset of 174 short videos (10 ? 60 seconds) from previous publications, Internet, and our own captures. To know the strength and weakness of a method in different situations, we roughly divide our data into 7 categories based on camera motion and scene type. They are: (I) simple, (II) quick rotation, (III) zooming, (IV) large parallax, (V) driving, (VI) crowd, and (VII) running. 2 To better measure stability on background motion (caused by camera shake), we use a manual foreground mask to exclude foreground motion. YouTube Stabilizer is a parameter-free online tool. But ?Warp Stabilizer? is an interactive system, and the user might carefully tune a few parameters. Here, we wish to examine its robustness as an automatic tool by fixing its parameters. We use the example videos in [Liu et al. 2011] to decide the best parameters. Finally, we choose the default parameters (smoothness: 50%, ?Smooth Motion? and ?Subspace Warp?) to produce results. Quantitative Comparison For each category, we compute the average metrics and standard deviation of three systems ( Figure 12 (a)). We discuss the results with regard to each system in detail below. All three systems perform well in category (I) ?simple?, since this category contains videos with relatively smooth camera motion and mild depth variations. Though our method has a minor advantage, the users can safely choose any of three to get a desired result. Among the remaining categories, we want to highlight the category (IV) ?large parallax?. The three systems achieve similar stability, while our system is clearly better in terms of distortion. We show two examples in Figure 12 (b) and (c) for visual comparison of our system and the YouTube Stabilizer. These examples show the limitation of a 1D array of homography mixtures ? it cannot model depth changes in horizontal direction. Warp Stabilizer also generates some shearing/skewing artifacts in some video frames, though in principle this 3D method should be able to handle parallax. Figure 12 (d) shows such an example (please note the shearing of the bookshelf). This is probably due to the subspace analysis failure caused by occlusion. Our method succeeds in all of these examples. Comparison in this category clearly demonstrates the advantages of our warping-based motion model in dealing with a large parallax. Categories (II?III) contain quick rotation or zooming, which are challenging cases for methods requiring long feature tracking. ?Warp Stabilizer? often generates significant cropping. Figure 12(e) is such an example. To alleviate this problem, we try to interactively tune its smoothing parameters. When applying a weaker smoothing, however, we find its result becomes shaky. In comparison, our method generates stable results with much less cropping. For categories (V?VII), the three systems generate similar stability levels (?Warp Stabilizer? is slightly better in category VII), while our sys- tem is consistently better with respect to either cropping ratio or distortion control. We notice that our method generates relatively smaller standard deviations of the three metrics for all categories. It suggests that our method generates more consistent results from various inputs. User Study We further conduct a user study with 40 participants to evaluate and compare our method with the YouTube Stabilizer and the ?Warp Stabilizer? in Adobe AfterEffects CS6. Every participant is required to evaluate results on 28 different input videos (randomly sampled from our dataset), in which there are 4 videos for each category mentioned above (The 4 video are prepared in the way that two of them compare our result to YouTube Stabilizer, and the other two to ?Warp Stabilizer?). In the user study, we use the scheme of forced two-alternative choice. Every participant is asked to pick a better one between the results of our method and YouTube Stabilizer, or between the results of our method and the ?Warp Stabilizer?. These videos are displayed to the subjects in a random order. The subjects are unaware of the video categories. Neither do they know which technique is used to produce the stabilized results. Figure 13 (a) shows such an interface for the user study. The original video is displayed on the top. The two stabilized ones are shown side-by-side below. Users can simultaneously play input video and both two results to better examine the difference. And these videos can be played back and forth, or be paused at a certain frame to help users carefully make their decision. The user can also play each of these videos individually to examine their quality without other distractions. We ask users to disregard differences in aspect ratio, or sharpness since each one may undergo different video codecs or further post-processing which makes uniform treatment difficult. The user study results are shown in 13 (b). For each category, we show the average percentage of user preference. In general, the majority of all users showed significant preference towards our results when compared to any of the other two systems respectively. In particular, the participants prefer the overall quality of our results for category (IV) ?large parallax? over YouTube Stabilizer (72% vs. 28%) and ?Warp Stabilizer? (69% vs. 31%). The result is consistent with our metric evaluation. For category (II?III) containing quick rotation or zooming, users show a strong bias in preference toward our results over ?Warp Stabilizer? (93% vs. 7% for rotation, 83% vs. 17% for zooming). This is possibly due to the significant cropping in the results of ?Warp Stabilizer?. For categories (V?VII), more participants prefer our results to the other two systems, although the three systems generate similar stability levels according to our stability metric. It is likely because of the superior distortion and cropping control in our method. In category (I) ?simple?, users express similar preference toward three results. After the user study, we also ask all participants to articulate the criteria for their feedbacks. We conclude the main criteria for unacceptable videos: 1) the video gets a smaller field of view or even contains frames with visible empty (black) area; 2) the video presents structure distortions in individual frames; 3) the motions in some video frames vibrate or oscillate; 4) the scene transition looks abrupt or not smoothed in the video. From these criteria, our proposed metrics can be partially related with human preferences. And both quantitative evaluation and user study results consistently indicate our system performs better than the other two systems. We find that when 3D reconstruction is successful, 3D methods often generate the best results. However, our system is more robust as we do not require feature tracking, and it produces comparable or only slightly worse results. It is interesting to note that our adaptive path optimization can also be applied to path smoothing for 3D methods [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012], which often use low-pass filtering (Gaussian smoothing), or curve fitting for path planning. In comparison, our adaptive camera  path smoothing technique can automatically adjust the smoothness strength by considering discontinuity and distortion. We show such an example video on our project webpage. There are cases where the warping-based motion model fails to handle severe occlusions or dis-occlusions, especially when combined with rolling shutter effects. Figure 14 shows two such examples. Our warping-based motion model chooses a large ? to enforce strong coherence between grid cells. In this way, we can minimize the geometrical distortion, but at the same time, we sacrifice motion accuracy and eventually the stability of the result. In general, we find geometrical distortion is more disruptive than some slight remaining jitters. Our path optimization does not strictly follow cinematography rules, which may be desirable in certain applications. But our discontinuity-preservation optimization produces visually pleasing results in most examples. If necessary, we could apply the strategy in [Gleicher and Liu 2007] as a post-process to solve this problem. We also do not deal with motion blur. Sometimes, the stabilized results contain visible blur artifacts. This problem can be addressed by the recent work [Cho et al. 2012]. We first verify the effectiveness of different components of the proposed approach. A Global Path vs. Bundled Paths For the example in Figure 1 , the result according to a global path has remaining jitters in some image regions. This is because the parallax makes the global homography motion model invalid, therefore some image regions cannot be stabilized very well. But our bundled paths can handle this kind of typical situation. Please refer to our accompanying video for a visual comparison. Spatially-variant Homographies vs. Homography Mixture Grundmann et al. [2012] proposed a homography mixture model for rolling shutter correction. They divide a video frame into a 1D array of horizontal blocks, and use a Gaussian mixture of homographies for each block. This model is beyond a single 2D transformation and able to partially handle parallax. 1 http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Stabilization/Stabili- zationSig13.html Compared with our 2D mesh-based, spatially-variant homographies, this model has two limitations: 1) it does not address horizontal depth variation; 2) it uses weaker feature points (which apply lower threshold level for feature detection) and a simple Gaussian mixture for the regularization. Weaker feature points may result in larger fitting errors and the ability to use simple Gaussian smoothing is limited. Figure 8 shows a comparison of these two models. In this example, the scene has horizontal depth variation and the sky region lacks feature points. Figure 8 (a) is the result of using YouTube Stabilizer (integrated Homography Mixture feature). We can observe severe geometrical distortions. To further verify our observation, we replace our spatially-variant model with the homography mixture model (our implementation) in our framework and generate the result in Figure 8 (d), where we observe similar distortion. In comparison, our warping-based motion estimation can fundamentally handle depth variation (not limited to vertical direction). Our result ( Figure 8 (c)) does not suffer from such distortion. Please also see the comparison in the accompanying video. Rolling Shutter Handling Figure 9 compares our methods with [Grundmann et al. 2012] on two example videos from their paper. Our model accounts for frame distortions such as skew (left example) and local wobble (right example). More examples are included in the supplementary video, which shows we achieve similar results on correcting rolling shutter distortion as [Grundmann et al. 2012]. To quantitatively evaluate and measure the result from different aspects, we define three objective metrics. Cropping and distortion Our first two metrics measure cropping ratio and global distortion. We first fit a global homography at each frame between input video and output video. We then compute the cropping ratio and distortion for each frame. The cropping ratio can be directly computed from the scale component of the homography. There is one global cropping ratio for the whole sequence, and each frame provides an estimation. We average these estimations at all frames as the final metric. The distortion is computed as defined in Section 4.1. Because any distortion in a single frame will destroy the perfection of the whole result, we choose their minimum across the whole sequence as the final metric. This ?worst-case? metric  allows us to easily see whether the whole result video is completely successful. For a good result, both metrics should be close to 1. Stability The third metric measures the stability of the result. Designing a good metric is non-trivial because it is hard to compare two different videos. We suggest an empirically good metric using frequency analysis on estimated 2D motion from a video. Our basic assumption is that the more energy is contained in the low frequency part of the motion, the more stable a video is. Computationally, we estimate our bundled camera paths to approximate the true motion (optical flow) in a video. We do not smooth out anything after the estimation. Then, we extract translation and rotation components from each path. Each component is a 1D temporal signal. Finally, we evaluate the energy percentage of the low frequency components (expect for DC component) in these 1D signals to measure the stability. Specifically, we take a few of the lowest (empirically set as from the 2nd to the 6th) frequencies and calculate the energy percentage over full frequencies (excluded by the DC component). Similar to the distortion, we take the smallest measurement among the translation and rotation as the final metric. For a good result, the metric should approach 1 here as well. The purpose of this comparison is to test whether our results are comparable with (if not better than) previous ?successful? results in [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012; Grundmann et al. 2011]. We collect eleven test videos from these papers (thumbnails in Figure 10 ), and compare our results with their published results (all from authors? project webpages). Overall, all methods generate similar stability both subjectively and quantitatively ( Figure 10 ) on these examples, while our results are slightly better on some videos in terms of cropping ratio and distortion. For video (2)-(4), 3D stabilization [Liu et al. 2009] achieves the best stability and distortion scores. It suggests that 3D methods are the first choice (in term of stability and distortion error), when the 3D motion can be successfully estimated. Although our results are slightly worse in stability, the visual difference is quite small (please verify from the supplementary video). Furthermore, the aggressive smoothing in 3D methods sometimes leads to an output FOV that is too small as demonstrated by the cropping score. Our method manages to provide a good trade-off. For video (5-9), [Liu et al. 2011], [Goldstein and Fattal 2012], and our method achieve similar stability, while our method is slightly better in cropping and distortion. For video (10-11) 2 , our method outperforms the L1optimization [Grundmann et al. 2011] in stability (slightly), cropping ratio, and distortion scores. Figure 11 highlights the most challenging video (10) in this dataset. Liu et al. [2011] refer this example as a failure case because a single subspace cannot account for the feature trajectories on both the face and the background. Their results have visible distortion. [Grundmann et al. 2011] produced better result on this example. But in the video result, we still observe large temporal distortion on the background region. (See our accompanying video.) In comparison, our method can successfully handle this example (achieve best in terms of all three metrics) because the warping-based motion model can represent this complicated motion. Due to no publicly available implementation of previous works, we compare our system with two well-known commercial systems ? YouTube Stabilizer and ?Warp Stabilizer? in Adobe After Effects CS6. The YouTube Stabilizer is based on the combination of the L 1 -norm path optimization [Grundmann et al. 2011] and homography mixtures [Grundmann et al. 2012]. The ?Warp Stabilizer? in Adobe After Effects is largely based on subspace stabilization [Liu et al. 2011]. We understand that commercial products are often different from a given research system. But we believe these two systems represent the essential elements of research conducted in this field, and the comparison makes sense for examining strengths or weaknesses and robustness (for various videos using a set of fixed parameters) of our system. Dataset We assemble a comprehensive dataset of 174 short videos (10 ? 60 seconds) from previous publications, Internet, and our own captures. To know the strength and weakness of a method in different situations, we roughly divide our data into 7 categories based on camera motion and scene type. They are: (I) simple, (II) quick rotation, (III) zooming, (IV) large parallax, (V) driving, (VI) crowd, and (VII) running. 2 To better measure stability on background motion (caused by camera shake), we use a manual foreground mask to exclude foreground motion. YouTube Stabilizer is a parameter-free online tool. But ?Warp Stabilizer? is an interactive system, and the user might carefully tune a few parameters. Here, we wish to examine its robustness as an automatic tool by fixing its parameters. We use the example videos in [Liu et al. 2011] to decide the best parameters. Finally, we choose the default parameters (smoothness: 50%, ?Smooth Motion? and ?Subspace Warp?) to produce results. Quantitative Comparison For each category, we compute the average metrics and standard deviation of three systems ( Figure 12 (a)). We discuss the results with regard to each system in detail below. All three systems perform well in category (I) ?simple?, since this category contains videos with relatively smooth camera motion and mild depth variations. Though our method has a minor advantage, the users can safely choose any of three to get a desired result. Among the remaining categories, we want to highlight the category (IV) ?large parallax?. The three systems achieve similar stability, while our system is clearly better in terms of distortion. We show two examples in Figure 12 (b) and (c) for visual comparison of our system and the YouTube Stabilizer. These examples show the limitation of a 1D array of homography mixtures ? it cannot model depth changes in horizontal direction. Warp Stabilizer also generates some shearing/skewing artifacts in some video frames, though in principle this 3D method should be able to handle parallax. Figure 12 (d) shows such an example (please note the shearing of the bookshelf). This is probably due to the subspace analysis failure caused by occlusion. Our method succeeds in all of these examples. Comparison in this category clearly demonstrates the advantages of our warping-based motion model in dealing with a large parallax. Categories (II?III) contain quick rotation or zooming, which are challenging cases for methods requiring long feature tracking. ?Warp Stabilizer? often generates significant cropping. Figure 12(e) is such an example. To alleviate this problem, we try to interactively tune its smoothing parameters. When applying a weaker smoothing, however, we find its result becomes shaky. In comparison, our method generates stable results with much less cropping. For categories (V?VII), the three systems generate similar stability levels (?Warp Stabilizer? is slightly better in category VII), while our sys- tem is consistently better with respect to either cropping ratio or distortion control. We notice that our method generates relatively smaller standard deviations of the three metrics for all categories. It suggests that our method generates more consistent results from various inputs. User Study We further conduct a user study with 40 participants to evaluate and compare our method with the YouTube Stabilizer and the ?Warp Stabilizer? in Adobe AfterEffects CS6. Every participant is required to evaluate results on 28 different input videos (randomly sampled from our dataset), in which there are 4 videos for each category mentioned above (The 4 video are prepared in the way that two of them compare our result to YouTube Stabilizer, and the other two to ?Warp Stabilizer?). In the user study, we use the scheme of forced two-alternative choice. Every participant is asked to pick a better one between the results of our method and YouTube Stabilizer, or between the results of our method and the ?Warp Stabilizer?. These videos are displayed to the subjects in a random order. The subjects are unaware of the video categories. Neither do they know which technique is used to produce the stabilized results. Figure 13 (a) shows such an interface for the user study. The original video is displayed on the top. The two stabilized ones are shown side-by-side below. Users can simultaneously play input video and both two results to better examine the difference. And these videos can be played back and forth, or be paused at a certain frame to help users carefully make their decision. The user can also play each of these videos individually to examine their quality without other distractions. We ask users to disregard differences in aspect ratio, or sharpness since each one may undergo different video codecs or further post-processing which makes uniform treatment difficult. The user study results are shown in 13 (b). For each category, we show the average percentage of user preference. In general, the majority of all users showed significant preference towards our results when compared to any of the other two systems respectively. In particular, the participants prefer the overall quality of our results for category (IV) ?large parallax? over YouTube Stabilizer (72% vs. 28%) and ?Warp Stabilizer? (69% vs. 31%). The result is consistent with our metric evaluation. For category (II?III) containing quick rotation or zooming, users show a strong bias in preference toward our results over ?Warp Stabilizer? (93% vs. 7% for rotation, 83% vs. 17% for zooming). This is possibly due to the significant cropping in the results of ?Warp Stabilizer?. For categories (V?VII), more participants prefer our results to the other two systems, although the three systems generate similar stability levels according to our stability metric. It is likely because of the superior distortion and cropping control in our method. In category (I) ?simple?, users express similar preference toward three results. After the user study, we also ask all participants to articulate the criteria for their feedbacks. We conclude the main criteria for unacceptable videos: 1) the video gets a smaller field of view or even contains frames with visible empty (black) area; 2) the video presents structure distortions in individual frames; 3) the motions in some video frames vibrate or oscillate; 4) the scene transition looks abrupt or not smoothed in the video. From these criteria, our proposed metrics can be partially related with human preferences. And both quantitative evaluation and user study results consistently indicate our system performs better than the other two systems. We find that when 3D reconstruction is successful, 3D methods often generate the best results. However, our system is more robust as we do not require feature tracking, and it produces comparable or only slightly worse results. It is interesting to note that our adaptive path optimization can also be applied to path smoothing for 3D methods [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012], which often use low-pass filtering (Gaussian smoothing), or curve fitting for path planning. In comparison, our adaptive camera  path smoothing technique can automatically adjust the smoothness strength by considering discontinuity and distortion. We show such an example video on our project webpage. There are cases where the warping-based motion model fails to handle severe occlusions or dis-occlusions, especially when combined with rolling shutter effects. Figure 14 shows two such examples. Our warping-based motion model chooses a large ? to enforce strong coherence between grid cells. In this way, we can minimize the geometrical distortion, but at the same time, we sacrifice motion accuracy and eventually the stability of the result. In general, we find geometrical distortion is more disruptive than some slight remaining jitters. Our path optimization does not strictly follow cinematography rules, which may be desirable in certain applications. But our discontinuity-preservation optimization produces visually pleasing results in most examples. If necessary, we could apply the strategy in [Gleicher and Liu 2007] as a post-process to solve this problem. We also do not deal with motion blur. Sometimes, the stabilized results contain visible blur artifacts. This problem can be addressed by the recent work [Cho et al. 2012]. We have presented a new 2D video stabilization method with a bundled camera paths model. The proposed method can simultaneously generate comparable results to 3D methods while keeping merits of 2D methods. Using image warping techniques for motion representation is an interesting finding in this paper. In the future, we would extend this kind of representation to other video-based applications.",
  "resources" : [ ]
}