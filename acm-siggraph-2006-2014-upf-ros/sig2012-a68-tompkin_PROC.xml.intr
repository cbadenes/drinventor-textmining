{
  "uri" : "sig2012-a68-tompkin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a68-tompkin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Videoscapes: Exploring Sparse, Unstructured Video Collections",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/James-Tompkin",
      "name" : "James",
      "surname" : "Tompkin"
    }, {
      "uri" : "http://drinventor/Kwang In-Kim",
      "name" : "Kwang In",
      "surname" : "Kim"
    }, {
      "uri" : "http://drinventor/Jan-Kautz",
      "name" : "Jan",
      "surname" : "Kautz"
    }, {
      "uri" : "http://drinventor/Christian-Theobalt",
      "name" : "Christian",
      "surname" : "Theobalt"
    } ]
  },
  "bagOfWords" : [ "recent", "year", "have", "be", "explosion", "mobile", "device", "capable", "record", "photograph", "can", "share", "community", "platform", "research", "community", "have", "start", "harvest", "immense", "amount", "datum", "from", "community", "photo", "collection", "have", "develop", "tool", "estimate", "spatial", "relation", "between", "photograph", "reconstruct", "3d", "geometry", "certain", "landmark", "sufficiently", "dense", "set", "photo", "available", "-lsb-", "Snavely", "et", "al.", "2006", "Goesele", "et", "al.", "2007", "Agarwal", "et", "al.", "2009", "Frahm", "et", "al.", "2010b", "-rsb-", "user", "can", "interactively", "explore", "location", "view", "reconstruct", "3d", "model", "spatially", "transition", "between", "photograph", "Navigation", "tool", "like", "Google", "Street", "View", "Bing", "Maps", "also", "use", "exploration", "paradigm", "reconstruct", "entire", "street", "network", "through", "alignment", "purposefully", "capture", "imagery", "via", "additionally", "record", "localization", "depth", "sensor", "datum", "photo", "exploration", "tool", "ideal", "view", "navigate", "static", "landmark", "Notre", "Dame", "can", "convey", "dynamics", "liveliness", "spatio-temporal", "relationship", "location", "event", "one", "solution", "employ", "video", "datum", "yet", "comparable", "browse", "experience", "casually", "capture", "video", "generation", "still", "open", "challenge", "one", "may", "tempt", "think", "video", "simply", "series", "image", "so", "straightforward", "extension", "image-based", "approach", "should", "serve", "purpose", "enable", "video", "tour", "however", "reality", "nature", "casually", "capture", "video", "different", "from", "photo", "prevent", "simple", "extension", "casually", "capture", "video", "collection", "usually", "sparse", "largely", "unstructured", "unlike", "dense", "photo", "collection", "use", "approach", "mention", "above", "preclude", "dense", "reconstruction", "registration", "all", "frame", "furthermore", "exploration", "interface", "should", "reflect", "dynamic", "temporal", "nature", "video", "paper", "we", "propose", "system", "explore", "unstructured", "video", "collection", "immersive", "visually", "compelling", "manner", "give", "sparse", "video", "collection", "certain", "-lrb-", "possibly", "large", "-rrb-", "area", "e.g.", "inner", "city", "London", "user", "can", "tour", "through", "video", "collection", "follow", "video", "transition", "between", "they", "correspond", "view", "while", "we", "system", "can", "provide", "direction", "from", "location", "sparse", "video", "collection", "may", "contain", "sufficient", "input", "do", "provide", "spatial", "arrangement", "landmark", "contain", "within", "video", "collection", "-lrb-", "distinct", "from", "geolocation", "video", "capture", "-rrb-", "unlike", "tour", "through", "image", "we", "system", "convey", "sense", "place", "dynamics", "liveliness", "while", "still", "maintain", "seamless", "browse", "video", "transition", "challenge", "build", "set", "technique", "analyze", "sparse", "unstructured", "video", "collection", "provide", "set", "interface", "exploit", "derive", "structure", "end", "we", "compute", "videoscape", "graph", "structure", "from", "collection", "video", "-lrb-", "figure", "-rrb-", "edge", "Videoscape", "video", "segment", "node", "mark", "possible", "transition", "point", "portal", "between", "video", "we", "automatically", "identify", "portal", "from", "appropriate", "subset", "video", "frame", "often", "great", "redundancy", "video", "process", "portal", "-lrb-", "corresponding", "video", "frame", "-rrb-", "enable", "smooth", "transition", "between", "video", "Videoscape", "can", "explore", "interactively", "play", "video", "clip", "transition", "other", "clip", "when", "portal", "arise", "when", "temporal", "context", "relevant", "temporal", "awareness", "event", "provide", "offer", "correctly", "order", "transition", "between", "temporally", "align", "video", "yield", "meaningful", "spatio-temporal", "viewing", "experience", "large", "unstructured", "video", "collection", "map-based", "viewing", "mode", "let", "user", "choose", "end", "video", "automatically", "find", "path", "video", "transition", "join", "they", "gp", "orientation", "datum", "enhance", "map", "view", "when", "available", "furthermore", "image", "can", "give", "system", "from", "which", "closest", "matching", "portal", "form", "path", "through", "Videoscape", "enhance", "experience", "when", "transition", "through", "portal", "we", "develop", "different", "video", "transition", "mode", "appropriate", "transition", "select", "base", "preference", "participant", "user", "study", "finally", "we", "evaluate", "videoscape", "system", "three", "further", "user", "study", "videoscape", "exploration", "explorer", "application", "enable", "intuitive", "seamless", "spatio-temporal", "exploration", "Videoscape", "base", "several", "novel", "exploration", "paradigm", "videoscape", "evaluation", "four", "user", "study", "provide", "quantitative", "qualitative", "datum", "compare", "Videoscapes", "exist", "system", "include", "user", "study", "analyze", "preferred", "transition", "type", "heuristic", "appropriate", "use", "we", "exemplify", "use", "we", "system", "database", "part", "London", "input", "material", "videoscape", "capture", "individual", "who", "be", "ask", "walk", "through", "city", "film", "thing", "like", "happen", "around", "they", "set", "video", "capture", "different", "day", "be", "process", "videoscape", "can", "interactively", "explore", "we", "demonstrate", "we", "interactive", "interface", "we", "supplemental", "video", "content-based", "Retrieval", "Finding", "portal", "between", "video", "relate", "content-based", "image", "video", "retrieval", "from", "off-line", "database", "internet", "see", "Datta", "et", "al.", "-lsb-", "2008", "-rsb-", "survey", "Video", "Google", "-lsb-", "Sivic", "Zisserman", "2003", "-rsb-", "one", "first", "system", "enable", "video", "retrieval", "can", "robustly", "detect", "recognize", "object", "from", "different", "viewpoint", "so", "provide", "image-based", "retrieval", "contents", "video", "database", "have", "also", "be", "research", "retrieve", "annotate", "geographic", "location", "spatial", "landmark", "Kennedy", "Naaman", "-lsb-", "2008", "-rsb-", "use", "visual", "feature", "metadatum", "user-tag", "clustering", "annotate", "photograph", "goal", "we", "work", "pure", "content", "retrieval", "instead", "we", "want", "structure", "video", "datum", "can", "explore", "intuitively", "seamlessly", "we", "employ", "robust", "key-point", "matching", "portal", "identification", "-lrb-", "section", "-rrb-", "approach", "have", "also", "be", "use", "recent", "work", "content-based", "geolocation", "image", "-lsb-", "Baatz", "et", "al.", "2010", "Zamir", "Shah", "2010", "Li", "et", "al.", "2008", "-rsb-", "increase", "retrieval", "performance", "Li", "et", "al.", "-lsb-", "2008", "-rsb-", "build", "graph", "structure", "-lrb-", "iconic", "scene", "graph", "-rrb-", "which", "relate", "image", "landmark", "only", "contain", "sparse", "set", "representative", "image", "through", "spectral", "refinement", "we", "also", "filter", "out", "erroneous", "portal", "we", "Videoscape", "graph", "which", "relate", "spirit", "identify", "iconic", "image", "however", "we", "setting", "different", "since", "we", "graph", "model", "entire", "video", "collection", "cover", "many", "landmark", "we", "filter", "match", "technique", "adapt", "specifically", "we", "sparse", "video", "datum", "structure", "Media", "Collections", "since", "casually", "capture", "community", "photo", "video", "collection", "stem", "largely", "from", "unconstrained", "environment", "analyze", "connection", "spatial", "arrangement", "camera", "challenging", "problem", "Photo", "Tourism", "work", "Snavely", "et", "al.", "-lsb-", "2006", "-rsb-", "take", "challenge", "give", "set", "photograph", "show", "same", "spatial", "location", "-lrb-", "e.g.", "image", "Notre", "Dame", "de", "Paris", "-rrb-", "perform", "structure-from-motion", "estimate", "camera", "sparse", "3d", "scene", "geometry", "set", "image", "arrange", "space", "spatially", "confine", "location", "can", "interactively", "navigate", "recent", "work", "have", "use", "stereo", "reconstruction", "from", "photo", "tourism", "datum", "-lsb-", "Goesele", "et", "al.", "2007", "-rsb-", "path", "finding", "through", "image", "take", "from", "same", "location", "-lsb-", "Snavely", "et", "al.", "2008", "-rsb-", "cloud", "computing", "enable", "significant", "speed-up", "reconstruction", "from", "community", "photo", "collection", "-lsb-", "Agarwal", "et", "al.", "2009", "-rsb-", "other", "work", "find", "novel", "strategy", "scale", "basic", "concept", "larger", "image", "set", "reconstruction", "-lsb-", "Frahm", "et", "al.", "2010b", "-rsb-", "include", "reconstruct", "geometry", "from", "frame", "video", "capture", "from", "roof", "vehicle", "additional", "sensor", "-lsb-", "Frahm", "et", "al.", "2010a", "-rsb-", "while", "some", "problem", "parallel", "ours", "transfer", "approach", "casually", "capture", "video", "non-trivial", "instance", "naive", "application", "-lsb-", "Frahm", "et", "al.", "2010b", "-rsb-", "we", "London", "video", "collection", "can", "yield", "full", "3d", "reconstruction", "depict", "environment", "video", "datum", "sparse", "contrast", "previous", "system", "which", "attempt", "reconstruct", "dense", "geometry", "confine", "location", "we", "approach", "aim", "recover", "navigate", "linkage", "structure", "video", "cover", "much", "larger", "area", "video", "coverage", "sporadic", "we", "reconstruct", "scene", "camera", "geometry", "only", "specific", "location", "-lrb-", "i.e.", "portal", "-rrb-", "Kennedy", "et", "al.", "-lsb-", "2009", "-rsb-", "use", "audio", "datum", "align", "video", "clip", "know", "have", "be", "record", "different", "people", "same", "event", "e.g.", "concert", "we", "system", "go", "farther", "than", "application", "scenario", "automatically", "link", "network", "video", "from", "unknown", "location", "compute", "immersive", "3d", "transition", "recently", "advance", "have", "be", "make", "analyze", "represent", "connectivity", "image", "graph", "Philibin", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "geometric", "latent", "dirichlet", "allocation", "which", "exploit", "geometrical", "collocation", "structure", "object", "image", "thereby", "enable", "accurate", "image", "match", "specific", "landmark", "Weyand", "Leibe", "-lsb-", "Weyand", "Leibe", "2011", "-rsb-", "propose", "algorithm", "select", "favorite", "view", "object", "base", "analysis", "how", "view", "overlap", "algorithm", "focus", "improve", "pairwise", "image", "matching", "construct", "representative", "view", "image", "collection", "discuss", "section", "can", "all", "benefit", "from", "we", "analysis", "global", "context", "graph", "structure", "perhaps", "most", "strongly", "related", "we", "algorithm", "image", "web", "-lsb-", "Heath", "et", "al.", "2010", "-rsb-", "which", "construct", "visualize", "graph", "structure", "reflect", "large-scale", "connectivity", "image", "system", "first", "build", "sparsely", "connect", "graph", "perform", "feature-based", "matching", "which", "make", "incrementally", "denser", "via", "connectivity", "analysis", "we", "portal", "identification", "scheme", "also", "rely", "key", "point", "matching", "follow", "connectivity", "analysis", "base", "graph", "laplacian", "however", "oppose", "image", "web", "we", "want", "filter", "out", "unreliable", "match", "rather", "than", "increase", "graph", "connectivity", "rendering", "explore", "Media", "Collections", "image/videobased", "rendering", "method", "synthesize", "new", "view", "from", "photos/videos", "scene", "we", "capitalize", "previous", "work", "area", "render", "portal", "while", "navigate", "Videoscape", "pioneering", "work", "Andrew", "Lippman", "-lsb-", "1980", "-rsb-", "realize", "one", "first", "system", "interactive", "navigation", "through", "database", "image", "subsequent", "research", "attempt", "automate", "process", "instance", "Kimber", "et", "al.", "FlyAbout", "-lsb-", "2001", "-rsb-", "capture", "panoramic", "video", "move", "360", "camera", "along", "continuous", "path", "synthesize", "novel", "view", "mosaicing", "user", "choose", "path", "through", "constrain", "set", "automatically", "pre-computed", "branch", "point", "point", "only", "novel", "view", "synthesis", "require", "we", "describe", "heuristic", "investigate", "through", "user", "study", "select", "appropriate", "transition", "render", "style", "telepresence", "context", "McCurdy", "Griswold?s", "Realityflythrough", "-lsb-", "2005", "-rsb-", "establish", "connection", "between", "video", "from", "mobile", "device", "base", "gp", "information", "provide", "simple", "transition", "between", "overlap", "video", "manner", "similar", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "transition", "video", "project", "onto", "respective", "image", "plane", "Aliaga", "et", "al.", "sea", "image", "-lsb-", "2003", "-rsb-", "require", "special", "robotic", "acquisition", "platform", "fiducial", "place", "scene", "consequence", "system", "operate", "spatially", "confine", "environment", "where", "dense", "set", "view", "can", "easily", "capture", "further", "related", "approach", "exist", "navigate", "through", "real", "scene", "capture", "photograph", "video", "-lsb-", "Debevec", "et", "al.", "1996", "Saurer", "et", "al.", "2010", "-rsb-", "however", "method", "rely", "constrain", "capture", "environment", "-lrb-", "e.g.", "special", "hardware", "confine", "spatial", "location", "-rrb-", "which", "facilitate", "processing", "rendering", "contrast", "we", "work", "we", "exploit", "vision", "technique", "automatically", "find", "connection", "between", "video", "capture", "under", "less", "constrain", "condition", "video", "browse", "system", "propose", "Pongnumkul", "et", "al.", "-lsb-", "2008", "-rsb-", "provide", "interface", "create", "geographical", "storyboard", "from", "single", "continuous", "video", "manually", "connect", "frame", "map", "landmark", "we", "system", "improve", "upon", "method", "automatically", "identify", "connection", "between", "many", "video", "join", "they", "visual", "transition", "we", "also", "exploit", "sensor", "datum", "provide", "richer", "viewing", "interface", "technique", "propose", "Ballan", "et", "al.", "-lsb-", "2010", "-rsb-", "enable", "blend", "between", "different", "video", "show", "single", "spatially", "confine", "scene", "event", "assume", "scene", "model", "billboard", "foreground", "3d", "geometry", "background", "background", "reconstruct", "from", "additional", "community", "photo", "scene", "video", "camera", "calibrate", "w.r.t.", "background", "model", "system", "state", "art", "tailor", "spatially", "confine", "set", "video", "all", "see", "same", "event", "same", "time", "from", "converge", "camera", "angle", "contrast", "we", "system", "operate", "collection", "show", "variety", "general", "scene", "film", "from", "much", "less", "constrain", "set", "camera", "position", "different", "time" ],
  "content" : "In recent years, there has been an explosion of mobile devices capable of recording photographs that can be shared on community platforms. The research community has started to harvest the immense amount of data from community photo collections, and has developed tools to estimate the spatial relation between photographs, or to reconstruct 3D geometry of certain landmarks if a sufficiently dense set of photos is available [Snavely et al. 2006; Goesele et al. 2007; Agarwal et al. 2009; Frahm et al. 2010b]. Users can then interactively explore these locations by viewing the reconstructed 3D models or spatially transitioning between photographs. Navigation tools like Google Street View or Bing Maps also use this exploration paradigm and reconstruct entire street networks through alignment of purposefully captured imagery via additionally recorded localization and depth sensor data. These photo exploration tools are ideal for viewing and navigating static landmarks, such as Notre Dame, but cannot convey the dynamics, liveliness, and spatio-temporal relationships of a location or an event. One solution is to employ video data; yet, there are no comparable browsing experiences for casually captured videos and their generation is still an open challenge. One may be tempted to think that videos are simply series of images, so straightforward extensions of image-based approaches should serve the purpose and enable video tours. However, in reality the nature of casually captured video is different from photos and prevents such a simple extension. Casually captured video collections are usually sparse and largely unstructured, unlike the dense photo collections used in the approaches mentioned above. This precludes a dense reconstruction or registration of all frames. Furthermore, the exploration interface should reflect the dynamic and temporal nature of video. In this paper, we propose a system to explore unstructured video collections in an immersive and visually compelling manner. Given a sparse video collection of a certain (possibly large) area, e.g., the inner city of London, the user can tour through the video collection by following videos and transitioning between them at corresponding views. While our system cannot provide directions from location A to B, as sparse video collections may not contain sufficient input, it does provide the spatial arrangement of landmarks contained within a video collection (distinct from the geolocations of video captures). Unlike tours through images, our system conveys a sense of place, dynamics and liveliness while still maintaining  seamless browsing with video transitions. The challenge is to build a set of techniques to analyze sparse unstructured video collections, and to provide a set of interfaces to exploit the derived structure. To this end, we compute a Videoscape graph structure from a collection of videos ( Figure 1 ). The edges of the Videoscape are video segments and the nodes mark possible transition points, or portals, between videos. We automatically identify portals from an appropriate subset of the video frames as there is often great redundancy in videos, and process the portals (and the corresponding video frames) to enable smooth transitions between videos. The Videoscape can be explored interactively by playing video clips and transitioning to other clips when a portal arises. When temporal context is relevant, temporal awareness of an event is provided by offering correctly ordered transitions between temporally aligned videos. This yields a meaningful spatio-temporal viewing experience of large, unstructured video collections. A map-based viewing mode lets the user choose start and end videos, and automatically find a path of videos and transitions that join them. GPS and orientation data enhances the map view when available. Furthermore, images can be given to the system, from which the closest matching portals form a path through the Videoscape. To enhance the experience when transitioning through a portal, we develop different video transition modes, with appropriate transitions selected based on the preference of participants in a user study. Finally, we evaluate the Videoscape system with three further user studies. ? Videoscape exploration: an explorer application that enables intuitive and seamless spatio-temporal exploration of the Videoscape, based on several novel exploration paradigms. ? Videoscape evaluation: four user studies providing quantitative and qualitative data comparing Videoscapes to existing systems, including a user study analyzing preferred transition types and heuristics for their appropriate use. We exemplify the use of our system on databases of parts of London. The input material to this Videoscape was captured by individuals who were asked to walk through the city and to film things they liked as they happened around them. Sets of videos captured on different days were processed into a Videoscape that can be interactively explored, and we demonstrate our interactive interfaces in our supplemental video. Content-based Retrieval Finding portals between videos relates to content-based image and video retrieval from an off-line database or the Internet, see Datta et al. [2008] for a survey. Video Google [Sivic and Zisserman 2003] is one of the first systems that enables video retrieval. It can robustly detect and recognize objects from different viewpoints and so provides image-based retrieval of contents in a video database. There has also been research into retrieving and annotating geographic locations or spatial landmarks. Kennedy and Naaman [2008] used visual features, metadata, and user-tags for clustering and annotating photographs. The goal of our work is not pure content retrieval; instead, we want to structure video data such that it can be explored intuitively and seamlessly. We employ robust key-point matching for portal identification (Section 4), an approach that has also been used in recent work on  content-based geolocation of images [Baatz et al. 2010; Zamir and Shah 2010; Li et al. 2008]. To increase retrieval performance, Li et al. [2008] build a graph structure (the iconic scene graph) which relates images of a landmark and only contains a sparse set of representative images. Through spectral refinement we also filter out erroneous portals in our Videoscape graph, which is related in spirit to identifying iconic images. However, our setting is different since our graph models entire video collections covering many landmarks, and our filtering and matching technique are adapted specifically to our sparse video data. Structuring Media Collections Since casually captured community photo and video collections stem largely from unconstrained environments, analyzing their connections and the spatial arrangement of cameras is a challenging problem. In their Photo Tourism work, Snavely et al. [2006] took on that challenge: Given a set of photographs showing the same spatial location (e.g., images of ?Notre Dame de Paris?), they performed structure-from-motion to estimate cameras and sparse 3D scene geometry. The set of images is arranged in space such that spatially confined locations can be interactively navigated. Recent work has used stereo reconstruction from photo tourism data [Goesele et al. 2007], path finding through images taken from the same location [Snavely et al. 2008], and cloud computing to enable significant speed-up of reconstruction from community photo collections [Agarwal et al. 2009]. Other work finds novel strategies to scale the basic concepts to larger image sets for reconstruction [Frahm et al. 2010b], including reconstructing geometry from frames of videos captured from the roof of a vehicle with additional sensors [Frahm et al. 2010a]. While some of these problems are parallel to ours, transfer of their approaches to casually captured videos is non-trivial. For instance, a naive application of [Frahm et al. 2010b] on our London video collection cannot yield a full 3D reconstruction of the depicted environment as the video data is sparse. In contrast to previous systems, which attempt to reconstruct a dense geometry for a confined location, our approach aims to recover and navigate the linkage structure of videos covering a much larger area. As video coverage is sporadic, we reconstruct scene and camera geometry only for specific locations (i.e., at portals). Kennedy et al. [2009] used audio data to align video clips that are known to have been recorded by different people at the same event, e.g., a concert. Our system goes farther than this application scenario by automatically linking networks of videos from unknown locations, and computing immersive 3D transitions. Recently, advances have been made in analyzing and representing the connectivity of images as a graph. Philibin et al. [2011] proposed geometric latent Dirichlet allocation, which exploits the geometrical collocation structure of objects in images and thereby enables accurate image matching for specific landmarks. Weyand and Leibe [Weyand and Leibe 2011] proposed an algorithm to select favorite views of an object based on the analysis of how views of it overlap. These algorithms focus on improving pairwise image matching or constructing representative views of image collections. As will be discussed in Section 4, they can all benefit from our analysis of global context in the graph structure. Perhaps most strongly related to our algorithm is Image Webs [Heath et al. 2010], which constructs and visualizes a graph structure reflecting the large-scale connectivity of images. The system first builds a sparsely connected graph by performing feature-based matching, which is made incrementally denser via connectivity analysis. Our portal identification scheme also relies on key point matching followed by connectivity analysis based on the graph Laplacian. However, as opposed Image Webs, we want to filter out unreliable matches rather than to increase the graph connectivity. Rendering and Exploring Media Collections Image/videobased rendering methods synthesize new views from photos/videos of a scene. We capitalize on previous work in this area to render portals while navigating the Videoscape. The pioneering work of Andrew Lippman [1980] realized one of the first systems for interactive navigation through a database of images. Subsequent research attempted to automate this process. For instance, Kimber et al.?s FlyAbout [2001] captured panoramic videos by moving a 360 ? camera along continuous paths and synthesized novel views by mosaicing. Users chose a path through a constrained set of automatically pre-computed branching points, and at these points only novel view synthesis is required. We describe heuristics, investigated through a user study, to select appropriate transition rendering styles. In a telepresence context, McCurdy and Griswold?s Realityflythrough [2005] establishes connections between videos from mobile devices based on GPS information and provides a simple transition between overlapping videos in a manner similar to [Snavely et al. 2006]. At transitions, videos are projected onto their respective image planes. Aliaga et al.?s Sea of Images [2003] requires a special robotic acquisition platform and fiducials placed into the scene. As a consequence, the system operates in a spatially confined environment where a dense set of views can be easily captured. Further related approaches exist for navigating through real scenes captured in photographs and videos [Debevec et al. 1996; Saurer et al. 2010]. However, these methods rely on a constrained capture environment (e.g., special hardware or confined spatial locations), which facilitates processing and rendering. In contrast, in our work we exploit vision techniques to automatically find the connections between videos captured under less constrained conditions. The video browsing system proposed by Pongnumkul et al. [2008] provides an interface to create a geographical storyboard from a single continuous video by manually connecting frames to map landmarks. Our system improves upon this method by automatically identifying connections between many videos and joining them with visual transitions. We also exploit sensor data to provide a richer viewing interface. The technique proposed by Ballan et al. [2010] enables blending between different videos showing a single spatially confined scene or event. They assume a scene model with a billboard in the foreground and 3D geometry in the background. The background is reconstructed from additional community photos of the scene, and the video cameras are calibrated w.r.t. the background model. The system is state of the art, but is tailored to spatially confined sets of videos that all see the same event at the same time from converging camera angles. In contrast, our system operates with a collection that shows a variety of general scenes filmed from a much less constrained set of camera positions at different times.",
  "resources" : [ ]
}