{
  "uri" : "sig2012-a68-tompkin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a68-tompkin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Videoscapes: Exploring Sparse, Unstructured Video Collections",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/James-Tompkin",
      "name" : "James",
      "surname" : "Tompkin"
    }, {
      "uri" : "http://drinventor/Kwang In-Kim",
      "name" : "Kwang In",
      "surname" : "Kim"
    }, {
      "uri" : "http://drinventor/Jan-Kautz",
      "name" : "Jan",
      "surname" : "Kautz"
    }, {
      "uri" : "http://drinventor/Christian-Theobalt",
      "name" : "Christian",
      "surname" : "Theobalt"
    } ]
  },
  "bagOfWords" : [ "from", "portal", "we", "construct", "Videoscape", "graph", "whose", "edge", "video", "clip", "whose", "node", "portal", "between", "clip", "cr", "category", "i.", "4.8", "-lsb-", "Computer", "Graphics", "-rsb-", "scene", "analysis", "time-varying", "imagery", "Keywords", "video", "collection", "spatio-temporal", "exploration", "however", "reality", "nature", "casually", "capture", "video", "different", "from", "photo", "prevent", "simple", "extension", "challenge", "build", "set", "technique", "analyze", "sparse", "unstructured", "video", "collection", "provide", "set", "interface", "exploit", "derive", "structure", "input", "material", "videoscape", "capture", "individual", "who", "be", "ask", "walk", "through", "city", "film", "thing", "like", "happen", "around", "they", "content-based", "Retrieval", "Finding", "portal", "between", "video", "relate", "content-based", "image", "video", "retrieval", "from", "off-line", "database", "internet", "see", "Datta", "et", "al.", "-lsb-", "2008", "-rsb-", "survey", "have", "also", "be", "research", "retrieve", "annotate", "geographic", "location", "spatial", "landmark", "Kennedy", "Naaman", "-lsb-", "2008", "-rsb-", "use", "visual", "feature", "metadatum", "user-tag", "clustering", "annotate", "photograph", "increase", "retrieval", "performance", "Li", "et", "al.", "-lsb-", "2008", "-rsb-", "build", "graph", "structure", "-lrb-", "iconic", "scene", "graph", "-rrb-", "which", "relate", "image", "landmark", "only", "contain", "sparse", "set", "representative", "image", "Photo", "Tourism", "work", "Snavely", "et", "al.", "-lsb-", "2006", "-rsb-", "take", "challenge", "give", "set", "photograph", "show", "same", "spatial", "location", "-lrb-", "e.g.", "image", "Notre", "Dame", "de", "Paris", "-rrb-", "perform", "structure-from-motion", "estimate", "camera", "sparse", "3d", "scene", "geometry", "recent", "work", "have", "use", "stereo", "reconstruction", "from", "photo", "tourism", "datum", "-lsb-", "Goesele", "et", "al.", "2007", "-rsb-", "path", "finding", "through", "image", "take", "from", "same", "location", "-lsb-", "Snavely", "et", "al.", "2008", "-rsb-", "cloud", "computing", "enable", "significant", "speed-up", "reconstruction", "from", "community", "photo", "collection", "-lsb-", "Agarwal", "et", "al.", "2009", "-rsb-", "Kennedy", "et", "al.", "-lsb-", "2009", "-rsb-", "use", "audio", "datum", "align", "video", "clip", "know", "have", "be", "record", "different", "people", "same", "event", "e.g.", "concert", "recently", "advance", "have", "be", "make", "analyze", "represent", "connectivity", "image", "graph", "Philibin", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "geometric", "latent", "dirichlet", "allocation", "which", "exploit", "geometrical", "collocation", "structure", "object", "image", "thereby", "enable", "accurate", "image", "match", "specific", "landmark", "Weyand", "Leibe", "-lsb-", "Weyand", "Leibe", "2011", "-rsb-", "propose", "algorithm", "select", "favorite", "view", "object", "base", "analysis", "how", "view", "overlap", "algorithm", "focus", "improve", "pairwise", "image", "matching", "construct", "representative", "view", "image", "collection", "perhaps", "most", "strongly", "related", "we", "algorithm", "image", "web", "-lsb-", "Heath", "et", "al.", "2010", "-rsb-", "which", "construct", "visualize", "graph", "structure", "reflect", "large-scale", "connectivity", "image", "subsequent", "research", "attempt", "automate", "process", "instance", "Kimber", "et", "al.", "FlyAbout", "-lsb-", "2001", "-rsb-", "capture", "panoramic", "video", "move", "360", "camera", "along", "continuous", "path", "synthesize", "novel", "view", "mosaicing", "telepresence", "context", "McCurdy", "Griswold?s", "Realityflythrough", "-lsb-", "2005", "-rsb-", "establish", "connection", "between", "video", "from", "mobile", "device", "base", "gp", "information", "provide", "simple", "transition", "between", "overlap", "video", "manner", "similar", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "transition", "video", "project", "onto", "respective", "image", "plane", "Aliaga", "et", "al.", "sea", "image", "-lsb-", "2003", "-rsb-", "require", "special", "robotic", "acquisition", "platform", "fiducial", "place", "scene", "further", "related", "approach", "exist", "navigate", "through", "real", "scene", "capture", "photograph", "video", "-lsb-", "Debevec", "et", "al.", "1996", "Saurer", "et", "al.", "2010", "-rsb-", "assume", "scene", "model", "billboard", "foreground", "3d", "geometry", "background", "background", "model", "section", "describe", "off-line", "component", "which", "construct", "Videoscape", "graph", "capture", "semantic", "link", "between", "database", "casually", "capture", "video", "edge", "graph", "video", "node", "possible", "transition", "point", "between", "video", "so-called", "portal", "input", "we", "system", "database", "video", "which", "each", "video", "may", "contain", "many", "different", "shot", "several", "location", "we", "expect", "most", "video", "have", "least", "one", "shot", "show", "similar", "location", "least", "one", "other", "video", "long", "video", "which", "may", "contain", "shot", "several", "scene", "mask", "during", "graph", "construction", "series", "shorter", "30", "second", "video", "clip", "provide", "portal", "opportunity", "regular", "interval", "after", "run", "sift", "we", "use", "ransac", "estimate", "match", "most", "consistent", "accord", "fundamental", "matrix", "-lsb-", "Hartley", "Zisserman", "2004", "-rsb-", "similar", "other", "related", "method", "-lsb-", "Snavely", "et", "al.", "2006", "Heath", "et", "al.", "2010", "Li", "et", "al.", "2008", "-rsb-", "similar", "Heath", "et", "al.", "-lsb-", "2010", "-rsb-", "however", "use", "graph", "opposite", "goal", "increase", "connectivity", "between", "matched", "photograph", "intuitively", "-lrb-", "-rrb-", "-lsb-", "-rsb-", "close", "when", "two", "input", "frame", "contain", "common", "feature", "similar", "many", "way", "accomplish", "literature", "describe", "many", "style", "camera", "transition", "-lsb-", "Morvan", "O?Sullivan", "2009", "Goesele", "et", "al.", "2010", "Veas", "et", "al.", "2010", "Vangorp", "et", "al.", "2011", "-rsb-", "cinema", "bestow", "certain", "experience", "upon", "viewer", "-lsb-", "Dmytryk", "1984", "Murch", "2001", "-rsb-", "inspire", "-lsb-", "Lipski", "et", "al.", "2010", "-rsb-", "warp", "transition", "proceed", "follow", "give", "2d", "image", "correspondence", "from", "SFM", "between", "portal", "frame", "we", "compute", "as-similar-as-possible", "moving-least-square", "-lrb-", "ml", "-rrb-", "transform", "-lsb-", "Schaefer", "et", "al.", "2006", "-rsb-", "base", "datum", "we", "support", "follow", "transition", "type", "plane", "transition", "where", "plane", "fit", "reconstructed", "geometry", "-lrb-", "similar", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "-rrb-", "two", "video", "project", "dissolve", "across", "transition", "ambient", "point", "cloud-based", "-lrb-", "apc", "-rrb-", "transition", "-lsb-", "Goesele", "et", "al.", "2010", "-rsb-", "which", "project", "video", "onto", "reconstructed", "geometry", "use", "apc", "area", "without", "reconstruction", "all", "transition", "case", "dynamic", "object", "either", "video", "handle", "explicitly", "dissolve", "implicitly", "across", "transition", "strategy", "support", "Morvan", "O?Sullivan", "-lsb-", "2009", "-rsb-", "who", "assess", "similar", "problem", "occlude", "object", "when", "transition", "between", "camera", "detail", "construct", "label", "datum", "structure", "-lrb-", "which", "label", "propagation", "-rrb-", "discuss", "supplemental", "material", "geometry", "reconstruction", "individual", "portal", "connect", "between", "two", "nine", "video", "each", "75", "identify", "portal", "connect", "two", "video", "participant", "drag", "drop", "video", "reorder", "list", "from", "most", "prefer", "least", "prefer", "spatial", "awareness", "we", "design", "study", "which", "attempt", "measure", "how", "much", "spatial", "awareness", "retain", "through", "video", "transition", "without", "geometry-based", "reconstruction", "q2", "which", "interface", "do", "you", "find", "provide", "greater", "spatial", "awareness", "sense", "orientation" ],
  "content" : "From these portals, we construct the Videoscape, a graph whose edges are video clips and whose nodes are portals between clips. CR Categories: I.4.8 [Computer Graphics]: Scene Analysis? Time-varying imagery; Keywords: video collections, spatio-temporal exploration. However, in reality the nature of casually captured video is different from photos and prevents such a simple extension. The challenge is to build a set of techniques to analyze sparse unstructured video collections, and to provide a set of interfaces to exploit the derived structure. The input material to this Videoscape was captured by individuals who were asked to walk through the city and to film things they liked as they happened around them. Content-based Retrieval Finding portals between videos relates to content-based image and video retrieval from an off-line database or the Internet, see Datta et al. [2008] for a survey. There has also been research into retrieving and annotating geographic locations or spatial landmarks. Kennedy and Naaman [2008] used visual features, metadata, and user-tags for clustering and annotating photographs. To increase retrieval performance, Li et al. [2008] build a graph structure (the iconic scene graph) which relates images of a landmark and only contains a sparse set of representative images. In their Photo Tourism work, Snavely et al. [2006] took on that challenge: Given a set of photographs showing the same spatial location (e.g., images of ?Notre Dame de Paris?), they performed structure-from-motion to estimate cameras and sparse 3D scene geometry. Recent work has used stereo reconstruction from photo tourism data [Goesele et al. 2007], path finding through images taken from the same location [Snavely et al. 2008], and cloud computing to enable significant speed-up of reconstruction from community photo collections [Agarwal et al. 2009]. Kennedy et al. [2009] used audio data to align video clips that are known to have been recorded by different people at the same event, e.g., a concert. Recently, advances have been made in analyzing and representing the connectivity of images as a graph. Philibin et al. [2011] proposed geometric latent Dirichlet allocation, which exploits the geometrical collocation structure of objects in images and thereby enables accurate image matching for specific landmarks. Weyand and Leibe [Weyand and Leibe 2011] proposed an algorithm to select favorite views of an object based on the analysis of how views of it overlap. These algorithms focus on improving pairwise image matching or constructing representative views of image collections. Perhaps most strongly related to our algorithm is Image Webs [Heath et al. 2010], which constructs and visualizes a graph structure reflecting the large-scale connectivity of images. Subsequent research attempted to automate this process. For instance, Kimber et al.?s FlyAbout [2001] captured panoramic videos by moving a 360 ? camera along continuous paths and synthesized novel views by mosaicing. In a telepresence context, McCurdy and Griswold?s Realityflythrough [2005] establishes connections between videos from mobile devices based on GPS information and provides a simple transition between overlapping videos in a manner similar to [Snavely et al. 2006]. At transitions, videos are projected onto their respective image planes. Aliaga et al.?s Sea of Images [2003] requires a special robotic acquisition platform and fiducials placed into the scene. Further related approaches exist for navigating through real scenes captured in photographs and videos [Debevec et al. 1996; Saurer et al. 2010]. They assume a scene model with a billboard in the foreground and 3D geometry in the background. the background model. Section 4 describes the off-line component which constructs the Videoscape: a graph capturing the semantic links between a database of casually captured videos. The edges of the graph are videos and the nodes are possible transition points between videos, so-called portals. Input to our system is a database of videos in which each video may contain many different shots of several locations. We expect most videos to have at least one shot that shows a similar location to at least one other video. Long videos, which may contain shots of several scenes, are masked during graph construction into a series of shorter 30 second video clips to provide portal opportunities at regular intervals. After running SIFT, we use RANSAC to estimate matches that are most consistent according to the fundamental matrix [Hartley and Zisserman 2004], similar to other related methods [Snavely et al. 2006; Heath et al. 2010; Li et al. 2008]. This is similar to Heath et al. [2010]; however, they use this graph for the opposite goal of increasing connectivity between matched photographs. Intuitively, k(?, ?) ? [0, 1] is close to 1 when two input frames contain common features and are similar. There are many ways to accomplish this: the literature describes many styles of camera transitions [Morvan and O?Sullivan 2009; Goesele et al. 2010; Veas et al. 2010; Vangorp et al. 2011] and cinema bestows certain experiences upon the viewer [Dmytryk 1984; Murch 2001]. Inspired by [Lipski et al. 2010], the warp transition proceeds as follows: Given 2D image correspondences from SFM between portal frames, we compute an as-similar-as-possible moving-least-squares (MLS) transform [Schaefer et al. 2006]. Based on this data, we support the following transition types: a plane transition, where a plane is fitted to the reconstructed geometry (similar to [Snavely et al. 2006]) and the two videos are projected and dissolved across the transition; an ambient point cloud-based (APC) transition [Goesele et al. 2010] which projects video onto the reconstructed geometry and uses APCs for areas without reconstruction. In all transition cases, dynamic objects in either video are not handled explicitly, but dissolved implicitly across the transition. This strategy is supported by Morvan and O?Sullivan [2009], who assess the similar problem of occluding objects when transitioning between cameras. Details of constructing the label data structure (which is label propagation) are discussed in the supplemental material. Geometry Reconstruction Individual portals connect between two and nine videos each, with 75% of identified portals connecting two videos. Participants drag and drop videos to reorders the list from most preferred to least preferred. Spatial Awareness We designed a study which attempts to measure how much spatial awareness is retained through video transitions with and without geometry-based reconstructions. ? and Q2: ?Which interface did you find provided the greater spatial awareness and sense of orientation?",
  "resources" : [ ]
}