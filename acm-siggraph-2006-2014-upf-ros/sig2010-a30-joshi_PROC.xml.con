{
  "uri" : "sig2010-a30-joshi_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2010/a30-joshi_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Image Deblurring using Inertial Measurement Sensors",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Ondrej-Sindelar",
      "name" : "Ondrej",
      "surname" : "Sindelar"
    }, {
      "uri" : "http://drinventor/Filip-Sroubek",
      "name" : "Filip",
      "surname" : "Sroubek"
    } ]
  },
  "bagOfWords" : [ "we", "now", "describe", "result", "we", "ground-truth", "camera-shake", "measurement", "compare", "result", "use", "we", "deblurring", "method", "ground-truth", "measurement", "we", "also", "compare", "we", "result", "those", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "use", "implementation", "author", "have", "available", "online", "we", "also", "show", "result", "we", "method", "run", "natural", "image", "acquire", "outside", "lab", "setup", "compare", "result", "use", "previous", "work", "well", "Figure", "we", "show", "visualization", "ground-truth", "spatiallyvarying", "psf", "image", "from", "we", "lab", "setup", "image", "show", "some", "interesting", "property", "significant", "variation", "across", "image", "plane", "also", "kernel", "display", "fix", "depth", "thus", "all", "spatial", "variance", "due", "rotation", "demonstrate", "importance", "accounting", "spatial", "variance", "bottom", "row", "Figure", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "psf", "different", "non-corresponding", "area", "result", "quite", "interesting", "show", "some", "common", "assumption", "make", "image", "deconvolution", "do", "always", "hold", "most", "deconvolution", "work", "assume", "spatially", "invariant", "kernel", "which", "really", "only", "apply", "camera", "motion", "under", "orthographic", "model", "however", "typical", "imaging", "setup", "-lrb-", "we", "use", "40mm", "lens", "-rrb-", "perspective", "effect", "strong", "enough", "induce", "spatially-varying", "blur", "we", "also", "note", "often", "roll", "component", "blur", "something", "also", "model", "spatially", "invariant", "kernel", "lastly", "we", "observe", "translation", "thus", "depth", "dependent", "effect", "can", "significant", "which", "interesting", "often", "think", "most", "camera-shake", "blur", "due", "rotation", "please", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "example", "Figure", "use", "same", "scene", "above", "we", "show", "comparison", "deconvolution", "result", "use", "we", "method", "ground-truth", "two", "other", "image", "show", "lab", "calibration", "scene", "be", "take", "exposure", "from", "1/3", "1/10", "second", "each", "image", "we", "show", "input", "result", "deconvolve", "psf", "from", "initial", "motion", "estimate", "after", "perform", "drift", "correction", "compare", "deconvolution", "use", "psf", "from", "recover", "groundtruth", "motion", "psf", "recover", "use", "method", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "latter", "two", "comparison", "we", "make", "best", "effort", "adjust", "parameter", "recover", "best", "blur", "kernel", "possible", "make", "comparison", "fair", "all", "result", "be", "deblurr", "use", "exactly", "same", "deconvolution", "method", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "result", "figure", "show", "wide", "variety", "blur", "yet", "we", "method", "recover", "accurate", "kernel", "provide", "deconvolution", "result", "very", "close", "ground-truth", "all", "case", "we", "result", "better", "than", "those", "use", "Shan", "et", "al.", "Fergus", "et", "al.", "method", "after", "calibrate", "we", "hardware", "system", "use", "method", "discuss", "section", "4.2", "we", "take", "camera", "outside", "lab", "use", "laptop", "Bluetooth", "adapter", "capture", "inertial", "sensor", "datum", "Figure", "we", "show", "several", "result", "where", "we", "have", "deblurr", "image", "use", "we", "method", "Shan", "et", "al.", "method", "Fergus", "et", "al.", "method", "Shan", "et", "al.", "Fergus", "et", "al.", "result", "be", "deconvolve", "use", "Levin", "et", "al.", "method", "-lsb-", "2007", "-rsb-", "we", "result", "deconvolve", "use", "we", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.1", "all", "image", "we", "result", "show", "clear", "improvement", "over", "input", "blurry", "image", "still", "some", "residually", "ring", "unavoidable", "due", "frequency", "loss", "during", "blur", "Shan", "et", "al.", "result", "vary", "quality", "many", "show", "large", "ring", "artifact", "due", "kernel", "misestimation", "over-sharpening", "Fergus", "et", "al.", "result", "generally", "blurrier", "than", "ours", "all", "image", "show", "here", "be", "shoot", "1/2", "1/10", "second", "exposure", "40mm", "lens", "Canon", "1Ds", "Mark", "III", "additional", "result", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "Figure", "we", "show", "visualization", "ground-truth", "spatiallyvarying", "psf", "image", "from", "we", "lab", "setup", "image", "show", "some", "interesting", "property", "significant", "variation", "across", "image", "plane", "also", "kernel", "display", "fix", "depth", "thus", "all", "spatial", "variance", "due", "rotation", "demonstrate", "importance", "accounting", "spatial", "variance", "bottom", "row", "Figure", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "psf", "different", "non-corresponding", "area", "result", "quite", "interesting", "show", "some", "common", "assumption", "make", "image", "deconvolution", "do", "always", "hold", "most", "deconvolution", "work", "assume", "spatially", "invariant", "kernel", "which", "really", "only", "apply", "camera", "motion", "under", "orthographic", "model", "however", "typical", "imaging", "setup", "-lrb-", "we", "use", "40mm", "lens", "-rrb-", "perspective", "effect", "strong", "enough", "induce", "spatially-varying", "blur", "we", "also", "note", "often", "roll", "component", "blur", "something", "also", "model", "spatially", "invariant", "kernel", "lastly", "we", "observe", "translation", "thus", "depth", "dependent", "effect", "can", "significant", "which", "interesting", "often", "think", "most", "camera-shake", "blur", "due", "rotation", "please", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "example", "Figure", "use", "same", "scene", "above", "we", "show", "comparison", "deconvolution", "result", "use", "we", "method", "ground-truth", "two", "other", "image", "show", "lab", "calibration", "scene", "be", "take", "exposure", "from", "1/3", "1/10", "second", "each", "image", "we", "show", "input", "result", "deconvolve", "psf", "from", "initial", "motion", "estimate", "after", "perform", "drift", "correction", "compare", "deconvolution", "use", "psf", "from", "recover", "groundtruth", "motion", "psf", "recover", "use", "method", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "latter", "two", "comparison", "we", "make", "best", "effort", "adjust", "parameter", "recover", "best", "blur", "kernel", "possible", "make", "comparison", "fair", "all", "result", "be", "deblurr", "use", "exactly", "same", "deconvolution", "method", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "result", "figure", "show", "wide", "variety", "blur", "yet", "we", "method", "recover", "accurate", "kernel", "provide", "deconvolution", "result", "very", "close", "ground-truth", "all", "case", "we", "result", "better", "than", "those", "use", "Shan", "et", "al.", "Fergus", "et", "al.", "method", "after", "calibrate", "we", "hardware", "system", "use", "method", "discuss", "section", "4.2", "we", "take", "camera", "outside", "lab", "use", "laptop", "Bluetooth", "adapter", "capture", "inertial", "sensor", "datum", "Figure", "we", "show", "several", "result", "where", "we", "have", "deblurr", "image", "use", "we", "method", "Shan", "et", "al.", "method", "Fergus", "et", "al.", "method", "Shan", "et", "al.", "Fergus", "et", "al.", "result", "be", "deconvolve", "use", "Levin", "et", "al.", "method", "-lsb-", "2007", "-rsb-", "we", "result", "deconvolve", "use", "we", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.1", "all", "image", "we", "result", "show", "clear", "improvement", "over", "input", "blurry", "image", "still", "some", "residually", "ring", "unavoidable", "due", "frequency", "loss", "during", "blur", "Shan", "et", "al.", "result", "vary", "quality", "many", "show", "large", "ring", "artifact", "due", "kernel", "misestimation", "over-sharpening", "Fergus", "et", "al.", "result", "generally", "blurrier", "than", "ours", "all", "image", "show", "here", "be", "shoot", "1/2", "1/10", "second", "exposure", "40mm", "lens", "Canon", "1Ds", "Mark", "III", "additional", "result", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "work", "we", "present", "aid", "blind", "deconvolution", "algorithm", "use", "hardware", "attachment", "conjunction", "corresponding", "blurry", "input", "image", "natural", "image", "prior", "compute", "perpixel", "spatially-varying", "blur", "deconvolve", "image", "produce", "sharp", "result", "several", "benefit", "we", "method", "over", "previous", "approach", "-lrb-", "-rrb-", "we", "method", "automatic", "have", "user-tuned", "parameter", "-lrb-", "-rrb-", "use", "inexpensive", "commodity", "hardware", "could", "easily", "build", "camera", "produce", "mass-market", "attachment", "more", "compact", "than", "we", "prototype", "we", "have", "show", "advantage", "over", "purely", "image-based", "method", "which", "some", "sense", "surprising?blind", "deconvolution", "inherently", "ill-posed", "problem", "thus", "extra", "information", "inertial", "measurement", "should", "helpful", "many", "challenge", "use", "datum", "properly", "many", "which", "we", "have", "address", "however", "we", "result", "also", "suggest", "several", "area", "future", "work", "biggest", "limitation", "we", "method", "sensor", "accuracy", "noise", "we", "method?s", "performance", "degrade", "under", "few", "case", "-lrb-", "-rrb-", "drift", "large", "enough", "search", "space", "we", "optimization", "process", "too", "large", "i.e.", "greater", "than", "couple", "mm", "-lrb-", "-rrb-", "we", "estimation", "initial", "camera", "rotation", "relative", "gravity", "incorrect", "similarly", "camera", "move", "way", "normally", "distribute", "about", "gravity", "vector", "-lrb-", "-rrb-", "significant", "depth", "variation", "scene", "camera", "undergo", "significant", "translation", "-lrb-", "-rrb-", "camera", "translate", "some", "initial", "constant", "velocity", "-lrb-", "-rrb-", "large", "image", "frequency", "information", "loss", "blur", "handle", "-lrb-", "-rrb-", "we", "interested", "use", "more", "sensor", "sensor", "inexpensive", "one", "could", "easily", "add", "sensor", "redundancy", "perform", "denoising", "average", "either", "analog", "digital", "domain", "-lrb-", "-rrb-", "one", "could", "consider", "add", "other", "sensor", "magnetometer", "get", "another", "measure", "orientation", "already", "common", "approach", "IMU", "base", "navigation", "system", "-lrb-", "-rrb-", "one", "could", "recover", "depth", "scene", "perform", "depth", "from", "motion", "blur", "algorithm", "similar", "depth", "from", "defocus", "we", "pursue", "problem", "however", "important", "note", "vary", "scene", "depth", "do", "always", "significantly", "affect", "blur", "typical", "situation", "we", "have", "find", "depth", "only", "need", "accurate", "deblurring", "object", "within", "meter", "from", "camera", "most", "often", "people", "take", "image", "where", "scene", "farther", "than", "distance", "-lrb-", "-rrb-", "we", "assume", "initial", "translation", "velocity", "zero", "we", "accelerometer", "give", "we", "measure", "while", "we", "currently", "only", "consider", "accelerometer", "datum", "during", "exposure", "we", "actually", "record", "all", "sensor", "datum", "before", "after", "each", "exposure", "well", "thus", "one", "way", "address", "issue", "try", "identify", "stationary", "before", "camera", "exposure", "track", "from", "get", "more", "accurate", "initial", "velocity", "estimate", "-lrb-", "-rrb-", "issue", "all", "deblurr", "method", "frequency", "loss", "cause", "unavoidable", "artifact", "during", "deconvolution", "could", "appear", "ringing", "banding", "over-smoothing", "depend", "deconvolution", "method", "would", "interesting", "combine", "we", "hardware", "Raskar", "et", "al.", "-lsb-", "2006", "-rsb-", "flutter", "shutter", "hardware", "reduce", "frequency", "loss", "during", "image", "capture" ],
  "content" : "We will now describe the results of our ground-truth camera-shake measurements and compare results using our deblurring method to the ground-truth measurements. We also compare our results to those of Shan et al. [2008] and Fergus et al. [2006], using the implementations the authors have available online. We also show results of our methods running on natural images acquired outside of a lab setup and compare these to results using previous work as well. In Figure 6 , we show visualizations of the ground-truth spatiallyvarying PSFs for an image from our lab setup. This image shows some interesting properties. There is a significant variation across the image plane. Also, the kernels displayed are for a fixed depth, thus all the spatial variance is due to rotation. To demonstrate the importance of accounting for spatial variance, on the bottom row of Figure 6 , we show a result where we have deconvolved using the PSF for the correct part of the image and the PSF for a different, non-corresponding area. These results are quite interesting as they show that some of the common assumptions made in image deconvolution do not always hold. Most deconvolution work assumes spatially invariant kernels, which really only applies for camera motion under an orthographic model; however, with a typical imaging setup (we use a 40mm lens), the perspective effects are strong enough to induce a spatially-varying blur. We also note that there is often a roll component to the blur, something that is also not modeled by spatially invariant kernels. Lastly, we observe that translation, and thus depth dependent effects can be significant, which is interesting as it is often thought that most camera-shake blur is due to rotation. Please visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/ for examples. In Figure 7 , using the same scene as above, we show comparisons of deconvolution results using our method, ground-truth, and two others. The images shown of the lab calibration scene were taken at exposures from 1/3 to 1/10 of a second. For each image we show the input, the result of deconvolving with PSFs from the initial motion estimate and after performing drift correction, and compare these to a deconvolution using PSFs from the recovered groundtruth motions, and PSFs recovered using the methods of Shan et al. [2008] and Fergus et al. [2006]. For these latter two comparisons, we made a best effort to adjust the parameters to recover the best blur kernel possible. To make the comparison fair, all results were deblurred using exactly the same deconvolution method, that of Levin et al. [2007]. The results in Figure 7 , show a wide variety of blurs, yet our method recovers an accurate kernel and provides deconvolution results that are very close to that of the ground-truth. In all cases, our results are better than those using Shan et al.?s and Fergus et al.?s methods. After calibrating our hardware system using the method discussed in Section 4.2, we took the camera outside of the lab, using a laptop with a Bluetooth adapter to capture the inertial sensor data. In Figure 8 , we show several results where we have deblurred images using our method, Shan et al.?s method, and Fergus et al.?s method. The Shan et al. and Fergus et al. results were deconvolved using Levin et al.?s method [2007], and our results are deconvolved using our spatially-varying deconvolution method discussed in Section 3.1. For all the images, our results show a clear improvement over the input blurry image. There is still some residually ringing that is unavoidable due to frequency loss during blurring. The Shan et al. results are of varying quality, and many show large ringing artifacts that are due to kernel misestimation and over-sharpening; the Fergus et al. results are generally blurrier than ours. All the images shown here were shot with 1/2 to 1/10 second exposures with a 40mm lens on a Canon 1Ds Mark III. For additional results, visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/. In Figure 6 , we show visualizations of the ground-truth spatiallyvarying PSFs for an image from our lab setup. This image shows some interesting properties. There is a significant variation across the image plane. Also, the kernels displayed are for a fixed depth, thus all the spatial variance is due to rotation. To demonstrate the importance of accounting for spatial variance, on the bottom row of Figure 6 , we show a result where we have deconvolved using the PSF for the correct part of the image and the PSF for a different, non-corresponding area. These results are quite interesting as they show that some of the common assumptions made in image deconvolution do not always hold. Most deconvolution work assumes spatially invariant kernels, which really only applies for camera motion under an orthographic model; however, with a typical imaging setup (we use a 40mm lens), the perspective effects are strong enough to induce a spatially-varying blur. We also note that there is often a roll component to the blur, something that is also not modeled by spatially invariant kernels. Lastly, we observe that translation, and thus depth dependent effects can be significant, which is interesting as it is often thought that most camera-shake blur is due to rotation. Please visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/ for examples. In Figure 7 , using the same scene as above, we show comparisons of deconvolution results using our method, ground-truth, and two others. The images shown of the lab calibration scene were taken at exposures from 1/3 to 1/10 of a second. For each image we show the input, the result of deconvolving with PSFs from the initial motion estimate and after performing drift correction, and compare these to a deconvolution using PSFs from the recovered groundtruth motions, and PSFs recovered using the methods of Shan et al. [2008] and Fergus et al. [2006]. For these latter two comparisons, we made a best effort to adjust the parameters to recover the best blur kernel possible. To make the comparison fair, all results were deblurred using exactly the same deconvolution method, that of Levin et al. [2007]. The results in Figure 7 , show a wide variety of blurs, yet our method recovers an accurate kernel and provides deconvolution results that are very close to that of the ground-truth. In all cases, our results are better than those using Shan et al.?s and Fergus et al.?s methods. After calibrating our hardware system using the method discussed in Section 4.2, we took the camera outside of the lab, using a laptop with a Bluetooth adapter to capture the inertial sensor data. In Figure 8 , we show several results where we have deblurred images using our method, Shan et al.?s method, and Fergus et al.?s method. The Shan et al. and Fergus et al. results were deconvolved using Levin et al.?s method [2007], and our results are deconvolved using our spatially-varying deconvolution method discussed in Section 3.1. For all the images, our results show a clear improvement over the input blurry image. There is still some residually ringing that is unavoidable due to frequency loss during blurring. The Shan et al. results are of varying quality, and many show large ringing artifacts that are due to kernel misestimation and over-sharpening; the Fergus et al. results are generally blurrier than ours. All the images shown here were shot with 1/2 to 1/10 second exposures with a 40mm lens on a Canon 1Ds Mark III. For additional results, visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/. In this work, we presented an aided blind deconvolution algorithm that uses a hardware attachment in conjunction with a corresponding blurry input image and a natural image prior to compute perpixel, spatially-varying blur and that deconvolves an image to produce a sharp result. There are several benefits to our method over previous approaches: (1) our method is automatic and has no user-tuned parameters and (2) it uses inexpensive commodity hardware that could easily be built into a camera or produced as a mass-market attachment that is more compact than our prototype. We have shown advantages over purely image-based methods, which in some sense is not surprising?blind deconvolution is an inherently ill-posed problem,  thus the extra information of inertial measurements should be helpful. There are many challenges to using this data properly, many of which we have addressed; however, our results also suggest several areas for future work. The biggest limitation of our method is sensor accuracy and noise. Our method?s performance will degrade under a few cases: (1) if the drift is large enough that the search space for our optimization process is too large, i.e., greater than a couple mm, (2) if our estimation of the initial camera rotation relative to gravity is incorrect or similarly, if the camera moves in a way that is not normally distributed about the gravity vector, (3) if there is significant depth variation in the scene and the camera undergoes significant translation, (4) if the camera is translating at some initial, constant velocity, and (5) if there is large image frequency information loss to blurring. To handle (1), we are interested in using more sensors. The sensors are inexpensive and one could easily add sensors for redundancy and perform denoising by averaging either in the analog or digital domain. For (2) one could consider adding other sensors, such as a magnetometer, to get another measure of orientation ? this is already a common approach in IMU based navigation systems. For (3) one could recover the depth in the scene by performing a ?depth from motion blur? algorithm, similar to depth from defocus. We are pursuing this problem; however, it is important to note that varying scene depth does not always significantly affect the blur. For typical situations, we have found that depth is only needed for accurate deblurring of objects within a meter from the camera. Most often people take images where the scene is farther than this distance. For (4) we assume the initial translation velocity is zero and our accelerometer gives us no measure of this. While we currently only consider the accelerometer data during an exposure, we actually record all the sensors? data before and after each exposure as well. Thus one way to address this issue is to try to identify a stationary period before the camera exposure and track from there to get a more accurate initial velocity estimate. (5) is an issue with all deblurring methods. Frequency loss will cause unavoidable artifacts during deconvolution that could appear as ringing, banding, or over-smoothing depending on the deconvolution method. It would be interesting to combine our hardware with the Raskar et al. [2006] flutter shutter hardware to reduce frequency loss during image capture.",
  "resources" : [ ]
}