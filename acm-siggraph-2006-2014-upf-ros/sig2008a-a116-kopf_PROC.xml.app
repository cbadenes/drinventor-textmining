{
  "uri" : "sig2008a-a116-kopf_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2008a/a116-kopf_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Deep Photo: Model-Based Photograph Enhancement and Viewing",
    "published" : "2008",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Johannes-Kopf",
      "name" : "Johannes",
      "surname" : "Kopf"
    }, {
      "uri" : "http://drinventor/Boris-Neubert",
      "name" : "Boris",
      "surname" : "Neubert"
    }, {
      "uri" : "http://drinventor/Billy-Chen",
      "name" : "Billy",
      "surname" : "Chen"
    }, {
      "uri" : "http://drinventor/Michael F.-Cohen",
      "name" : "Michael F.",
      "surname" : "Cohen"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Oliver-Deussen",
      "name" : "Oliver",
      "surname" : "Deussen"
    }, {
      "uri" : "http://drinventor/Matthew-Uyttendaele",
      "name" : "Matthew",
      "surname" : "Uyttendaele"
    }, {
      "uri" : "http://drinventor/Dani-Lischinski",
      "name" : "Dani",
      "surname" : "Lischinski"
    } ]
  },
  "bagOfWords" : [ "paper", "we", "introduce", "novel", "system", "browse", "enhance", "manipulate", "casual", "outdoor", "photograph", "combine", "they", "already", "exist", "georeferenced", "digital", "terrain", "urban", "model", "simple", "interactive", "registration", "process", "use", "align", "photograph", "model", "information", "turn", "enable", "variety", "operation", "range", "from", "dehaze", "relight", "photograph", "novel", "view", "synthesis", "overlay", "geographic", "information", "we", "describe", "implementation", "number", "application", "discuss", "possible", "extension", "keyword", "image-based", "modeling", "image-based", "rendering", "image", "completion", "dehazing", "relighting", "photo", "browse", "ACM", "Reference", "Format", "Kopf", "J.", "Neubert", "B.", "Chen", "B.", "Cohen", "M.", "Cohen-Or", "D.", "Deussen", "O.", "Uyttendaele", "M.", "Lischinski", "D.", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "ACM", "Trans", "27", "Article", "116", "-lrb-", "December", "2008", "-rrb-", "10", "page", "dous", "10.1145", "1409060.1409069", "http://doi.acm.org/10.1145/1409060.1409069", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "despite", "increase", "ubiquity", "digital", "photography", "metaphor", "we", "use", "browse", "interact", "we", "photograph", "have", "change", "much", "few", "exception", "we", "still", "treat", "they", "2d", "entity", "whether", "display", "computer", "monitor", "print", "hard", "copy", "well", "understand", "augment", "photograph", "depth", "can", "open", "way", "variety", "new", "exciting", "manipulation", "however", "infer", "depth", "information", "from", "single", "image", "capture", "ordinary", "camera", "still", "longstanding", "unsolved", "problem", "computer", "vision", "luckily", "we", "witness", "great", "increase", "number", "accuracy", "geometric", "model", "world", "include", "terrain", "building", "register", "photograph", "model", "depth", "become", "available", "each", "pixel", "Deep", "Photo", "system", "describe", "paper", "consist", "number", "application", "afford", "newfound", "depth", "value", "well", "many", "other", "type", "information", "typically", "associate", "model", "deep", "Photo", "motivate", "several", "recent", "trend", "now", "reach", "critical", "mass", "many", "photo", "share", "web", "site", "now", "enable", "user", "manually", "add", "location", "information", "photo", "some", "digital", "camera", "RICOH", "Caplio", "500se", "Nokia", "N95", "feature", "built-in", "GPS", "allow", "automatic", "location", "tagging", "expect", "future", "more", "camera", "have", "functionality", "most", "photograph", "geo-tagged", "thanks", "commercial", "project", "Google", "Earth", "Microsoft?s", "Virtual", "Earth", "both", "quantity", "quality", "model", "rapidly", "increase", "combination", "geo-tagging", "availability", "fairly", "accurate", "3d", "model", "allow", "many", "photograph", "precisely", "georegister", "we", "envision", "near", "future", "automatic", "georegistration", "available", "online", "service", "thus", "although", "we", "briefly", "describe", "simple", "interactive", "geo-registration", "technique", "we", "currently", "employ", "emphasis", "paper", "application", "enable", "include", "dehazing", "-lrb-", "add", "haze", "-rrb-", "image", "approximate", "change", "lighting", "novel", "view", "synthesis", "expand", "field", "view", "add", "new", "object", "image", "integration", "GIS", "datum", "photo", "browser", "we", "goal", "work", "have", "be", "enable", "application", "single", "outdoor", "image", "take", "casual", "manner", "without", "require", "any", "special", "equipment", "any", "particular", "setup", "thus", "we", "system", "applicable", "large", "body", "exist", "outdoor", "photograph", "so", "long", "we", "know", "rough", "location", "where", "each", "photograph", "take", "we", "choose", "New", "York", "City", "Yosemite", "National", "Park", "two", "many", "location", "around", "world", "which", "detailed", "textured", "model", "already", "available", "should", "note", "while", "model", "we", "use", "fairly", "detail", "still", "far", "cry", "from", "degree", "accuracy", "level", "detail", "one", "would", "need", "order", "use", "model", "directly", "render", "photographic", "image", "thus", "one", "we", "challenge", "work", "have", "be", "understand", "how", "best", "leverage", "3d", "information", "afford", "use", "model", "while", "same", "time", "preserve", "photographic", "quality", "original", "image", "addition", "explore", "application", "list", "above", "paper", "also", "make", "number", "specific", "technical", "contribution", "two", "main", "one", "new", "data-driven", "stable", "dehazing", "procedure", "new", "model-guided", "layered", "depth", "image", "completion", "technique", "novel", "view", "synthesis", "before", "continue", "we", "should", "note", "some", "limitation", "Deep", "Photo", "its", "current", "form", "we", "current", "implementation", "foreground", "object", "mat", "out", "before", "combine", "rest", "photograph", "model", "may", "composit", "back", "onto", "photograph", "later", "stage", "so", "some", "image", "user", "must", "spend", "some", "time", "interactive", "matting", "fidelity", "some", "we", "manipulation", "foreground", "may", "reduce", "below", "we", "attempt", "provide", "some", "representative", "reference", "discuss", "detail", "only", "one", "most", "closely", "related", "we", "goal", "technique", "other", "system", "use", "panoramic", "mosaic", "-lsb-", "Shum", "et", "al.", "1998", "-rsb-", "combine", "image", "range", "datum", "-lsb-", "stamo", "Allen", "2000", "-rsb-", "merge", "ground", "aerial", "view", "-lsb-", "fr?h", "Zakhor", "2003", "-rsb-", "name", "few", "any", "approach", "may", "use", "create", "kind", "textured", "3d", "model", "we", "use", "we", "system", "however", "work", "we", "concern", "creation", "model", "rather", "way", "which", "combination", "single", "photograph", "may", "useful", "casual", "digital", "photographer", "one", "might", "say", "rather", "than", "attempt", "automatically", "manually", "reconstruct", "model", "from", "single", "photo", "we", "exploit", "availability", "digital", "terrain", "urban", "model", "effectively", "replace", "difficult", "3d", "reconstruction/modeling", "process", "much", "simpler", "registration", "process", "philosophy", "behind", "we", "work", "somewhat", "similar", "we", "attempt", "leverage", "large", "amount", "textured", "geometric", "model", "have", "already", "be", "create", "weather", "other", "atmospheric", "phenomenon", "haze", "greatly", "reduce", "visibility", "distant", "region", "image", "outdoor", "scene", "since", "we", "interested", "dehaze", "single", "image", "take", "without", "any", "special", "equipment", "method", "suitable", "we", "need", "several", "work", "attempt", "remove", "effect", "haze", "fog", "etc.", "from", "single", "image", "use", "some", "form", "depth", "information", "however", "method", "involve", "estimate", "large", "number", "parameter", "quality", "report", "result", "unlikely", "satisfy", "today?s", "digital", "photography", "enthusiast", "novel", "view", "synthesis", "more", "recently", "Hoiem", "et", "al.", "-lsb-", "2005", "-rsb-", "use", "machine", "learn", "technique", "order", "construct", "simple", "pop-up", "3d", "model", "completely", "automatically", "from", "single", "photograph", "work", "we", "use", "already", "available", "3d", "model", "order", "add", "depth", "photograph", "we", "present", "new", "model-guided", "image", "completion", "technique", "enable", "we", "expand", "field", "view", "perform", "high-quality", "novel", "view", "synthesis", "thus", "we", "can", "hope", "correctly", "recover", "reflectance", "each", "pixel", "photograph", "which", "necessary", "order", "perform", "physically", "accurate", "relighting", "therefore", "work", "we", "propose", "very", "simple", "relighting", "approximation", "which", "nevertheless", "able", "produce", "fairly", "compelling", "result", "example", "Cho", "-lsb-", "Cho", "2007", "-rsb-", "note", "absolute", "geo-location", "can", "assign", "individual", "pixel", "gi", "annotation", "building", "street", "name", "may", "project", "onto", "image", "plane", "application", "demonstrate", "paper", "matte", "do", "have", "too", "accurate", "so", "long", "conservative", "-lrb-", "i.e.", "all", "foreground", "pixel", "contain", "-rrb-", "many", "typical", "image", "we", "take", "spectacular", "often", "well", "know", "landscape", "cityscape", "unfortunately", "many", "case", "lighting", "condition", "weather", "optimal", "when", "photograph", "take", "result", "may", "dull", "hazy", "depth", "each", "image", "pixel", "know", "theory", "should", "easy", "remove", "effect", "haze", "fitting", "analytical", "model", "-lrb-", "e.g.", "-lsb-", "McCartney", "1976", "Nayar", "Narasimhan", "1999", "-rsb-", "-rrb-", "thus", "more", "suitable", "short", "range", "distance", "might", "fail", "correctly", "approximate", "attenuation", "scene", "point", "more", "than", "few", "kilometer", "away", "while", "reduce", "degree", "dehaze", "-lsb-", "Schechner", "et", "al.", "2003", "-rsb-", "regularization", "-lsb-", "schechner", "Averbuch", "2007", "Kaftory", "et", "al.", "2007", "-rsb-", "may", "use", "alleviate", "problem", "we", "approach", "estimate", "stable", "value", "haze", "curve", "-lrb-", "-rrb-", "directly", "from", "relationship", "between", "color", "photograph", "those", "model", "texture", "more", "specifically", "we", "compute", "curve", "-lrb-", "-rrb-", "airlight", "eq", "before", "explain", "detail", "we", "method", "we", "would", "like", "point", "out", "model", "texture", "typically", "have", "global", "color", "bias", "now", "we", "ready", "explain", "how", "compute", "haze", "curve", "-lrb-", "-rrb-", "we", "goal", "find", "-lrb-", "-rrb-", "would", "map", "hazy", "photo", "color", "-lrb-", "-rrb-", "color-corrected", "texture", "color", "-lrb-", "-rrb-", "have", "same", "value", "every", "color", "channel", "one", "can", "underestimate", "importance", "role", "lighting", "play", "creation", "interesting", "photograph", "possible", "modify", "lighting", "photograph", "approximate", "what", "scene", "might", "look", "like", "another", "time", "day", "explain", "earlier", "we", "goal", "work", "single", "image", "augment", "detailed", "yet", "completely", "accurate", "geometric", "model", "scene", "other", "word", "define", "greater", "among", "cosine", "angle", "between", "normal", "original", "view", "direction", "cosine", "angle", "between", "two", "view", "direction", "move", "cursor", "either", "two", "view", "highlight", "corresponding", "location", "other", "view" ],
  "content" : "In this paper, we introduce a novel system for browsing, enhancing, and manipulating casual outdoor photographs by combining them with already existing georeferenced digital terrain and urban models. A simple interactive registration process is used to align a photograph with such a model. This information, in turn, enables a variety of operations, ranging from dehazing and relighting the photograph, to novel view synthesis, and overlaying with geographic information. We describe the implementation of a number of these applications and discuss possible extensions. Keywords: image-based modeling, image-based rendering, image completion, dehazing, relighting, photo browsing\n      ACM Reference Format Kopf, J., Neubert, B., Chen, B., Cohen, M., Cohen-Or, D., Deussen, O., Uyttendaele, M., Lischinski, D. 2008. Deep Photo: Model-Based Photograph Enhancement and Viewing. ACM Trans. 27, 5, Article 116 (December 2008), 10 pages. DOI = 10.1145/1409060.1409069 http://doi.acm.org/10.1145/1409060.1409069. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept. , ACM, Inc. , 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . Despite the increasing ubiquity of digital photography, the metaphors we use to browse and interact with our photographs have not changed much. With few exceptions, we still treat them as 2D entities, whether they are displayed on a computer monitor or printed as a hard copy. It is well understood that augmenting a photograph with depth can open the way for a variety of new exciting manipulations. However, inferring the depth information from a single image that was captured with an ordinary camera is still a longstanding unsolved problem in computer vision. Luckily, we are witnessing a great increase in the number and the accuracy of geometric models of the world, including terrain and buildings. By registering photographs to these models, depth becomes available at each pixel. The Deep Photo system described in this paper, consists of a number of applications afforded by these newfound depth values, as well as the many other types of information that are typically associated with such models. Deep Photo is motivated by several recent trends now reaching critical mass. Many photo sharing web sites now enable users to manually add location information to photos. Some digital cameras, such as the RICOH Caplio 500SE and the Nokia N95, feature a built-in GPS, allowing automatic location tagging. It is expected that, in the future, more cameras will have such functionality, and that most photographs will be geo-tagged. Thanks to commercial projects, such as Google Earth and Microsoft?s Virtual Earth, both the quantity and the quality of such models is rapidly increasing. The combination of geo-tagging and the availability of fairly accurate 3D models allows many photographs to be precisely georegistered. We envision that in the near future automatic georegistration will be available as an online service. Thus, although we briefly describe the simple interactive geo-registration technique that we currently employ, the emphasis of this paper is on the applications that it enables, including: ? dehazing (or adding haze to) images, ? approximating changes in lighting, ? novel view synthesis, ? expanding the field of view, ? adding new objects into the image, ? integration of GIS data into the photo browser. Our goal in this work has been to enable these applications for single outdoor images, taken in a casual manner without requiring any special equipment or any particular setup. Thus, our system is applicable to a large body of existing outdoor photographs, so long as we know the rough location where each photograph was taken. We chose New York City and Yosemite National Park as two of the many locations around the world, for which detailed textured models are already available 1 . It should be noted that while the models that we use are fairly detailed, they are still a far cry from the degree of accuracy and the level of detail one would need in order to use these models directly to render photographic images. Thus, one of our challenges in this work has been to understand how to best leverage the 3D information afforded by the use of these models, while at the same time preserving the photographic qualities of the original image. In addition to exploring the applications listed above, this paper also makes a number of specific technical contributions. The two main ones are a new data-driven stable dehazing procedure, and a new model-guided layered depth image completion technique for novel view synthesis. Before continuing, we should note some of the limitations of Deep Photo in its current form. In our current implementation such foreground objects are matted out before combining the rest of the photograph with a model, and may be composited back onto the photograph at a later stage. So, for some images, the user must spend some time on interactive matting, and the fidelity of some of our manipulations in the foreground may be reduced. Below, we attempt to provide some representative references, and discuss in detail only the ones most closely related to our goals and techniques. Other systems use panoramic mosaics [Shum et al. 1998], combine images with range data [Stamos and Allen 2000], or merge ground and aerial views [Fr?h and Zakhor 2003], to name a few. Any of these approaches may be used to create the kinds of textured 3D models that we use in our system; however, in this work we are not concerned with the creation of such models, but rather with the ways in which their combination with a single photograph may be useful for the casual digital photographer. One might say that rather than attempting to automatically or manually reconstruct the model from a single photo, we exploit the availability of digital terrain and urban models, effectively replacing the difficult 3D reconstruction/modeling process by a much simpler registration process. The philosophy behind our work is somewhat similar: we attempt to leverage the large amount of textured geometric models that have already been created. Weather and other atmospheric phenomena, such as haze, greatly reduce the visibility of distant regions in images of outdoor scenes. Since we are interested in dehazing single images, taken without any special equipment, such methods are not suitable for our needs. There are several works that attempt to remove the effects of haze, fog, etc., from a single image using some form of depth information. However, their method involves estimating a large number of parameters, and the quality of the reported results is unlikely to satisfy today?s digital photography enthusiasts. Novel view synthesis. More recently Hoiem et al. [2005] use machine learning techniques in order to construct a simple ?pop-up? 3D model, completely automatically from a single photograph. In this work, we use already available 3D models in order to add depth to photographs. We present a new model-guided image completion technique that enables us to expand the field of view and to perform high-quality novel view synthesis. Thus, we cannot hope to correctly recover the reflectance at each pixel of the photograph, which is necessary in order to perform physically accurate relighting. Therefore, in this work we propose a very simple relighting approximation, which is nevertheless able to produce fairly compelling results. For example, Cho [Cho 2007] notes that absolute geo-locations can be assigned to individual pixels and that GIS annotations, such as building and street names, may be projected onto the image plane. For the applications demonstrated in this paper the matte does not have to be too accurate, so long as it is conservative (i.e., all the foreground pixels are contained). Many of the typical images we take are of a spectacular, often well known, landscape or cityscape. Unfortunately in many cases the lighting conditions or the weather are not optimal when the photographs are taken, and the results may be dull or hazy. If the depth at each image pixel is known, in theory it should be easy to remove the effects of haze by fitting an analytical model (e.g., [McCartney 1976; Nayar and Narasimhan 1999]): Thus, it is more suitable for short ranges of distance and might fail to correctly approximate the attenuation of scene points that are more than a few kilometers away. While reducing the degree of dehazing [Schechner et al. 2003] and regularization [Schechner and Averbuch 2007; Kaftory et al. 2007] may be used to alleviate these problems, our approach is to estimate stable values for the haze curve f (z) directly from the relationship between the colors in the photograph and those of the model textures. More specifically, we compute a curve f (z) and an airlight A, such that eq. Before explaining the details of our method, we would like to point out that the model textures typically have a global color bias. Now we are ready to explain how to compute the haze curve f (z). Our goal is to find A and f (z) that would map the hazy photo colors I ? h (z) to the color-corrected texture colors C I ? m (z). has the same value for every color channel. One cannot underestimate the importance of the role that lighting plays in the creation of an interesting photograph. it is possible to modify the lighting of a photograph, approximating what the scene might look like at another time of day. As explained earlier, our goal is to work on single images, augmented with a detailed, yet not completely accurate geometric model of the scene. In other words, g is defined as the greater among the cosine of the angle between the normal and the original view direction, and the cosine of the angle between the two view directions. Moving the cursor in either of the two views highlights the corresponding location in the other view.",
  "resources" : [ ]
}