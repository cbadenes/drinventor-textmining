{
  "uri" : "sig2011-a31-shiratori_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a31-shiratori_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Motion Capture from Body-Mounted Cameras",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Takaaki-Shiratori",
      "name" : "Takaaki",
      "surname" : "Shiratori"
    }, {
      "uri" : "http://drinventor/Hyun Soo-Park",
      "name" : "Hyun Soo",
      "surname" : "Park"
    }, {
      "uri" : "http://drinventor/Leonid-Sigal",
      "name" : "Leonid",
      "surname" : "Sigal"
    }, {
      "uri" : "http://drinventor/Yaser-Sheikh",
      "name" : "Yaser",
      "surname" : "Sheikh"
    }, {
      "uri" : "http://drinventor/Jessica K.-Hodgins",
      "name" : "Jessica K.",
      "surname" : "Hodgins"
    } ]
  },
  "bagOfWords" : [ "section", "we", "evaluate", "we", "system", "quantitatively", "use", "conventional", "motion", "capture", "system", "ground", "truth", "show", "additional", "result", "collect", "out", "door", "first", "we", "evaluate", "effect", "global", "optimization", "step", "Figure", "-lrb-", "-rrb-", "show", "comparison", "between", "camera", "center", "estimate", "Vicon", "marker", "we", "reconstruction", "before", "global", "optimization", "after", "camera", "center", "be", "adjust", "base", "estimate", "skeleton", "Figure", "-lrb-", "-rrb-", "show", "reduction", "error", "after", "global", "optimization", "smoothness", "term", "Figure", "-lrb-", "-rrb-", "compare", "joint", "angle", "trajectory", "obtain", "we", "system", "measurement", "from", "Vicon", "motion", "capture", "system", "top", "row", "show", "joint", "angle", "trajectory", "upper", "body", "bottom", "row", "show", "joint", "angle", "trajectory", "lower", "body", "joint", "angle", "illustrate", "figure", "angle", "axis-angle", "representation", "normalize", "angle", "first", "frame", "capture", "session", "mean", "median", "error", "3.0093", "1.8076", "respectively", "minimum", "maximum", "error", "0.038", "9.52", "respectively", "standard", "deviation", "2.1891", "because", "error", "parent", "joint", "angle", "propagate", "child", "joint", "joint", "angle", "error", "may", "sufficient", "characterize", "error", "overall", "system", "therefore", "we", "also", "evaluate", "error", "joint", "position", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "error", "do", "propagate", "significantly", "because", "optimization", "equation", "-lrb-", "-rrb-", "find", "solution", "all", "camera", "satisfy", "image", "measurement", "mean", "median", "position", "error", "1.76", "cm", "1.42", "cm", "respectively", "minimum", "maximum", "error", "0.053", "cm", "12.24", "cm", "respectively", "standard", "deviation", "1.26", "cm", "method", "comparison", "we", "now", "describe", "how", "we", "obtain", "quantitative", "comparison", "we", "system", "produce", "camera", "pose", "SfM", "space", "while", "motion", "capture", "system", "output", "3d", "marker", "position", "motion", "capture", "space", "compare", "two", "different", "reconstruction", "we", "need", "compute", "follow", "transform", "between", "two", "space", "we", "attach", "three", "marker", "each", "camera", "several", "marker", "static", "object", "collect", "image", "from", "camera", "corresponding", "marker", "position", "from", "motion", "capture", "system", "subject", "move", "use", "3d", "position", "static", "marker", "motion", "capture", "space", "corresponding", "image", "measurement", "specify", "manually", "camera", "center", "position", "orientation", "motion", "capture", "space", "be", "estimate", "thus", "we", "could", "convert", "three", "marker", "position", "motion", "capture", "datum", "camera", "pose", "recover", "similarity", "transform", "from", "SfM", "space", "motion", "capture", "space", "we", "estimate", "scale", "from", "distance", "between", "camera", "center", "pair", "both", "space", "we", "estimate", "translation", "orientation", "from", "SfM", "space", "motion", "capture", "space", "apply", "iterative", "closest", "point", "algorithm", "two", "set", "camera", "center", "parameter", "be", "use", "similarity", "transform", "after", "non-linear", "refinement", "major", "benefit", "we", "system", "portable", "selfcontained", "allow", "prolonged", "capture", "outdoor", "environment", "illustrate", "benefit", "we", "capture", "two", "sequence", "local", "playground", "result", "illustrate", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "also", "test", "ability", "capture", "fast", "motion", "run", "motion", "street", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "top", "row", "show", "photo", "subject", "perform", "motion", "bottom", "row", "illustrate", "pose", "skin", "character", "use", "joint", "angle", "estimate", "we", "system", "some", "motion", "be", "quite", "dynamic", "we", "observe", "faster", "than", "2.4", "m/s", "instantaneous", "velocity", "camera", "swing", "run", "sequence", "though", "motion", "result", "image", "blur", "roll", "shutter", "effect", "we", "be", "able", "properly", "reconstruct", "sequence", "Figure", "show", "reconstructed", "long", "walking", "motion", "along", "wind", "path", "uneven", "terrain", "subject", "traverse", "considerable", "distance", "far", "greater", "than", "what", "would", "possible", "traditional", "indoor", "motion", "capture", "setup", "we", "superimpose", "sparse", "3d", "structure", "manually", "match", "view", "angle", "photo", "take", "during", "capture", "reference", "sparse", "structure", "provide", "context", "motion", "show", "path", "along", "which", "subject", "have", "walk", "first", "we", "evaluate", "effect", "global", "optimization", "step", "Figure", "-lrb-", "-rrb-", "show", "comparison", "between", "camera", "center", "estimate", "Vicon", "marker", "we", "reconstruction", "before", "global", "optimization", "after", "camera", "center", "be", "adjust", "base", "estimate", "skeleton", "Figure", "-lrb-", "-rrb-", "show", "reduction", "error", "after", "global", "optimization", "smoothness", "term", "Figure", "-lrb-", "-rrb-", "compare", "joint", "angle", "trajectory", "obtain", "we", "system", "measurement", "from", "Vicon", "motion", "capture", "system", "top", "row", "show", "joint", "angle", "trajectory", "upper", "body", "bottom", "row", "show", "joint", "angle", "trajectory", "lower", "body", "joint", "angle", "illustrate", "figure", "angle", "axis-angle", "representation", "normalize", "angle", "first", "frame", "capture", "session", "mean", "median", "error", "3.0093", "1.8076", "respectively", "minimum", "maximum", "error", "0.038", "9.52", "respectively", "standard", "deviation", "2.1891", "because", "error", "parent", "joint", "angle", "propagate", "child", "joint", "joint", "angle", "error", "may", "sufficient", "characterize", "error", "overall", "system", "therefore", "we", "also", "evaluate", "error", "joint", "position", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "error", "do", "propagate", "significantly", "because", "optimization", "equation", "-lrb-", "-rrb-", "find", "solution", "all", "camera", "satisfy", "image", "measurement", "mean", "median", "position", "error", "1.76", "cm", "1.42", "cm", "respectively", "minimum", "maximum", "error", "0.053", "cm", "12.24", "cm", "respectively", "standard", "deviation", "1.26", "cm", "method", "comparison", "we", "now", "describe", "how", "we", "obtain", "quantitative", "comparison", "we", "system", "produce", "camera", "pose", "SfM", "space", "while", "motion", "capture", "system", "output", "3d", "marker", "position", "motion", "capture", "space", "compare", "two", "different", "reconstruction", "we", "need", "compute", "follow", "transform", "between", "two", "space", "we", "attach", "three", "marker", "each", "camera", "several", "marker", "static", "object", "collect", "image", "from", "camera", "corresponding", "marker", "position", "from", "motion", "capture", "system", "subject", "move", "use", "3d", "position", "static", "marker", "motion", "capture", "space", "corresponding", "image", "measurement", "specify", "manually", "camera", "center", "position", "orientation", "motion", "capture", "space", "be", "estimate", "thus", "we", "could", "convert", "three", "marker", "position", "motion", "capture", "datum", "camera", "pose", "recover", "similarity", "transform", "from", "SfM", "space", "motion", "capture", "space", "we", "estimate", "scale", "from", "distance", "between", "camera", "center", "pair", "both", "space", "we", "estimate", "translation", "orientation", "from", "SfM", "space", "motion", "capture", "space", "apply", "iterative", "closest", "point", "algorithm", "two", "set", "camera", "center", "parameter", "be", "use", "similarity", "transform", "after", "non-linear", "refinement", "major", "benefit", "we", "system", "portable", "selfcontained", "allow", "prolonged", "capture", "outdoor", "environment", "illustrate", "benefit", "we", "capture", "two", "sequence", "local", "playground", "result", "illustrate", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "also", "test", "ability", "capture", "fast", "motion", "run", "motion", "street", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "top", "row", "show", "photo", "subject", "perform", "motion", "bottom", "row", "illustrate", "pose", "skin", "character", "use", "joint", "angle", "estimate", "we", "system", "some", "motion", "be", "quite", "dynamic", "we", "observe", "faster", "than", "2.4", "m/s", "instantaneous", "velocity", "camera", "swing", "run", "sequence", "though", "motion", "result", "image", "blur", "roll", "shutter", "effect", "we", "be", "able", "properly", "reconstruct", "sequence", "Figure", "show", "reconstructed", "long", "walking", "motion", "along", "wind", "path", "uneven", "terrain", "subject", "traverse", "considerable", "distance", "far", "greater", "than", "what", "would", "possible", "traditional", "indoor", "motion", "capture", "setup", "we", "superimpose", "sparse", "3d", "structure", "manually", "match", "view", "angle", "photo", "take", "during", "capture", "reference", "sparse", "structure", "provide", "context", "motion", "show", "path", "along", "which", "subject", "have", "walk" ],
  "content" : "In this section, we evaluate our system quantitatively using a conventional motion capture system as ground truth, and show additional results collected out of doors. First, we evaluate the effect of the global optimization step. Figure 6(a) shows the comparison between the camera centers estimated by the Vicon markers and our reconstruction before the global optimization but after the camera centers were adjusted based on the estimated skeleton. Figure 6(b) shows the reduction in error after global optimization with the smoothness terms. Figure 7(a) compares the joint angle trajectories obtained by our system with the measurements from the Vicon motion capture system. The top row shows the joint angle trajectories of the upper body and the bottom row shows the joint angle trajectories of the lower body. The joint angles illustrated in the figure are the angle of the axis-angle representation normalized by the angle of the first frame in the capture session. The mean and median errors are 3.0093 ? and 1.8076 ? , respectively, and the minimum and the maximum errors are 0.038 ? and 9.52 ? , respectively. The standard deviation is 2.1891 ? . Because the error of a parent joint angle propagates to a child joint, the joint angle errors may not be sufficient to characterize the error of the overall system. Therefore, we also evaluate the errors of the joint positions ( Figure 7(b) ). The error does not propagate significantly, because the optimization of Equation (1) finds a solution such that all cameras satisfy the image measurements. The mean and median position errors are 1.76 cm and 1.42 cm, respectively, and the minimum and the maximum errors are 0.053 cm and 12.24 cm, respectively. The standard deviation is 1.26 cm. Method of Comparison: We now describe how we obtained these quantitative comparisons. Our system produces camera poses in the SfM space, while the motion capture system outputs 3D marker positions in the motion capture space. To compare the two different reconstructions, we needed to compute the following transforms between the two spaces. We attached three markers on each of the cameras and several markers on static objects and collected images from the cameras and the corresponding marker positions from the motion capture system as the subject moved. Using the 3D positions of the static markers in the motion capture space and the corresponding image measurements specified manually, the camera center positions and orientations in the motion capture space were estimated. Thus we could convert the three marker positions in the motion capture data to the camera poses. To recover the similarity transform from the SfM space to the motion capture space, we estimated a scale from the distances between the camera center pairs in both of the spaces. Then, we estimated translation and orientation from the SfM space to the motion capture space by applying the iterative closest point algorithm to the two sets of the camera centers. The parameters were used for the similarity transform after non-linear refinement. The major benefit of our system is that it is portable and selfcontained, allowing prolonged captures in outdoor environments. To illustrate these benefits we captured two sequences in the local playground; the results are illustrated in Figures 8(a) and 8(b). We also tested the ability to capture fast motions with a running motion on a street ( Figure 8(c) ). The top rows show the photos of the subject performing the motions, and the bottom rows illustrate a posed skinned character using the joint angles estimated by our system. Some of these motions were quite dynamic and we observed faster than 2.4 m/s instantaneous velocity of a camera in the swing and running sequences. Though these motions resulted in image blur, and the rolling shutter effect, we were able to properly reconstruct the sequences. Figure 9 shows a reconstructed long walking motion along the winding path on an uneven terrain. The subject traversed a considerable distance that is far greater than what would be possible in a traditional indoor motion capture setup. We superimposed the sparse 3D structure and manually matched the viewing angle to a photo taken during the capture for reference. The sparse structure provides the context for the motion by showing the path along which the subject has walked. First, we evaluate the effect of the global optimization step. Figure 6(a) shows the comparison between the camera centers estimated by the Vicon markers and our reconstruction before the global optimization but after the camera centers were adjusted based on the estimated skeleton. Figure 6(b) shows the reduction in error after global optimization with the smoothness terms. Figure 7(a) compares the joint angle trajectories obtained by our system with the measurements from the Vicon motion capture system. The top row shows the joint angle trajectories of the upper body and the bottom row shows the joint angle trajectories of the lower body. The joint angles illustrated in the figure are the angle of the axis-angle representation normalized by the angle of the first frame in the capture session. The mean and median errors are 3.0093 ? and 1.8076 ? , respectively, and the minimum and the maximum errors are 0.038 ? and 9.52 ? , respectively. The standard deviation is 2.1891 ? . Because the error of a parent joint angle propagates to a child joint, the joint angle errors may not be sufficient to characterize the error of the overall system. Therefore, we also evaluate the errors of the joint positions ( Figure 7(b) ). The error does not propagate significantly, because the optimization of Equation (1) finds a solution such that all cameras satisfy the image measurements. The mean and median position errors are 1.76 cm and 1.42 cm, respectively, and the minimum and the maximum errors are 0.053 cm and 12.24 cm, respectively. The standard deviation is 1.26 cm. Method of Comparison: We now describe how we obtained these quantitative comparisons. Our system produces camera poses in the SfM space, while the motion capture system outputs 3D marker positions in the motion capture space. To compare the two different reconstructions, we needed to compute the following transforms between the two spaces. We attached three markers on each of the cameras and several markers on static objects and collected images from the cameras and the corresponding marker positions from the motion capture system as the subject moved. Using the 3D positions of the static markers in the motion capture space and the corresponding image measurements specified manually, the camera center positions and orientations in the motion capture space were estimated. Thus we could convert the three marker positions in the motion capture data to the camera poses. To recover the similarity transform from the SfM space to the motion capture space, we estimated a scale from the distances between the camera center pairs in both of the spaces. Then, we estimated translation and orientation from the SfM space to the motion capture space by applying the iterative closest point algorithm to the two sets of the camera centers. The parameters were used for the similarity transform after non-linear refinement. The major benefit of our system is that it is portable and selfcontained, allowing prolonged captures in outdoor environments. To illustrate these benefits we captured two sequences in the local playground; the results are illustrated in Figures 8(a) and 8(b). We also tested the ability to capture fast motions with a running motion on a street ( Figure 8(c) ). The top rows show the photos of the subject performing the motions, and the bottom rows illustrate a posed skinned character using the joint angles estimated by our system. Some of these motions were quite dynamic and we observed faster than 2.4 m/s instantaneous velocity of a camera in the swing and running sequences. Though these motions resulted in image blur, and the rolling shutter effect, we were able to properly reconstruct the sequences. Figure 9 shows a reconstructed long walking motion along the winding path on an uneven terrain. The subject traversed a considerable distance that is far greater than what would be possible in a traditional indoor motion capture setup. We superimposed the sparse 3D structure and manually matched the viewing angle to a photo taken during the capture for reference. The sparse structure provides the context for the motion by showing the path along which the subject has walked.",
  "resources" : [ ]
}