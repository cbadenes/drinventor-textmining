{
  "uri" : "sig2014a-a200-wu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014a/a200-wu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Real-time Shading-based Refinement for Consumer Depth Cameras",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shahram-Izadi",
      "name" : "Shahram",
      "surname" : "Izadi"
    } ]
  },
  "bagOfWords" : [ "we", "algorithm", "make", "few", "scene", "assumption", "handle", "arbitrary", "scene", "object", "even", "under", "motion", "enable", "type", "real-time", "depth", "map", "enhancement", "we", "contribute", "new", "highly", "parallel", "algorithm", "reformulate", "inverse", "render", "optimization", "problem", "prior", "work", "allow", "we", "estimate", "lighting", "shape", "temporally", "coherent", "way", "video", "frame-rate", "we", "optimization", "problem", "minimize", "use", "new", "regular", "grid", "gauss-newton", "solver", "implement", "fully", "GPU", "we", "demonstrate", "result", "show", "enhance", "depth", "map", "which", "comparable", "offline", "method", "computed", "order", "magnitude", "faster", "well", "baseline", "comparison", "online", "filtering-based", "method", "order", "refine", "depth", "map", "base", "shade", "real-time", "order", "magnitude", "faster", "than", "state-of-the-art", "offline", "system", "-lsb-", "Wu", "et", "al.", "2011", "-rsb-", "we", "must", "redesign", "shading-based", "energy", "function", "well", "its", "optimization", "method", "careful", "design", "choice", "enable", "refinement", "depth", "map", "real-time", "make", "ideally", "suit", "modern", "commodity", "range", "sensor", "run", "30Hz", "specifically", "we", "algorithm", "provide", "follow", "contribution", "rephrase", "inverse", "render", "optimization", "problem", "use", "offline", "method", "-lsb-", "Wu", "et", "al.", "2011", "-rsb-", "highly", "parallelized", "manner", "enable", "real-time", "lighting", "estimation", "through", "spherical", "harmonic", "direct", "solve", "refine", "depth", "rather", "than", "displacement", "3d", "mesh", "space-time", "coherent", "estimation", "shape", "lighting", "use", "temporal", "correspondence", "derive", "from", "real-time", "alignment", "depth", "map", "beyond", "technical", "contribution", "we", "show", "versatility", "we", "method", "reconstruct", "arbitrary", "scene", "even", "under", "motion", "demonstrate", "improve", "accuracy", "compare", "filter", "base", "refinement", "method", "we", "show", "integration", "real-time", "scanning", "framework", "akin", "kinectfusion", "-lsb-", "Newcombe", "et", "al.", "2011", "Izadi", "et", "al.", "2011", "Nie?ner", "et", "al.", "2013", "-rsb-", "show", "improved", "quality", "during", "realtime", "capture", "finally", "we", "demonstrate", "how", "we", "method", "enable", "improvement", "spatio-temporal", "reconstruction", "recent", "live", "non-rigid", "performance", "capture", "system", "-lsb-", "zollh?fer", "et", "al.", "2014a", "-rsb-", "Park", "et", "al.", "-lsb-", "2011", "-rsb-", "formulate", "depth", "upsampling", "color", "image", "resolution", "optimization", "problem", "enforce", "discontinuity", "similarity", "mention", "earlier", "well", "additional", "regularization", "term", "multi-frame", "superresolution", "technique", "estimate", "higher", "resolution", "depth", "image", "from", "stack", "align", "low", "resolution", "image", "capture", "under", "slight", "lateral", "displacement", "-lsb-", "Cui", "et", "al.", "2013", "-rsb-", "real-time", "computation", "have", "be", "possible", "so", "far", "we", "approach", "do", "impose", "strong", "prior", "shape", "recovery", "we", "work", "we", "demonstrate", "real-time", "shading-based", "refinement", "rgb-d", "datum", "capture", "general", "scene", "unknown", "timevarying", "lighting", "use", "only", "commodity", "hardware", "we", "further", "employ", "new", "effective", "approximation", "parameterization", "well", "fast", "geometric", "correspondence", "search", "GPU", "which", "enable", "we", "even", "enforce", "temporal", "prior", "we", "reconstruction", "input", "we", "algorithm", "noisy", "low", "resolution", "depth", "map", "from", "depth", "camera", "align", "RGB", "image", "i.", "unlike", "previous", "offline", "method", "use", "multi-camera", "input", "refine", "full", "3d", "mesh", "we", "rephrase", "shading-based", "refinement", "depth", "map", "enhancement", "process", "we", "solve", "inverse", "rendering", "problem", "use", "effective", "parameterization", "shade", "equation", "-lrb-", "sect", "thereafter", "coarse", "geometry", "refine", "use", "shade", "information", "-lrb-", "sect", "real-time", "estimation", "illumination", "refine", "geometry", "necessitate", "efficient", "formulation", "light", "transport", "model", "i.e.", "shade", "equation", "similar", "previous", "offline", "method", "we", "assume", "surface", "scene", "Lambertian", "we", "parameterize", "incident", "lighting", "spherical", "harmonic", "-lrb-", "sh", "-rrb-", "-lsb-", "Wu", "et", "al.", "2011", "-rsb-", "fact", "we", "estimate", "incident", "irradiance", "function", "surface", "normal", "incident", "light", "filter", "cosine", "normal", "previous", "approach", "we", "henceforth", "estimate", "lighting", "from", "grayscale", "version", "thus", "assume", "gray", "lighting", "equal", "value", "each", "RGB", "channel", "some", "step", "full", "rgb", "image", "use", "which", "we", "denote", "unlike", "offline", "multi-view", "method", "we", "employ", "triangulated", "depth", "map", "geometry", "parameterization", "mean", "fixed", "depth", "pixel", "mesh", "vertex", "relation", "we", "can", "express", "reflect", "irradiance", "-lrb-", "-rrb-", "depth", "pixel", "-lrb-", "-rrb-", "normal", "-lrb-", "-rrb-", "albedo", "-lrb-", "-rrb-", "where", "nine", "2nd", "order", "spherical", "harmonic", "coefficient", "incident", "illumination", "note", "we", "real-time", "setting", "we", "can", "afford", "local", "visibility", "computation", "so", "illumination", "depend", "only", "normal", "direction", "during", "light", "estimation", "-lrb-", "sect", "4.1", "-rrb-", "we", "therefore", "initially", "assume", "scene", "have", "uniform", "albedo", "subsequently", "dense", "albedo", "image", "compute", "divide", "rgb", "value", "through", "lighting", "term", "high-frequency", "detail", "depth", "map", "compute", "shading-based", "refinement", "per-pixel", "depth", "value", "-lrb-", "Sect", "unlike", "previous", "sf", "method", "solve", "surface", "normal", "orientation", "we", "directly", "optimize", "depth", "link", "depth", "normal", "only", "computationally", "much", "more", "efficient", "also", "allow", "we", "implicitly", "enforce", "surface", "integrability", "during", "depth", "optimization", "illumination", "coefficient", "compute", "minimize", "difference", "between", "render", "image", "-lrb-", "give", "we", "current", "lighting", "estimate", "geometry", "-rrb-", "capture", "RGB", "image", "where", "-lrb-", "-rrb-", "image", "size", "solve", "least-square", "problem", "equivalent", "solve", "follow", "system", "linear", "equation", "we", "exclude", "pixel", "grazing", "angle", "light", "estimation", "both", "shade", "depth", "unreliable", "region", "performance", "reason", "when", "input", "RGB", "image", "resolution", "higher", "than", "640", "480", "we", "downsample", "image", "factor", "three", "lighting", "estimation", "stage", "sh", "lighting", "coefficient", "obtain", "-lrb-", "-rrb-", "i.", "calculation", "we", "use", "parallel", "reduction", "solve", "CPU", "input", "image", "estimate", "albedo", "map", "right", "spatial", "neighborhood", "geometric", "regularizer", "estimation", "we", "optionally", "add", "temporal", "prior", "term", "-lrb-", "-rrb-", "Eq", "linear", "system", "we", "need", "solve", "follow", "where", "identity", "matrix", "example", "illumination", "environment", "map", "correspond", "show", "fig.", "example", "albedo", "image", "show", "fig.", "give", "estimate", "lighting", "albedo", "image", "we", "refine", "coarse", "depth", "through", "second", "error", "minimization", "use", "shade", "cue", "from", "intensity", "image", "normal", "field", "computation", "image", "pixel", "require", "optimize", "energy", "2n", "unknown", "refine", "depth", "base", "normal", "constraint", "mean", "solve", "another", "sparse", "linear", "system", "variable", "achieve", "real-time", "performance", "we", "choose", "more", "efficient", "strategy", "directly", "optimize", "depth", "value", "each", "pixel", "enable", "we", "use", "regular", "image", "structure", "efficient", "parallelism", "we", "optimization", "note", "depend", "camera", "physical", "depth", "resolution", "may", "lower", "than", "RGB", "resolution", "we", "always", "sample", "depth", "color", "same", "higher", "resolution", "obtain", "refined", "depth", "map", "we", "minimize", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "vector", "depth", "value", "break", "down", "follow", "four", "term", "shade", "gradient", "constraint", "we", "datum", "term", "penalize", "difference", "between", "render", "shade", "gradient", "intensity", "image", "gradient", "-lrb-", "-rrb-", "-lsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-rsb-", "-lsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-rsb-", "-lrb-", "-rrb-", "gradient-based", "metric", "more", "robust", "against", "inaccuracy", "we", "approximate", "shade", "model", "which", "do", "account", "all", "lighting", "effect", "real", "scene", "-lrb-", "-rrb-", "we", "first", "establish", "link", "between", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "unnormalized", "surface", "normal", "-lrb-", "-rrb-", "can", "compute", "from", "3d", "point", "neighbor", "depth", "pixel", "-lrb-", "fig.", "-rrb-", "after", "substitute", "eq", "we", "enforce", "laplacian", "smoothness", "constraint", "each", "pixel", "which", "compute", "fig.", "show", "neighborhood", "geometric", "regularizer", "depth", "constraint", "we", "also", "define", "depth", "constraint", "which", "enforce", "refined", "depth", "stay", "close", "initial", "depth", "before", "refinement", "temporal", "constraint", "reduce", "temporal", "aliasing", "we", "reconstruction", "static", "scene", "we", "employ", "temporal", "constraint", "stabilize", "refined", "depth", "we", "image", "formation", "model", "have", "take", "albedo", "variation", "account", "we", "method", "may", "interpret", "albedo", "change", "shade", "variation", "produce", "artificial", "detail", "around", "albedo", "boundary", "order", "reduce", "texture-copy", "artifact", "we", "modify", "we", "shade", "energy", "term", "eq", "so", "modify", "shading", "energy", "define", "-lrb-", "-rrb-", "ij", "-lsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-rsb-", "ij", "-lsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-rsb-", "-lrb-", "14", "-rrb-", "where", "ij", "ij", "-lcb-", "-rcb-", "binary", "weight", "each", "row", "column", "which", "set", "zero", "albedo", "boundary", "edge", "albedo", "change", "usually", "result", "large", "difference", "RGB", "color", "space", "-lsb-", "Horn", "1974", "-rsb-", "therefore", "we", "detect", "apply", "user-defined", "threshold", "edge", "map", "compute", "albedo", "image", "fig.", "show", "example", "how", "strategy", "can", "reduce", "texture-copy", "artifact", "vary", "threshold", "solve", "non-linear", "energy", "-lrb-", "eq", "we", "refinement", "energy", "-lrb-", "-rrb-", "-lrb-", "eq", "-rrb-", "non-linear", "give", "image", "formation", "model", "its", "dependence", "orientation", "surface", "normal", "we", "use", "row-major", "order", "pixel", "depth", "image", "target", "resolution", "obtain", "parameter", "vector", "unknown", "per-pixel", "depth", "value", "follow", "next", "section", "describe", "we", "efficient", "parallel", "patch-based", "gauss-newton", "solver", "allow", "we", "minimize", "energy", "more", "than", "500,000", "parameter", "real-time", "rate", "refine", "depth", "value", "compute", "minimize", "explicit", "linearization", "vector", "field", "-lrb-", "-rrb-", "use", "Taylor", "expansion", "yield", "-lrb-", "-rrb-", "Jacobian", "evaluate", "solution", "after", "iteration", "can", "solve", "jointly", "complete", "domain", "use", "iterative", "solution", "technique", "like", "preconditioned", "conjugate", "gradient", "-lrb-", "pcg", "-rrb-", "previous", "work", "-lsb-", "Weber", "et", "al.", "2013", "Zollh?fer", "et", "al.", "2014a", "-rsb-", "demonstrate", "feasibility", "strategy", "GPU", "optimization", "framework", "dynamics", "simulation", "non-rigid", "registration", "respectively", "one", "important", "observation", "switch", "kernel", "have", "significant", "impact", "performance", "aforementioned", "method", "optimize", "require", "kernel", "call", "initialization", "3-4", "kernel", "call", "inner", "pcg", "loop", "depend", "whether", "system", "matrix", "explicitly", "evaluate", "sequentially", "apply", "result", "even", "several", "thousand", "variable", "optimization", "problem", "can", "solve", "interactive", "rate", "however", "we", "problem", "we", "should", "able", "optimize", "more", "than", "half", "million", "value", "real", "time", "which", "possible", "approach", "we", "error", "term", "computation", "single", "pixel", "only", "depend", "image", "neighborhood", "thus", "we", "can", "subdivide", "domain", "square", "patch", "-lrb-", "cp", "fig.", "-rrb-", "perform", "optimization", "patchwise", "use", "variant", "Schwarz", "Alternating", "Procedure", "optimization", "single", "patch", "happen", "per", "thread", "block", "where", "all", "datum", "can", "keep", "shared", "GPU", "memory", "procedure", "exploit", "locality", "optimization", "constraint", "uniform", "tessellation", "optimization", "domain", "scale", "well", "higher", "number", "unknown", "reduce", "kernel", "call", "overhead", "global", "memory", "access", "exploit", "fast", "shared", "memory", "optimization", "domain", "-lrb-", "see", "Fig.", "-rrb-", "partition", "small", "rectangular", "sub-region", "-lrb-", "patch", "-rrb-", "linear", "system", "correspond", "sub-region", "-lrb-", "without", "boundary", "-rrb-", "solve", "independently", "impose", "Neumann", "constraint", "boundary", "??", "optimization", "only", "happen", "inner", "variable", "boundary", "value", "remain", "unchanged", "each", "Schwarz", "iteration", "inner", "boundary", "variable", "patch", "first", "read", "store", "share", "memory", "inner", "variable", "optimize", "keep", "boundary", "value", "fix", "finally", "inner", "variable", "write", "back", "global", "memory", "since", "boundary", "variable", "consider", "fix", "corresponding", "block", "entry", "can", "move", "right-hand", "side", "each", "local", "sub-problem", "sub-region", "-lrb-", "patch", "-rrb-", "assign", "one", "thread", "block", "solve", "parallel", "use", "one", "thread", "per", "variable", "patch", "size", "set", "base", "GPU", "l-1", "cache", "thus", "we", "hardware", "setup", "we", "use", "16", "16", "patch", "include", "boundary", "value", "result", "20", "20", "grid", "have", "keep", "share", "memory", "entire", "algorithm", "show", "pseudocode", "algorithm", "illustrate", "Fig.", "note", "Gauss-Newton", "Schwarz", "iteration", "happen", "concurrently", "after", "each", "Schwarz", "iteration", "we", "also", "apply", "delta", "update", "so", "each", "Schwarz", "iteration", "step", "implicitly", "perform", "gauss-newton", "step", "do", "incur", "any", "additional", "computation", "because", "pcg", "solver", "have", "re-evaluate", "Jacobian", "residual", "anyway", "result", "faster", "convergence", "fig.", "show", "convergence", "behavior", "depend", "number", "pcg", "step", "figure", "we", "use", "20", "outer", "GaussNewton/Schwarz", "iteration", "10", "inner", "pcg", "iteration", "10", "we", "observe", "further", "improvement", "note", "we", "use", "synchronization", "when", "write", "depth", "value", "result", "some", "patch", "might", "already", "read", "update", "boundary", "value", "which", "lead", "mixture", "multiplicative", "additive", "Schwarz", "other", "hand", "avoid", "synchronization", "improve", "performance", "we", "shift", "initial", "patch", "grid", "each", "iteration", "sub-patch", "step", "base", "Halton", "sequence", "improve", "convergence", "avoid", "patch", "structure", "from", "become", "visible", "solution", "algorithm", "share", "memory", "pcg", "Kernel", "do", "all", "patch", "parallel", "do", "Fetch", "Data", "share", "memory", "-lrb-", "-rrb-", "compute", "RHS", "Preconditioner", "-lrb-", "-rrb-", "per", "patch", "we", "solve", "result", "linear", "optimization", "problem", "use", "fast", "shared", "memory", "pcg", "solver", "all", "per-patch", "pcg", "correspond", "one", "Schwarz", "iteration", "launch", "single", "kernel", "call", "can", "see", "algorithm", "include", "shared", "memory", "initialization", "well", "run", "pcg", "iteration", "write", "back", "local", "patch", "result", "global", "memory", "pcg", "solver", "we", "use", "simple", "Jacobi", "preconditioner", "can", "readily", "parallelize", "we", "exploit", "memory", "hierarchy", "cache", "all", "per-pixel", "datum", "register", "load", "all", "datum", "have", "access", "neighbor", "thread", "share", "memory", "each", "pcg", "iteration", "per-patch", "scalar", "product", "require", "which", "we", "use", "fast", "block", "reduction", "share", "memory", "exclude", "block", "reduction", "inner", "pcg", "loop", "require", "synchronization", "point", "system", "matrix", "apply", "efficiently", "on-the-fly", "each", "pcg", "step", "optimize", "kernel", "exploit", "sparsity", "we", "run", "propose", "rgb-d", "shading-based", "refinement", "strategy", "hierarchical", "coarse-to-fine", "manner", "allow", "faster", "convergence", "we", "method", "end", "we", "build", "image", "pyramid", "successively", "restrict", "input", "rgb-d", "datum", "coarser", "level", "after", "we", "sweep", "from", "coarse-to-fine", "-lrb-", "nested", "iteration", "-rrb-", "through", "hierarchy", "alternate", "between", "we", "patch-based", "gauss-newton", "solver", "apply", "prolongation", "operator", "currently", "we", "use", "hierarchy", "three", "level", "aside", "from", "refinement", "complete", "depth", "map", "we", "perform", "we", "optimization", "only", "block", "contain", "foreground", "pixel", "-lrb-", "cp", "base", "input", "depth", "we", "mark", "all", "patch", "contain", "foreground", "pixel", "compute", "linear", "order", "use", "fast", "prefix", "sum", "execute", "refinement", "only", "foreground", "block", "we", "test", "we", "real-time", "enhancement", "software", "datum", "from", "PrimeSense", "Carmine", "1.09", "short", "Range", "-lrb-", "RGB", "re", "1280", "1024", "depth", "re", "640", "480", "framerate", "12", "fp", "-rrb-", "Kinect", "one", "-lrb-", "rgb", "re", "1920", "1080", "depth", "re", "512", "424", "frame", "rate", "30", "fp", "-rrb-", "well", "Asus", "Xtion", "pro", "-lrb-", "RGB", "re", "640", "480", "depth", "re", "640", "480", "framerate", "30", "fp", "-rrb-", "camera", "we", "approach", "run", "real-time", "rate", "excess", "30", "fp", "both", "static", "dynamic", "scene", "all", "rgb-d", "sensor", "significant", "enhancement", "detail", "compare", "raw", "depth", "datum", "achieve", "see", "Fig.", "12", "fig.", "supplemental", "video", "document", "which", "show", "screen", "capture", "visualization", "reconstruction", "before", "after", "refinement", "total", "qualitative", "test", "be", "do", "scene", "see", "Tab", "we", "always", "enable", "prior", "term", "lighting", "estimation", "set", "10", "we", "set", "empirically", "find", "weight", "depth", "refinement", "follow", "400", "10", "100", "all", "static", "scene", "100", "10", "all", "dynamic", "scene", "please", "refer", "Tab", "detail", "sequence", "timing", "individual", "step", "measure", "Intel", "Core", "i7-3770", "cpu", "3.4", "ghz", "-lrb-", "16gb", "Ram", "-rrb-", "Nvidia", "Geforce", "GTX", "780", "list", "preprocessing", "step", "include", "depth-to-color", "alignment", "filter", "depth", "color", "resampling", "image", "foreground", "segmentation", "those", "result", "gauss-newton", "optimizer", "run", "follow", "parameter", "hierarchy", "level", "10", "outer", "iteration", "pcg", "iteration", "-lrb-", "coarse-to-fine", "-rrb-", "we", "enable", "temporal", "smoothness", "prior", "term", "capture", "static", "scene", "i.e.", "vase", "sequence", "Lucy", "sequence", "Socrates", "sequence", "require", "icp", "alignment", "add", "3.5", "m", "three", "sequence", "total", "computation", "time", "yield", "effective", "frame", "rate", "between", "15", "fp", "full", "hd", "93", "fp", "SVGA", "quantitative", "evaluation", "we", "quantitatively", "evaluate", "accuracy", "we", "method", "two", "synthetic", "sequence", "400", "frame", "long", "we", "use", "ground", "truth", "detailed", "performance-captured", "face", "geometry", "-lsb-", "valgaert", "et", "al.", "2012", "-rsb-", "ground", "truth", "lighting", "from", "St.", "Peter?s", "basilica", "-lsb-", "Debevec", "1998", "-rsb-", "render", "two", "rgb-d", "sequence", "first", "one", "-lrb-", "coa", "-rrb-", "albedo", "uniform", "second", "one", "-lrb-", "da", "-rrb-", "we", "use", "dense", "albedo", "map", "obtain", "from", "one", "capture", "face", "image", "synthesize", "depth", "map", "sequence", "we", "first", "obtain", "quantize", "depth", "map", "from", "stereo", "result", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "add", "gaussian", "noise", "mimic", "noise", "from", "depth", "sensor", "we", "compare", "we", "method", "space-time", "multi-lateral", "rgbd", "filter", "method", "-lsb-", "Richardt", "et", "al.", "2012", "-rsb-", "-lrb-", "stfilt", "-rrb-", "reconstruction", "coa", "da", "use", "single-frame", "shade", "base", "refinement", "algorithm", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "-lrb-", "sbrol", "-rrb-", "error", "metric", "we", "employ", "average", "pixel-wise", "euclidean", "distance", "mm", "per", "frame", "-lrb-", "-rrb-", "well", "average", "angular", "difference", "normal", "degree", "-lrb-", "-rrb-", "distance", "normal", "error", "average", "over", "all", "frame", "summarize", "Tab", "compare", "stfilt", "we", "method", "produce", "result", "much", "lower", "distance", "normal", "error", "obtain", "metrically", "faithful", "reconstruction", "oppose", "only", "plausible", "result", "-lrb-", "see", "Fig.", "-rrb-", "comparison", "more", "involved", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "we", "result", "exhibit", "comparable", "distance", "error", "we", "real-time", "capability", "come", "price", "slightly", "higher", "error", "reconstruct", "normal", "orientation", "respective", "error", "curve", "over", "time", "both", "DA", "-lrb-", "Fig.", "-rrb-", "coa", "-lrb-", "additional", "material", "-rrb-", "further", "confirm", "above", "conclusion", "qualitative", "comparison", "we", "also", "compare", "we", "method", "stfilt", "real-world", "datum", "-lrb-", "talk", "sequence", "see", "Tab", "-rrb-", "use", "same", "hardware", "previously", "describe", "we", "approach", "only", "have", "runtime", "advantage", "-lrb-", "55.8", "fp", "against", "8.5", "fp", "-rrb-", "also", "produce", "much", "more", "detailed", "result", "-lrb-", "see", "Fig.", "10", "video", "-rrb-", "state-of-the-art", "online", "method", "-lrb-", "stfilt", "-lsb-", "Richardt", "et", "al.", "2012", "-rsb-", "-rrb-", "even", "come", "close", "offline", "shading-based", "refinement", "method", "-lrb-", "sbrol", "-lsb-", "valgaert", "et", "al.", "2012", "-rsb-", "-rrb-", "real-time", "3d", "reconstruction", "we", "use", "we", "algorithm", "together", "real-time", "voxel-hashing-based", "hand-held", "scanning", "approach", "depth", "camera", "propose", "-lsb-", "nie?ner", "et", "al.", "2013", "-rsb-", "we", "drastically", "enhance", "depth", "map", "quality", "full", "3d", "model", "more", "detail", "can", "reconstruct", "-lrb-", "voxel", "size", "0.5", "mm", "-rrb-", "see", "fig.", "Fig.", "11", "deformable", "3d", "reconstruction", "we", "also", "integrate", "we", "algorithm", "real-time", "deformable", "tracking", "approach", "-lsb-", "zollh?fer", "et", "al.", "2014a", "-rsb-", "which", "lead", "improve", "space-time", "coherent", "reconstruction", "non-rigidly", "deform", "template", "see", "Fig.", "13", "we", "approach", "enable", "leap", "forward", "real-time", "scene", "reconstruction", "depth", "camera", "still", "subject", "several", "well-known", "shape", "from", "shade", "limitation", "we", "adaptive", "refinement", "-lrb-", "sect", "4.2", "-rrb-", "efficient", "mitigate", "visual", "presence", "artifact", "however", "we", "still", "can", "completely", "prevent", "they", "instance", "see", "fig.", "Fig.", "12", "-lrb-", "top", "left", "-rrb-", "generally", "we", "believe", "underconstrained", "nature", "contrast", "some", "offline", "method", "we", "real-time", "constraint", "allow", "only", "simplify", "light", "transport", "model", "we", "initial", "constant", "albedo", "assumption", "may", "exacerbate", "texture", "copying", "general", "scene", "however", "we", "result", "show", "practice", "very", "faithful", "surface", "reconstruction", "spatially-varying", "albedo", "feasible", "due", "second", "order", "spherical", "harmonic", "representation", "non-diffuse", "surface", "still", "challenge", "we", "method", "addition", "we", "able", "improve", "depth", "map", "around", "silhouette", "since", "normal", "undefined", "we", "further", "assume", "one-bounce", "local", "illumination", "ignore", "lighting", "visibility", "which", "may", "lead", "error", "some", "case", "example", "hard", "shadow", "may", "result", "artificial", "detail", "around", "boundary", "we", "present", "first", "method", "real-time", "shading-based", "refinement", "rgb-d", "datum", "capture", "commodity", "depth", "camera", "general", "uncontrolled", "scene", "enable", "new", "real-time", "inverse", "rendering", "framework", "approximate", "time-varying", "incident", "lighting", "well", "albedo", "scene", "algorithm", "refine", "raw", "depth", "camera", "optimize", "complex", "non-linear", "energy", "use", "new", "highly", "parallel", "Gauss-Newton", "solver", "GPU", "result", "superior", "previous", "online", "depth", "map", "enhancement", "algorithm", "par", "offline", "shape-from-shading", "approach", "we", "experiment", "further", "show", "approach", "enable", "new", "level", "accuracy", "handheld", "3d", "scanning", "well", "deformable", "surface", "tracking", "research", "co-fund", "German", "Research", "Foundation", "-lrb-", "DFG", "-rrb-", "grant", "grk-1773", "heterogeneous", "image", "Systems", "ERC", "start", "Grant", "335545", "CapReal", "Max", "Planck", "Center", "Visual", "Computing", "Communication", "we", "would", "also", "like", "thank", "Angela", "Dai", "video", "voice", "over", "uus", "Y.", "CHUON", "S.", "hrun", "S.", "tricker", "D.", "heobalt", "C.", "2013", "algorithm", "3d", "shape", "scanning", "depth", "camera", "IEEE", "Trans", "PAMI", "35", "1039", "1050", "ebevec", "P.", "1998", "render", "synthetic", "object", "real", "scene", "bridge", "traditional", "image-based", "graphic", "global", "illumination", "high", "dynamic", "range", "photography", "SIGGRAPH", "189", "198", "ebevec", "P.", "2012", "light", "stage", "application", "photoreal", "digital", "actor", "SIGGRAPH", "Asia", "Technical", "Briefs", "iebel", "J.", "hrun", "S.", "2006", "application", "Markov", "Random", "Fields", "range", "sense", "nip", "291", "298", "olson", "J.", "AEK", "J.", "lagemann", "C.", "hrun", "S.", "2010", "upsample", "range", "datum", "dynamic", "environment", "ANELLO", "S.", "ESKIN", "C.", "ZADI", "S.", "OHLI", "P.", "ET", "AL", "learn", "depth", "camera", "close-range", "human", "capture", "interaction", "ACM", "Trans", "hosh", "a.", "yffe", "G.", "unwattanapong", "B.", "USCH", "J.", "X.", "EBEVEC", "P.", "2011", "multiview", "face", "capture", "use", "polarize", "spherical", "gradient", "illumination", "ACM", "Trans", "udmundsson", "S.", "A.", "ANAES", "H.", "ARSEN", "R.", "2008", "fusion", "stereo", "vision", "time-of-flight", "imaging", "improve", "3d", "estimation", "J.", "Intell", "425", "433", "Y.", "EE", "J.-Y.", "WEON", "I.", "S.", "2013", "high", "quality", "shape", "from", "single", "rgb-d", "image", "under", "uncalibrated", "natural", "illumination", "ern", "andez", "C.", "OGIATZIS", "G.", "ipollum", "R.", "2008", "multiview", "photometric", "stereo", "IEEE", "PAMI", "30", "548", "554", "orn", "B.", "K.", "1974", "determine", "lightness", "from", "image", "computer", "graphic", "image", "processing", "277", "299", "orn", "B.", "K.", "1975", "obtain", "shape", "from", "shade", "information", "psychology", "computer", "vision", "115", "155", "zadi", "S.", "IM", "D.", "ILLIGES", "O.", "OLYNEAUX", "D.", "EW", "COMBE", "R.", "OHLI", "P.", "HOTTON", "J.", "ODGES", "S.", "reeman", "D.", "AVISON", "a.", "itzgibbon", "a.", "2011", "Kinectfusion", "real-time", "3d", "reconstruction", "interaction", "use", "move", "depth", "camera", "UIST", "ACM", "559", "568", "HAN", "N.", "ran", "L.", "appen", "M.", "2009", "train", "manyparameter", "shape-from-shading", "model", "use", "surface", "database", "ICCV", "Workshop", "OPF", "J.", "OHEN", "M.", "F.", "ISCHINSKI", "D.", "YTTENDAELE", "M.", "2007", "Joint", "bilateral", "upsampling", "ACM", "Trans", "indner", "M.", "OLB", "a.", "artmann", "K.", "2007", "datafusion", "pmd-based", "distance-information", "high-resolution", "rgb-image", "ISSCS", "121", "124", "ulligan", "J.", "ROLLY", "X.", "2004", "surface", "determination", "photometric", "range", "CVPR", "Workshop", "ehab", "D.", "USINKIEWICZ", "S.", "AVIS", "J.", "AMAMOORTHI", "R.", "2005", "efficiently", "combine", "position", "normal", "precise", "3d", "geometry", "SIGGRAPH", "24", "ewcombe", "R.", "A.", "ZADI", "S.", "ET", "AL", "Kinectfusion", "Realtime", "dense", "surface", "mapping", "tracking", "mixed", "augmented", "reality", "-lrb-", "ismar", "-rrb-", "ieee", "international", "symposium", "127", "136", "iessner", "m.", "OLLH", "OFER", "M.", "ZADI", "S.", "TAMMINGER", "M.", "2013", "real-time", "3d", "reconstruction", "scale", "use", "voxel", "hash", "ACM", "transaction", "graphic", "-lrb-", "tog", "-rrb-", "32", "169", "ark", "J.", "IM", "H.", "aus", "y.-w.", "rown", "M.", "S.", "WEON", "I.S.", "2011", "high", "quality", "depth", "map", "upsampling", "3d-tof", "camera", "iccv", "IEEE", "1623", "1630", "rado", "E.", "augera", "O.", "2005", "shape", "from", "shade", "well-posed", "problem", "amamoorthus", "R.", "ANRAHAN", "P.", "2001", "signalprocessing", "framework", "inverse", "rendering", "SIGGRAPH", "117", "128", "ichardt", "C.", "toll", "C.", "ODGSON", "N.", "A.", "eidel", "h.-p.", "heobalt", "C.", "2012", "coherent", "spatiotemporal", "filter", "upsampling", "rendering", "rgbz", "video", "Computer", "Graphics", "Forum", "-lrb-", "Proceedings", "Eurographics", "-rrb-", "31", "-lrb-", "May", "-rrb-", "unwattanapong", "B.", "yffe", "G.", "RAHAM", "P.", "USCH", "J.", "X.", "HOSH", "a.", "ebevec", "P.", "2013", "acquire", "reflectance", "shape", "from", "continuous", "spherical", "harmonic", "illumination", "ACM", "transaction", "graphic", "-lrb-", "tog", "-rrb-", "32", "109", "ALGAERTS", "L.", "C.", "RUHN", "a.", "eidel", "h.-p.", "heobalt", "C.", "2012", "lightweight", "binocular", "facial", "performance", "capture", "under", "uncontrolled", "lighting", "ACM", "Trans", "eber", "D.", "ENDER", "J.", "CHNOES", "M.", "tork", "a.", "ell", "ner", "D.", "2013", "efficient", "gpu", "datum", "structure", "method", "solve", "sparse", "linear", "system", "dynamics", "application", "Computer", "Graphics", "Forum", "32", "16", "26", "eus", "g.-q.", "irzinger", "G.", "1996", "Learning", "shape", "from", "shade", "multilayer", "network", "IEEE", "Trans", "neural", "Networks", "C.", "ARANASI", "K.", "IU", "Y.", "eidel", "h.-p.", "heobalt", "C.", "2011", "shading-based", "dynamic", "shape", "refinement", "from", "multiview", "video", "under", "general", "illumination", "C.", "toll", "C.", "ALGAERTS", "L.", "heobalt", "C.", "2013", "on-set", "performance", "capture", "multiple", "actor", "stereo", "camera", "ACM", "transaction", "graphic", "-lrb-", "tog", "-rrb-", "32", "161", "ang", "Q.", "ang", "R.", "AVIS", "J.", "istr", "D.", "2007", "spatialdepth", "super", "resolution", "range", "image", "CVPR", "IEEE", "l.-f.", "EUNG", "S.-K.", "aus", "y.-w.", "S.", "2013", "shading-based", "shape", "refinement", "rgb-d", "image", "hang", "Z.", "SA", "P.-S.", "RYER", "J.", "E.", "HAH", "M.", "1999", "shape", "from", "shade", "survey", "IEEE", "PAMI", "21", "690", "706", "hu", "J.", "ang", "L.", "ang", "R.", "AVIS", "J.", "2008", "fusion", "time-of-flight", "depth", "stereo", "high", "accuracy", "depth", "map", "OLLH", "OFER", "M.", "IESSNER", "M.", "ZADI", "S.", "EHMANN", "C.", "ach", "C.", "ISHER", "M.", "C.", "itzgibbon", "a.", "oop", "C.", "heobalt", "C.", "TAMMINGER", "M.", "2014", "real-time", "non-rigid", "reconstruction", "use", "rgb-d", "camera", "ACM", "TOG", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "33", "OLLH", "OFER", "M.", "hie", "J.", "OLAIANNI", "M.", "tamminger", "m.", "reiner", "G.", "2014", "interactive", "model-based", "reconstruction", "human", "head", "use", "rgb-d", "sensor", "computer", "animation", "virtual", "world", "25", "3-4", "213", "222" ],
  "content" : "Our algorithm makes few scene assumptions, handling arbitrary scene objects even under motion. To enable this type of real-time depth map enhancement, we contribute a new highly parallel algorithm that reformulates the inverse rendering optimization problem in prior work, allowing us to estimate lighting and shape in a temporally coherent way at video frame-rates. Our optimization problem is minimized using a new regular grid Gauss-Newton solver implemented fully on the GPU. We demonstrate results showing enhanced depth maps, which are comparable to offline methods but are computed orders of magnitude faster, as well as baseline comparisons with online filtering-based methods. In order to refine a depth map based on the shading in real-time, orders of magnitude faster than state-of-the-art offline systems [Wu et al. 2011], we must redesign the shading-based energy function as well as its optimization method. This careful design choice enables the refinement of depth maps in real-time, making it ideally suited to modern commodity range sensors that run at ? 30Hz. Specifically, our algorithm provides the following contributions: ? rephrasing the inverse rendering optimization problems used in offline methods [Wu et al. 2011] in a highly parallelized manner to enable real-time lighting estimation through spherical harmonics, and direct solving for refined depth rather than displacements on 3D meshes. ? space-time coherent estimation of shape and lighting using temporal correspondences derived from a real-time alignment of depth maps. Beyond these technical contributions, we show the versatility of our method for reconstructing arbitrary scenes, even under motion, and demonstrate improved accuracy compared to filtering based refinement methods. We show integration into a real-time scanning framework akin to KinectFusion [Newcombe et al. 2011; Izadi et al. 2011; Nie?ner et al. 2013], and show improved quality during realtime capture. Finally, we demonstrate how our method enables improvement of the spatio-temporal reconstructions of a recent live non-rigid performance capture system [Zollh?fer et al. 2014a]. Park et al. [2011] formulate depth upsampling to color image resolution as an optimization problem enforcing the discontinuity similarity mentioned earlier, as well as additional regularization terms. Multi-frame superresolution techniques estimate higher resolution depth images from a stack of aligned low resolution images captured under slight lateral displacement [Cui et al. 2013], but real-time computation has not been possible so far. Our approach does not impose strong priors on shape recovery. In our work, we demonstrate real-time shading-based refinement of RGB-D data, captured in general scenes with unknown and timevarying lighting, using only commodity hardware. We further employ new effective approximations and parameterizations, as well as fast geometric correspondence search on the GPU, which enables us to even enforce temporal priors in our reconstructions. Input to our algorithm is a noisy low resolution depth map D r from a depth camera and an aligned RGB image I. Unlike previous offline methods that used multi-camera input to refine full 3D meshes, we rephrase shading-based refinement as a depth map enhancement process. We solve the inverse rendering problem using an effective parameterization of the shading equation (Sect. Thereafter, the coarse geometry is refined using shading information (Sect. Real-time estimation of illumination and refined geometry necessitates an efficient formulation of the light transport model, i.e., the shading equation. Similar to previous offline methods, we assume that surfaces in a scene are Lambertian, and we parameterize the incident lighting with spherical harmonics (SH) [Wu et al. 2011]. In fact, we estimate incident irradiance as a function of the surface normal, that is the incident light, filtered by the cosine with the normal. As with previous approaches, we henceforth estimate lighting from a grayscale version of I, and thus assume gray lighting with equal values in each RGB channel. In some steps, full RGB images are used, which we denote I c . Unlike offline multi-view methods, we employ a triangulated depth map as geometry parameterization. This means there is a fixed depth pixel to mesh vertex relation, and we can express the reflected irradiance B(i, j) of a depth pixel (i, j) with normal n(i, j) and albedo k(i, j) as: where l k are the nine 2nd order spherical harmonics coefficients of the incident illumination. Note that in our real-time setting, we cannot afford local visibility computation, so illumination depends only on the normal direction. During lighting estimation (Sect. 4.1), we therefore initially assume that the scene has uniform albedo. Subsequently, a dense albedo image is computed by dividing the RGB values through the lighting term. High-frequency detail in the depth map is then computed by shading-based refinement of the per-pixel depth values (Sect. Unlike previous SfS methods that solve for surface normal orientations, we directly optimize the depth by linking the depth to the normal. This is not only computationally much more efficient, but also allows us to implicitly enforce surface integrability during depth optimization. The illumination coefficients l k are computed by minimizing the difference between the rendered image B (given our current lighting estimate and geometry) and the captured RGB image I: where (N x , N y ) is the image size. Solving this least-squares problem is equivalent to solving the following system of linear equations: We exclude pixels at grazing angles for lighting estimation, as both shading and depth are unreliable in these regions. For performance reasons, when the input RGB image resolution is higher than 640 ? 480, we downsample the image by a factor of three in the lighting estimation stage. The SH lighting coefficients are then obtained as l = (A T A) ?1 A T I. For the calculation of A T A and A T I, we use a parallel reduction and solve for the l k on the CPU. input image and estimated albedo map. Right: spatial neighborhood of geometric regularizer. estimation, we optionally add a temporal prior term ? L (l ? l p ) 2 to Eq. Then, the linear system we need to solve is as follows: where M I ? R 9?9 is an identity matrix. An example illumination environment map corresponding to l is shown in Fig. 2 . Example albedo images are shown in Fig. 3 . Given the estimated lighting and albedo image, we refine the coarse depth through a second error minimization that uses shading cues from the intensity image. Normal field computation for an image with N pixels requires optimizing an energy in 2N unknowns, and refining the depth based on the normal constraint means solving another sparse linear system with N variables. To achieve real-time performance we choose a more efficient strategy, and directly optimize for the depth value of each of the N pixels in I. This enables us to use the regular image structure for efficient parallelism of our optimization. Note that, depending on the camera, the physical depth resolution may be lower than the RGB resolution; we always sample depth and color at the same higher resolution. To obtain the refined depth map D ? , we minimize: E(D) = w g E g (i, j)+w s E s (i, j)+w p E p (i, j)+w r E r (i, j), (i,j) (6) where D is the vector of depth values. This is broken down into the following four terms: Shading Gradient Constraint Our data term penalizes differences between rendered shading gradients and intensity image gradients: E g (i, j) = [B(i, j) ? B(i + 1, j) ? (I(i, j) ? I(i + 1, j))] 2 +[B(i, j) ? B(i, j + 1) ? (I(i, j) ? I(i, j + 1))] 2 , (7) This gradient-based metric is more robust against inaccuracies of our approximate shading model which does not account for all lighting effects in a real scene. D(i, j), we first establish the link between D(i, j) and n(i, j). The unnormalized surface normal at (i, j) can be computed from the 3D points of the neighboring depth pixels ( Fig. 3 ): After substituting Eq. We enforce a Laplacian smoothness constraint for each pixel, which is computed as: Fig. 3 shows the neighborhood of this geometric regularizer. Depth Constraint We also define a depth constraint, which enforces that the refined depth stays close to the initial depth before refinement D i : Temporal Constraint To reduce temporal aliasing in our reconstructions, for static scenes we employ a temporal constraint to stabilize the refined depth. As our image formation model has not taken albedo variation into account, our method may interpret albedo changes as shading variation and produce artificial details around albedo boundaries. In order to reduce these texture-copy artifacts, we modified our shading energy term in Eq. So the modified shading energy is defined as: E g (i, j) = w ij r [B(i, j) ? B(i + 1, j) ? (I(i, j) ? I(i + 1, j))] 2 +w ij c [B(i, j) ? B(i, j + 1) ? (I(i, j) ? I(i, j + 1))] 2 , (14) where w ij r , w ij c ? {0, 1} are binary weights for each row and column, which are set to zero for albedo boundary edges. Albedo changes usually result in large difference in RGB color space [Horn 1974]. Therefore, we detect these by applying a user-defined threshold to an edge map computed on the albedo image I a . Fig. 4 shows an example of how this strategy can reduce the texture-copy artifacts with varying thresholds. Solving the non-linear energy (Eq. Our refinement energy E(d) : R N ? R (Eq. 6) is non-linear given the image formation model and its dependence on the orientation of the surface normal. We use a row-major ordering of the pixels in the depth image D at the target resolution to obtain the parameter vector of the N unknown per-pixel depth values as follows: The next sections describe our efficient parallel patch-based Gauss-Newton solver, that allows us to minimize this energy for more than 500,000 parameters at real-time rates. Refined depth values are then computed by minimizing: Explicit linearization of the vector field F (d) using Taylor expansion yields: J(d k ) is the Jacobian of F evaluated at the solution after k iterations. These can be solved jointly on the complete domain using iterative solution techniques like preconditioned conjugate gradient (PCG). Previous work [Weber et al. 2013; Zollh?fer et al. 2014a] demonstrated the feasibility of this strategy in a GPU optimization framework for dynamics simulation and non-rigid registration, respectively. One important observation is that switching kernels has a significant impact on performance. The aforementioned methods are optimized such that they require 2 kernel calls for initialization and 3-4 kernel calls in the inner PCG loop, depending on whether the system matrix J T J is explicitly evaluated or by sequentially applying J T and J. As a result, even for several thousands of variables, the optimization problem can be solved at interactive rates. However, for our problem we should be able to optimize more than half a million values in real time, which is not possible with these approaches. With our error term, the computation for a single pixel only depends on a 5 ? 5 image neighborhood. Thus, we can subdivide the domain into square patches (cp. Fig. 5 ), and perform the optimization patchwise using a variant of the Schwarz Alternating Procedure. The optimization of a single patch happens per thread block, where all data can be kept in shared GPU memory. This procedure exploits the locality of the optimization constraints and the uniform tessellation of the optimization domain. It scales well to a higher number of unknowns and reduces kernel call overhead and global memory accesses by exploiting fast shared memory. The optimization domain ? (see Fig. 5 ) is partitioned into small rectangular sub-regions (patches) The linear systems corresponding to the sub-regions (without boundary) are solved independently by imposing Neumann constraints on the boundaries ?? i . Optimization only happens on the inner variables; boundary values remain unchanged. In each Schwarz iteration, inner, and boundary variables of a patch are first read and stored to shared memory. Then the inner variables are optimized, keeping the boundary values fixed. Finally, the inner variables are written back to global memory. Since the boundary variables are considered to be fixed, the corresponding block entries can be moved to the right-hand side: A i,i d i = b i ? A i,b d b . Each local sub-problem on a sub-region (or patch) ? i is assigned to one thread block and solved in parallel using one thread per variable. The patch size is set based on the GPU L-1 cache; thus for our hardware setup, we use 16 ? 16 patches. Including the boundary values, this results in a 20 ? 20 grid that has to be kept in shared memory. The entire algorithm is shown as pseudocode in Algorithm 1 and illustrated in Fig. 6 . Note that Gauss-Newton and Schwarz iterations happen concurrently. After each Schwarz iteration, we also apply the delta updates, so each Schwarz iteration step implicitly performs a Gauss-Newton step. This does not incur any additional computation, because the PCG solver has to re-evaluate the Jacobian and the residuals anyway, but results in faster convergence. Fig. 7 shows the convergence behavior depending on the number K of PCG steps. For the figure, we used N e = 20 outer GaussNewton/Schwarz iterations, and K = 1, 3, 5, 10 inner PCG iterations. For K > 10 we observed no further improvement. Note that we use no synchronization when writing depth values. As a result, some patches might already read updated boundary values, which leads to a mixture of Multiplicative and Additive Schwarz. On the other hand, avoiding synchronization improves performance. We shift the initial patch grid in each iteration by sub-patch steps based on a Halton sequence to improve convergence and to avoid patch structures from becoming visible in the solution. Algorithm 1 Shared Memory PCG Kernel for i = 1 . N e do for all patches p in parallel do Fetch Data To Shared Memory(p); Compute RHS And Preconditioner(p); for k = 1 . Per patch, we solve the resulting linear optimization problem using a fast shared memory PCG solver. All per-patch PCGs corresponding to one Schwarz iteration are launched with a single kernel call. As can be seen in Algorithm 1, this includes shared memory initialization as well as running K PCG iterations and writing back the local patch results to global memory. In the PCG solver, we use a simple Jacobi preconditioner that can be readily parallelized. We exploit the memory hierarchy by caching all per-pixel data to registers and loading all data that has to be accessed by neighboring threads to shared memory. In each PCG iteration, a per-patch scalar product is required, for which we use a fast block reduction in shared memory. Excluding the block reductions, the inner PCG loop requires 6 synchronization points. The system matrix J T J is applied efficiently on-the-fly in each PCG step in an optimized kernel exploiting the sparsity of J. We run the proposed RGB-D shading-based refinement strategy in a hierarchical coarse-to-fine manner to allow for a faster convergence of our method. To this end, we build an image pyramid by successively restricting the input RGB-D data to the coarser levels. After, we sweep from coarse-to-fine (nested iteration) through the hierarchy and alternate between our patch-based Gauss-Newton solver and applying the prolongation operator. Currently, we use a hierarchy with three levels. Aside from the refinement of complete depth maps, we perform our optimization only on blocks containing foreground pixels (cp. Based on the input depth, we mark all patches containing foreground pixels, compute a linear ordering of these using a fast prefix sum, and execute the refinement only on these foreground blocks. We tested our real-time enhancement software on data from a PrimeSense Carmine 1.09 Short Range (RGB res. 1280 ? 1024, depth res 640 ? 480, framerate 12 fps), a Kinect One (RGB res 1920 ? 1080, depth res. 512 ? 424, frame rate 30 fps), as well as an Asus Xtion Pro (RGB res. 640 ? 480, depth res 640 ? 480, framerate 30 fps) camera. Our approach runs at real-time rates in excess of 30 fps. On both static and dynamic scenes and for all RGB-D sensors, a significant enhancement of detail compared to the raw depth data was achieved, see Fig. 12 , Fig. 1 , and the supplemental video and document, which show screen captured visualizations of the reconstructions before and after refinement. In total, qualitative tests were done on 9 scenes, see Tab. 1. We always enable the prior term in lighting estimation by setting ? l = 10. We set the empirically found weights for depth refinement as follows: w g = 1, w s = 400, w p = 10, w r = 100 for all static scenes; w g = 1, w s = 100, w p = 10, w r = 0 for all dynamic scenes. Please refer to Tab. 1 for details of the sequences and timings of the individual steps measured on an Intel Core i7-3770 CPU with 3.4GHz (16GB Ram) and an Nvidia Geforce GTX 780. The listed preprocessing steps include: depth-to-color alignment, filtering of depth and color, resampling of images, and foreground segmentation. For those results, the Gauss-Newton optimizer ran with the following parameters: 3 hierarchy levels, N e = 10, 8, 6 outer iterations, and K = 5, 5, 5 PCG iterations (coarse-to-fine). We enable the temporal smoothness prior term for capturing static scenes, i.e., the Vase sequence, Lucy sequence and Socrates sequence. The required ICP alignment adds 3.5 ms for these three sequences to the total computation time. This yields effective frame rates between 15 fps at full HD and 93 fps at SVGA. Quantitative Evaluation We quantitatively evaluate the accuracy of our method on two synthetic sequences that are 400 frames long. We use ground truth, detailed performance-captured face geometry [Valgaerts et al. 2012], and the ground truth lighting from St. Peter?s Basilica [Debevec 1998] and render two RGB-D sequences. In the first one (CoA) the albedo is uniform, and in the second one (DA) we use a dense albedo map obtained from one of the captured face images. To synthesize the depth map sequences, we first obtain a quantized depth map from the stereo results of [Valgaerts et al. 2012], and then add Gaussian noise to mimic the noise from a depth sensor. We compare our method with the space-time multi-lateral RGBD filtering method of [Richardt et al. 2012] (STFilt), and with reconstructions of CoA and DA using the single-frame shading based refinement algorithm in the offline method of [Valgaerts et al. 2012](SBRol). As an error metric, we employ the average pixel-wise Euclidean distance in mm per frame (d e ), as well as the average angular difference of normals in degrees (d n ). The distance and normal errors averaged over all frames are summarized in Tab. 2. Compared to STFilt, our method produces results with much lower distance and normal errors, as it obtains metrically faithful reconstructions as opposed to only plausible results (see Fig. 8 ). In comparison to the more involved offline method by [Valgaerts et al. 2012], our results exhibit comparable distance error, but our real-time capability comes at the price of a slightly higher error in reconstructed normal orientation. The respective error curves over time on both DA ( Fig. 9 ) and CoA (additional material) further confirm the above conclusions. Qualitative Comparison We also compared our method with STFilt on real-world data (talking sequence, see Tab. 1). Using the same hardware as previously described, our approach not only has a runtime advantage (55.8 fps against 8.5 fps), but also produces much more detailed results (see Fig. 10 and video). state-of-the-art online method (STFilt [Richardt et al. 2012]), and even comes close to an offline shading-based refinement method (SBRol [Valgaerts et al. 2012]). Real-time 3D reconstruction We used our algorithm together with the real-time voxel-hashing-based hand-held scanning approach for depth cameras proposed by [Nie?ner et al. 2013]. With our drastically enhanced depth map quality, full 3D models with more detail can be reconstructed (voxel size of 0.5mm); see Fig. 1 and Fig. 11 . Deformable 3D reconstruction We also integrated our algorithm into the real-time deformable tracking approach by [Zollh?fer et al. 2014a], which leads to improved space-time coherent reconstructions of a non-rigidly deforming template, see Fig. 13 . Our approach enables a leap forward in real-time scene reconstruction with depth cameras, but is still subject to several well-known shape from shading limitations. Our adaptive refinement (Sect. 4.2) is efficient to mitigate the visual presence of these artifacts. However, we still cannot completely prevent them; for instance see Fig. 4 and Fig. 12 (top left). Generally, we believe that the underconstrained nature of In contrast to some offline methods, our real-time constraint allows only for a simplified light transport model. That is, our initial constant albedo assumption may exacerbate texture copying on general scenes; however, our results show that in practice, very faithful surface reconstructions with spatially-varying albedo are feasible. Due to the second order spherical harmonics representation, non-diffuse surfaces are still challenging for our method. In addition, we are not able to improve depth maps around silhouettes since the normal is undefined. We further assume a one-bounce local illumination and ignore lighting visibility, which may lead to errors in some cases. For example, hard shadows may result in artificial detail around their boundaries. We presented the first method for real-time shading-based refinement of RGB-D data captured with commodity depth cameras in general uncontrolled scenes. This is enabled by a new real-time inverse rendering framework that approximates time-varying incident lighting as well as albedo in the scene. The algorithm then refines the raw depth of the camera by optimizing a complex non-linear energy using a new highly parallel Gauss-Newton solver on the GPU. The results are superior to previous online depth map enhancement algorithms, and on par with offline shape-from-shading approaches. Our experiments further show that the approach enables a new level of accuracy in handheld 3D scanning as well as deformable surface tracking. This research was co-funded by the German Research Foundation (DFG), grant GRK-1773 Heterogeneous Image Systems, the ERC Starting Grant 335545 CapReal, and the Max Planck Center for Visual Computing and Communication. We would also like to thank Angela Dai for the video voice over. C UI , Y., S CHUON , S., T HRUN , S., S TRICKER , D., AND T HEOBALT , C. 2013. Algorithms for 3d shape scanning with a depth camera. IEEE Trans. PAMI 35, 5, 1039?1050. D EBEVEC , P. 1998. Rendering synthetic objects into real scenes: Bridging traditional and image-based graphics with global illumination and high dynamic range photography. SIGGRAPH, 189?198. D EBEVEC , P. 2012. The light stages and their applications to photoreal digital actors. In SIGGRAPH Asia Technical Briefs. D IEBEL , J., AND T HRUN , S. 2006. An application of Markov Random Fields to range sensing. NIPS, 291?298. D OLSON , J., B AEK , J., P LAGEMANN , C., AND T HRUN , S. 2010. Upsampling range data in dynamic environments. F ANELLO , S., K ESKIN , C., I ZADI , S., K OHLI , P., ET AL . Learning to be a depth camera for close-range human capture and interaction. ACM Trans. G HOSH , A., F YFFE , G., T UNWATTANAPONG , B., B USCH , J., Y U , X., AND D EBEVEC , P. 2011. Multiview face capture using polarized spherical gradient illumination. ACM Trans. G UDMUNDSSON , S. A., A ANAES , H., AND L ARSEN , R. 2008. Fusion of stereo vision and time-of-flight imaging for improved 3d estimation. J. Intell. 5, 425?433. H AN , Y., L EE , J.-Y., AND K WEON , I. S. 2013. High quality shape from a single rgb-d image under uncalibrated natural illumination. H ERN ANDEZ  ? , C., V OGIATZIS , G., AND C IPOLLA , R. 2008. Multiview photometric stereo. IEEE PAMI 30, 3, 548?554. H ORN , B. K. 1974. Determining lightness from an image. Computer graphics and image processing 3, 4, 277?299. H ORN , B. K. 1975. Obtaining shape from shading information. The psychology of computer vision, 115?155. I ZADI , S., K IM , D., H ILLIGES , O., M OLYNEAUX , D., N EW COMBE , R., K OHLI , P., S HOTTON , J., H ODGES , S., F REEMAN , D., D AVISON , A., AND F ITZGIBBON , A. 2011. Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera. UIST, ACM, 559?568. K HAN , N., T RAN , L., AND T APPEN , M. 2009. Training manyparameter shape-from-shading models using a surface database. ICCV Workshop. K OPF , J., C OHEN , M. F., L ISCHINSKI , D., AND U YTTENDAELE , M. 2007. Joint bilateral upsampling. ACM Trans. L INDNER , M., K OLB , A., AND H ARTMANN , K. 2007. Datafusion of PMD-based distance-information and high-resolution RGB-images. ISSCS, 121?124. M ULLIGAN , J., AND B ROLLY , X. 2004. Surface determination by photometric ranging. CVPR Workshop. N EHAB , D., R USINKIEWICZ , S., D AVIS , J., AND R AMAMOORTHI , R. 2005. Efficiently combining positions and normals for precise 3D geometry. SIGGRAPH 24, 3. N EWCOMBE , R. A., I ZADI , S., ET AL . Kinectfusion: Realtime dense surface mapping and tracking. In Mixed and augmented reality (ISMAR), IEEE international symposium on, 127? 136. N IESSNER , M., Z OLLH OFER  ? , M., I ZADI , S., AND S TAMMINGER , M. 2013. Real-time 3d reconstruction at scale using voxel hashing. ACM Transactions on Graphics (TOG) 32, 6, 169. P ARK , J., K IM , H., T AI , Y.-W., B ROWN , M. S., AND K WEON , I.S. 2011. High quality depth map upsampling for 3d-tof cameras. In ICCV, IEEE, 1623?1630. P RADOS , E., AND F AUGERAS , O. 2005. Shape from shading: a well-posed problem? R AMAMOORTHI , R., AND H ANRAHAN , P. 2001. A signalprocessing framework for inverse rendering. SIGGRAPH, 117?128. R ICHARDT , C., S TOLL , C., D ODGSON , N. A., S EIDEL , H.-P., AND T HEOBALT , C. 2012. Coherent spatiotemporal filtering, upsampling and rendering of RGBZ videos. Computer Graphics Forum (Proceedings of Eurographics) 31, 2 (May). T UNWATTANAPONG , B., F YFFE , G., G RAHAM , P., B USCH , J., Y U , X., G HOSH , A., AND D EBEVEC , P. 2013. Acquiring reflectance and shape from continuous spherical harmonic illumination. ACM Transactions on Graphics (TOG) 32, 4, 109. V ALGAERTS , L., W U , C., B RUHN , A., S EIDEL , H.-P., AND T HEOBALT , C. 2012. Lightweight binocular facial performance capture under uncontrolled lighting. ACM Trans. W EBER , D., B ENDER , J., S CHNOES , M., S TORK , A., AND F ELL NER , D. 2013. Efficient gpu data structures and methods to solve sparse linear systems in dynamics applications. Computer Graphics Forum 32, 1, 16?26. W EI , G.-Q., AND H IRZINGER , G. 1996. Learning shape from shading by a multilayer network. IEEE Trans. Neural Networks. W U , C., V ARANASI , K., L IU , Y., S EIDEL , H.-P., AND T HEOBALT , C. 2011. Shading-based dynamic shape refinement from multiview video under general illumination. W U , C., S TOLL , C., V ALGAERTS , L., AND T HEOBALT , C. 2013. On-set performance capture of multiple actors with a stereo camera. ACM Transactions on Graphics (TOG) 32, 6, 161. Y ANG , Q., Y ANG , R., D AVIS , J., AND N ISTR , D. 2007. Spatialdepth super resolution for range images. CVPR, IEEE. Y U , L.-F., Y EUNG , S.-K., T AI , Y.-W., AND L IN , S. 2013. Shading-based shape refinement of rgb-d images. Z HANG , Z., T SA , P.-S., C RYER , J. E., AND S HAH , M. 1999. Shape from shading: A survey. IEEE PAMI 21, 8, 690?706. Z HU , J., W ANG , L., Y ANG , R., AND D AVIS , J. 2008. Fusion of time-of-flight depth and stereo for high accuracy depth maps. Z OLLH OFER  ? , M., N IESSNER , M., I ZADI , S., R EHMANN , C., Z ACH , C., F ISHER , M., W U , C., F ITZGIBBON , A., L OOP , C., T HEOBALT , C., AND S TAMMINGER , M. 2014. Real-time non-rigid reconstruction using an rgb-d camera. ACM TOG (Proc. SIGGRAPH) 33, 4. Z OLLH OFER  ? , M., T HIES , J., C OLAIANNI , M., S TAMMINGER , M., AND G REINER , G. 2014. Interactive model-based reconstruction of the human head using an rgb-d sensor. Computer Animation and Virtual Worlds 25, 3-4, 213?222.",
  "resources" : [ ]
}