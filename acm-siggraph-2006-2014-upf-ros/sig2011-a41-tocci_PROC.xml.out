{
  "uri" : "sig2011-a41-tocci_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a41-tocci_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "A Versatile HDR Video Production System",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Michael D.-Tocci",
      "name" : "Michael D.",
      "surname" : "Tocci"
    }, {
      "uri" : "http://drinventor/Chris-Kiser",
      "name" : "Chris",
      "surname" : "Kiser"
    }, {
      "uri" : "http://drinventor/Nora-Tocci",
      "name" : "Nora",
      "surname" : "Tocci"
    }, {
      "uri" : "http://drinventor/Pradeep-Sen",
      "name" : "Pradeep",
      "surname" : "Sen"
    } ]
  },
  "bagOfWords" : [ "extension", "dynamic", "range", "digital", "image", "have", "be", "subject", "significant", "research", "both", "academia", "industry", "we", "propose", "system", "simple", "use", "only", "off-the-shelf", "technology", "flexible", "term", "sensor", "use", "specifically", "we", "HDR", "optical", "architecture", "-lrb-", "-rrb-", "capture", "optically-aligned", "multipleexposure", "image", "simultaneously", "do", "need", "image", "manipulation", "account", "motion", "-lrb-", "-rrb-", "extend", "dynamic", "range", "available", "image", "sensor", "-lrb-", "over", "photographic", "stop", "we", "current", "prototype", "-rrb-", "-lrb-", "-rrb-", "inexpensive", "implement", "-lrb-", "-rrb-", "utilize", "single", "standard", "camera", "lens", "-lrb-", "-rrb-", "efficiently", "use", "light", "from", "lens", "complement", "we", "system", "we", "also", "propose", "novel", "hdr", "imagemerge", "algorithm", "-lrb-", "-rrb-", "combine", "image", "separate", "more", "than", "stop", "exposure", "-lrb-", "-rrb-", "spatially", "blend", "pre-demosaiced", "pixel", "datum", "reduce", "unwanted", "artifact", "-lrb-", "-rrb-", "produce", "HDR", "image", "radiometrically", "correct", "-lrb-", "-rrb-", "use", "highest-fidelity", "-lrb-", "lowest", "quantized-noise", "-rrb-", "pixel", "datum", "available", "common", "method", "merge", "multiple", "ldr", "image", "single", "composite", "HDR", "image", "one", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "which", "first", "solve", "camera", "response", "curve", "translate", "pixel", "value", "log", "scene", "irradiance", "blend", "irradiance", "from", "image", "together", "approach", "typically", "work", "each", "pixel", "final", "HDR", "image", "independently", "use", "only", "information", "contain", "within", "respective", "pixel", "each", "ldr", "image", "unlike", "approach", "we", "propose", "use", "additional", "information", "available", "neighborhood", "pixel", "reduce", "noise", "we", "final", "irradiance", "estimate", "finally", "other", "have", "present", "algorithm", "fuse", "ldr", "image", "together", "without", "explicitly", "create", "hdr", "image", "first", "-lrb-", "e.g.", "-lsb-", "Agarwala", "et", "al.", "2004", "Mertens", "et", "al.", "2008", "-rsb-", "-rrb-", "because", "space", "limit", "we", "focus", "only", "principal", "technology", "currently", "available", "HDR", "video", "refer", "interested", "reader", "text", "subject", "-lrb-", "e.g.", "-lsb-", "Myszkowski", "et", "al.", "2008", "-rsb-", "-rrb-", "more", "information", "although", "method", "work", "well", "static", "scene", "well-suited", "video", "because", "different", "moment", "time", "exposure", "length", "each", "photograph", "which", "result", "vary", "amount", "motion", "blur", "other", "timerelated", "effect", "here", "beamsplitt", "prism", "break", "up", "light", "three", "part", "one", "each", "sensor", "fit", "different", "filter", "design", "use", "absorptive", "filter", "like", "one", "make", "inefficient", "use", "light", "some", "approach", "place", "array", "neutral-density", "filter", "over", "individual", "pixel", "sensor", "vary", "amount", "absorption", "-lrb-", "e.g.", "-lsb-", "Nayar", "Mitsunaga", "2000", "-rsb-", "-rrb-", "which", "can", "require", "complex", "demosaicing", "algorithm", "sensor", "have", "filter", "pattern", "four", "differently-exposed", "pixel", "-lrb-", "one", "four", "fully", "expose", "-rrb-", "only", "pixel", "would", "receive", "full", "exposure", "level", "from", "scene", "approach", "similar", "we", "own", "light", "camera", "split", "pyramid-shaped", "mirror", "refract", "prism", "redirect", "toward", "set", "sensor", "fit", "absorptive", "filter", "produce", "image", "different", "exposure", "-lrb-", "e.g.", "Harvey", "-lsb-", "1998", "-rsb-", "Aggarwal", "Ahuja", "-lsb-", "2001", "2004", "-rsb-", "Wang", "et", "al.", "-lsb-", "2005", "-rsb-", "-rrb-", "design", "previous", "system", "all", "suffer", "from", "parallax", "error", "due", "fact", "image-forming", "beam", "split", "spatially-distinct", "subsection", "each", "individual", "sensor", "look", "through", "camera", "lens", "from", "slightly", "different", "angle", "show", "recent", "work", "handheld", "plenoptic", "camera", "-lrb-", "e.g.", "-lsb-", "Ng", "et", "al.", "2005", "-rsb-", "-rrb-", "provide", "each", "sensor", "slightly", "different", "information", "which", "significantly", "affect", "imaging", "scene", "close", "camera", "Watts", "radiative", "power", "enter", "aperture", "camera", "three-way", "system", "show", "fig.", "-lrb-", "configure", "same", "dynamic", "range", "ours", "-rrb-", "allow", "only", "0.3622", "Watts", "sensor", "waste", "almost", "available", "light", "example", "McGuire", "et", "al.", "-lsb-", "2007", "-rsb-", "present", "design", "tool", "create", "efficient", "beamsplitt", "tree", "separate", "lens", "each", "sensor", "show", "example", "HDR", "imaging", "finally", "unclear", "how", "system", "could", "develop", "single", "hand-held", "unit", "we", "system", "place", "beamsplitter", "behind", "single", "camera", "lens", "so", "do", "suffer", "from", "limitation", "finally", "early", "prototype", "hdr", "system", "industry", "eventually", "intend", "commercial", "use", "spheronvr", "hdrv", "camera", "-lsb-", "spheron", "vr", "2011", "-rsb-", "however", "method", "achieve", "HDR", "capture", "have", "be", "publish", "while", "all", "system", "we", "mention", "section", "capable", "produce", "hdr", "video", "date", "method", "produce", "high-quality", "hdr", "video", "have", "be", "demonstrate", "robust", "yet", "simple", "enough", "readily", "introduce", "wide", "commercial", "audience", "implement", "modern", "optics", "laboratory", "we", "optical", "architecture", "base", "beamsplitter", "located", "between", "camera", "lens", "sensor", "which", "have", "be", "use", "previous", "HDR", "camera", "design", "well", "3-sensor", "color-splitting", "camera", "-lsb-", "Kell", "Sziklai", "1951", "-rsb-", "like", "previous", "method", "we", "optical", "system", "use", "set", "partially-reflecting", "surface", "split", "light", "from", "single", "photographic", "lens", "so", "focus", "onto", "three", "optical", "splitting", "prism", "use", "3ccd", "camera", "employ", "dichroic", "filter", "split", "incoming", "light", "from", "camera", "lens", "red", "green", "blue", "portion", "so", "each", "color", "image", "onto", "its", "own", "image", "sensor", "imaging", "sensor", "simultaneously", "design", "show", "fig.", "allow", "we", "capture", "HDR", "image", "use", "most", "light", "enter", "camera", "we", "use", "term", "high", "medium", "low", "exposure", "-lrb-", "he", "ME", "LE", "respectively", "-rrb-", "refer", "sensor", "base", "amount", "light", "each", "one", "receive", "optical", "splitting", "system", "we", "current", "implementation", "use", "two", "uncoated", "2-micron", "thick", "plastic", "beamsplitter", "which", "rely", "fresnel", "reflection", "air/plastic", "interface", "so", "actual", "transmittance/reflectance", "-lrb-", "t/r", "-rrb-", "value", "function", "angle", "we", "arrangement", "first", "beamsplitter", "45", "angle", "have", "approximate", "t/r", "ratio", "92/8", "which", "mean", "92", "light", "from", "camera", "lens", "transmit", "through", "first", "beamsplitter", "focus", "directly", "onto", "high-exposure", "-lrb-", "he", "-rrb-", "sensor", "beamsplitter", "reflect", "light", "from", "lens", "upward", "show", "fig.", "toward", "second", "uncoated", "beamsplitter", "which", "have", "same", "optical", "property", "first", "position", "90", "angle", "light", "path", "have", "approximate", "t/r", "ratio", "94/6", "total", "light", "reflect", "upward", "94", "-lrb-", "7.52", "total", "light", "-rrb-", "transmit", "through", "second", "beamsplitter", "focus", "onto", "medium-exposure", "-lrb-", "me", "-rrb-", "sensor", "arrangement", "he", "ME", "LE", "sensor", "capture", "image", "92", "7.52", "0.44", "total", "light", "gather", "camera", "lens", "respectively", "therefore", "he", "me", "exposure", "separate", "12.2", "-lrb-", "3.61", "stop", "-rrb-", "me", "le", "separate", "17.0", "-lrb-", "4.09", "stop", "-rrb-", "which", "mean", "configuration", "design", "extend", "dynamic", "range", "sensor", "7.7", "stop", "beamsplitter", "arrangement", "make", "we", "design", "light", "efficient", "negligible", "0.04", "total", "light", "gather", "lens", "waste", "also", "allow", "all", "three", "sensor", "see", "same", "scene", "so", "all", "three", "image", "optically", "identical", "except", "light", "level", "course", "ME", "image", "have", "undergo", "odd", "number", "reflection", "so", "flip", "left-right", "compare", "other", "image", "fix", "easily", "software", "three", "sensor", "gen-locked", "capture", "perfectly", "synchronize", "video", "frame", "identical", "exposure", "time", "since", "t/r", "also", "dependent", "wavelength", "light", "we", "calculate", "t/r", "value", "full", "visible", "spectrum", "integrate", "over", "filter", "spectrum", "Bayer", "pattern", "arrive", "separate", "t/r", "value", "each", "color", "channel", "use", "we", "design", "implementation", "result", "transmittance", "value", "91.85", "92.38", "respectively", "because", "exact", "transmission/reflection", "property", "we", "beamsplitter", "vary", "angle", "we", "examine", "how", "might", "vary", "over", "area", "sensor", "simulate", "propose", "optical", "architecture", "ZEMAX", "-lsb-", "2011", "-rsb-", "calculate", "range", "transmittance", "value", "function", "angle", "we", "examine", "largest", "angular", "variation", "possible", "pellicle", "beamsplitter", "unlike", "approach", "we", "system?s", "internal", "beamsplitter", "receive", "light", "much", "smaller", "range", "field", "angle", "because", "geometrical", "configuration", "system", "show", "scale", "Fig.", "we", "case", "top-left", "bottom-right", "corner", "point", "sensor", "have", "chief-ray", "angle", "pellicle", "46.4", "43.7", "difference", "2.7", "f/2", ".8", "each", "two", "point", "receive", "10", "cone", "ray", "from", "lens", "show", "blue", "red", "-lrb-", "cone", "constant", "angle", "over", "entire", "sensor", "-rrb-", "we", "calculate", "transmittance", "beamsplitter", "integrate", "over", "cone", "ray", "use", "ZEMAX", "simulation", "million", "random", "ray", "2-micron", "thick", "uncoated", "plastic", "pellicle", "beamsplitter", "which", "yield", "transmittance", "91.85", "top-left", "92.38", "bottom-right", "point", "difference", "about", "0.5", "close", "92", "value", "we", "use", "we", "design", "calculation", "polarization", "incident", "light", "might", "affect", "transmission", "property", "beamsplitter", "well", "although", "we", "simulation", "be", "all", "do", "unpolarized", "light", "possible", "encounter", "linearly", "polarize", "light", "outdoor", "scene", "-lrb-", "e.g.", "from", "glance", "reflection", "off", "water", "-rrb-", "which", "may", "change", "exposure", "difference", "between", "sensor", "however", "practice", "we", "do", "see", "polarization", "effect", "scene", "we", "capture", "we", "note", "all", "effect", "may", "reduce", "eliminate", "use", "thin-film", "coating", "beamsplitter", "thin-film", "coating", "could", "design", "have", "more", "constant", "transmission", "property", "over", "range", "angle", "system", "reduce", "polarization", "effect", "examination", "different", "beamsplitter", "coating", "address", "factor", "topic", "future", "work", "advantage", "propose", "optical", "splitting", "system", "its", "cost", "complexity", "relatively", "low", "compatible", "standard", "camera", "lens", "compact", "light", "path", "allow", "integration", "single", "hand-held", "unit", "something", "difficult", "do", "design", "place", "beamsplitter", "outside", "lens", "-lsb-", "McGuire", "et", "al.", "2007", "Cole", "Safai", "2010", "-rsb-", "optical", "architecture", "also", "flexible", "term", "kind", "sensor", "use", "use", "low-cost", "sensor", "example", "could", "allow", "design", "integrate", "consumer", "electronics", "bring", "HDR", "video", "wide", "audience", "addition", "optical", "architecture", "we", "also", "propose", "novel", "algorithm", "merge", "together", "image", "acquire", "we", "system", "order", "automatically", "create", "hdr", "image", "from", "widely-separated", "component", "ldr", "image", "without", "artifact", "we", "find", "necessary", "depart", "from", "standard", "method", "merge", "hdr", "image", "4.1", "Limitations", "current", "approach", "most", "previous", "algorithm", "merge", "hdr", "image", "from", "set", "ldr", "image", "different", "exposure", "typically", "do", "so", "after", "demosaice", "ldr", "image", "merge", "datum", "pixel-by-pixel", "without", "take", "neighboring", "pixel", "information", "account", "discussion", "we", "focus", "original", "merge", "algorithm", "propose", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "although", "algorithm", "work", "well", "when", "ldr", "image", "have", "small", "exposure", "separation", "we", "find", "quality", "final", "HDR", "image", "degrade", "when", "ldr", "image", "separate", "more", "than", "photographic", "stop", "capture", "widest", "dynamic", "range", "possible", "smallest", "number", "camera", "sensor", "necessary", "position", "ldr", "image", "further", "apart", "exposure", "we", "system", "than", "traditional", "hdr", "acquisition", "method", "devebec", "malik?s", "merge", "algorithm", "can", "yield", "undesired", "artifact", "when", "apply", "we", "camera?s", "datum", "because", "quantization", "noise", "effect", "show", "Fig.", "tmo?s", "amplify", "small", "gradient", "difference", "image", "make", "they", "visible", "when", "dynamic", "range", "compress", "amplify", "merge", "artifact", "well", "fig.", "show", "underlie", "cause", "artifact", "present", "range", "scene", "illumination", "measure", "three", "sensor", "we", "architecture", "illustration", "we", "simplify", "system", "4-bit", "sensor", "-lrb-", "oppose", "12-bit", "sensor", "we", "implementation", "-rrb-", "which", "measure", "only", "16", "unique", "brightness", "value", "we", "separate", "sensor", "only", "stop", "-lrb-", "factor", "-rrb-", "exposure", "since", "cmo", "sensor", "exhibit", "approximately", "linear", "relationship", "between", "incident", "exposure", "output", "value", "we", "graph", "value", "from", "three", "sensor", "linear", "function", "incident", "irradiance", "instead", "traditional", "logarithmic", "scale", "figure", "we", "see", "LE", "sensor", "measure", "scene", "irradiance", "more", "coarsely", "than", "other", "two", "sensor", "quantization", "problem", "when", "merge", "ldr", "image", "figure", "show", "source", "artifact", "when", "merge", "we", "LDR", "image", "-lrb-", "simplify", "here", "4-bit", "sensor", "-rrb-", "algorithm", "always", "use", "datum", "from", "all", "three", "sensor", "simple", "weighting", "function", "Debevec", "Malik", "case", "datum", "from", "each", "sensor", "weight", "triangle", "function", "show", "so", "non-zero", "contribution", "from", "LE", "sensor", "low", "brightness", "value", "-lrb-", "like", "sample", "illumination", "level", "indicate", "-rrb-", "even", "though", "datum", "from", "LE", "sensor", "quantize", "more", "coarsely", "than", "he", "sensor", "we", "approach", "contrast", "use", "datum", "from", "higherexposure", "sensor", "much", "possible", "blend", "datum", "from", "next", "darker", "sensor", "when", "near", "saturation", "fore", "LE", "sensor", "record", "single", "increment", "addition", "always", "some", "small", "amount", "noise", "pixel", "value", "error", "LE", "sensor", "span", "12", "value", "range", "he", "sensor", "example", "more", "importantly", "we", "algorithm", "only", "examine", "individual", "pixel", "when", "merge", "ldr", "image", "also", "take", "account", "neighbor", "pixel", "might", "provide", "additional", "information", "help", "denoising", "process", "while", "new", "idea", "one", "key", "aspect", "we", "image-merging", "algorithm", "use", "pixel", "datum", "exclusively", "from", "brightest", "most", "well-exposed", "sensor", "possible", "therefore", "we", "use", "pixel", "from", "he", "image", "much", "possible", "only", "use", "pixel", "me", "image", "he", "pixel", "close", "saturation", "corresponding", "me", "pixel", "below", "saturation", "level", "multiply", "factor", "adjust", "relation", "he", "pixel", "base", "camera?s", "response", "curve", "-lrb-", "fig.", "-rrb-", "give", "me", "pixel", "receive", "12.2", "less", "irradiance", "than", "he", "pixel", "corresponding", "me", "pixel", "above", "saturation", "level", "we", "simply", "apply", "similar", "process", "same", "pixel", "low-exposure", "LE", "image", "when", "merge", "datum", "between", "sensor", "may", "seem", "sufficient", "follow", "na?ve", "winner", "take", "all", "approach", "exclusively", "use", "val", "ue", "from", "he", "sensor", "until", "become", "saturated", "simply", "switch", "next", "sensor", "-lsb-", "JAI", "2009", "-rsb-", "we", "find", "do", "work", "well", "practice", "because", "result", "banding", "artifact", "where", "transition", "occur", "do", "we", "algorithm", "scan", "region", "around", "pixel", "be", "evaluate", "any", "neighbor", "pixel", "region", "saturate", "pixel", "under", "consideration", "may", "subject", "pixel", "crosstalk", "leakage", "algorithm", "estimate", "value", "pixel", "base", "its", "neighbor", "describe", "below", "algorithm", "detail", "we", "approach", "hdr-merging", "perform", "prior", "demosaice", "individual", "Bayer", "color", "filter", "array", "image", "because", "we", "find", "demosaicing", "can", "corrupt", "color", "saturated", "region", "-lrb-", "also", "note", "Ajdin", "et", "al.", "-lsb-", "2008", "-rsb-", "-rrb-", "image", "demosaice", "before", "be", "merge", "HDR", "demosaiced", "orange", "color", "compute", "from", "saturated", "red-pixel", "datum", "non-saturated", "green/blue-pixel", "datum", "result", "hue", "orange", "section", "incorrectly", "reproduce", "since", "image", "merge", "prior", "demosaicing", "step", "we", "algorithm", "work", "pixel", "value", "instead", "irradiance", "produce", "radiometrically-correct", "hdr", "image", "we", "must", "correctly", "match", "irradiance", "level", "he", "ME", "LE", "sensor", "use", "appropriate", "beamsplitter", "transmittance", "value", "each", "pixel", "color", "since", "change", "slightly", "function", "wavelength", "although", "we", "use", "different", "value", "match", "each", "color", "channel", "simplicity", "we", "explain", "process", "average", "value", "we", "consider", "convert", "pixel", "value", "through", "camera", "response", "curve", "where", "result", "irradiance", "adjust", "exposure", "level", "ratio", "-lrb-", "average", "12.2", "he/me", "-rrb-", "new", "irradiance", "value", "convert", "back", "through", "camera", "response", "curve", "new", "pixel", "value", "fig.", "show", "3-step", "process", "graphically", "conversion", "process", "may", "next", "do", "all", "he", "pixel", "value", "-lrb-", "from", "through", "4096", "-rrb-", "arrive", "pixel-ratio", "curve", "which", "give", "scaling", "factor", "convert", "each", "me", "pixel?s", "value", "corresponding", "pixel", "value", "he", "sensor", "same", "irradiance", "-lrb-", "fig.", "-rrb-", "practice", "separate", "pixel-ratio", "curve", "calculate", "each", "color", "-lrb-", "-rrb-", "Bayer", "pattern", "when", "compare", "pixel", "value", "between", "he", "me", "image", "-lrb-", "between", "me", "le", "image", "-rrb-", "we", "use", "pixel-ratio", "curve", "lookup", "table", "-lrb-", "lut", "-rrb-", "convert", "he", "pixel", "value", "less", "than", "4096", "me", "pixel", "value", "vice", "versa", "when", "he", "pixel", "value", "saturate", "we", "simply", "extend", "pixelratio", "curve", "use", "last", "value", "obtain", "-lrb-", "approximately", "-rrb-", "fig.", "show", "curve", "compute", "from", "raw", "camera", "datum", "although", "curve", "compute", "from", "linear", "best-fit", "could", "also", "use", "we", "call", "function", "-lrb-", "-rrb-", "where", "we", "pixel", "value", "three-step", "process", "describe", "earlier", "can", "reverse", "map", "me", "pixel", "value", "he", "pixel", "value", "write", "me?he", "-lrb-", "-rrb-", "-lrb-", "12.2", "-lrb-", "-rrb-", "-rrb-", "once", "irradiance", "level", "three", "image", "have", "be", "match", "we", "ready", "begin", "merging", "process", "explain", "we", "merge", "approach", "we", "assume", "two", "registered", "ldr", "image", "-lrb-", "one", "high-exposure", "image", "he", "second", "medium", "500 1000 1500 2000", "2500 3000 3500 4000", "4500", "Pixel", "Value", "Camera", "response", "curve", "curve", "show", "how", "camera", "convert", "scene", "irradiance", "pixel", "value", "compute", "what", "me", "pixel", "value", "should", "give", "he", "value", "he", "pixel", "value", "-lrb-", "-rrb-", "first", "convert", "scene", "irradiance", "-lrb-", "-rrb-", "which", "next", "divide", "we", "he/me", "attenuation", "ratio", "12.2", "new", "irradiance", "value", "-lrb-", "-rrb-", "convert", "through", "camera", "response", "curve", "expect", "me", "pixel", "value", "-lrb-", "-rrb-", "although", "graph", "approximately", "linear", "perfectly", "so", "because", "compute", "from", "raw", "datum", "without", "significant", "smoothing", "apply", "linear", "fit", "exposure", "image", "me", "-rrb-", "merge", "hdr", "image", "hdr", "we", "approach", "information", "high-exposure", "image", "he", "combine", "datum", "from", "next", "darker-exposure", "image", "me", "need", "reduce", "transition", "artifact", "describe", "earlier", "we", "algorithm", "work", "each", "pixel", "location", "-lrb-", "-rrb-", "look", "information", "from", "surrounding", "-lrb-", "2k", "-rrb-", "-lrb-", "2k", "-rrb-", "pixel", "neighborhood", "denote", "-lrb-", "-rrb-", "we", "implementation", "we", "use", "pixel", "neighborhood", "-lrb-", "-rrb-", "we", "define", "pixel", "saturate", "its", "value", "greater", "than", "90", "maximum", "pixel", "value", "-lrb-", "4096", "we", "sensor", "-rrb-", "we", "now", "specify", "algorithm", "each", "follow", "state", "pixel", "its", "neighborhood", "Case", "pixel", "under", "consideration", "he", "-lrb-", "-rrb-", "saturate", "he", "-lrb-", "-rrb-", "have", "saturated", "pixel", "so", "pixel", "value", "use", "as-is", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "case", "he", "-lrb-", "-rrb-", "saturate", "he", "-lrb-", "-rrb-", "have", "more", "saturated", "pixel", "although", "most", "hdr", "merge", "algorithm", "would", "use", "pixel", "as-is", "we", "find", "call", "question", "actual", "value", "we", "pixel", "due", "proximity", "effect", "-lrb-", "e.g.", "leakage", "pixel", "cross-talk", "sensor", "-rrb-", "we", "therefore", "blend", "between", "pixel", "value", "he", "-lrb-", "-rrb-", "one", "next", "darker-exposure", "me", "-lrb-", "-rrb-", "depend", "amount", "saturation", "present", "neighborhood", "do", "three", "step", "let", "set", "unsaturated", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "where", "number", "unsaturated", "pixel", "let", "he", "-lrb-", "-rrb-", "number", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "output", "pixel", "give", "blended", "value", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "-lrb-", "??", "-rrb-", "me?he", "-lrb-", "me", "-lrb-", "-rrb-", "-rrb-", "where", "we", "use", "pixel-ratio", "lut", "map", "me", "value", "he", "range", "blend", "measurement", "higher", "exposure", "he", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "base", "number", "saturated", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "we", "calculate", "ratio", "pixel", "value", "me", "image", "between", "unsaturated", "pixel", "neighborhood", "center", "pixel", "use", "map", "me", "ratio", "estimate", "actual", "value", "saturated", "pixel", "under", "consideration", "do", "four", "step", "case", "compute", "coefficient", "500 1000 1500 2000", "2500 3000 3500 4000", "4500", "he", "Pixel", "Value", "Pixel", "ratio", "curve", "blue", "curve", "obtain", "when", "3-step", "process", "-lrb-", "fig.", "-rrb-", "apply", "all", "red", "pixel", "value", "he", "image", "use", "camera", "response", "curve", "ratio", "between", "he", "me", "pixel", "value", "calculate", "red", "curve", "measure", "experimentally", "take", "ratio", "large", "set", "red", "he", "me", "pixel", "actual", "image", "capture", "we", "system", "two", "match", "reasonably", "validate", "we", "approach", "red", "curve", "use", "algorithm", "look-up", "table", "-lrb-", "lut", "-rrb-", "scale", "factor", "convert", "red", "me", "pixel", "value", "blend", "they", "smoothly", "red", "he", "pixel", "value", "similar", "curve", "be", "compute", "green", "blue", "channel", "compute", "ratio", "map", "ratio", "between", "center", "pixel", "each", "pixel", "neighborhood", "from", "me", "image", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "all", "pixel", "ME", "neighborhood", "me", "-lrb-", "-rrb-", "now", "compute", "estimate", "he", "-lrb-", "-rrb-", "saturated", "pixel", "scale", "unsaturated", "pixel", "value", "neighborhood", "he", "he", "-lrb-", "-rrb-", "ratio", "i?u", "compute", "he", "-lrb-", "step", "-rrb-", "finally", "blend", "estimate", "he", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "use", "equation", "similar", "case", "step", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "me?he", "-lrb-", "me", "-lrb-", "-rrb-", "-rrb-", "Case", "he", "-lrb-", "-rrb-", "saturated", "all", "pixel", "he", "-lrb-", "-rrb-", "saturate", "so", "we", "do", "have", "any", "valid", "information", "from", "highexposure", "image", "we", "algorithm", "combine", "datum", "from", "two", "sensor", "when", "three", "LDR", "image", "process", "above", "simply", "repeat", "second", "iteration", "substitute", "hdr", "he", "le", "me", "manner", "we", "merge", "datum", "from", "higher", "exposure", "we", "work", "we", "way", "toward", "lowest", "exposure", "only", "use", "datum", "from", "lower", "exposure", "when", "higher-exposure", "datum", "near", "saturation", "we", "find", "algorithm", "reduce", "artifact", "merged", "image", "considerably", "show", "Fig.", "output", "we", "algorithm", "hdr", "image", "can", "demosaice", "convert", "from", "pixel", "value", "irradiance", "use", "camera", "response", "curve", "similar", "fig.", "accounting", "all", "color", "channel", "final", "HDR", "full-color", "image", "may", "tonemapp", "commercial", "software", "package", "-lrb-", "e.g.", "fdrtool", "HDR", "expose", "Photomatix", "etc.", "-rrb-", "describe", "literature", "-lrb-", "e.g.", "-lsb-", "Smith", "et", "al.", "2006", "-rsb-", "-rrb-", "we", "implement", "prototype", "we", "design", "use", "three", "Silicon", "Imaging", "si-1920hd", "high-end", "cinema", "cmo", "sensor", "mount", "inside", "custom", "camera", "body", "sensor", "have", "1920", "1080", "pixel", "-lrb-", "micron", "square", "-rrb-", "standard", "Bayer", "color", "filter", "array", "can", "measure", "dynamic", "range", "around", "10", "stop", "-lrb-", "exclude", "noise", "-rrb-", "sensor", "be", "align", "aim", "camera", "small", "pinhole", "light", "source", "lock", "down", "he", "sensor", "adjust", "setscrew", "align", "me", "le", "sensor", "we", "prototype", "final", "image", "registration", "accurate", "less", "than", "micron", "rotation", "error", "less", "than", "0.1", "note", "general", "require", "registration", "accuracy", "modulo", "pixel", "array", "Bayer", "pattern", "because", "while", "pixel", "one", "color", "sensor", "must", "match", "same", "color", "another", "shift", "any", "number", "Bayer", "pattern", "can", "handle", "software", "we", "discuss", "how", "registration", "tolerance", "affect", "we", "result", "Sec", "camera", "body", "have", "Hasselblad", "lens", "mount", "allow", "use", "high-performance", "interchangeable", "commercial", "lens", "beamsplitter", "we", "current", "prototype", "system", "employ", "uncoated", "pellicle", "beamsplitter", "one", "sell", "Edmund", "Optics", "-lsb-", "part", "number", "nt39-482", "-rsb-", "however", "we", "find", "they", "robust", "we", "application", "we", "be", "able", "use", "we", "system", "successfully", "various", "location", "environmental", "condition", "picture", "complete", "camera", "prototype", "which", "cost", "less", "than", "us$", "15k", "part", "show", "Fig.", "we", "calculate", "camera?s", "response", "curve", "describe", "devebec", "Malik", "-lsb-", "1997", "-rsb-", "set", "bracket", "exposure", "from", "he", "sensor", "-lrb-", "fig.", "-rrb-", "we", "also", "measure", "ratio", "between", "he", "me", "pixel", "value", "experimentally", "each", "color", "channel", "observe", "match", "curve", "predict", "we", "3-step", "process", "-lrb-", "fig.", "-rrb-", "curve", "implement", "LUT", "translate", "value", "from", "me", "he", "sensor", "range", "merge", "similar", "process", "use", "obtain", "implement", "he/le", "ratio", "curve", "LE", "sensor", "value", "also", "adjust", "slight", "offset", "before", "be", "use", "merge", "algorithm", "account", "small", "amount", "stray", "light", "workflow", "process", "datum", "come", "from", "camera", "show", "alg", "most", "step", "self-explanatory", "typical", "hdr", "system", "non-uniform", "correction", "-lrb-", "nuc", "-rrb-", "step", "apply", "each", "image", "correct", "non-uniformity", "sensor", "2-point", "correction", "which", "involve", "take", "dark", "field", "image", "offset", "uniformly-lit", "diffuse", "scene", "gain", "HDR", "merge", "algorithm", "present", "Sec", "implement", "MATLAB", "demosaicing", "step", "use", "algorithm", "Malvar", "et", "al.", "-lsb-", "2004", "-rsb-", "we", "tonemapp", "final", "hdr", "image", "use", "hdr", "processing", "tool", "HDR", "expose", "Photomatix", "once", "system", "build", "we", "perform", "radiometric", "calibration", "test", "measure", "its", "dynamic", "range", "do", "we", "aim", "camera", "step", "array", "neutral", "density", "filter", "step", "stop", "each", "focus", "down", "sun?s", "light", "integrate", "sphere", "behind", "array", "provide", "uniform", "bright", "light", "source", "single", "frame", "image", "datum", "capture", "all", "sensor", "simultaneously", "exposure", "time", "1/30", "second", "fig.", "10", "show", "result", "experiment", "we", "current", "prototype", "we", "able", "clearly", "measure", "dynamic", "range", "over", "17", "equivalent", "over", "17", "photographic", "stop", "over", "100db", "more", "importantly", "we", "demonstrate", "we", "optical", "architecture", "can", "increase", "dynamic", "range", "partic", "demosaic", "hdr", "image", "produce", "full-color", "image", "White", "balancing", "color", "correction", "conversion", "hdr", "format", "tonemapping", "filter", "-lrb-", "saturation", "brightness", "sharpness", "-rrb-", "build", "final", "video", "from", "individual", "frame", "era", "test", "imaging", "set", "neutral", "density", "filter", "vary", "transmittance", "power", "we", "focus", "sun", "onto", "integrate", "sphere", "behind", "filter", "uniform", "illumination", "left", "tonemapped", "hdr", "image", "filter", "take", "we", "prototype", "right", "log", "plot", "measure", "intensity", "across", "HDR", "image", "-lrb-", "kink", "curve", "step", "10", "due", "wrap-around", "image", "-rrb-", "dynamic", "range", "camera", "can", "measure", "find", "last", "pair", "filter", "where", "noticeable", "difference", "measure", "intensity", "happen", "after", "filter", "17", "which", "mean", "we", "prototype", "can", "capture", "over", "17", "stop", "dynamic", "range", "ular", "camera", "sensor", "stop", "-lrb-", "7.7", "calculate", "from", "design", "-rrb-", "use", "available", "optical", "electronic", "component", "we", "also", "test", "we", "propose", "merge", "algorithm", "against", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "every", "scene", "we", "test", "we", "approach", "able", "produce", "higher", "quality", "image", "than", "algorithm", "because", "quantization", "artifact", "introduce", "widely-separated", "exposure", "image", "-lrb-", "see", "Figs.", "11", "-rrb-", "we", "find", "useful", "visualize", "merging", "process", "indicate", "which", "part", "image", "be", "be", "select", "from", "different", "ldr", "image", "-lrb-", "fig.", "11", "-rrb-", "finally", "fig.", "11", "13", "show", "frame", "from", "video", "capture", "we", "prototype", "tonemapped", "different", "way", "all", "video", "be", "capture", "1920", "1080", "24", "30", "fp", "show", "wide", "variety", "hdr", "scene", "complex", "movement", "lens", "effect", "UT", "TING", "orch", "scene", "fig.", "show", "oxy-acetylene", "torch", "cut", "steel", "plate", "tonemapp", "Photomatix", "spark", "correctly", "motion-blurred", "because", "simultaneous", "capture", "all", "sensor", "we", "also", "able", "capture", "detail", "bright", "molten", "metal", "dark", "surroundings", "same", "time", "suggest", "proposed", "system", "could", "use", "industrial", "manufacturing", "application", "ALLS", "ATER", "scene", "Fig.", "11", "show", "rapidly-moving", "color", "object", "all", "correctly", "motion-blurred", "bright", "sunlight", "uniform-color", "sphere", "present", "challenging", "gradient", "merge", "algorithm", "film", "snowy", "scene", "bright", "sunlight", "notoriously", "difficult", "especially", "dark-colored", "subject", "yet", "we", "system", "capture", "detail", "both", "snow", "dark", "area", "hip", "lame", "scene", "show", "intense", "flame", "set", "object", "direct", "sunlight", "deep", "shadow", "elting", "now", "very", "challenging", "scene", "impossible", "capture", "conventional", "camera", "because", "take", "from", "inside", "dimly-lit", "room", "look", "out", "bright", "outdoor", "scene", "we", "system", "can", "simultaneously", "capture", "both", "wall", "decoration", "inside", "structure", "cloud", "outside", "which", "several", "order", "magnitude", "brighter", "scene", "also", "feature", "focus-pull", "during", "shot", "from", "distant", "mountain", "foreground", "which", "difficult", "do", "system", "split", "beam", "before", "lens", "-lrb-", "e.g.", "-lsb-", "Cole", "Safai", "2010", "-rsb-", "-rrb-", "onkey", "show", "stuff", "animal", "bright", "white", "fur", "direct", "sunlight", "fine", "detail", "fur", "dark", "bucket", "accurately", "capture", "finally", "ighting", "orch", "scene", "show", "focus", "pull", "from", "bright", "background", "dimly-lit", "subject", "foreground", "bright", "torch", "flame", "add", "which", "provide", "flicker", "illumination", "subject?s", "otherwise", "darken", "face", "result", "all", "demonstrate", "quality", "image", "we", "prototype", "system", "can", "capture", "sensor", "alignment", "merge", "we", "prototype", "align", "so", "mis-registration", "between", "image", "less", "than", "micron", "we", "note", "mis-registration", "could", "cause", "sub-pixel", "artifact", "region", "sharp", "detail", "when", "datum", "merge", "HDR", "prior", "demosaicing", "process", "however", "general", "we", "spatially-blending", "merge", "algorithm", "help", "reduce", "use", "value", "from", "single", "sensor", "much", "possible", "only", "blend", "datum", "from", "other", "sensor", "when", "approach", "saturation", "limit", "region", "where", "blend", "mis-registered", "value", "would", "cause", "problem", "those", "near", "saturated", "pixel", "have", "sharp", "detail", "furthermore", "although", "can", "artifact", "from", "merge", "datum", "prior", "demosaicing", "color", "artifact", "cause", "merge", "datum", "postdemosaic", "-lrb-", "which", "happen", "area", "where", "only", "some", "color", "channel", "saturate", "-rrb-", "more", "serious", "because", "can", "happen", "large", "smooth", "region", "image", "overall", "mis-registration", "artifact", "can", "mitigate", "simply", "use", "sensor", "larger", "pixel", "also", "same", "challenge", "sensor", "alignment", "address", "3ccd", "system", "full-resolution", "color", "commercial", "product", "typically", "align", "very", "tight", "tolerance", "-lrb-", "much", "smaller", "than", "single", "pixel", "-rrb-", "we", "expect", "production", "version", "we", "proposed", "system", "benefit", "from", "similar", "commercial", "alignment", "technique", "merge", "algorithm", "noise", "reduction", "although", "we", "algorithm", "rely", "datum", "from", "only", "single", "sensor", "until", "approach", "saturation", "reduce", "quantization", "artifact", "noise", "from", "darker", "sensor", "might", "possible", "leverage", "information", "from", "other", "sensor", "appropriately", "further", "noise", "reduction", "course", "we", "image", "be", "space", "very", "closely", "together", "simple", "approach", "like", "Debevec", "Malik", "would", "reduce", "noise", "we", "spacing", "might", "possible", "combine", "we", "spatial-blending", "approach", "-lrb-", "which", "look", "neighborhood", "pixel", "just", "pixel", "be", "merge", "-rrb-", "merge", "algorithm", "weigh", "other", "image", "base", "noise", "content", "-lrb-", "e.g.", "-lsb-", "Granados", "et", "al.", "2010", "Hasinoff", "et", "al.", "2010", "-rsb-", "-rrb-", "produce", "better", "result", "flexible", "design", "we", "present", "simple", "blueprint", "hdr", "video", "imaging", "system", "use", "off-the-shelf", "component", "can", "produce", "high-quality", "hdr", "video", "footage", "use", "commerciallyavailable", "sensor", "make", "easy", "change", "they", "extend", "capability", "camera", "furthermore", "number", "sensor", "can", "extend", "minor", "modification", "optical", "path", "fig.", "12", "show", "optical", "design", "configure", "sensor", "which", "would", "allow", "camera", "have", "even", "higher", "dynamic", "range", "produce", "higher", "quality", "image", "same", "dynamic", "range", "first", "propose", "architecture", "might", "enable", "high-quality", "low-cost", "imaging", "because", "allow", "three", "cheap", "sensor", "limited", "dynamic", "range", "capture", "image", "dynamic", "range", "comparable", "single", "high-end", "sensor", "Limitations", "any", "hdr", "system", "-lrb-", "include", "human", "eye", "-rrb-", "ultimately", "limit", "its", "ability", "capture", "wide", "dynamic", "range", "veil", "glare", "optics", "-lsb-", "McCann", "Rizzi", "2007", "-rsb-", "we", "design", "compatible", "higher-performance", "lens", "which", "may", "help", "reduce", "veil", "glare", "we", "optical", "architecture", "also", "provide", "highexposure", "sensor", "only", "92", "light", "scene", "which", "compare", "favorably", "33", "provide", "previous", "internal", "beamsplitting", "system", "less", "light", "than", "would", "capture", "single", "sensor", "lens", "traditional", "camera", "3.1", "-rrb-", "could", "explore", "optical", "system", "would", "also", "benefit", "from", "improvement", "sensor", "alignment", "finally", "would", "useful", "develop", "software", "package", "hdr", "cinematography", "use", "system", "like", "one", "propose", "would", "give", "director", "photography", "same", "kind", "tool", "post-process", "use", "have", "disposal", "when", "light", "set", "conclusion", "we", "have", "present", "optical", "architecture", "highdynamic", "range", "video", "make", "efficient", "use", "incoming", "light", "produce", "cinema-quality", "image", "we", "also", "propose", "novel", "hdr", "merge", "algorithm", "minimize", "quantization", "artifact", "from", "LDR", "datum", "use", "highest-exposure", "datum", "until", "saturation", "before", "spatially-blending", "darker-exposure", "datum", "test", "we", "design", "we", "build", "work", "prototype", "system", "show", "image", "from", "video", "acquire", "system", "Acknowledgment", "we", "thank", "anonymous", "reviewer", "helpful", "comment", "improve", "final", "version", "paper", "image", "from", "video", "capture", "we", "camera", "sample", "tonemapp", "image", "demonstrate", "different", "imaging", "hdr", "effect", "ldr", "image", "capture", "three", "sensor", "we", "system", "show", "right", "scene", "from", "top", "bottom", "now", "OG", "ASH", "hip", "lame", "elting" ],
  "content" : "The extension of the dynamic range of digital images has been the subject of significant research in both academia and industry. Our proposed system is simple, uses only off-the-shelf technology, and is flexible in terms of the sensors that are used. Specifically, our HDR optical architecture: (1) captures optically-aligned, multipleexposure images simultaneously that do not need image manipulation to account for motion, (2) extends the dynamic range of available image sensors (by over 7 photographic stops in our current prototype), (3) is inexpensive to implement, (4) utilizes a single, standard camera lens, and (5) efficiently uses the light from the lens. To complement our system, we also propose a novel HDR imagemerging algorithm that: (1) combines images separated by more than 3 stops in exposure, (2) spatially blends pre-demosaiced pixel data to reduce unwanted artifacts, (3) produces HDR images that are radiometrically correct, and (4) uses the highest-fidelity (lowest quantized-noise) pixel data available. A common method for merging multiple LDR images into a single composite HDR image is the one of Debevec and Malik [1997], which first solves for the camera response curve that translates pixel values to the log of scene irradiance and then blends irradiances from the images together. These approaches typically work on each pixel of the final HDR image independently and use only the information contained within the respective pixel in each of the LDR images. Unlike these approaches, we propose to use additional information available in the neighborhood of a pixel to reduce the noise in our final irradiance estimate. Finally, others have presented algorithms for fusing the LDR images together without explicitly creating an HDR image first (e.g., [Agarwala et al. 2004; Mertens et al. 2008]). Because of space limits, we focus only on the principal technologies currently available for HDR video and refer interested readers to texts on the subject (e.g., [Myszkowski et al. 2008]) for more information. Although this method works well for static scenes, it is not well-suited for video because of the different moments in time and exposure lengths for each photograph, which result in varying amounts of motion blur and other timerelated effects. Here a beamsplitting prism breaks up the light into three parts, one for each sensor fitted with different filters. Designs that use absorptive filters like this one make inefficient use of light. Some approaches place an array of neutral-density filters over the individual pixels of the sensor with varying amounts of absorption (e.g., [Nayar and Mitsunaga 2000]), which can require complex demosaicing algorithms. If the sensor has a filter pattern with four differently-exposed pixels (one of the four fully exposed), then only 1 4 pixels would receive the full exposure level from the scene. In approaches similar to our own, the light in the camera is split with a pyramid-shaped mirror or refracting prism and redirected toward a set of sensors fitted with absorptive filters to produce images with different exposures (e.g., Harvey [1998], Aggarwal and Ahuja [2001; 2004], and Wang et al. [2005]). The designs of these previous systems all suffer from parallax error, due to the fact that the image-forming beam is split into spatially-distinct subsections; each individual sensor ?looks? through the camera lens from a slightly different angle. As shown in recent work on handheld plenoptic cameras (e.g., [Ng et al. 2005]), this provides each of the sensors with slightly different information, which significantly affects the imaging of scenes close to the camera. If Q Watts of radiative power enters the aperture of the camera, the three-way system shown in Fig. 2 (configured for the same dynamic range as ours) allows only 0.3622Q Watts to the sensors, wasting almost 2 3 of the available light. For example, McGuire et al. [2007] present a design tool to create efficient beamsplitting trees with separate lenses for each sensor, and show examples of HDR imaging. Finally, it is unclear how such as system could be developed into a single, hand-held unit. Our system places the beamsplitter behind a single camera lens, so it does not suffer from these limitations. Finally, there are early prototype HDR systems in industry eventually intended for commercial use, such as the SpheronVR HDRv camera [Spheron VR 2011 ]. However, their method for achieving HDR capture has not been published. While all the systems we mention in this section are capable of producing HDR video, to date no method for producing high-quality HDR video has been demonstrated that is robust and yet simple enough to be readily introduced to a wide commercial audience or implemented in a modern optics laboratory. Our optical architecture is based on beamsplitters located between the camera lens and the sensors, which have been used in previous HDR camera designs as well as in 3-sensor color-splitting cameras [Kell and Sziklai 1951] 1 . Like these previous methods, our optical system uses a set of partially-reflecting surfaces to split the light from a single photographic lens so that it is focused onto three\n        1 The optical splitting prisms used in 3CCD cameras employ dichroic filters to split the incoming light from a camera lens into red, green, and blue portions so that each color is imaged onto its own image sensor. imaging sensors simultaneously. This design, shown in Fig. 3 , allows us to capture HDR images using most of the light entering the camera. We use the terms ?high,? ?medium,? and ?low? exposure (HE, ME, LE, respectively) to refer to the sensors based on the amount of light each one receives. The optical splitting system in our current implementation uses two uncoated, 2-micron thick plastic beamsplitters which rely on Fresnel reflections at air/plastic interfaces so their actual transmittance/reflectance (T/R) values are a function of angle. In our arrangement, the first beamsplitter is at a 45 ? angle and has an approximate T/R ratio of 92/8, which means that 92% of the light from the camera lens is transmitted through the first beamsplitter and focused directly onto the high-exposure (HE) sensor 2 . This beamsplitter reflects 8% of the light from the lens upwards, as shown in Fig. 3 , toward the second uncoated beamsplitter, which has the same optical properties as the first but is positioned at a 90 ? angle to the light path and has an approximate T/R ratio of 94/6. Of the 8% of the total light that is reflected upwards, 94% (or 7.52% of the total light) is transmitted through the second beamsplitter and focused onto the medium-exposure (ME) sensor. With this arrangement, the HE, ME and LE sensors capture images with 92%, 7.52%, and 0.44% of the total light gathered by the camera lens, respectively. Therefore, the HE and ME exposures are separated by 12.2? (3.61 stops) and the ME and LE are separated by 17.0? (4.09 stops), which means that this configuration is designed to extend the dynamic range of the sensor by 7.7 stops. This beamsplitter arrangement makes our design light efficient: a negligible 0.04% of the total light gathered by the lens is wasted. It also allows all three sensors to ?see? the same scene, so all three images are optically identical except for their light levels. Of course, the ME image has undergone an odd number of reflections and so it is flipped left-right compared to the other images, but this is fixed easily in software. The three sensors are gen-locked to capture perfectly synchronized video frames with identical exposure times. 2 Since T/R is also dependent on the wavelength of the light, we calculate T/R values for the full visible spectrum and integrate over the R, G, and B filter spectra in the Bayer pattern to arrive at separate T/R values for each color channel for use in our design and implementation. result in transmittance values of 91.85% and 92.38%, respectively. Because the exact transmission/reflection properties of our beamsplitters vary with angle, we examine how these might vary over the area of the sensor by simulating the proposed optical architecture in ZEMAX [2011]. To calculate the range of transmittance values as a function of angle, we examine the largest angular variation possible on the pellicle beamsplitter. Unlike these approaches, our system?s internal beamsplitters receive light in a much smaller range of field angles because of the geometrical configuration of the system, shown to scale in Fig. 4 . In our case, the top-left and bottom-right corner points on the sensor have chief-ray angles at the pellicle of 46.4 ? and 43.7 ? , a difference of 2.7 ? . At f/2.8, each of these two points receives a ?10 ? cone of rays from the lens, shown in blue and red (these cones are constant in angle over the entire sensor). We calculate the transmittance of the beamsplitter by integrating over this cone of rays using a ZEMAX simulation with 1 million random rays on a 2-micron thick, uncoated plastic pellicle beamsplitter, which yields a transmittance of 91.85% for the top-left and 92.38% for the bottom-right points, a difference of about 0.5%, and close to the 92% value we used in our design calculations. Polarization of the incident light might affect the transmission properties of the beamsplitter as well. Although our simulations were all done with unpolarized light, it is possible to encounter linearly polarized light in outdoor scenes (e.g., from glancing reflections off water), which may change the exposure difference between sensors. However, in practice we did not see such polarization effects in the scenes we captured. We note that all of these effects may be reduced or eliminated by using a thin-film coating on the beamsplitter. This thin-film coating could be designed to have more constant transmission properties over the range of angles in the system or to reduce polarization effects. An examination of different beamsplitter coatings to address these factors is a topic for future work. Advantages of the proposed optical splitting system are that its cost and complexity are relatively low, and it is compatible with standard camera lenses. The compact light path allows integration into a single hand-held unit, something difficult to do with designs that place the beamsplitters outside the lens [McGuire et al. 2007; Cole and Safai 2010]. The optical architecture is also flexible in terms of the kind of sensor used. The use of low-cost sensors, for example, could allow the design to be integrated into consumer electronics and bring HDR video to a wide audience. In addition to the optical architecture, we also propose a novel algorithm for merging together images acquired with our system. In order to automatically create HDR images from the widely-separated component LDR images without artifacts, we found it necessary to depart from the standard methods for merging HDR images. 4.1 Limitations of current approaches Most previous algorithms for merging HDR images from a set of LDR images with different exposures typically do so after demosaicing the LDR images and merge data pixel-by-pixel without taking neighboring pixel information into account. For this discussion, we focus on the original merging algorithm proposed by Debevec and Malik [1997]. Although this algorithm works well when the LDR images have a small exposure separation, we found the quality of the final HDR image degrades when the LDR images are separated by more than 3 photographic stops. To capture the widest dynamic range possible with the smallest number of camera sensors, it is necessary to position the LDR images further apart in exposure in our system than with traditional HDR acquisition methods. Devebec and Malik?s merging algorithm can yield undesired artifacts when applied to our camera?s data because of quantization and noise effects, as shown in Fig. 5 . These TMO?s amplify small gradient differences in the image to make them visible when the dynamic range is compressed, amplifying merging artifacts as well. Fig. 6 shows the underlying cause for these artifacts by presenting the range of scene illumination measured by the three sensors in our architecture. For illustration, we simplify the system with 4-bit sensors (as opposed to the 12-bit sensors in our implementation), which measure only 16 unique brightness values and we separate the sensors by only 1 stop (a factor of 2) in exposure. Since CMOS sensors exhibit an approximately linear relationship between incident exposure and their output value, we graph the values from the three sensors as a linear function of incident irradiance instead of the traditional logarithmic scale. In this figure, we see that the LE sensor measures the scene irradiance more coarsely than the other two sensors. Quantization problems when merging LDR images. This figure shows the source of artifacts when merging our LDR images (simplified here to 4-bit sensors) with algorithms that always use data from all three sensors with simple weighting functions, such as that of Debevec and Malik. In this case, data from each sensor is weighted with a triangle function as shown, so there are non-zero contributions from the LE sensor at low brightness values (like the sample illumination level indicated), even though the data from the LE sensor is quantized more coarsely than that of the HE sensor. Our approach, in contrast, uses data from the higherexposure sensor as much as possible and blends in data from the next darker sensor when near saturation. fore the LE sensor records a single increment. In addition, there is always some small amount of noise in the pixel values, and an error of ?1 in the LE sensor spans a 12 value range in the HE sensor for this example. More importantly, our algorithm not only examines individual pixels when merging the LDR images, but also takes into account neighboring pixels that might provide additional information to help in the denoising process. While not a new idea, one key aspect of our image-merging algorithm is to use pixel data exclusively from the brightest, most well-exposed sensor possible. Therefore, we use pixels from the HE image as much as possible, and only use pixels in the ME image if the HE pixel is close to saturation. If the corresponding ME pixel is below the saturation level, it is multiplied by a factor that adjusts it in relation to the HE pixel based on the camera?s response curve ( Fig. 7 ), given that the ME pixel receives 12.2? less irradiance than the HE pixel. If the corresponding ME pixel is above the saturation level, then we simply apply a similar process to the same pixel in the low-exposure LE image. When merging data between sensors, it may seem sufficient to follow a na?ve ?winner take all? approach and exclusively use the val- ues from the HE sensor until they become saturated and then simply switch to the next sensor [ JAI 2009 ]. We found that this does not work well in practice because it results in banding artifacts where transitions occur. To do this, our algorithm scans a region around the pixel being evaluated. If any neighboring pixels in this region are saturated, then the pixel under consideration may be subject to pixel crosstalk or leakage, and the algorithm will estimate a value for the pixel based on its neighbors as described below. Algorithm details ? In our approach, HDR-merging is performed prior to demosaicing the individual Bayer color filter array images, because we found that demosaicing can corrupt colors in saturated regions (also noted by Ajdin et al. [2008]). If the image is demosaiced before being merged into HDR, the demosaiced orange color will be computed from saturated red-pixel data and non-saturated green/blue-pixel data. As a result, the hue of the orange section will be incorrectly reproduced. Since the images are merged prior to the demosaicing step, our algorithm works with pixel values instead of irradiance. To produce a radiometrically-correct HDR image, we must correctly match the irradiance levels of the HE, ME, and LE sensors using the appropriate beamsplitter transmittance values for each pixel color, since these change slightly as a function of wavelength. Although we use different values to match each of the color channels, for simplicity we explain the process with average values. We consider converting a pixel value through the camera response curve, where the resulting irradiance is adjusted by the exposure level ratio (average of 12.2? for HE/ME), and this new irradiance value is converted back through the camera response curve to a new pixel value. Fig. 7 shows this 3-step process graphically. This conversion process may next be done for all HE pixel values (from 1 through 4096), to arrive at a pixel-ratio curve, which gives the scaling factor for converting each ME pixel?s value to the corresponding pixel value on the HE sensor for the same irradiance ( Fig. 8 ). In practice, separate pixel-ratio curves are calculated for each color (R,G,B) in the Bayer pattern. When comparing pixel values between HE and ME images (or between ME and LE images), we use the pixel-ratio curves as lookup tables (LUTs) to convert HE pixel values less than 4096 into ME pixel values, or vice versa. When the HE pixel values are saturated, we simply extend the pixelratio curve using the last value obtained there (approximately 8). Fig. 7 shows the curve computed from the raw camera data, although a curve computed from a linear best-fit could also be used. If we call this function f (x), where x is our pixel value, then the three-step process described earlier can be reversed to map ME pixel values to HE pixel values, written as g ME?HE (x) = f ?1 (12.2f (x)). Once the irradiance levels of the three images have been matched, we are ready to begin the merging process. To explain our merging approach, we assume two registered LDR images (one high-exposure image I HE and a second medium- 3 0 4 1 0 500 1000 1500 2000 2500 3000 3500 4000 4500 Pixel Value Camera response curve. This curve shows how the camera converts scene irradiance into pixel values. To compute what the ME pixel value should be for a given HE value, the HE pixel value (1) is first converted to a scene irradiance (2), which is next divided by our HE/ME attenuation ratio of 12.2. This new irradiance value (3) is converted through the camera response curve into the expected ME pixel value (4). Although this graph is approximately linear, it is not perfectly so because it is computed from the raw data, without significant smoothing or applying a linear fit. exposure image I ME ) that are to be merged into an HDR image I HDR . Our approach starts with the information in the high-exposure image I HE and then combines in data from the next darker-exposure image I ME , as needed. To reduce the transition artifacts described earlier, our algorithm works on each pixel location (x, y) by looking at the information from the surrounding (2k + 1) ? (2k + 1) pixel neighborhood, denoted as N (x, y). In our implementation, we used a 5 ? 5 pixel neighborhood (k = 2), and we define a pixel to be saturated if its value is greater than 90% of the maximum pixel value (4096, for our sensor). We now specify the algorithm for each of the following 4 states for the pixel and its neighborhood: Case 1: The pixel under consideration I HE (x, y) is not saturated and N HE (x, y) has no saturated pixels, so the pixel value is used as-is: I HDR (x, y) = I HE (x, y). Case 2: I HE (x, y) is not saturated, but N HE (x, y) has 1 or more saturated pixels. Although most HDR merging algorithms would use this pixel as-is, we find that this calls into question the actual value of our pixel due to proximity effects (e.g., leakage or pixel cross-talk on the sensor). We therefore blend between the pixel value at I HE (x, y) and the one at the next darker-exposure I ME (x, y) depending on the amount of saturation present in the neighborhood. This is done in three steps: 1. Let U be the set of unsaturated pixels in neighborhood N HE (x, y), where |U | is the number of unsaturated pixels. Let |N HE (x, y)| be the number of pixels in neighborhood N HE (x, y). The output pixel is then given by the blended value: I HDR (x, y) = ?I HE (x, y)+(1??)g ME?HE (I ME (x, y)), where we use the pixel-ratio LUT to map the ME value into the HE range. This blends the measurement at the higher exposure I HE (x, y) with I ME (x, y) based on the number of saturated pixels in the neighborhood N HE (x, y). We calculate the ratios of pixel values in the ME image between the unsaturated pixels in the neighborhood and the center pixel, and use this map of ME ratios to estimate the actual value of the saturated pixel under consideration. This is done in four steps: 1. As in Case 2, compute U , |U |, and the coefficient ? 0 0 500 1000 1500 2000 2500 3000 3500 4000 4500 HE Pixel Value Pixel ratio curve. The blue curve is obtained when the 3-step process ( Fig. 7 ) is applied to all red pixel values in the HE image using the camera response curve, and the ratio between HE and ME pixel values is calculated. The red curve is measured experimentally by taking the ratio of a large set of red HE and ME pixels in actual images captured by our system. The two match reasonably, validating our approach. This red curve is used by the algorithm as a look-up table (LUT) of scale factors to convert red ME pixel values to blend them smoothly with red HE pixel values. Similar curves were computed for the green and blue channels. Compute a ratio map R of the ratios between the center pixel and each pixel of the neighborhood from the ME image: R(x, y) i = I ME (x, y)/N ME (x, y) i , for all pixels i in the ME neighborhood N ME (x, y). Now compute an estimated I ? HE (x, y) for the saturated pixel by scaling the unsaturated pixel values in the neighborhood I N ? HE HE (x, with y) the = |U ratios 1 | P i?U computed R i N HE in (x, step y) i 2: . Finally, blend estimated I ? HE (x, y) with I ME (x, y) using an equation similar to that of Case 2, step 3: I HDR (x, y) = ? I ? HE (x, y) + (1 ? ?)g ME?HE (I ME (x, y)). Case 4: I HE (x, y) is saturated and all pixels in N HE (x, y) are saturated, so we do not have any valid information from the highexposure image. This is our algorithm for combining data from two sensors. When there are three LDR images, the process above is simply repeated in a second iteration, substituting I HDR for I HE and I LE for I ME . In this manner, we merge data from the higher exposures as we work our way toward the lowest exposure, and only use data from lower exposures when the higher-exposure data is at or near saturation. We found that this algorithm reduces artifacts in the merged image considerably, as shown in Fig. 5 . The output of our algorithm is an HDR image that can be demosaiced and converted from pixel values to irradiance using a camera response curve similar to that of Fig. 7 accounting for all 3 color channels. The final HDR full-color image may then be tonemapped with commercial software packages (e.g., FDRTools, HDR Expose, Photomatix, etc.), or as described in the literature (e.g., [Smith et al. 2006]). We implemented a prototype of our design using three Silicon Imaging SI-1920HD high-end cinema CMOS sensors mounted inside a custom camera body. These sensors have 1920 ? 1080 pixels (5 microns square) with a standard Bayer color filter array, and can measure a dynamic range of around 10 stops (excluding noise). The sensors were aligned by aiming the camera at small pinhole light sources, locking down the HE sensor and then adjusting setscrews to align the ME and LE sensors. In our prototype, the final image registration was accurate to less than  5 microns with a rotation error of less than 0.1 ? . Note that, in general, the required registration accuracy is modulo the 2 ? 2 pixel array of the Bayer pattern, because while pixels of one color in a sensor must match with the same color in another, a shift of any number of 2 ? 2 Bayer patterns can be handled in software. We discuss how registration tolerances affect our results in Sec. The camera body has a Hasselblad lens mount to allow the use of high-performance, interchangeable commercial lenses. For beamsplitters, our current prototype system employs uncoated pellicle beamsplitters, such as the ones sold by Edmund Optics [part number NT39-482]. However, we found them to be robust for our application, as we were able to use our system successfully in various locations and environmental conditions. A picture of the completed camera prototype, which cost less than US$15k in parts, is shown in Fig. 9 . We calculated the camera?s response curve as described by Devebec and Malik [1997] with a set of bracketed exposures from the HE sensor ( Fig. 7 ). We also measured the ratio between the HE and ME pixel values experimentally for each color channel and observed that it matched the curve predicted by our 3-step process ( Fig. 8 ). This curve was implemented as a LUT to translate values from the ME to the HE sensor range for merging, and a similar process was used to obtain and implement the HE/LE ratio curve. The LE sensor values are also adjusted with a slight offset before being used in the merging algorithm to account for a small amount of stray light. The workflow for processing the data coming from the camera is shown in Alg. Most of the steps are self-explanatory and are typical for HDR systems. The non-uniform correction (NUC) step is applied to each image to correct for non-uniformities in the sensor. This is a 2-point correction, which involves taking a dark field image for offset and a uniformly-lit diffuse scene for gain. The HDR merging algorithm presented in Sec. 4 was implemented in MATLAB. The demosaicing step used the algorithm of Malvar et al. [2004], and we tonemapped the final HDR images using HDR processing tools such as HDR Expose and Photomatix. Once the system was built, we performed a radiometric calibration test to measure its dynamic range. To do this, we aimed the camera at a stepped array of neutral density filters, in steps of 1 stop each, and focused down the sun?s light to an integrating sphere behind the array to provide a uniform, bright light source. A single frame of image data was captured with all 3 sensors simultaneously at an exposure time of 1/30 second. Fig. 10 shows the result of this experiment. In our current prototype, we are able to clearly measure a dynamic range of over 2 17 to 1, equivalent to over 17 photographic stops or over 100dB. More importantly, we demonstrate that our optical architecture can increase the dynamic range of these partic- 4 4: Demosaic HDR image to produce full-color image 5: White balancing, color correction, conversion to HDR format 6: Tonemapping and filtering (saturation, brightness, sharpness) 7: Build final video from individual frames era was tested by imaging a set of neutral density filters varying in transmittance by powers of 2. We focused the sun onto an integrating sphere behind the filters for uniform illumination. On the left is a tonemapped HDR image of the filters taken by our prototype. On the right is a log plot of the measured intensity across the HDR image (the kink in the curve at step 10 is due to the wrap-around in the image). The dynamic range of the camera can be measured by finding the last pair of filters where there is no noticeable difference in the measured intensity. This happens after filter 17, which means our prototype can capture over 17 stops of dynamic range. ular camera sensors by 7 stops (7.7 was calculated from the design) using available optical and electronic components. We also tested our proposed merging algorithm against that of Debevec and Malik [1997]. In every scene we tested, our approach was able to produce higher quality images than this algorithm because of the quantization artifacts it introduces with widely-separated exposure images (see Figs. 5 and 11). We found it useful to visualize the merging process by indicating which parts of the image were being selected from the different LDR images ( Fig. 11 ). Finally, Figs. 1, 11, and 13 show frames from video captured with our prototype and tonemapped in different ways. All videos were captured at 1920 ? 1080 at 24 or 30 fps and show a wide variety of HDR scenes with complex movement and lens effects. The C UT TING T ORCH scene in Fig. 1 shows an oxy-acetylene torch cutting a steel plate, tonemapped with Photomatix. The sparks are correctly motion-blurred because of the simultaneous capture on all sensors. We are also able to capture detail in the bright molten metal and the dark surroundings at the same time, suggesting that the proposed system could be used in industrial and manufacturing applications. The B ALLS & W ATER scene in Fig. 11 shows rapidly-moving, colored objects all correctly motion-blurred in bright sunlight. The uniform-color spheres present a challenging gradient for the merging algorithm. Filming snowy scenes in bright sunlight is notoriously difficult, especially with dark-colored subjects, yet our system captures detail in both the snow and the dark areas. The G O -C HIPS & F LAME scene shows an intense flame and a set of objects in direct sunlight and deep shadow. M ELTING S NOW is a very challenging scene that is impossible to capture with conventional cameras, because it was taken from inside a dimly-lit room looking out at a bright outdoor scene. Our system can simultaneously capture both the wall decoration inside and the structure of the clouds outside which are several orders of magnitude brighter. This scene also features a focus-pull during the shot from the distant mountains to the foreground, which is difficult to do with systems that split the beam before the lens (e.g., [Cole and Safai 2010]). M ONKEY shows a stuffed animal with bright white fur in direct sunlight. Fine details in the fur and the dark bucket are accurately captured. Finally, the L IGHTING T ORCH scene shows a focus pull from a bright background to a dimly-lit subject in the foreground. A bright torch flame is added, which provides flickering illumination to the subject?s otherwise darkened face. These results all demonstrate the quality of images that our prototype system can capture. Sensor alignment and merging ? Our prototype was aligned so that mis-registration between images was less than 5 microns. We note that mis-registration could cause sub-pixel artifacts in regions of sharp detail when the data is merged into HDR prior to the demosaicing process. However, in general, our spatially-blending merging algorithm helps reduce these by using values from a single sensor as much as possible, and only blending in data from other sensors when it approaches saturation. This limits the regions where blending mis-registered values would cause a problem to those that are near saturated pixels and have sharp detail. Furthermore, although there can be artifacts from merging the data prior to demosaicing, the color artifacts caused by merging data postdemosaic (which happens in areas where only some color channels are saturated) are more serious because they can happen in large, smooth regions in the image. Overall, mis-registration artifacts can be mitigated by simply using sensors with larger pixels. Also, the same challenge of sensor alignment is addressed in 3CCD systems for full-resolution color. These commercial products are typically aligned to very tight tolerances (much smaller than a single pixel), and we expect production versions of our proposed system to benefit from similar commercial alignment techniques. Merging algorithm and noise reduction ? Although our algorithm relies on data from only a single sensor until it approaches saturation to reduce quantization artifacts and noise from darker sensors, it might be possible to leverage information from the other sensors appropriately for further noise reduction. Of course, if our images were spaced very closely together, simple approaches like that of Debevec and Malik would reduce noise. With our spacing, it might  be possible to combine our spatial-blending approach (which looks at a neighborhood of pixels, not just the pixel being merged) with merging algorithms that weigh other images based on their noise content (e.g., [Granados et al. 2010; Hasinoff et al. 2010]) to produce better results. Flexible design ? We present a simple blueprint for an HDR video imaging system using off-the-shelf components that can produce high-quality HDR video footage. The use of commerciallyavailable sensors makes it easy to change them to extend the capabilities of the camera. Furthermore, the number of sensors can be extended with minor modifications to the optical path. Fig. 12 shows an optical design configured with 5 sensors, which would allow the camera to have an even higher dynamic range, or to produce higher quality images at the same dynamic range. First, the proposed architecture might enable high-quality, low-cost imaging because it allows three cheap sensors with limited dynamic range to capture images with dynamic range comparable to that of a single, high-end sensor. Limitations ? Any HDR system (including the human eye) is ultimately limited in its ability to capture wide dynamic range by veiling glare in the optics [McCann and Rizzi 2007]. Our design is compatible with higher-performance lenses, which may help reduce veiling glare. Our optical architecture also provides the highexposure sensor with only 92% of the light of the scene, which compares favorably with the 33% provided by previous internal beamsplitting systems, but is less light than would be captured with a single sensor and lens in a traditional camera. 3.1) could be explored. The optical system would also benefit from improvements in the sensor alignment. Finally, it would be useful to develop software packages for HDR cinematography for use with systems like the one proposed. These would give directors of photography the same kinds of tools in post-process that they are used to having at their disposal when lighting the set. In conclusion, we have presented an optical architecture for highdynamic range video that makes efficient use of the incoming light and produces cinema-quality images. We also proposed a novel HDR merging algorithm that minimizes quantization artifacts from the LDR data by using the highest-exposure data until saturation before spatially-blending in darker-exposure data. To test our design, we built a working prototype of the system and showed images from video acquired with the system. Acknowledgment ? We thank the anonymous reviewers for their helpful comments that improved the final version of the paper. Images from videos captured with our camera. These are sample tonemapped images that demonstrate different imaging HDR effects. The LDR images captured by three sensors in our system are shown on the right. The scenes are, from top to bottom, S NOW D OG , W ASH M E , G O C HIPS & F LAME , M ELTING S , M , and L T .",
  "resources" : [ ]
}