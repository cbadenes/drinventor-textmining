{
  "uri" : "sig2014-a37-su_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a37-su_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Estimating Image Depth Using Shape Collections",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Hao-Su",
      "name" : "Hao",
      "surname" : "Su"
    }, {
      "uri" : "http://drinventor/Qixing-Huang",
      "name" : "Qixing",
      "surname" : "Huang"
    }, {
      "uri" : "http://drinventor/Niloy J.-Mitra",
      "name" : "Niloy J.",
      "surname" : "Mitra"
    }, {
      "uri" : "http://drinventor/Yangyan-Li",
      "name" : "Yangyan",
      "surname" : "Li"
    }, {
      "uri" : "http://drinventor/Leonidas J.-Guibas",
      "name" : "Leonidas J.",
      "surname" : "Guibas"
    } ]
  },
  "bagOfWords" : [ "dbb9797eacf04351a22d9cc44e6114e9339e33af7815c218d10ea3d09a9e2e93", "p3d", "10.1145", "2601097.2601159", "name", "identification", "possible", "estimate", "image", "depth", "use", "shape", "collection", "Hao", "Su", "Qixing", "Huang", "Niloy", "J.", "Mitra", "Stanford", "University", "University", "College", "London", "Figure", "we", "attribute", "single", "2d", "image", "object", "-lrb-", "left", "-rrb-", "depth", "transport", "information", "from", "3d", "shape", "deformation", "subspace", "learn", "analyze", "network", "related", "different", "shape", "-lrb-", "middle", "-rrb-", "visualization", "we", "color", "code", "estimate", "depth", "value", "increase", "from", "red", "blue", "-lrb-", "right", "-rrb-", "image", "while", "easy", "acquire", "view", "publish", "share", "lack", "critical", "depth", "information", "pose", "serious", "bottleneck", "many", "image", "manipulation", "editing", "retrieval", "task", "paper", "we", "consider", "problem", "add", "depth", "image", "object", "effectively", "lifting", "back", "3d", "exploit", "collection", "align", "3d", "model", "related", "object", "we", "key", "insight", "even", "when", "image", "object", "contain", "shape", "collection", "network", "shape", "implicitly", "characterize", "shape-specific", "deformation", "subspace", "regularize", "problem", "enable", "robust", "diffusion", "depth", "information", "from", "shape", "collection", "input", "image", "we", "evaluate", "we", "fully", "automatic", "approach", "diverse", "challenging", "input", "image", "validate", "result", "against", "kinect", "depth", "reading", "demonstrate", "several", "imaging", "application", "include", "depth-enhanced", "image", "editing", "image", "relighting", "cr", "category", "i.", "3.5", "-lsb-", "Computer", "Graphics", "-rsb-", "computational", "geometry", "object", "modeling?geometric", "algorithm", "keyword", "data-driven", "shape", "analysis", "pose", "estimation", "depth", "estimation", "image", "retrieval", "shape", "collection", "Links", "dl", "pdf", "EB", "IDEO", "ata", "introduction", "image", "remain", "far", "most", "popular", "visual", "medium", "nowadays", "easy", "acquire", "distribute", "contain", "rich", "visual", "detail", "can", "easily", "view", "understand", "result", "ubiquitous", "web", "2d", "projection", "we", "3d", "world", "however", "may", "lack", "certain", "semantical", "information", "example", "important", "part", "object", "may", "occluded", "depth", "datum", "typically", "miss", "pose", "serious", "challenge", "application", "involve", "image", "recognition", "manipulation", "editing", "etc.", "could", "greatly", "benefit", "from", "omit", "information", "hence", "strong", "motivation", "lift", "image", "3d", "infer", "attribute", "lose", "projection", "paper", "we", "specifically", "interested", "infer", "depth", "visible", "object", "area", "key", "coordinate", "miss", "projection", "problem", "recover", "depth", "from", "single", "image", "naturally", "ill-posed", "various", "prior", "have", "be", "propose", "regularization", "most", "common", "classical", "approach", "match", "input", "image", "set", "3d", "object", "database", "-lrb-", "i.e.", "prior", "-rrb-", "use", "best", "matching", "shape", "fill", "miss", "depth", "information", "however", "large-scale", "deployment", "method", "fundamentally", "limit", "because", "only", "limited", "number", "3d", "model", "available", "most", "often", "we", "do", "even", "have", "3d", "model", "same", "sufficiently", "similar", "object", "from", "which", "image", "take", "paper", "we", "consider", "problem", "estimate", "depth", "image", "object", "exploit", "novel", "joint", "fashion", "collection", "3d", "model", "related", "largely", "different", "object", "-lrb-", "see", "Figure", "-rrb-", "key", "we", "approach", "estimation", "correspondence", "between", "image", "multiple", "model", "help", "correspondence", "estimate", "between", "model", "themselves", "we", "address", "depth", "inference", "problem", "its", "purest", "form", "where", "we", "assume", "object", "image", "have", "be", "segmented", "from", "its", "background", "-lrb-", "image", "now", "commonplace", "shopping", "web", "site", "-rrb-", "while", "we", "3d", "model", "typically", "untextured", "come", "from", "shape", "collection", "Trimble", "3d", "warehouse", "we", "image-based", "shape-driven", "modeling", "technique", "fully", "automatic", "reconstruct", "3d", "point", "cloud", "from", "image", "object", "algorithm", "consist", "preprocessing", "stage", "which", "align", "input", "shape", "each", "other", "learn", "deformation", "model", "each", "shape", "reconstruction", "stage", "which", "use", "continuous", "optimization", "recover", "image", "object", "pose", "reconstruct", "point", "cloud", "from", "image", "align", "relevant", "3d", "model", "extract", "from", "collection", "we", "show", "how", "formulate", "appropriate", "objective", "function", "how", "obtain", "initial", "solution", "how", "effectively", "refine", "solution", "use", "alternate", "optimization", "we", "approach", "we", "jointly", "match", "depth-augmented", "image", "i.e.", "popup", "point", "cloud", "image", "group", "related", "shape", "collection", "we", "pose", "task", "joint", "non-rigid", "registration", "problem", "which", "each", "shape", "can", "deform", "formulation", "have", "two", "key", "feature", "first", "contrast", "utilize", "single", "similar", "shape", "incorporate", "collection", "similar", "shape", "offer", "better", "coverage", "relevant", "neighborhood", "shape", "space", "second", "since", "we", "have", "already", "align", "3d", "model", "each", "other", "enable", "we", "apply", "consistency", "constraint", "-lsb-", "Kim", "et", "al.", "2012a", "Huang", "Guibas", "2013", "-rsb-", "regularize", "image", "3d", "model", "matching", "use", "shape-shape", "correspondence", "joint", "non-rigid", "registration", "formulation", "we", "introduce", "key", "concept", "deformation", "prior", "which", "govern", "deformation", "each", "shape", "-lrb-", "c.f.", "-lsb-", "Averkiou", "et", "al.", "2014", "-rsb-", "-rrb-", "intuitively", "we", "aim", "preserve", "key", "structural", "property", "each", "shape", "deformation", "so", "round", "shape", "stay", "round", "left-to-right", "symmetry", "preserve", "etc.", "particular", "instead", "detect", "property", "form", "each", "shape", "alone", "which", "turn", "out", "unreliable", "we", "learn", "they", "from", "optimal", "deformation", "each", "shape", "other", "shape", "test", "performance", "propose", "approach", "we", "have", "create", "benchmark", "dataset", "consist", "Microsoft", "Kinect", "scan", "various", "category", "object", "include", "chair", "table", "lamp", "cup", "experimental", "result", "show", "propose", "approach", "recover", "depth", "information", "close", "kinect", "scan", "significantly", "more", "accurate", "than", "state-of-the-art", "image-based", "modeling", "technique", "moreover", "propose", "approach", "robust", "variation", "texture", "lighting", "condition", "we", "demonstrate", "depth-enhanced", "image", "editing", "illustrate", "possibility", "offer", "we", "approach", "addition", "we", "show", "we", "work", "key", "intermediate", "step", "towards", "goal", "obtain", "full", "3d", "model", "use", "popup", "point", "cloud", "input", "we", "can", "reconstruct", "certain", "case", "full", "mesh", "exploit", "shape", "symmetry", "learn", "from", "shape", "network", "contribution", "we", "present", "first", "best", "we", "knowledge", "fully", "automatic", "method", "utilize", "network", "related", "different", "3d", "object", "order", "reconstruct", "depth", "information", "from", "single", "image", "object", "key", "novelty", "show", "how", "single", "modestly-sized", "shape", "network", "can", "help", "infer", "depth", "information", "variety", "image", "object", "same", "class", "use", "learn", "deformation", "model", "base", "align", "shape", "network", "compensate", "fact", "image", "from", "model", "directly", "present", "database", "regularize", "model", "deformation", "use", "multi-way", "3d", "alignment", "between", "initial", "image", "point", "cloud", "shape", "neighborhood", "shape", "network", "process", "extract", "depth", "information", "image", "we", "also", "discover", "good", "correspondence", "between", "image", "network", "shape", "enable", "we", "connect", "image", "network", "transfer", "complementary", "information", "back", "forth", "example", "information", "transfer", "can", "include", "texture", "segmentation", "material", "property", "label", "etc.", "ACM", "Reference", "Format", "Su", "H.", "Huang", "Q.", "Mitra", "N.", "Li", "Y.", "Guibas", "L.", "2014", "estimate", "image", "depth", "use", "shape", "collection", "ACM", "Trans", "graph", "33", "Article", "37", "-lrb-", "July", "2014", "-rrb-", "11", "page", "dous", "10.1145", "2601097.2601159", "http://doi.acm.org/10.1145/2601097.2601159", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "copyright", "ACM", "0730-0301/14", "07-art37", "15.00", "DOI", "http://doi.acm.org/10.1145/2601097.2601159", "Yangyan", "Li", "Leonidas", "Guibas", "Stanford", "University", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "37:2", "H.", "Su", "et", "al.", "related", "work", "data-driven", "geometry", "processing", "emergence", "large", "shape", "collection", "provide", "we", "platform", "aggregate", "information", "from", "multiple", "shape", "improve", "analysis", "processing", "individual", "shape", "already", "Trimble", "3d", "warehouse", "contain", "many", "thousand", "example", "model", "per", "category", "most", "indoor", "object", "some", "popular", "outdoor", "category", "car", "airplane", "recently", "we", "have", "witness", "success", "data-driven", "technique", "shape", "analysis", "-lsb-", "Huang", "et", "al.", "2011", "Kim", "et", "al.", "2012a", "Kim", "et", "al.", "2013", "Huang", "et", "al.", "2013", "Wang", "et", "al.", "2013", "-rsb-", "shape", "model", "ing", "-lsb-", "Chaudhuri", "et", "al.", "2011", "Kalogerakis", "et", "al.", "2012", "Averkiou", "et", "al.", "2014", "-rsb-", "shape", "reconstruction", "-lsb-", "Nan", "et", "al.", "2012", "Kim", "et", "al.", "2012b", "Shen", "et", "al.", "2012", "-rsb-", "key", "task", "data-driven", "geometry", "processing", "technique", "establish", "high-quality", "correspondence", "-lrb-", "either", "pointor", "segment-level", "-rrb-", "across", "geometric", "object", "although", "exist", "rich", "technique", "align", "match", "3d", "shape", "problem", "match", "image", "object", "3d", "shape", "which", "major", "focus", "paper", "far", "from", "be", "solve", "image-shape", "matching", "most", "exist", "image-shape", "match", "approach", "-lsb-", "cyr", "Kimia", "2004", "Xu", "et", "al.", "2011", "Wang", "et", "al.", "2013", "-rsb-", "convert", "problem", "image", "matching", "problem", "i.e.", "match", "image", "project", "view", "3d", "shape", "typically", "from", "estimate", "dense", "correspondence", "between", "silhouette", "curve", "interpolate", "correspondence", "interior", "pixel", "-lsb-", "Sun", "et", "al.", "2011", "-rsb-", "use", "icp-like", "approach", "recently", "Wang", "et", "al.", "-lsb-", "2013", "-rsb-", "propose", "technique", "directly", "estimate", "correspondence", "between", "entire", "image", "object", "major", "limitation", "approach", "project", "view", "3d", "shape", "only", "contain", "partial", "information", "from", "original", "shape", "practice", "technique", "limit", "match", "very", "similar", "object", "contrast", "we", "formulate", "image", "shape", "match", "problem", "solve", "non-rigid", "alignment", "problem", "3d", "i.e.", "simultaneously", "estimate", "optimize", "depth", "image", "object", "deformation", "3d", "shape", "align", "they", "3d", "space", "particular", "we", "show", "match", "image", "collection", "3d", "shape", "boost", "matching", "quality", "enforce", "consistency", "between", "image-shape", "map", "shape-shape", "map", "pose", "estimation", "exist", "vast", "body", "work", "determine", "pose", "object", "image", "relative", "calibrate", "camera", "problem", "commonly", "formulate", "feature", "correspondence", "problem", "thus", "can", "distinguish", "type", "local", "image", "feature", "point", "line", "curve", "segment", "whole", "contour", "-lsb-", "Chen", "et", "al.", "2003", "Dalal", "Triggs", "2005", "Oliva", "Torralba", "2006", "-rsb-", "recently", "researcher", "use", "learning-based", "scheme", "cast", "classification", "learn", "good", "feature", "viewpoint", "estimation", "-lsb-", "Zia", "et", "al.", "2013", "-rsb-", "task", "pose", "estimation", "closely", "couple", "other", "task", "image-shape", "matching", "problem", "depth", "estimation", "point", "correspondence", "we", "therefore", "model", "part", "global", "optimization", "problem", "iteratively", "refine", "result", "large", "improvement", "depth", "estimation", "estimate", "depth", "image", "object", "long", "standing", "problem", "computer", "vision", "computer", "graphic", "problem", "ill-pose", "when", "input", "single", "image", "exist", "approach", "typically", "incorporate", "additional", "information", "user", "interaction", "-lsb-", "Wu", "et", "al.", "2008", "-rsb-", "shade", "-lsb-", "Lensch", "et", "al.", "2003", "Goldman", "et", "al.", "2005", "-rsb-", "use", "abstracted", "proxy", "shape", "-lsb-", "Zheng", "et", "al.", "2012", "-rsb-", "however", "approach", "design", "object", "simple", "texture", "shape", "and/or", "under", "specific", "lighting", "condition", "other", "word", "do", "apply", "well", "man-made", "object", "real", "image", "which", "exhibit", "complicated", "geometry", "texture", "availability", "large", "collection", "depth", "image", "recent", "depth", "estimation", "approach", "base", "supervised", "learning", "-lsb-", "Hoiem", "et", "al.", "2005", "Saxena", "et", "al.", "2009", "-rsb-", "give", "exemplar", "depth", "image", "approach", "learn", "conditional", "probabilistic", "distribution", "pixel", "depths", "relative", "depths", "between", "neighbor", "pixel", "apply", "learn", "distribution", "infer", "depth", "information", "new", "image", "we", "take", "different", "approach", "since", "obtain", "3d", "shape", "similar", "global", "structure", "image", "object", "easy", "we", "estimate", "depth", "information", "unsupervised", "manner", "i.e.", "directly", "match", "image", "3d", "shape", "thus", "avoid", "tedious", "task", "perform", "instance", "specific", "learning", "Pipeline", "Overview", "propose", "image-based", "shape-driven", "modeling", "approach", "take", "single", "image", "object", "segmented", "from", "background", "collection", "shape", "-lcb-", "-rcb-", "same", "class", "input", "simultaneously", "estimate", "object", "pose", "show", "reconstruct", "3d", "point", "cloud", "from", "i.", "simplicity", "we", "assume", "all", "input", "shape", "support", "same", "ground", "plane", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "so", "common", "vertical", "direction", "available", "shape", "collection", "typically", "do", "contain", "shape", "exactly", "same", "object", "reconstruct", "we", "formulate", "task", "joint", "non-rigid", "alignment", "problem", "variable", "optimize", "point", "cloud", "parameterize", "camera", "pose", "z-coordinate", "-lrb-", "pixel", "depths", "-rrb-", "image", "object", "deformation", "set", "similar", "shape", "objective", "function", "minimize", "distance", "between", "point", "cloud", "deform", "shape", "however", "several", "challenge", "first", "depth", "coordinate", "point", "cloud", "unconstrained", "yet", "we", "can", "allow", "shape", "deform", "arbitrarily", "since", "otherwise", "both", "point", "cloud", "shape", "may", "stretch", "undesirably", "when", "be", "align", "second", "success", "non-rigid", "alignment", "depend", "good", "initialization", "both", "camera", "pose", "point", "cloud", "Third", "even", "good", "initialization", "challenge", "solve", "induce", "optimization", "problem", "involve", "depth", "each", "pixel", "effectively", "what", "help", "we", "situation", "fundamental", "difference", "between", "propose", "approach", "other", "shape-driven", "image", "base", "modeling", "technique", "we", "utilize", "information", "provide", "collection", "regularize", "problem", "illustrate", "figure", "pipeline", "consist", "preprocessing", "stage", "reconstruction", "stage", "goal", "preprocessing", "stage", "align", "shape", "learn", "smart", "deformation", "prior", "-lrb-", "local", "model", "-rrb-", "each", "shape", "motivation", "come", "from", "fact", "plausible", "deformation", "each", "shape", "typically", "lie", "lowdimensional", "space", "when", "compare", "number", "parameter", "general", "deformation", "model", "-lsb-", "Averkiou", "et", "al.", "2014", "-rsb-", "we", "learn", "deformation", "prior", "each", "shape", "perform", "covariance", "analysis", "over", "its", "optimize", "deformation", "neighbor", "shape", "deformation", "prior", "directly", "learn", "shape", "inherit", "several", "structure-preserving", "property", "-lrb-", "e.g.", "symmetry", "part", "structure", "-rrb-", "from", "shape", "collection", "essentially", "we", "learn", "local", "structure", "shape", "space", "reconstruction", "stage", "proceeds", "three", "step", "where", "first", "two", "step", "provide", "initial", "solution", "-lrb-", "set", "similar", "shape", "initial", "point", "cloud", "-rrb-", "third", "step", "optimize", "point", "cloud", "minimize", "its", "distance", "deform", "similar", "shape", "specifically", "first", "step", "initialize", "camera", "configuration", "extract", "set", "similar", "shape", "consider", "pose", "estimation", "problem", "although", "pose", "estimation", "use", "single", "shape", "hard", "we", "find", "when", "collection", "orient", "shape", "available", "simple", "cumulative", "score", "which", "sum", "weighted", "similarity", "score", "render", "image", "input", "image", "work", "remarkably", "well", "can", "understand", "fact", "input", "shape", "align", "best", "camera", "pose", "vote", "all", "relevant", "input", "shape", "together", "pose", "tend", "much", "more", "stable", "than", "those", "generate", "from", "individual", "shape", "same", "spirit", "when", "generate", "similar", "shape", "we", "combine", "both", "image-shape", "distance", "shape-shape", "distance", "generate", "more", "robust", "set", "similar", "shape", "later", "step", "pipeline", "give", "render", "image", "similar", "align", "shape", "second", "step", "proceed", "initialize", "depth", "information", "-lrb-", "z-coordinate", "-rrb-", "build", "dense", "correspondence", "between", "image", "object", "similar", "shape", "transfer", "depth", "information", "due", "difference", "between", "input", "image", "3d", "shape", "we", "observe", "extremely", "hard", "obtain", "reliable", "correspondence", "via", "pair-wise", "image-shape", "matching", "however", "input", "shape", "align", "we", "exploit", "consistency", "correspondence", "across", "set", "similar", "shape", "so", "we", "can", "obtain", "much", "more", "reliable", "depth", "information", "conceptually", "similar", "state-of-the-art", "technique", "data-driven", "shape", "match", "technique", "-lsb-", "Kim", "et", "al.", "2012a", "Huang", "Guibas", "2013", "-rsb-", "enforce", "consistency", "correspondence", "along", "cycle", "improve", "quality", "isolate", "correspondence", "finally", "third", "step", "we", "refine", "camera", "pose", "depth", "information", "use", "non-rigid", "registration", "formulate", "solve", "continuous", "optimization", "problem", "objective", "function", "combine", "distance", "term", "which", "evaluate", "distance", "between", "corresponding", "point", "induce", "point", "cloud", "deform", "similar", "shape", "two", "prior", "term", "deformation", "model", "depth", "formation", "respectively", "despite", "non-linearity", "scale", "optimization", "problem", "we", "show", "can", "optimize", "effectively", "use", "alternate", "optimization", "strategy", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Figure", "Algorithm", "Pipeline", "we", "reconstruct", "3d", "point", "cloud", "from", "image", "object", "utilize", "collection", "related", "shape", "preprocessing", "stage", "we", "jointly", "align", "input", "shape", "collection", "learn", "structure-preserving", "deformation", "model", "shape", "reconstruction", "stage", "we", "lift", "single", "image", "3d", "three", "step", "first", "step", "initialize", "camera", "pose", "coordinate", "system", "associate", "align", "shape", "extract", "set", "similar", "shape", "second", "step", "perform", "image-image", "match", "build", "dense", "correspondence", "between", "image", "object", "similar", "shape", "generate", "initial", "3d", "point", "cloud", "final", "step", "jointly", "align", "initial", "point", "cloud", "select", "shape", "simultaneously", "optimize", "depth", "each", "image", "pixel", "camera", "pose", "shape", "deformation", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "37:4", "H.", "Su", "et", "al.", "Figure", "Preprocessing", "Stage", "we", "learn", "deformation", "model", "each", "shape", "via", "its", "optimize", "deformation", "other", "shape", "each", "deformation", "model", "characterize", "small", "set", "typical", "deformation", "field", "-lrb-", "show", "vector", "mesh", "vertex", "-rrb-", "derive", "from", "covariance", "analysis", "model", "serve", "regularizer", "local", "shape", "space", "around", "each", "shape", "enforce", "during", "reconstruction", "stage", "preprocessing", "stage", "goal", "preprocessing", "stage", "understand", "plausible", "deformation", "each", "shape", "context", "provide", "input", "shape", "collection", "we", "achieve", "goal", "align", "all", "input", "shape", "learn", "deformation", "prior", "each", "shape", "deformation", "model", "joint", "shape", "alignment", "we", "use", "embedded", "deformation", "model", "-lsb-", "Sumner", "et", "al.", "2007", "-rsb-", "parameterize", "deformation", "each", "shape", "embedded", "deformation", "consist", "list", "control", "point", "associate", "basis", "function", "-lrb-", "-rrb-", "give", "point", "its", "deform", "counterpart", "linear", "combination", "control", "point", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "Refer", "-lsb-", "Sumner", "et", "al.", "2007", "-rsb-", "how", "construct", "embedded", "deformation", "model", "shape", "paper", "we", "use", "200", "control", "point", "hence", "each", "shape", "control", "600", "parameter", "align", "input", "shape", "we", "employ", "method", "describe", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "which", "jointly", "optimize", "deformation", "all", "input", "shape", "minimize", "sum", "distance", "between", "corresponding", "point", "compute", "use", "pair-wise", "alignment", "we", "denote", "optimize", "embedded", "deformation", "shape", "deformation-prior", "learning", "we", "assume", "plausible", "deformation", "each", "shape", "-lrb-", "parameterize", "vector", "collect", "all", "control", "point", "-rrb-", "lie", "low", "dimensional", "space", "define", "shape?s", "neighborhood", "-lrb-", "c.f.", "-lsb-", "Ovsjanikov", "et", "al.", "2011", "-rsb-", "-rrb-", "we", "learn", "space", "from", "optimal", "deformation", "each", "shape", "other", "shape", "which", "provide", "sample", "plausible", "deformation", "-lrb-", "see", "Figure", "-rrb-", "we", "directly", "obtain", "deformation", "sample", "compose", "absolute", "optimal", "deformation", "inverse", "deformation", "each", "shape", "each", "neighbor", "shape", "we", "transform", "original", "control", "point", "-lrb-", "i.e.", "rest", "state", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "let", "vector", "collect", "all", "transform", "control", "point", "let", "original", "control", "point", "each", "neighbor", "shape", "give", "rise", "deformation", "sample", "learn", "prior", "model", "from", "similar", "shape", "we", "only", "consider", "deformation", "sample", "from", "128", "most", "similar", "shape", "each", "shape", "term", "d2", "descriptor", "-lsb-", "Osada", "et", "al.", "2002", "-rsb-", "give", "deformation", "sample", "we", "perform", "covariance", "analysis", "extract", "principal", "value", "principal", "direction", "deformation", "space", "prior", "model", "deformation", "each", "shape", "give", "rior", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "intuitively", "deformation", "lead", "small", "objective", "value", "follow", "majority", "deformation", "sample", "note", "shift", "introduce", "make", "prior", "term", "well-defined", "we", "only", "have", "limit", "deformation", "sample", "many", "principal", "value", "zero", "reconstruction", "stage", "reconstruction", "stage", "solve", "joint", "optimization", "recover", "geometry", "image", "object", "align", "deform", "version", "set", "similar", "shape", "we", "begin", "introduce", "camera", "model", "we", "show", "how", "initialize", "approximate", "solution", "section", "5.2", "section", "5.3", "refine", "obtain", "final", "solution", "section", "5.4", "5.1", "Camera", "Configuration", "we", "use", "simplify", "nine", "parameter", "camera", "configuration", "-lrb-", "-rrb-", "here", "-lrb-", "-rrb-", "specify", "rigid", "motion", "from", "common", "coordinate", "system", "associate", "input", "shape", "camera", "coordinate", "system", "specify", "focal", "length", "specify", "effective", "size", "pixel", "horizontal", "vertical", "direction", "give", "point", "-lrb-", "-rrb-", "its", "corresponding", "pixel", "coordinate", "-lrb-", "-rrb-", "give", "-lrb-", "-rrb-", "other", "direction", "give", "pixel", "-lrb-", "-rrb-", "depth", "parameter", "specify", "its", "coordinate", "corresponding", "point", "give", "rp", "convenience", "we", "denote", "map", "from", "-lrb-", "-rrb-", "5.2", "step", "Camera", "Initialization", "Candidate", "generation", "we", "candidate", "camera", "pose", "sampling", "strategy", "similar", "most", "pose", "estimation", "algorithm", "-lsb-", "Zia", "et", "al.", "2013", "-rsb-", "which", "sample", "view", "direction", "fix", "rest", "parameter", "default", "value", "specifically", "we", "let", "camera", "position", "move", "view", "sphere", "center", "origin", "radius", "5d", "where", "average", "shape", "diameter", "rest", "parameter", "fix", "follow", "focal", "point", "place", "origin", "fix", "we", "let", "up-right", "direction", "camera", "system", "lie", "plane", "view", "direction", "axis", "finally", "we", "set", "3d", "set", "so", "average", "each", "object", "occupy", "half", "render", "image", "we", "generate", "candidate", "camera", "configuration", "each", "shape", "uniformly", "sampling", "500", "view", "direction", "view", "sphere", "let", "cand", "collect", "all", "candidate", "camera", "configuration", "each", "cand", "we", "denote", "render", "image", "shape", "crop", "use", "tight", "bound", "box", "surround", "object", "optimal", "candidate", "when", "pick", "optimal", "candidate", "we", "follow", "common", "strategy", "evaluate", "render", "image", "compare", "they", "input", "image", "due", "difference", "between", "real", "image", "render", "image", "standard", "single", "shape", "base", "approach", "typically", "require", "feature", "learning", "however", "we", "find", "when", "collection", "align", "shape", "present", "simple", "cumulative", "similarity", "score", "between", "input", "image", "render", "image", "sufficient", "-lrb-", "see", "Figure", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Estimating", "image", "Depth", "use", "shape", "collection", "37:5", "Figure", "Camera", "Initialization", "Voting", "leave", "give", "image", "object", "-lrb-", "sofa", "center", "-rrb-", "we", "can", "find", "multiple", "similar", "shape", "each", "which", "independently", "propose", "camera", "pose", "candidate", "-lrb-", "blue", "arrow", "each", "candidate", "-rrb-", "note", "some", "candidate", "far", "from", "optimal", "right", "since", "shape", "already", "jointly", "orient", "pre-processing", "step", "can", "use", "more", "accurately", "vote", "optimal", "pose", "-lrb-", "red", "arrow", "-rrb-", "argmin", "exp", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "c?c", "cand", "where", "-lrb-", "-rrb-", "give", "feature", "descriptor", "min", "-lcb-", "???", "-rcb-", "c?c", "cand", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "exponential", "operator", "introduce", "down-weight", "contribution", "image", "less", "similar", "input", "image", "we", "have", "various", "image", "descriptor", "include", "gist", "-lsb-", "oliva", "Torralba", "2001", "-rsb-", "hog", "-lsb-", "Dalal", "Triggs", "2005", "-rsb-", "light-field", "descriptor", "-lsb-", "Chen", "et", "al.", "2003", "-rsb-", "experimentally", "we", "find", "feature", "descriptor", "combine", "all", "three", "feature", "together", "yield", "best", "result", "5.3", "step", "ii", "point", "cloud", "initialization", "give", "initial", "camera", "configuration", "we", "generate", "initial", "point", "cloud", "from", "do", "select", "set", "similar", "shape", "input", "image", "establish", "dense", "correspondence", "between", "similar", "shape", "transfer", "depth", "information", "performance", "step", "crucial", "since", "govern", "global", "behavior", "final", "reconstruction", "although", "both", "image-based", "retrieval", "image-image", "matching", "have", "be", "study", "considerably", "past", "we", "find", "even", "state-of-the-art", "algorithm", "insufficient", "purpose", "transfer", "depth", "instead", "key", "idea", "propose", "approach", "utilize", "regularization", "provide", "collection", "align", "shape", "boost", "performance", "each", "step", "-rrb-", "similar", "shape", "extract", "from", "match", "render", "image", "have", "similar", "each", "other", "3d", "space", "ii", "-rrb-", "pixel", "render", "image", "different", "shape", "correspond", "same", "object", "image", "pixel", "should", "come", "from", "point", "close", "each", "other", "3d", "space", "where", "align", "model", "live", "experimental", "result", "show", "even", "standard", "pair-wise", "technique", "overall", "performance", "joint", "matching", "approach", "sufficient", "purpose", "depth", "initialization", "-lrb-", "see", "Figure", "-rrb-", "similar", "shape", "extraction", "naive", "approach", "extract", "similar", "shape", "compare", "input", "image", "render", "image", "-lrb-", "accord", "select", "view", "-rrb-", "one-by-one", "however", "even", "learn", "feature", "similarity", "metric", "approach", "insufficient", "due", "diversity", "lighting", "texture", "input", "image", "since", "we", "input", "shape", "align", "we", "use", "distance", "between", "shape", "guide", "selection", "similar", "shape", "specifically", "we", "first", "use", "pairwise", "similarity", "score", "define", "-lrb-", "-rrb-", "extract", "32", "similar", "shape", "-lrb-", "i.e.", "initial", "similar", "shape", "set", "-rrb-", "we", "build", "small", "weighted", "clique", "graph", "which", "consist", "input", "image", "initial", "set", "similar", "shape", "use", "diffusion", "distance", "-lsb-", "Coifman", "et", "al.", "2005", "-rsb-", "sort", "initial", "similar", "shape", "weight", "each", "image-shape", "edge", "give", "-lrb-", "-rrb-", "while", "image", "descriptor", "replace", "d2", "shape", "descriptor", "-lsb-", "Osada", "et", "al.", "2002", "-rsb-", "shape-shape", "edge", "give", "sort", "shape", "we", "select", "top", "shape", "final", "similar", "shape", "set", "simplify", "notation", "let", "similar", "shape", "denote", "correspondence", "initialization", "we", "initialize", "image-shape", "correspondence", "match", "input", "image", "object", "-lrb-", "background", "remove", "-rrb-", "render", "image", "object", "each", "shape", "give", "two", "image", "object", "we", "first", "apply", "-lsb-", "Munich", "Perona", "1999", "-rsb-", "build", "dense", "correspondence", "between", "silhouette", "curve", "treat", "correspondence", "landmark", "correspondence", "we", "employ", "laplacian", "deformation", "-lsb-", "sorkine", "et", "al.", "2004", "-rsb-", "align", "after", "alignment", "we", "derive", "initial", "pixel-shape", "correspondence", "from", "overlaid", "image", "object", "we", "denote", "initial", "correspondence", "from", "note", "some", "pixel", "may", "have", "correspondence", "due", "hole", "shape", "correspondence", "pruning", "so", "far", "we", "only", "compute", "imageshape", "correspondence", "between", "input", "image", "each", "shape", "isolation", "constraint", "we", "can", "use", "improve", "correspondence", "make", "they", "consistent", "optimal", "deformation", "-lcb-", "-rcb-", "align", "input", "shape", "more", "precisely", "give", "two", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "distance", "between", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "large", "least", "one", "two", "correspondence", "incorrect", "addition", "enforce", "consistency", "property", "we", "also", "prioritize", "smoothness", "correspondence", "i.e.", "give", "two", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "neighbor", "so", "should", "we", "favor", "either", "both", "they", "select", "both", "they", "prune", "both", "consistency", "property", "smoothness", "prior", "only", "involve", "pair", "correspondence", "we", "formulate", "correspondence", "pruning", "step", "solve", "binary", "second-order", "mrf", "problem", "we", "introduce", "binary", "random", "variable", "-lcb-", "-rcb-", "each", "initial", "correspondence", "where", "select", "otherwise", "we", "define", "two", "type", "pair-wise", "potential", "function", "each", "correspondence", "pair", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "define", "consistency", "potential", "otherwise", "where", "set", "0.05", "time", "average", "shape", "diameter", "each", "pair", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "two", "pair", "neighbor", "pixel", "we", "define", "smoothness", "potential", "function", "otherwise", "Figure", "point", "cloud", "initialization", "we", "perform", "image-image", "match", "obtain", "initial", "dense", "correspondence", "Figure", "effect", "point", "registration", "2d", "versus", "3d", "we", "com", "total", "potential", "function", "simply", "sum", "all", "pair-wise", "potential", "function", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "collect", "all", "pair", "correspondence", "consideration", "optimization", "we", "apply", "tree-reweighted", "belief", "propagation", "-lrb-", "trbp", "-rrb-", "-lsb-", "Szeliski", "et", "al.", "2008", "-rsb-", "which", "very", "effectively", "binary", "mrf", "problem", "convenience", "we", "still", "use", "denote", "remain", "correspondence", "between", "after", "stage", "geometry", "initialization", "use", "dense", "correspondence", "we", "compute", "coordinate", "corresponding", "point", "each", "pixel", "-lrb-", "-rrb-", "-lrb-", "camera", "coordinate", "system", "-rrb-", "average", "z-coordinate", "corresponding", "point", "similar", "shape", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "give", "note", "each", "pixel", "do", "belong", "any", "correspondence", "we", "copy", "value", "from", "closest", "pixel", "have", "correspondence", "we", "generate", "initial", "point", "cloud", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "accord", "-lrb-", "-rrb-", "5.4", "step", "iii", "point", "cloud", "optimization", "we", "refine", "initial", "image-shape", "correspondence", "initial", "point", "cloud", "solve", "joint", "alignment", "problem", "whose", "objective", "function", "minimize", "distance", "-lrb-", "define", "via", "correspondence", "-rrb-", "between", "point", "cloud", "deform", "similar", "shape", "we", "employ", "icp-like", "procedure", "alternate", "between", "continuous", "optimization", "step", "which", "optimize", "continuous", "variable", "include", "camera", "configuration", "z-coordinate", "each", "pixel", "-lcb-", "-rcb-", "deformation", "each", "shape", "discrete", "optimization", "step", "which", "update", "image-shape", "correspondence", "continuous", "optimization", "step", "we", "consider", "multiple", "objective", "align", "induce", "point-cloud", "similar", "shape", "first", "term", "evaluate", "sum", "square", "distance", "between", "corresponding", "point", "here", "weight", "adjust", "higher", "more", "reliable", "correspondence", "-lrb-", "-rrb-", "we", "set", "interior", "point", "20", "point", "close", "silhouette", "note", "another", "option", "measure", "distance", "image", "domain", "however", "due", "distance", "distortion", "projection", "two", "point", "close", "each", "other", "image", "domain", "may", "far", "from", "each", "other", "original", "shape", "turn", "out", "measure", "distance", "image", "domain", "lead", "far", "less", "accurate", "result", "-lrb-", "see", "Figure", "-rrb-", "datum", "consider", "each", "pixel", "independently", "we", "next", "introduce", "second", "term", "regularize", "z-coordinate", "neighbor", "pixel", "finally", "third", "term", "apply", "key", "deformation", "prior", "learn", "preprocessing", "stage", "datum", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "prior", "-lrb-", "-rrb-", "combine", "datum", "regu", "prior", "energy", "minimization", "problem", "continuous", "optimization", "step", "take", "form", "-lcb-", "-rcb-", "-lcb-", "-rcb-", "min", "datum", "regu", "prior", "we", "experiment", "we", "use", "throughout", "same", "set", "parameter", "0.01", "we", "again", "apply", "alternate", "optimization", "effectively", "optimize", "-lrb-", "-rrb-", "each", "step", "we", "first", "fix", "z-coordinate", "optimize", "camera", "configuration", "shape", "deformation", "min", "datum", "prior", "-lcb-", "-rcb-", "we", "fix", "optimize", "z-coordinate", "10", "min", "datum", "regu", "-lcb-", "-rcb-", "key", "advantage", "alternate", "optimization", "strategy", "-lrb-", "-rrb-", "-lrb-", "10", "-rrb-", "either", "sparse", "-lrb-", "constrain", "neighbor", "pixel", "-rrb-", "small-scale", "-lrb-", "camera", "configuration", "deformation", "parameter", "-rrb-", "enable", "we", "apply", "second-order", "Newton", "method", "optimize", "they", "effectively", "i.e.", "we", "solve", "sparse", "small-scale", "linear", "system", "each", "Newton", "iteration", "objective", "term", "consist", "non-linear", "least", "square", "we", "apply", "gauss-newton", "method", "optimize", "equation", "-lrb-", "-rrb-", "-lrb-", "10", "-rrb-", "derivation", "quite", "standard", "we", "omit", "detail", "discrete", "optimization", "step", "give", "optimize", "point-cloud", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "deform", "shape", "we", "proceed", "optimize", "image-shape", "correspondence", "we", "first", "convert", "each", "deform", "shape", "-lrb-", "-rrb-", "point-cloud", "simulate", "scan", "from", "current", "camera", "configuration", "we", "initialize", "collect", "closest", "point-pair", "init", "-lcb-", "-lrb-", "-rrb-", "argmin", "p?q", "argmin", "-rcb-", "may", "only", "exist", "partial", "similarity", "between", "image", "object", "each", "shape", "we", "adopt", "median", "thresholding", "scheme", "-lsb-", "rusinkiewicz", "Levoy", "2001", "-rsb-", "remove", "correspondence", "far", "from", "each", "other", "leave", "init", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rcb-", "init", "where", "median", "among", "each", "figure", "show", "example", "non-rigid", "alignment", "process", "practice", "only", "4-6", "alternate", "update", "sufficient", "good", "result", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Estimating", "image", "Depth", "use", "shape", "collection", "37:7", "image", "similar", "shape", "popup", "point", "cloud", "-lrb-", "ours", "-rrb-", "figure", "Representative", "result", "we", "have", "evaluate", "we", "approach", "five", "category", "object", "figure", "show", "representative", "result", "each", "category", "every", "object", "we", "show", "input", "2d", "image", "extract", "similar", "shape", "reconstructed", "point", "cloud", "finally", "ground", "truth", "kinect", "scan", "kinect", "point", "cloud", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Figure", "Intermediate", "Optimization", "result", "we", "employ", "iterative", "scheme", "simultaneously", "refine", "point", "cloud", "reconstruction", "deform", "similar", "shape", "figure", "show", "image", "view", "-lrb-", "row", "-rrb-", "side", "view", "-lrb-", "row", "-rrb-", "point", "cloud", "deform", "shape", "different", "iteration", "since", "strong", "reliable", "constraint", "impose", "along", "silhouette", "estimate", "image", "view", "result", "look", "correct", "from", "original", "image", "view", "initialization", "step", "however", "interior", "point", "cloud", "initialize", "poorly", "can", "observe", "from", "side", "view", "when", "optimization", "proceeds", "significant", "improvement", "can", "observe", "from", "side", "view", "shape", "deformation", "subspace", "prior", "from", "similar", "shape", "regularize", "solution", "propagate", "information", "otherwise", "under-determined", "interior", "region", "evaluation", "we", "evaluate", "propose", "shape-driven", "image-based", "modeling", "various", "kinect", "scan", "associate", "color", "information", "6.1", "experimental", "setup", "image", "we", "consider", "five", "category", "object", "chair", "table", "cup", "lamp", "car", "each", "category", "consist", "4-6", "kinect", "scan", "object", "different", "shape", "Figure", "show", "representative", "result", "each", "category", "we", "assume", "all", "object", "capture", "we", "standard", "setting", "where", "background", "easy", "remove", "kinect", "scan", "evaluation", "purpose", "only", "shape", "3d", "shape", "from", "Trimble", "warehouse", "each", "category", "contain", "2k-7k", "shape", "-lrb-", "see", "Table", "-rrb-", "where", "Chair", "datum", "set", "from", "-lsb-", "Kim", "et", "al.", "2013", "-rsb-", "car", "datum", "set", "from", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "three", "remain", "dataset", "be", "collect", "use", "similar", "strategy", "describe", "-lsb-", "Kim", "et", "al.", "2013", "-rsb-", "note", "even", "thousand", "shape", "shape", "space", "densely", "cover", "can", "see", "from", "extract", "similar", "shape", "-lrb-", "see", "Figure", "-rrb-", "evaluation", "protocol", "we", "evaluate", "reconstruct", "point-cloud", "each", "object", "image", "against", "kinect", "depth", "scan", "factor", "out", "free", "scaling", "degree", "freedom", "we", "first", "compute", "similarity", "transform", "align", "reconstructed", "point", "cloud", "Kinect", "scan", "give", "calibrate", "reconstruction", "Kinect", "scan", "kinect", "we", "propose", "two", "metric", "evaluate", "quality", "first", "metric", "evaluate", "Hausdorff", "distance", "between", "kinect", "-lrb-", "kinect", "-rrb-", "min", "kinect", "P.", "q?P", "second", "metric", "evaluate", "deviation", "between", "pair", "corresponding", "point", "-lrb-", "-rrb-", "kinect", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "P.", "Table", "statistics", "various", "dataset", "shape", "normalize", "have", "diameter", "mean", "standard", "deviation", "metric", "define", "Sec", "6.1", "b", "b", "number", "best", "match", "single", "shape", "those", "superscript", "number", "we", "algorithm", "#shapes", "haus", "b", "haus", "b", "haus", "haus", "deviation", "deviation", "chair", "7.3", "0.17", "0.15", "0.05", "0.03", "0.11", "0.10", "Table", "4.2", "0.14", "0.12", "0.06", "0.06", "0.12", "0.13", "Cup", "1.1", "0.15", "0.11", "0.05", "0.04", "0.09", "0.09", "lamp", "2.0", "0.13", "0.15", "0.06", "0.03", "0.10", "0.11", "car", "1.7", "0.12", "0.11", "0.05", "0.03", "0.09", "0.08", "clear", "Hausdorff", "distance", "invariant", "interior", "drift", "surface", "while", "correspondence", "deviation", "more", "strict", "each", "distance", "metric", "we", "collect", "statistics", "mean", "variance", "-lrb-", "-rrb-", "over", "all", "point", "-lrb-", "see", "Table", "-rrb-", "baseline", "we", "calculate", "Hausdorff", "metric", "obtain", "most", "similar", "shape", "6.2", "analysis", "result", "Table", "Figure", "show", "representative", "result", "propose", "approach", "overall", "result", "reasonably", "good", "despite", "obvious", "difficulty", "problem", "68.2", "correspondence", "fall", "below", "0.02", "time", "average", "shape", "diameter", "all", "dataset", "Hausdorff", "distance", "error", "considerably", "lower", "than", "deviation", "error", "shape", "point", "cloud", "drive", "shape", "collection", "show", "use", "shape", "collection", "good", "prior", "distribution", "point", "restricted", "drift", "along", "common", "shape", "space", "deviation", "error", "large", "because", "correspondence", "may", "glide", "along", "shape", "which", "exactly", "same", "we", "next", "discuss", "result", "each", "category", "chair", "table", "we", "evaluate", "chair", "table", "category", "because", "fine", "geometric", "detail", "present", "shape", "like", "other", "man-made", "object", "chair", "table", "usually", "have", "strong", "symmetry", "imply", "lower-dimensional", "deformation", "space", "other", "hand", "four", "leg", "may", "introduce", "match", "ambiguity", "category", "we", "find", "we", "algorithm", "limit", "when", "selfocclusion", "present", "lower", "board", "occlude", "front", "leg", "row", "figure", "consequently", "part", "attach", "leg", "cup", "cup", "relatively", "small", "household", "item", "usually", "have", "circular", "symmetrical", "body", "interestingly", "we", "method", "produce", "visually", "more", "please", "result", "compare", "Kinect", "because", "object", "size", "reach", "resolution", "limit", "sensor", "surface", "specular", "which", "challenge", "structural", "light", "mechanism", "Kinect", "lamp", "we", "choose", "category", "because", "have", "large", "variation", "possible", "shape", "particularly", "curvature", "pole", "can", "see", "we", "algorithm", "succeed", "both", "lamp", "example", "figure", "success", "can", "attribute", "two", "reason", "first", "we", "use", "data-driven", "approach", "implicitly", "combine", "part", "from", "different", "shape", "second", "we", "use", "non-rigid", "deformation", "field", "which", "allow", "bend", "pole", "car", "we", "choose", "category", "common", "outdoor", "object", "have", "fine", "geometric", "detail", "-lrb-", "e.g.", "wheel", "side", "mirror", "-rrb-", "we", "algorithm", "could", "accurately", "estimate", "depth", "car", "other", "hand", "Kinect", "have", "problem", "detect", "window", "wheel", "because", "too", "reflective", "too", "dark", "respectively", "comparison", "Automatic", "Pop-Up", "Automatic", "Pop-Up", "-lsb-", "Hoiem", "et", "al.", "2005", "-rsb-", "automatically", "reconstruct", "3d", "information", "use", "single", "image", "initially", "design", "outdoor", "scene", "use", "plane", "classifier", "software", "assume", "simple", "geometric", "prior", "tend", "work", "poorly", "complicated", "indoor", "object", "thin", "fine", "feature", "we", "show", "effect", "Automatic", "Pop-Up", "chair", "model", "Figure", "use", "pre-trained", "classifier", "we", "algorithm", "visu", "ally", "significantly", "better", "than", "output", "from", "software", "-lrb-", "compare", "last", "column", "figure", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Figure", "6.3", "discussion", "image-shape", "match", "versus", "image-image", "matching", "imageimage", "matching", "far", "less", "accurate", "than", "image-shape", "matching", "-lrb-", "figure", "-rrb-", "two", "reason", "account", "difference", "first", "projection", "from", "2d", "3d", "perspective", "two", "point", "close", "2d", "may", "far", "away", "3d", "second", "3d", "point", "cloud", "shape", "obtainable", "we", "setting", "project", "2d", "lose", "important", "information", "fact", "strong", "perspective", "projection", "might", "still", "hurt", "performance", "we", "algorithm", "example", "seat", "second", "row", "Figure", "estimate", "very", "thick", "which", "can", "attribute", "very", "strong", "perspective", "effect", "close", "chair", "leg", "deformation", "prior", "important", "we", "find", "we", "deformation", "shape", "prior", "key", "success", "illustrate", "Figure", "deformation", "space", "low-dimensional", "point", "generally", "restricted", "move", "parallel", "meaningful", "axis", "scale", "coordinate", "manner", "thus", "constraint", "make", "sure", "local", "deformation", "maintain", "global", "symmetry", "evidently", "Figure", "10", "because", "problem", "single", "image", "depth", "reconstruction", "intrinsically", "underdetermined", "poor", "result", "obtain", "without", "prior", "-lrb-", "top", "-rrb-", "contrary", "isotropic", "symmetry", "learn", "prior", "ensure", "deformation", "subspace", "have", "only", "1d", "which", "coordinate", "scale", "around", "y-axis", "timing", "all", "experiment", "be", "conduct", "standard", "desktop", "platform", "2.4", "GHz", "Intel", "Core", "duo-core", "12gb", "RAM", "each", "category", "pre-processing", "stage", "share", "all", "image", "which", "take", "3507", "chair", "2184", "table", "516", "cup", "1223", "lamp", "1109", "car", "camera", "initialization", "stage", "take", "average", "0.3", "each", "input", "image", "most", "time", "spend", "extract", "image", "feature", "each", "image", "object", "point", "cloud", "initialization", "stage", "take", "7", "average", "1", "correspondence", "initialization", "6", "correspondence", "pruning", "point", "cloud", "optimization", "stage", "take", "25", "average", "total", "run", "time", "process", "image", "object", "33", "application", "section", "we", "use", "series", "application", "show", "usefulness", "reconstructed", "point", "cloud", "include", "relight", "image", "object", "synthesize", "unseen", "novel", "view", "depth-aware", "image", "composition", "end", "we", "show", "some", "situation", "reasonable", "full", "mesh", "image", "object", "can", "recover", "use", "we", "point", "cloud", "input", "exploit", "shape", "symmetry", "relighting", "give", "image", "we", "estimate", "depth", "each", "pixel", "use", "local", "pca", "analysis", "estimate", "normal", "simulate", "lighting", "effect", "under", "different", "illumination", "condition", "Figure", "11", "we", "assume", "ambient", "light", "diffusion", "reflection", "surface", "Notice", "synthesize", "image", "almost", "photo-realistic", "except", "some", "artifact", "top-right", "corner", "due", "inaccurate", "depth", "estimation", "novel", "View", "synthesis", "since", "full", "3d", "information", "each", "pixel", "available", "we", "can", "simulate", "movement", "camera", "3d", "synthesize", "novel", "view", "Figure", "12", "synthesize", "view", "from", "we", "depth", "estimation", "use", "inverse", "warping", "method", "-lsb-", "Marcato", "Jr", "1998", "-rsb-", "almost", "photo-realistic", "particular", "we", "can", "even", "accurately", "recover", "depth", "information", "back", "mirror", "car", "appearance", "around", "back", "mirror", "quite", "natural", "when", "car", "rotate", "counter-clockwise", "direction", "-lrb-", "-30", "-15", "deg", "-rrb-", "note", "miss", "part", "invisible", "image", "can", "possibly", "recover", "exploit", "model", "symmetry", "Depth-Aware", "image", "composition", "Figure", "13", "we", "demonstrate", "experiment", "which", "we", "compose", "3d", "model", "woman", "sofa", "image", "so", "woman", "sit", "sofa", "correct", "composition", "should", "make", "sure", "woman?s", "body", "leg", "cover", "back", "arm", "she", "hip", "cover", "front", "chair", "arm", "since", "depth", "sofa", "can", "recover", "we", "can", "compute", "correct", "occlusion", "each", "pixel", "-lrb-", "right", "-rrb-", "oppose", "unnatural", "occlusion", "pattern", "depth", "information", "available", "-lrb-", "left", "-rrb-", "symmetry-based", "surface", "reconstruction", "infer", "point", "cloud", "only", "have", "point", "visible", "from", "camera", "view", "however", "we", "show", "experiment", "key", "intermediate", "step", "towards", "full", "3d", "model", "reconstruction", "we", "hallucinate", "miss", "part", "exploit", "model", "symmetry", "we", "use", "-lsb-", "Mitra", "et", "al.", "2006", "-rsb-", "extract", "symmetry", "pattern", "from", "similar", "shape", "transport", "they", "point", "cloud", "Figure", "14", "we", "discover", "circular", "symme", "try", "cup", "body", "plane", "symmetry", "handle", "which", "also", "induce", "segmentation", "cup", "thus", "we", "can", "transport", "symmetry", "parameter", "achieve", "full", "shape", "recovery", "use", "-lsb-", "Mitra", "et", "al.", "2007", "-rsb-", "finally", "we", "apply", "Poisson", "reconstruction", "-lsb-", "Kazhdan", "et", "al.", "2006", "-rsb-", "extract", "point", "cloud", "surface", "reconstruction", "smoothing", "final", "result", "bottom", "row", "Figure", "14", "we", "see", "reconstructed", "mesh", "generally", "look", "natural", "from", "all", "view", "however", "because", "we", "only", "apply", "plane", "symmetry", "handle", "part", "gap", "bottom", "handle", "reconstructed", "surface", "connect", "discover", "better", "structural", "predictor", "close", "small", "gap", "interesting", "open", "problem", "further", "exploration", "Figure", "10", "effect", "Deformation", "prior", "circular", "symmetric", "circularly", "symmetric", "Figure", "11", "relighting", "use", "infer", "depth", "information", "we", "can", "build", "normal", "map", "simulate", "different", "lighting", "condition", "Leftmost", "column", "input", "image", "three", "column", "right", "simulated", "illumination", "directional", "light", "source", "move", "from", "leave", "right", "-lrb-", "top", "row", "-rrb-", "up", "down", "-lrb-", "bottom", "row", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "37:10", "H.", "Su", "et", "al.", "Figure", "12", "novel", "View", "synthesize", "simulated", "image", "rotate", "camera", "around", "y-axis", "conclusion", "future", "work", "paper", "we", "have", "present", "data-driven", "algorithm", "add", "depth", "information", "image", "object", "algorithm", "take", "input", "image", "segmented", "object", "collection", "3d", "shape", "same", "object", "class", "compute", "various", "geometric", "prior", "from", "shape", "collection", "optimize", "depth", "estimation", "image", "object", "procedure", "fully", "automatic", "we", "have", "evaluate", "performance", "present", "approach", "benchmark", "consist", "Kinect", "scan", "variety", "common", "object", "take", "under", "different", "lighting", "condition", "experimental", "result", "show", "we", "approach", "produce", "depth", "close", "ground-truth", "superior", "state-of-the-art", "depth", "estimator", "we", "have", "also", "show", "usefulness", "we", "approach", "various", "application", "besides", "application", "demonstrate", "paper", "present", "depth", "estimator", "enable", "variety", "other", "application", "both", "computer", "graphic", "computer", "vision", "example", "shape", "collection", "can", "serve", "hub", "link", "many", "image", "object", "particularly", "useful", "retrieve", "similar", "image", "object", "be", "take", "from", "drastically", "different", "view", "point", "can", "match", "well", "pure", "image", "method", "another", "example", "help", "image-shape", "network", "we", "can", "propagate", "rich", "image", "label", "purpose", "categorize", "shape", "challenging", "problem", "shape", "analysis", "due", "lack", "label", "shape", "datum", "combine", "3d", "shape", "label", "Limitations", "course", "state", "we", "approach", "require", "segmented", "image", "object", "knowledge", "object", "class", "well", "study", "problem", "computer", "vision", "future", "work", "can", "combine", "we", "approach", "present", "method", "work", "best", "man-made", "object", "whose", "3d", "model", "can", "well", "align", "where", "variation", "shape", "pose", "modest", "do", "apply", "well", "object", "high", "variability", "tree", "building", "high", "articulation", "animal", "object", "important", "utilize", "more", "specialized", "domain", "knowledge", "-lrb-", "i.e.", "skeleton", "regular", "structure", "-rrb-", "establish", "correspondence", "estimate", "depth", "finally", "we", "experience", "minimum", "couple", "hundred", "shape", "necessary", "algorithm", "succeed", "intuition", "each", "part", "object", "image", "need", "have", "multiple", "correspondence", "good", "regularization", "future", "work", "ample", "opportunity", "future", "research", "while", "so", "far", "we", "have", "focus", "estimate", "depth", "single", "segmented", "object", "would", "very", "interesting", "generalize", "approach", "estimate", "depth", "entire", "scene", "would", "require", "we", "automate", "object", "detection", "process", "take", "account", "spatial", "relation", "among", "object", "Figure", "13", "Depth-Aware", "image", "composition", "image", "composition", "may", "tedious", "direct", "overlay", "may", "lead", "incorrect", "occlusion", "-lrb-", "leave", "-rrb-", "give", "image", "sofa", "3d", "model", "sit", "woman", "occlusion", "between", "they", "can", "correctly", "compute", "base", "upon", "depth", "infer", "we", "algorithm", "-lrb-", "right", "-rrb-", "Figure", "14", "symmetry-based", "Completion", "surface", "reconstruction", "top", "point", "cloud", "view", "from", "different", "angle", "bottom", "sur", "acknowledgement", "we", "thank", "reviewer", "comment", "suggestion", "paper", "work", "support", "part", "NSF", "grant", "IIS", "1016324", "DMS", "1228304", "AFOSR", "grant", "fa9550-12-1-0372", "NSFC", "grant", "61202221", "Max", "Plack", "Center", "Visual", "Computing", "Communications", "Google", "Motorola", "research", "award", "gift", "from", "HTC", "corporation", "Marie", "Curie", "Career", "integration", "Grant", "303541", "ERC", "start", "Grant", "SmartGeometry", "-lrb-", "stg-2013335373", "-rrb-", "gift", "from", "Adobe", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014", "Estimating", "image", "Depth", "use", "shape", "collection", "37:11", "reference", "VERKIOU", "M.", "IM", "V.", "HENG", "Y.", "itra", "N.", "J.", "2014", "Shapesynth", "parameterize", "model", "collection", "couple", "shape", "exploration", "synthesis", "cgf", "haudhurus", "S.", "ALOGERAKIS", "E.", "UIBAS", "L.", "OLTUN", "V.", "2011", "probabilistic", "reasoning", "assembly-based", "3d", "modeling", "ACM", "ToG", "30", "-lrb-", "Aug.", "-rrb-", "35:1", "35:10", "hen", "d.-y.", "IAN", "X.-P.", "hen", "y.-t.", "uhyoung", "M.", "2003", "visual", "similarity", "base", "3d", "model", "retrieval", "cgf", "22", "223", "232", "oifman", "R.", "R.", "AFON", "S.", "EE", "A.", "B.", "AGGIONI", "M.", "ADLER", "B.", "ARNER", "F.", "ucker", "S.", "W.", "2005", "geometric", "diffusion", "tool", "harmonic", "analysis", "structure", "definition", "datum", "Diffusion", "map", "pna", "102", "21", "7426", "7431", "yr", "C.", "M.", "IMIA", "B.", "B.", "2004", "similarity-based", "aspectgraph", "approach", "3d", "object", "recognition", "ijcv", "57", "-lrb-", "apr.", "-rrb-", "22", "alal", "N.", "rigg", "B.", "2005", "histogram", "orient", "gradient", "human", "detection", "Proc", "CVPR", "886", "893", "unkhouser", "T.", "AZHDAN", "M.", "hilane", "P.", "P.", "IEFER", "W.", "AL", "a.", "usinkiewicz", "S.", "OBKIN", "D.", "2004", "modeling", "example", "ACM", "ToG", "23", "-lrb-", "Aug.", "-rrb-", "652", "663", "oldman", "D.", "B.", "URLESS", "B.", "ERTZMANN", "a.", "eitz", "S.", "M.", "2005", "shape", "spatially-varying", "brdf", "from", "photometric", "stereo", "Proc", "iccv", "341", "348", "oiem", "D.", "FROS", "A.", "A.", "EBERT", "M.", "2005", "Automatic", "photo", "pop-up", "ACM", "ToG", "24", "-lrb-", "July", "-rrb-", "577", "584", "uang", "Q.", "UIBAS", "L.", "2013", "consistent", "shape", "map", "via", "semidefinite", "programming", "cgf", "32", "177", "186", "uang", "Q.", "OLTUN", "V.", "UIBAS", "L.", "2011", "Joint", "shape", "segmentation", "use", "linear", "programming", "ACM", "ToG", "30", "uang", "q.-x.", "H.", "UIBAS", "L.", "2013", "fine-grained", "semi-supervised", "labeling", "large", "shape", "collection", "ACM", "ToG", "32", "-lrb-", "Nov.", "-rrb-", "190:1", "190:10", "ALOGERAKIS", "E.", "HAUDHURI", "S.", "OLLER", "D.", "OLTUN", "V.", "2012", "probabilistic", "model", "component-based", "shape", "synthesis", "ACM", "ToG", "31", "-lrb-", "July", "-rrb-", "55:1", "55:11", "AZHDAN", "M.", "OLITHO", "M.", "OPPE", "H.", "2006", "Poisson", "surface", "reconstruction", "cgf", "61", "70", "IM", "V.", "G.", "W.", "ITRA", "N.", "J.", "ERDI", "S.", "unkhouser", "t.", "2012", "explore", "collection", "3d", "model", "use", "fuzzy", "correspondence", "ACM", "ToG", "31", "-lrb-", "July", "-rrb-", "54:1", "54:11", "IM", "Y.", "M.", "ITRA", "N.", "J.", "d.-m.", "uiba", "l.", "2012", "acquire", "3d", "indoor", "environment", "variability", "repetition", "ACM", "ToG", "31", "-lrb-", "Nov.", "-rrb-", "138:1", "138:11", "IM", "V.", "G.", "W.", "ITRA", "N.", "J.", "HAUDHURI", "S.", "ERDI", "S.", "unkhouser", "t.", "2013", "Learning", "part-based", "template", "from", "large", "collection", "3d", "shape", "ACM", "ToG", "32", "70:1", "70:12", "ensch", "H.", "P.", "A.", "AUTZ", "J.", "OESELE", "M.", "EIDRICH", "W.", "EIDEL", "h.-p", "2003", "image-based", "reconstruction", "spatial", "appearance", "geometric", "detail", "ACM", "ToG", "22", "arcato", "R.", "W.", "1998", "optimize", "inverse", "warper", "phd", "thesis", "Massachusetts", "Institute", "Technology", "itra", "N.", "J.", "UIBAS", "L.", "J.", "auly", "M.", "2006", "partial", "approximate", "symmetry", "detection", "3d", "geometry", "ACM", "ToG", "560", "568", "itra", "N.", "J.", "UIBAS", "L.", "auly", "M.", "2007", "symmetrization", "ACM", "ToG", "26", "63", "unich", "M.", "E.", "erona", "P.", "1999", "continuous", "dynamic", "time", "warp", "translation-invariant", "curve", "alignment", "application", "signature", "verification", "iccv", "vol", "L.", "ie", "K.", "harf", "a.", "2012", "search-classify", "approach", "cluttered", "indoor", "scene", "understanding", "ACM", "ToG", "31", "-lrb-", "Nov.", "-rrb-", "137:1", "137:10", "liva", "a.", "orralba", "a.", "2001", "model", "shape", "scene", "holistic", "representation", "spatial", "envelope", "ijcv", "42", "-lrb-", "May", "-rrb-", "145", "175", "liva", "a.", "orralba", "a.", "2006", "build", "gist", "scene", "role", "global", "image", "feature", "recognition", "Progress", "Brain", "Research", "155", "23", "36", "SADA", "R.", "unkhouser", "T.", "HAZELLE", "B.", "OBKIN", "D.", "2002", "shape", "distribution", "ACM", "ToG", "21", "-lrb-", "October", "-rrb-", "807", "832", "vsjanikov", "m.", "W.", "UIBAS", "L.", "itra", "N.", "J.", "2011", "exploration", "continuous", "variability", "collection", "3d", "shape", "ACM", "ToG", "30", "33:1", "33:10", "usinkiewicz", "S.", "EVOY", "M.", "2001", "efficient", "variant", "ICP", "algorithm", "3dim", "145", "152", "axena", "a.", "UN", "M.", "a.", "Y.", "2009", "make3d", "Learning", "3d", "scene", "structure", "from", "single", "still", "image", "IEEE", "TPAMI", "31", "-lrb-", "May", "-rrb-", "824", "840", "hen", "c.-h.", "H.", "HEN", "K.", "s.-m", "2012", "structure", "recovery", "part", "assembly", "ACM", "ToG", "31", "180:1", "180:11", "orkine", "O.", "OHEN", "D.", "IPMAN", "Y.", "LEXA", "M.", "OSSL", "C.", "EIDEL", "h.-p", "2004", "laplacian", "surface", "editing", "cgf", "175", "184", "umner", "R.", "W.", "chmid", "J.", "auly", "M.", "2007", "embed", "deformation", "shape", "manipulation", "ACM", "ToG", "26", "-lrb-", "July", "-rrb-", "UN", "M.", "UMAR", "S.", "S.", "RADSKI", "G.", "AVARESE", "S.", "2011", "toward", "automatic", "3d", "generic", "object", "modeling", "from", "one", "single", "image", "3DIMPVT", "IEEE", "zeliskus", "R.", "ABIH", "R.", "charstein", "D.", "EKSLER", "O.", "OLMOGOROV", "V.", "garwalum", "a.", "APPEN", "M.", "OTHER", "C.", "2008", "comparative", "study", "energy", "minimization", "method", "markov", "random", "field", "smoothness-based", "prior", "IEEE", "TPAMI", "30", "-lrb-", "June", "-rrb-", "1068", "1080", "ang", "Y.", "ONG", "M.", "ang", "T.", "OHEN", "D.", "HANG", "H.", "hen", "B.", "2013", "projective", "analysis", "3d", "shape", "segmentation", "ACM", "ToG", "32", "-lrb-", "Nov.", "-rrb-", "192:1", "192:12", "T.-P.", "UN", "J.", "ang", "c.-k.", "hum", "h.-y", "2008", "interactive", "normal", "reconstruction", "from", "single", "image", "ACM", "ToG", "119:1", "119:9", "K.", "HENG", "H.", "HANG", "H.", "OHEN", "D.", "IU", "L.", "iong", "Y.", "2011", "photo-inspired", "model-driven", "3d", "object", "modeling", "ACM", "ToG", "30", "-lrb-", "July", "-rrb-", "80:1", "80:10", "heng", "Y.", "HEN", "X.", "HENG", "M.-M.", "HOU", "K.", "S.-M.", "itra", "N.", "J.", "2012", "interactive", "image", "cuboid", "proxy", "smart", "image", "manipulation", "ACM", "ToG", "31", "99:1", "99:11", "ia", "Z.", "TARK", "M.", "chiele", "B.", "CHINDLER", "K.", "2013", "detailed", "3d", "representation", "object", "recognition", "modeling", "IEEE", "TPAMI", "35", "11", "2608", "2623", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "37", "publication", "date", "July", "2014" ],
  "content" : "\n  \n    dbb9797eacf04351a22d9cc44e6114e9339e33af7815c218d10ea3d09a9e2e93\n    p3d\n    10.1145/2601097.2601159\n    Name identification was not possible. \n  \n  \n    \n      \n        Estimating Image Depth Using Shape Collections\n      \n      Hao Su Qixing Huang Niloy J. Mitra Stanford University University College London\n      \n        \n        Figure 1: We attribute a single 2D image of an object (left) with depth by transporting information from a 3D shape deformation subspace learned by analyzing a network of related but different shapes (middle). For visualization, we color code the estimated depth with values increasing from red to blue (right).\n      \n      Images, while easy to acquire, view, publish, and share, they lack critical depth information. This poses a serious bottleneck for many image manipulation, editing, and retrieval tasks. In this paper we consider the problem of adding depth to an image of an object, effectively ?lifting? it back to 3D, by exploiting a collection of aligned 3D models of related objects. Our key insight is that, even when the imaged object is not contained in the shape collection, the network of shapes implicitly characterizes a shape-specific deformation subspace that regularizes the problem and enables robust diffusion of depth information from the shape collection to the input image. We evaluate our fully automatic approach on diverse and challenging input images, validate the results against Kinect depth readings, and demonstrate several imaging applications including depth-enhanced image editing and image relighting. CR Categories: I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling?Geometric algorithms. Keywords: data-driven shape analysis, pose estimation, depth estimation, image retrieval, shape collections\n      Links:\n      \n        \n      \n      DL PDF W\n      \n        \n        \n      \n      EB\n      \n        \n      \n      V IDEO\n      \n        \n      \n      D ATA\n    \n    \n      \n        1 Introduction\n      \n      Images remain by far the most popular visual medium. Nowadays they are easy to acquire and distribute, contain rich visual detail, can easily be viewed and understood and, as a result, are ubiquitous  in the Web. As 2D projections of our 3D world, however, they may lack certain semantical information. For example, important parts of objects may be occluded, and depth data is typically missing. This poses serious challenges to applications involving image recognition, manipulation, editing, etc. that could greatly benefit from this omitted information. Hence, there is a strong motivation to lift images to 3D by inferring attributes lost in the projection. In this paper we are specifically interested in inferring depth for the visible object areas ? the key coordinate missing in the projection. As the problem of recovering depth from single image is naturally ill-posed, various priors have been proposed for regularization. The most common and classical approach is to match the input image to a set of 3D objects in a database (i.e., priors), and use the best matching shape to fill in missing depth information. However, large-scale deployment of such a method is fundamentally limited because only a limited number of 3D models is available. Most often, we do not even have a 3D model of the same or sufficiently similar object from which the image was taken. In this paper we consider the problem of estimating depth for an image of an object by exploiting, in a novel joint fashion, a collection of 3D models of related but largely different objects (see Figure 1 ). Key to our approach is the estimation of correspondences between the image and multiple models, with the help of correspondences estimated between the models themselves. We address the depth inference problem in its purest form, where we assume that the object image has been segmented from its background (such images are now commonplace in shopping web sites), while our 3D models are typically untextured and come from shape collections, such as the Trimble 3D warehouse. Our image-based but shape-driven modeling technique is fully automatic and reconstructs a 3D point cloud from the imaged object. The algorithm consists of a preprocessing stage, which aligns the input shapes to each other and learns a deformation model for each shape; and a reconstruction stage, which uses a continuous optimization to recover the image object pose and reconstruct a point cloud from the image that aligns with relevant 3D models extracted from the collection. We show how to formulate an appropriate objective function, how to obtain an initial solution, and how to effectively refine the solution using an alternating optimization. In our approach, we jointly match the depth-augmented image, i.e., the popup point cloud of the image, with a group of related shapes in the collection. We pose the task as a joint non-rigid registration problem, in which each shape can be deformed. The formulation has two key features. First, in contrast to utilizing a single similar shape, incorporating a collection of similar shapes offers a better coverage of the relevant neighborhood of shape space. Second, since we have already aligned the 3D models to each other, it enables us to apply consistency constraints [Kim et al. 2012a; Huang and Guibas 2013] to regularize the image to 3D model matching by using the shape-shape correspondences. In the joint non-rigid registration formulation, we introduce the key concept of deformation priors, which govern the deformation of each shape (c.f., [Averkiou et al. 2014]). Intuitively, we aim to preserve the key structural properties of each shape in the deformation, so that round shapes stay round, left-to-right symmetries are preserved, etc. In particular, instead of detecting these properties form each shape alone, which turns out to be unreliable, we learn them from the optimal deformations of each shape to other shapes. To test the performance of the proposed approach, we have created a benchmark dataset consisting of Microsoft Kinect scans of various categories of objects including chairs, tables, lamps, and cups. Experimental results show that the proposed approach recovers depth information that is close to Kinect scans, and is significantly more accurate than state-of-the-art image-based modeling techniques. Moreover, the proposed approach is robust to variations in textures and lighting conditions. We demonstrate depth-enhanced image editing to illustrate the possibilities offered by our approach. In addition, we show that our work is a key intermediate step towards the goal of obtaining full 3D models. Using a popup point cloud as input, we can reconstruct in certain cases a full mesh by exploiting shape symmetries learned from the shape network. Contributions. We present the first, to the best of our knowledge, fully automatic method to utilize a network of related but different 3D objects in order to reconstruct depth information from a single imaged object. The key novelties are: ? showing how a single modestly-sized shape network can help infer depth information for a variety of image objects of the same class; ? using learned deformation models based on an aligned shape network to compensate for the fact that the image is not from a model directly present in the database; ? regularizing model deformations using multi-way 3D alignment between the initial image point cloud and the shapes in a neighborhood of the shape network; In the process of extracting depth information on an image, we also discover good correspondences between the image and the network shapes, enabling us to connect the image to the network and transfer complementary information back and forth. Example of such information transfer can include textures, segmentations, material properties, labels, etc.\n      ACM Reference Format Su, H., Huang, Q., Mitra, N., Li, Y., Guibas, L. 2014. Estimating Image Depth Using Shape Collections. ACM Trans. Graph. 33, 4, Article 37 (July 2014), 11 pages. DOI = 10.1145/2601097.2601159 http://doi.acm.org/10.1145/2601097.2601159. Copyright Notice Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . Copyright ? ACM 0730-0301/14/07-ART37 $15.00. DOI: http://doi.acm.org/10.1145/2601097.2601159\n      Yangyan Li Leonidas Guibas Stanford University\n      ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n      37:2\n      ?\n      H. Su et al.\n      \n        2 Related Work\n        Data-driven geometry processing. The emergence of large shape collections provides us with a platform to aggregate information from multiple shapes to improve the analysis and processing of individual shapes. Already the Trimble 3D warehouse contains many thousands of example models per category for most indoor objects and some popular outdoor categories such as car and airplane. Recently, we have witnessed the success of data-driven techniques in shape analysis [Huang et al. 2011; Kim et al. 2012a; Kim et al. 2013; Huang et al. 2013; Wang et al. 2013], shape model ing [Chaudhuri et al. 2011; Kalogerakis et al. 2012; Averkiou et al. 2014] and shape reconstruction [Nan et al. 2012; Kim et al. 2012b; Shen et al. 2012]. The key task in data-driven geometry processing technique is to establish high-quality correspondences (at either pointor segment-level) across geometric objects. Although there exist rich techniques for aligning and matching 3D shapes, the problem of matching image objects and 3D shapes, which is the major focus of this paper, is far from being solved. Image-shape matching. Most existing image-shape matching approaches [Cyr and Kimia 2004; Xu et al. 2011; Wang et al. 2013] convert the problem into an image matching problem, i.e., matching images with projected views of 3D shapes. They typically start from estimating dense correspondences between silhouette curves, and then interpolate correspondences to interior pixels. [Sun et al. 2011] used an ICP-like approach. Recently, Wang et al. [2013] proposed a technique that directly estimates correspondences between entire image objects. The major limitation of these approaches is that a projected view of a 3D shape only contains partial information from the original shape. In practice, these techniques are limited to matching very similar objects. In contrast, we formulate the image shape matching problem as solving a non-rigid alignment problem in 3D, i.e., simultaneously estimating optimizing the depth of the image object and the deformations of 3D shapes to align them in the 3D space. In particular, we show that matching an image with a collection of 3D shapes boosts the matching quality by enforcing consistency between image-shape maps and shape-shape maps. Pose estimation. There exists a vast body of work to determine the pose of an object in an image relative to a calibrated camera. The problem is commonly formulated as a feature correspondence problem. Thus, they can be distinguished by the type of local image features, such as points, lines, curve segments, whole contours [Chen et al. 2003; Dalal and Triggs 2005; Oliva and Torralba 2006]. Recently, researchers used learning-based scheme to cast it as a classification and learn good features for viewpoint estimation [Zia et al. 2013]. The task of pose estimation is closely coupled with other tasks in the image-shape matching problem, such as depth estimation and point correspondence. We therefore model it as a part of the global optimization problem and iteratively refine it, resulting in large improvement. Depth estimation. Estimating the depth of an image object is a long standing problem in computer vision and computer graphics. This problem is ill-posed when the input is a single image, and existing approaches typically incorporate additional information such as user interaction [Wu et al. 2008] and shading [Lensch et al. 2003; Goldman et al. 2005], or using abstracted proxy shapes [Zheng et al. 2012]. However, these approaches are designed for objects with simple textures and shapes and/or under specific lighting conditions. In other words, they do not apply well on man-made objects in real images, which exhibit complicated geometries and textures. With the availability of large collection of depth images, recent depth estimation approaches are based on supervised learning [Hoiem et al. 2005; Saxena et al. 2009]. Given exemplar depth images, these approaches learn conditional probabilistic distributions of pixel depths and relative depths between neighboring pixels, and apply the learned distributions to infer the depth information of new images. We take a different approach. Since obtaining 3D shapes that are similar in global structure to image objects is easy, we estimate depth information in an unsupervised manner, i.e., by directly matching images with 3D shapes, and thus avoiding the tedious task of performing instance specific learning.\n      \n      \n        3 Pipeline Overview\n        The proposed image-based but shape-driven modeling approach takes a single image object I segmented from background and a collection of shapes S = {S 1 , S 2 , ? ? ? , S N } of the same class as  input, and simultaneously estimates the object pose shown in I and reconstructs a 3D point cloud P from I. For simplicity, we assume that all input shapes are supported by the same ground plane [Huang et al. 2013], so a common vertical direction is available. As the shape collection typically does not contain a shape that is exactly same as the object to be reconstructed, we formulate the task as a joint non-rigid alignment problem. The variables to be optimized are the point cloud, parameterized by the camera pose and the z-coordinates (pixel depths) of the image object, and deformations of a set of similar shapes. The objective function minimizes the distance between the point cloud and the deformed shapes. However, there are several challenges. First, the depth coordinates of the point cloud are unconstrained yet we cannot allow the shapes to be deformed arbitrarily, since otherwise both the point cloud and the shapes may be stretched undesirably when being aligned. Second, the success of the non-rigid alignment depends on a good initialization for both the camera pose and the point cloud. Third, even with good initialization, it is challenging to solve the induced optimization problem involving the depth of each pixel effectively. What helps in our situation, and the fundamental difference between the proposed approach and other shape-driven image based modeling techniques, is that we utilize the information provided by the collection to regularize the problem. As illustrated in Figure 2 , the pipeline consists of a preprocessing stage and a reconstruction stage. The goal of the preprocessing stage is to align the shapes and to learn a smart deformation prior (local model) for each shape. The motivation comes from the fact that plausible deformations of each shape typically lie in a lowdimensional space, when compared with the number of parameters in a general deformation model [Averkiou et al. 2014]. We learn the deformation prior of each shape by performing covariance analysis over its optimized deformations to neighboring shapes. As the deformation prior is directly learned by shapes, it inherits several structure-preserving properties (e.g., symmetry, part structure) from the shape collection. Essentially, we learn the local structure of the shape space. The reconstruction stage proceeds in three steps, where the first two steps provide an initial solution (a set of similar shapes and an initial point cloud) and the third step optimizes this point cloud to minimize its distance to the deformed similar shapes. Specifically, the first step initializes a camera configuration and extracts a set of similar shapes. This is considered as a pose estimation problem. Although pose estimation using a single shape is hard, we found that when a collection of oriented shapes are available, a simple cumulative score, which sums the weighted similarity scores of the rendered images to the input image, works remarkably well. This can be understood by the fact that the input shapes are aligned, and the best camera pose is voted on by all relevant input shapes together, then the pose tends to be much more stable than those generated from individual shapes. In the same spirit, when generating similar shapes, we combine both the image-shape distances and shape-shape distances to generate a more robust set of similar shapes for later steps of the pipeline. Given the rendered images of similar aligned shapes, the second step proceeds to initialize the depth information (z-coordinates) by building dense correspondences between the image object and similar shapes and transferring the depth information. Due to the differences between the input image and 3D shapes, we observed that it is extremely hard to obtain reliable correspondences via pair-wise image-shape matching. However, as the input shapes are aligned, we exploit the consistency of correspondences across the set of similar shapes, so that we can obtain much more reliable depth information. This is conceptually similar to state-of-the-art techniques in data-driven shape matching techniques [Kim et al. 2012a; Huang and Guibas 2013] to enforce consistency of correspondences along cycles to improve quality of isolated correspondences. Finally, in the third step we refine the camera pose and depth information using non-rigid registration formulated as solving a continuous optimization problem. The objective function combines a distance term, which evaluates the distance between the corresponding points on the induced point cloud and the deformed similar shapes, and two prior terms on the deformation models and the depth in- formation, respectively. Despite the non-linearity and scale of this optimization problem, we show that it can be optimized effectively using an alternating optimization strategy.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        \n          \n          Figure 2: Algorithm Pipeline. We reconstruct a 3D point cloud from an image object by utilizing a collection of related shapes. In the preprocessing stage, we jointly align the input shape collection and learn structure-preserving deformation models for the shapes. Then, in the reconstruction stage, we lift a single image to 3D in three steps. The first step initializes the camera pose in the coordinate system associated with the aligned shapes and extracts a set of similar shapes. The second step performs image-image matching to build dense correspondences between the image object and the similar shapes, and generate an initial 3D point cloud. The final step jointly aligns the initial point cloud and the selected shapes by simultaneously optimizing the depth of each image pixel, the camera pose, and the shape deformations.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        37:4\n        ?\n        H. Su et al.\n        \n          \n          Figure 3: Preprocessing Stage. We learn a deformation model of each shape via its optimized deformations to other shapes. Each deformation model is characterized by a small set of typical deformation fields (shown as vectors on mesh vertices) derived from covariance analysis. This model serves as the regularizer for the local shape space around each shape and is enforced during the reconstruction stage.\n        \n      \n      \n        4 Preprocessing Stage\n        The goal of the preprocessing stage is to understand the plausible deformations of each shape in the context provided by the input shape collection. We achieve this goal by aligning all input shapes and then learning a deformation prior for each shape. Deformation model and joint shape alignment. We use the embedded deformation model [Sumner et al. 2007] to parameterize the deformation of each shape. An embedded deformation consists of a list of control points p ? ? J and the associated basis functions B ? (?). Given a point x ? R 3 , its deformed counterpart is a linear combination of the control points:\n        \n          1\n          D(x) = B ? (x)p ? . p ? ?J\n        \n        Refer to [Sumner et al. 2007] on how to construct embedded deformation models on shapes. In this paper, we use 200 control points, and hence each shape is controlled by M = 600 parameters. To align the input shapes, we employ the method described in [Huang et al. 2013], which jointly optimizes the deformations of all input shapes to minimize the sum of distances between corresponding points computed using pair-wise alignment. We denote the optimized embedded deformation of shape S i by D i ? . Deformation-prior learning. We assume that plausible deformations of each shape (parameterized by a vector that collects all control points) lie in a low dimensional space defined by the shape?s neighborhood (c.f., [Ovsjanikov et al. 2011]). We learn this space from the optimal deformations of each shape S i to other shapes, which provide samples of plausible deformations (see Figure 3 ). We directly obtain these deformation samples by composing the absolute optimal deformations D i ? and their inverse deformations D i ? ?1 . For each shape S i and each neighboring shape S j , we transform the original control point p ? (i.e., in the rest state) of D i ? to (D j ? ?1 ? D i ? )(p ? ). Let c i,j be the vector that collects all transformed control points. Let c 0 i be the original control points. Then each neighboring shape S j gives rise to a deformation sample c i,j ? c 0 i . To learn the prior model from similar shapes, we only consider the deformation samples from the 128 most similar shapes to each shape, in terms of the D2 descriptor [Osada et al. 2002]. Given the deformation samples, we perform covariance analysis to extract the principal values ? 1 ? ? 2 ? ? ? ? ? M and principal directions u 1 , u 2 , ? ? ? , u M of the deformation space. The prior model on the deformation of each shape is given by\n        \n          2\n          M P rior(D i ) = ? ? 1 (c i ? c 0 i ) T u j 2 , ? j + ? j=1\n        \n        Intuitively, a deformation leads to a small objective value if it follows the majority of the deformation samples. Note that the shift ? is introduced to make the prior term well-defined as we only have limited deformation samples and many principal values are zero.\n      \n      \n        5 Reconstruction Stage\n        The reconstruction stage solves a joint optimization to recover the geometry of an image object that aligns with the deformed versions of a set of similar shapes. We begin by introducing the camera model. Then we show how to initialize an approximate solution in Section 5.2 and Section 5.3, and refine it to obtain the final solution in Section 5.4.\n      \n      \n        5.1 Camera Configuration\n        We use a simplified nine parameter camera configuration C = (R, t, z f , s x , s y ). Here (R, t) specifies the rigid motion from the common coordinate system ? associated with the input shapes to the camera coordinate system ? C ; z f specifies the focal length; s x and s y specify the effective size of the pixels in the horizontal and vertical directions. Given a point q = (q x , q y , q z ) T in ?, its corresponding pixel coordinate p = (p x , p y ) T is given by\n        \n          3\n          p x = q s x ? z q f z ? , p y = q s y ? z q f z ? , q ? = R T (q ? t).\n        \n        In the other direction, given a pixel p = (p x , p y ) T and a depth parameter z p specifying its z coordinate in ? C , the corresponding point in ? is given by\n        \n          4\n          T q = Rp ? + t, p ? = s x p x z p , s y p y z p , z p . z f z f\n        \n        For convenience, we denote the map from p, z p to q as C(p, z p ).\n      \n      \n        5.2 Step I: Camera Initialization\n        Candidate generation. Our candidate camera pose sampling strategy is similar to most pose estimation algorithms [Zia et al. 2013], which sample the viewing direction and fix the rest of the parameters to default values. Specifically, we let the camera position move on a viewing sphere centered at the origin with radius 5d, where d is the averaged shape diameter. The rest of the parameters are fixed as follows. The focal point t is placed at the origin. To fix R, we let the up-right direction of the camera system lie in the plane of the viewing direction and the z axis. Finally, we set z p = 3d, and set s x , s y so that, on the average, each object occupies half of the rendered image. We generate candidate camera configurations for each shape by uniformly sampling 500 viewing directions on the viewing sphere. Let C cand collect all candidate camera configurations. For each C ? C cand , we denote I i C as the rendered image of shape S i cropped using a tight bounding box surrounding object. Optimal candidate. When picking the optimal candidate, we follow the common strategy of evaluating rendered images by comparing them with the input image. Due to differences between real images and rendered images, standard single shape based approaches  typically require feature learning. However, we found that when a collection of aligned shapes are present, a simple cumulative similarity score between the input image and the rendered images is sufficient (see Figure 4 ): N\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        Estimating Image Depth Using Shape Collections\n        ?\n        37:5\n        \n          \n          \n          Figure 4: Camera Initialization by Voting. Left: given an image object (the sofa in the center), we can find multiple similar shapes, each of which independently proposes a camera pose candidate (the blue arrow for each candidate). Note that some candidates are far from optimal. Right: since shapes are already jointly oriented in the pre-processing step, they can be used to more accurately vote for the optimal pose (the red arrow).\n        \n        \n          5\n          C ? = argmin exp(? f (I) ? f (I i C ) 2 /2? 2 ), C?C cand i=1\n        \n        where f (?) is a given feature descriptor, ? = min i?{1,??? ,n},C?C cand f (I) ? f (I i C ) , and the exponential operator is introduced to down-weight the contribution of images that are less similar to the input image. We have various image descriptors including GIST [Oliva and Torralba 2001], HOG [Dalal and Triggs 2005], and the light-field descriptor [Chen et al. 2003]. Experimentally, we found that the feature descriptor that combines all the three features together yields the best result.\n      \n      \n        5.3 Step II: Point Cloud Initialization\n        C,\n        Given the initial camera configuration we generate an initial point cloud P from I. This is done by selecting a set of similar shapes to the input image, and then establishing dense correspondences between I and the similar shapes for transferring depth information. The performance of this step is crucial since it governs the global behavior of the final reconstruction. Although both image-based retrieval and image-image matching have been studied considerably in the past, we found that even state-of-the-art algorithms are insufficient for the purpose of transferring depth. Instead, the key idea of the proposed approach is to utilize the regularization provided by a collection of aligned shapes to boost the performance in each step: i) the similar shapes extracted from matching rendered images have to be similar with each other in the 3D space, and ii) pixels in the rendered images of different shapes corresponding to the same object image pixel should come from points close to each other in the 3D space where the aligned models live. Experimental results show that even with standard pair-wise techniques, the overall performance of joint matching approaches is sufficient for the purpose of depth initialization (see Figure 5 ). Similar shape extraction. A naive approach to extract the similar shapes is to compare the input image with rendered images (according to the selected view) one-by-one. However, even with learned feature similarity metrics, such an approach is insufficient due to the diversity in lighting and texture of the input image. Since our input shapes are aligned, we use the distances between shapes to guide the selection of similar shapes. Specifically, we first use the pairwise similarity score defined in (5) to extract K 0 = 32 similar shapes (i.e., an initial similar shape set). We then build a small weighted clique graph, which consists of the input image and the initial set of similar shapes, and use the diffusion distance [Coifman et al. 2005] to sort the initial similar shapes. The weight of each image-shape edge is given by (5), while the image descriptor is replaced by the D2 shape descriptor [Osada et al. 2002] for a shape-shape edge. Given the sorted shapes, we select the top K = 6 shapes as the final similar shape set. To simplify the notation let the similar shapes be denoted as S 1 , ? ? ? , S K . Correspondence initialization. We initialize the image-shape correspondences by matching the input image object (background is removed) and the rendered image object I i C of each shape S i . Given two image objects, we first apply [Munich and Perona 1999] to build dense correspondences between silhouette curves. Treating these correspondences as landmark correspondences, we then employ Laplacian deformation [Sorkine et al. 2004] to align I and I i C . After alignment, we derive the initial pixel-shape correspondences from the overlaid image objects. With M i ? I ? S i we denote the initial correspondences from I ? S i . Note that some pixels may not have correspondences due to ?holes? in the shapes. Correspondence pruning. So far we only compute the imageshape correspondences between the input image and each shape in isolation. A constraint that we can use to improve these correspondence is to make them consistent with the optimal deformations {D i ? } that align the input shapes. More precisely, given two correspondences (p, q i ) ? M i and (p, q j ) ? M j , if the distance between D i ? (q i ) and D j ? (q j ) is large, then at least one of these two correspondences is incorrect. In addition to enforcing this consistency property, we also prioritize the smoothness of correspondences, i.e., given two correspondences (p, q i ), (p ? , q ? i ) ? M i where p and p ? are neighbors and so should be q i and q ? i , we favor that either both of them are selected or both of them are pruned. As both the consistency property and the smoothness prior only involve pairs of correspondences, we formulate the correspondence pruning step as solving a binary second-order MRF problem. We introduce a binary random variable x c ? {0, 1} for each initial correspondence c ? ? K i=1 M i , where x c = 1 if c is selected and x c = 0 otherwise. We then define the two types of pair-wise potential functions. For each correspondence pair c i = (p, q i ) ? M i and c j = (p, q j ) ? M j , we define a consistency potential: otherwise, where ? is set as the 0.05 times the averaged shape diameter. For each pair of correspondences c = (p, q i ) ? M i and c ? = (p ? , q ? i ) ? M i where p, p ? and q i , q ? i are two pairs of neighboring pixels, we define a smoothness potential function as\n        \n          \n        \n        otherwise.\n        \n          Figure 5: Point Cloud Initialization. We start by performing image-image matching to obtain initial dense correspondences be-\n        \n        \n          Figure 6: Effect of point registration in 2D versus in 3D. We com-\n          \n        \n        Then the total potential function simply sums all pair-wise potential functions\n        \n          6\n          f = ?(x c , x ? ),\n        \n        \n          6\n          c\n        \n        (c,c ? )?P\n        where P collects all pairs of correspondences of consideration. For optimization, we apply tree-reweighted belief propagation (TRBP) [Szeliski et al. 2008], which is very effectively on binary MRF problems. For convenience, we still use M i to denote the remaining correspondences between I and S i after this stage. Geometry initialization. Using the dense correspondences, we compute the z coordinate of the corresponding point of each pixel p = (p x , p y ) (in the camera coordinate system of C ? ) by averaging z-coordinates of the corresponding points of similar shapes:\n        \n          7\n          z p = q z ? /|M(p)|, (q x ? , q y ? , q z ? ) T = R(q ? t), (p,q)?M(p)\n        \n        where R, t are given by C ? . Note that for each pixel p that does not belong to any correspondence, we copy the value of z p from the closest pixel that has correspondences. We then generate the initial point cloud P = {C(p, z p )|p ? I} according to (4).\n      \n      \n        5.4 Step III: Point Cloud Optimization\n        We refine the initial image-shape correspondences and the initial point cloud by solving a joint alignment problem, whose objective function minimizes the distance (defined via correspondences) between the point cloud and deformed similar shapes. We employ an ICP-like procedure, alternating between a continuous optimization step, which optimizes the continuous variables including camera configuration C, the z-coordinates of each pixel {z p |p ? I} and the deformation of each shape D i , 1 ? i ? K; and a discrete optimization step, which updates image-shape correspondences. Continuous optimization step. We consider multiple objectives for aligning the induced point-cloud and the similar shapes. The first term evaluates the sum of squared distances between the corresponding points:  Here, p is a weight that is adjusted to be higher for more reliable correspondence (p, q i ). We set w p = 1 for interior points and w p = 20 for points close to silhouettes. Note that another option is to measure the distance in the image domain. However, due to distance distortions in projection, two points that are close to each other in the image domain may be far from each other on the original shape. It turns out measuring the distance in the image domain leads to far less accurate results (see Figure 6 ). As f data considers each pixel independently, we next introduce a second term to regularize the z-coordinate of neighboring pixels: Finally, the third term applies the key deformation priors learned in the preprocessing stage:\n        K f data = 1 1 w p C(p, z p ) ? D i (q i ) 2 . K |M i | i=1 (p,q i )?M i\n        z p ? ) 2 .\n        prior(D i ).\n        Combining data regu , and prior , the energy minimization problem in the continuous optimization step takes the form:\n        \n          8\n          {z p },C,{D i }\n        \n        \n          8\n          min f data + ? r f regu + ? p f prior .\n        \n        In our experiments, we use throughout the same set of parameters: ? r = 0.01 and ? p = 1. We again apply alternating optimization to effectively optimize (8). In each step, we first fix the z-coordinates z p to optimize the camera configuration C and the shape deformations D i :\n        \n          9\n          min f data + ? p f prior .\n        \n        \n          9\n          C,{D i }\n        \n        We then fix C and D i to optimize the z-coordinates z p :\n        \n          10\n          min f data + ? r f regu . {z p }\n        \n        The key advantage of this alternating optimization strategy is that (9) and (10) are either sparse (constraining neighboring pixels) or of small-scale (camera configuration and deformation parameters). This enables us to apply second-order Newton methods to optimize them effectively, i.e., we solve a sparse or a small-scale linear system at each Newton iteration. As the objective terms consists of non-linear least squares, we apply a Gauss-Newton method for optimizing Equations (9) and (10). The derivation is quite standard and we omit the details. Discrete optimization step. Given the optimized point-cloud P = {C(p, z p )|p ? I} and deformed shapes, we proceed to optimize the image-shape correspondences. We first convert each deformed shape D i (S i ) into a point-cloud S i ? by simulating a scan from the current camera configuration. We then initialize M i to collect closest point-pairs M i init = {(p, q)|q = argmin p?q ? or p = argmin p ? ?q }. q ? ?S ? p ? ?P i  As there may only exist partial similarity between the image object I and each shape S i , we adopt the median thresholding scheme [Rusinkiewicz and Levoy 2001] to remove correspondences that are far from each other, leaving init M i = {(p, q)| p ? q ? 2? i , (p, q) ? M i }, init where ? i is the median of p ? q among each M i . Figure 8 shows an example of the non-rigid alignment process. In practice, only 4-6 alternating updates are sufficient for good results.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        Estimating Image Depth Using Shape Collections\n        ?\n        37:7\n        \n          \n        \n        image similar shapes popup point cloud (ours)\n        \n          Figure 7: Representative Results. We have evaluated our approach on five categories of objects. This figure shows representative results in each category. For every object we show the input 2D image, the extracted similar shapes, the reconstructed point cloud and finally the ground truth Kinect scan.\n        \n        Kinect point cloud\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        \n          \n          Figure 8: Intermediate Optimization Results. We employ an iterative scheme to simultaneously refine point cloud reconstruction and to deform similar shapes. This figure shows the image view (rows 1 and 3) and side view (rows 2 and 4) of the point cloud and the deformed shape in different iterations. Since strong and reliable constraints are imposed along the silhouette of the estimated image view, the results look correct from the original image view at the initialization step. However, the interior of the point cloud is initialized poorly, as can be observed from the side view. When the optimization proceeds, significant improvement can be observed from the side view as the shape deformation subspace prior from similar shapes regularizes the solution and ?propagates? the information to the otherwise under-determined interior regions.\n        \n      \n      \n        6 Evaluation\n        We evaluated the proposed shape-driven image-based modeling on various Kinect scans with associated color information.\n      \n      \n        6.1 Experimental Setup\n        Images. We consider five categories of objects: chairs, tables, cups, lamps and cars. Each category consists of 4-6 Kinect scans of objects with different shapes. Figure 7 shows representative results in each category. We assume all the objects are captured in our standard setting, where background is easy to remove. The Kinect scans are for evaluation purposes only. Shapes. The 3D shapes are from Trimble warehouse. Each category contains 2K-7K shapes (see Table 1 ), where the Chair data set is from [Kim et al. 2013], the Car data set is from [Huang et al. 2013], and the three remaining datasets were collected using a similar strategy to that described in [Kim et al. 2013]. Note that even with thousands of shapes, the shape space is not densely covered as can be seen from the extracted similar shapes (see Figure 7 ). Evaluation protocol. We evaluate the reconstructed point-cloud of each object image against the Kinect depth scans. To factor out the free scaling degree of freedom we first compute a similarity transform that aligns the reconstructed point cloud with the Kinect scan. Given the calibrated reconstruction P and the Kinect scan P Kinect , we propose two metrics to evaluate the quality of P. The first metric evaluates the Hausdorff distance between P and P Kinect : d(p, P Kinect ) = min Kinect p ? q , ?p ? P. q?P The second metric evaluates the deviation between the pair of corresponding points p and f (p) in P and P Kinect : d(p, f (p))) = p ? f (p) , ?p ? P.\n        \n          Table 1: Statistics on various datasets. Shapes are normalized to have diameter 1. ? ? and ? ? are the mean and standard deviation of the metrics defined in Sec 6.1. ? bs ? and ? ? bs are the numbers for\n        \n        the best matched single shape and those with no superscript are numbers by our algorithm. #shapes ? haus bs /? haus bs ? haus /? haus ? deviation /? deviation Chair 7.3K 0.17 / 0.15 0.05 / 0.03 0.11 / 0.10 Table 4.2K 0.14 / 0.12 0.06 / 0.06 0.12 / 0.13 Cup 1.1K 0.15 / 0.11 0.05 / 0.04 0.09 / 0.09 Lamp 2.0K 0.13 / 0.15 0.06 / 0.03 0.10 / 0.11 Car 1.7K 0.12 / 0.11 0.05 / 0.03 0.09 / 0.08\n        It is clear that the Hausdorff distance is invariant to interior drifting on the surface, while the correspondence deviation is more strict. For each distance metric we collect statistics on the mean and variance of d(p, P) over all points (see Table 1 ). As a baseline, we calculated the Hausdorff metric obtained by the most similar shape.\n      \n      \n        6.2 Analysis of Results\n         Table 1 and Figure 7 shows representative results for the proposed approach. Overall the results are reasonably good despite the obvious difficulty of the problem, with 68.2% correspondences falling below 0.02 times the averaged shape diameter. For all datasets, the Hausdorff distance error is considerably lower than that of the deviation error. As the shape of the point cloud is driven by the shape collection, this shows that using the shape collection as a good prior, the distribution of points is restricted to drift along the common shape space. The deviation error is large because the correspondences may glide along the shapes, which are not exactly the same. We next discuss the results for each category. Chair and tables. We evaluate on the chair and table categories because fine geometric details are present in these shapes. Like other man-made objects, chair and tables usually have strong symmetries, implying a lower-dimensional deformation space. On the other hand, the four legs may introduce matching ambiguities. On these categories, we find that our algorithm is limited when selfocclusion presents: the lower board is occluded by the front leg in Row 6 of Figure 7 and consequently part of it is attached to the leg. Cups. Cups are relatively small household items and usually have a circular symmetrical body. Interestingly, our method produces visually more pleasing results compared with the Kinect, because the object size is reaching the resolution limit of the sensor and the surface is specular, which is challenging for the structural light mechanism of the Kinect. Lamps. We choose this category because it has large variations in the possible shapes, particularly in the curvature of the pole. It can be seen that our algorithm succeeds in both lamp examples in Figure 7 . The success can be attributed to two reasons. First, we use a data-driven approach to implicitly combine parts from different shapes. Second, we use a non-rigid deformation field, which allows the bending of the pole. Cars. We choose this category as a common outdoor object having fine geometric details (e.g., wheels, side mirrors). Our algorithm could accurately estimates the depth of cars. On the other hand, the Kinect has problems in detecting windows and wheels, because they are too reflective or too dark respectively. Comparison to Automatic Pop-Up. Automatic Pop-Up [Hoiem et al. 2005] automatically reconstructs 3D information using a single image and was initially designed for outdoor scenes using plane classifiers. The software assumes simple geometric priors and tend to work poorly for complicated indoor objects with thin and fine features. We show the effect of Automatic Pop-Up on a chair model in Figure 9 using the pre-trained classifiers. Our algorithm is visu- ally significantly better than the output from this software (compare the last column of Figures 2 and 9).\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        \n          Figure 9:\n          \n        \n      \n      \n        6.3 Discussion\n        Image-Shape matching versus Image-Image matching. Imageimage matching is far less accurate than image-shape matching ( Figure 6 ). Two reasons accounts for the difference. First, the projection from 2D to 3D is perspective and two points close in 2D may be far away in 3D. Second, a 3D point cloud for a shape is obtainable in our setting and projecting it to 2D loses important information. In fact, strong perspective projection might still hurt the performance of our algorithm. For example, the seat of the second row in Figure 7 is estimated to be very thick, which can be attributed to very strong perspective effect close to the chair leg. Deformation prior is important. We find that our deformation shape prior is key to success. As illustrated in Figure 3 , the deformation space is low-dimensional and points are generally restricted to move parallel to meaningful axes or scale in a coordinated manner. Thus, such constraints makes sure that the local deformation maintain global symmetries. Evidently, in Figure 10 , because the problem of single image depth reconstruction is intrinsically underdetermined, poor results are obtained without a prior (top). On the contrary, the isotropic symmetry learned by the prior ensures that the deformation subspace has only 1D, which is a coordinated scaling around the y-axis. Timing. All experiments were conducted on a standard desktop platform with a 2.4GHz Intel Core 2 Duo-core and 12GB of RAM. For each category, the pre-processing stage is shared by all images, which takes 3507s for chairs, 2184s for tables, 516s for cups, 1223s for lamps and 1109s for cars. The camera initialization stage takes on average 0.3s for each input image, and most of the time was spent on extracting image features. For each image object, the point cloud initialization stage took ?7s in average, with ?1s on correspondence initialization and ? 6s on correspondence pruning. The point cloud optimization stage took ? 25s in average. The total running time for processing an image object was ? 33s.\n      \n      \n        7 Applications\n        In this section, we use a series of applications to show the usefulness of the reconstructed point cloud, including relighting image object, synthesizing unseen novel views, and depth-aware image composition. In the end, we show that, in some situations, a reasonable full mesh of an image object can be recovered using our point cloud as input, by exploiting shape symmetries. Relighting. Given an image, we estimate the depth of each pixel and use local PCA analysis to estimate normals and simulate the lighting effects under different illumination conditions. In Figure 11, we assume an ambient light and a diffusion reflection on the surface. Notice that the synthesized image is almost photo-realistic,  except some artifact at the top-right corner due to inaccurate depth estimation. Novel View Synthesis. Since the full 3D information for each pixel is available, we can simulate the movement of the camera in 3D and synthesize novel views. In Figure 12 , the synthesized view from our depth estimation using the inverse warping method [Marcato Jr 1998] is almost photo-realistic. In particular, as we can even accurately recover the depth information of the back mirror of the car, the appearance around the back mirror is quite natural when the car is rotating in the counter-clockwise direction (-30 and -15 deg). Note that the missing parts that are invisible in the image can possibly be recovered by exploiting model symmetry. Depth-Aware Image Composition. In Figure 13 we demonstrate an experiment in which we compose a 3D model of a woman with a sofa image, so that the woman is ?sitting? on the sofa. A correct composition should make sure that the woman?s body and legs cover the back arm, and her hip is covered by the front chair arm. Since the depth of the sofa can be recovered, we can compute this correct occlusion for each pixel (right), as opposed to unnatural occlusion patterns if no depth information is available (left). Symmetry-based Surface Reconstruction. The inferred point cloud only has points visible from the camera view. However, we show in this experiment that it is a key intermediate step towards full 3D model reconstruction. We hallucinate the missing parts by exploiting the model symmetry. We use [Mitra et al. 2006] to extract symmetry patterns from similar shapes and transport them to the point cloud. In Figure 14 , we discover a circular symme- try for the cup body and a plane symmetry for the handle, which also induces a segmentation of the cup. Thus, we can transport the symmetry parameters and achieve full shape recovery using [Mitra et al. 2007]. Finally, we apply Poisson reconstruction [Kazhdan et al. 2006] on the extracted point cloud for surface reconstruction with smoothing. The final result is in the bottom row of Figure 14 . We see that the reconstructed mesh generally looks natural from all views. However, because we only apply plane symmetry at the handle part, there is gap at the bottom of the handle and the reconstructed surface is not connected. Discovering better structural predictors to close the small gap is an interesting open problem for further exploration.\n        \n          Figure 10: Effect of Deformation Prior. For a circular symmetric\n        \n        circularly symmetric.\n        \n          \n          Figure 11: Relighting. Using the inferred depth information, we can build the normal map and simulate different lighting conditions. Leftmost column is the input image and the three columns on the right are the simulated illumination. A directional light source moves from left to right (top row), or up to down (bottom row).\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        37:10\n        ?\n        H. Su et al.\n        \n          \n          Figure 12: Novel View Synthesize. Simulated images by rotating cameras around the y-axis.\n        \n      \n      \n        8 Conclusion and Future Work\n        In this paper, we have presented a data-driven algorithm for adding depth information to an image object. The algorithm takes as input an image of a segmented object and a collection of 3D shapes of the same object class, and computes various geometric priors from the shape collection to optimize the depth estimation of the image object. This procedure is fully automatic. We have evaluated the performance of the presented approach on a benchmark that consists of Kinect scans of a variety of common objects taken under different lighting conditions. Experimental results show that our approach produces depth that is close to the ground-truth, and is superior to state-of-the-art depth estimators. We have also shown the usefulness of the our approach for various applications.  Besides the applications demonstrated in this paper, the presented depth estimator enables a variety of other applications in both computer graphics and computer vision. As an example, the shape collection can serve as the hub that links many image objects. This is particularly useful for retrieving similar image objects that were taken from drastically different view points that cannot be matched well by pure image methods. As another example, with the help of the image-shape network, we can propagate rich image labels for the purpose of categorizing shapes ? a challenging problem in shape analysis due to the lack of labeled shapes or of data combining 3D shapes and labels. Limitations. Of course, as stated, our approach requires a segmented image of an object and a knowledge of the object class. These are well studied problems in computer vision and future work can combine these with our approach. The presented method works best with man-made objects whose 3D models can be well aligned and where the variation in shape poses is modest. It does not apply well to objects of high variability, such as trees, or buildings, or of high articulation, such as animals. For these objects, it is important to utilize more specialized domain knowledge (i.e., skeletons and regular structures) to establish correspondences and estimate depth. Finally, in our experience, a minimum of a couple of hundreds of shapes is necessary for the algorithm to succeed. The intuition is that each part of the object in the image needs to have multiple correspondences for good regularization. Future work. There are ample opportunities for future research. While so far we have focused on estimating the depth of a single segmented object, it would be very interesting to generalize this approach to estimate the depth of an entire scene. This would require us to automate the object detection process and to take into account spatial relations among objects.\n        \n          \n          Figure 13: Depth-Aware Image Composition. Image composition may be tedious as direct overlay may lead to incorrect occlusions (Left). Given an image of a sofa and a 3D model of a sitting woman, occlusions between them can be correctly computed based upon the depth inferred by our algorithm (Right).\n        \n        \n          Figure 14: Symmetry-based Completion and Surface Reconstruction. Top: point clouds viewed from different angles. Bottom: sur-\n        \n      \n      \n        Acknowledgements\n        We thank the reviewers for their comments and suggestions on the paper. This work was supported in part by NSF grants IIS 1016324 and DMS 1228304, AFOSR grant FA9550-12-1-0372, NSFC grant 61202221, the Max Plack Center for Visual Computing and Communications, Google and Motorola research awards, a gift from HTC corporation, the Marie Curie Career Integration Grant 303541, the ERC Starting Grant SmartGeometry (StG-2013335373), and gifts from Adobe.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n        Estimating Image Depth Using Shape Collections\n        ?\n        37:11\n      \n      \n        References\n        \n          A VERKIOU , M., K IM , V., Z HENG , Y., AND M ITRA , N. J. 2014. Shapesynth: Parameterizing model collections for coupled shape exploration and synthesis. CGF.\n          C HAUDHURI , S., K ALOGERAKIS , E., G UIBAS , L., AND K OLTUN , V. 2011. Probabilistic reasoning for assembly-based 3d modeling. ACM ToG 30, 4 (Aug.), 35:1?35:10.\n          C HEN , D.-Y., T IAN , X.-P., S HEN , Y.-T., AND O UHYOUNG , M. 2003. On visual similarity based 3d model retrieval. CGF 22, 3, 223?232.\n          C OIFMAN , R. R., L AFON , S., L EE , A. B., M AGGIONI , M., N ADLER , B., W ARNER , F., AND Z UCKER , S. W. 2005. Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps. PNAS 102, 21, 7426?7431.\n          C YR , C. M., AND K IMIA , B. B. 2004. A similarity-based aspectgraph approach to 3d object recognition. IJCV 57, 1 (Apr.), 5?22.\n          D ALAL , N., AND T RIGGS , B. 2005. Histograms of oriented gradients for human detection. In Proc. CVPR, 886?893.\n          F UNKHOUSER , T., K AZHDAN , M., S HILANE , P., M IN , P., K IEFER , W., T AL , A., R USINKIEWICZ , S., AND D OBKIN , D. 2004. Modeling by example. ACM ToG 23, 3 (Aug.), 652?663.\n          G OLDMAN , D. B., C URLESS , B., H ERTZMANN , A., AND S EITZ , S. M. 2005. Shape and spatially-varying brdfs from photometric stereo. In Proc. ICCV, 341?348.\n          H OIEM , D., E FROS , A. A., AND H EBERT , M. 2005. Automatic photo pop-up. ACM ToG 24, 3 (July), 577?584.\n          H UANG , Q., AND G UIBAS , L. 2013. Consistent shape maps via semidefinite programming. CGF 32, 5, 177?186.\n          H UANG , Q., K OLTUN , V., AND G UIBAS , L. 2011. Joint shape segmentation using linear programming. ACM ToG 30, 6.\n          H UANG , Q.-X., S U , H., AND G UIBAS , L. 2013. Fine-grained semi-supervised labeling of large shape collections. ACM ToG 32, 6 (Nov.), 190:1?190:10.\n          K ALOGERAKIS , E., C HAUDHURI , S., K OLLER , D., AND K OLTUN , V. 2012. A probabilistic model for component-based shape synthesis. ACM ToG 31, 4 (July), 55:1?55:11.\n          K AZHDAN , M., B OLITHO , M., AND H OPPE , H. 2006. Poisson surface reconstruction. CGF, 61?70.\n          K IM , V. G., L I , W., M ITRA , N. J., D I V ERDI , S., AND F UNKHOUSER , T. 2012. Exploring collections of 3d models using fuzzy correspondences. ACM ToG 31, 4 (July), 54:1?54:11.\n          K IM , Y. M., M ITRA , N. J., Y AN , D.-M., AND G UIBAS , L. 2012. Acquiring 3d indoor environments with variability and repetition. ACM ToG 31, 6 (Nov.), 138:1?138:11.\n          K IM , V. G., L I , W., M ITRA , N. J., C HAUDHURI , S., D I V ERDI , S., AND F UNKHOUSER , T. 2013. Learning part-based templates from large collections of 3d shapes. ACM ToG 32, 4, 70:1?70:12.\n          L ENSCH , H. P. A., K AUTZ , J., G OESELE , M., H EIDRICH , W., AND S EIDEL , H.-P. 2003. Image-based reconstruction of spatial appearance and geometric detail. ACM ToG 22, 2.\n          M ARCATO J R , R. W. 1998. Optimizing an inverse warper. PhD thesis, Massachusetts Institute of Technology.\n          M ITRA , N. J., G UIBAS , L. J., AND P AULY , M. 2006. Partial and approximate symmetry detection for 3d geometry. ACM ToG, 560?568.\n          M ITRA , N. J., G UIBAS , L., AND P AULY , M. 2007. Symmetrization. ACM ToG 26, 3, #63, 1?8.\n          M UNICH , M. E., AND P ERONA , P. 1999. Continuous dynamic time warping for translation-invariant curve alignment with applications to signature verification. In ICCV, vol. 1.\n          N AN , L., X IE , K., AND S HARF , A. 2012. A search-classify approach for cluttered indoor scene understanding. ACM ToG 31, 6 (Nov.), 137:1?137:10.\n          O LIVA , A., AND T ORRALBA , A. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV 42, 3 (May), 145?175.\n          O LIVA , A., AND T ORRALBA , A. 2006. Building the gist of a scene: The role of global image features in recognition. Progress in Brain Research 155, 23?36.\n          O SADA , R., F UNKHOUSER , T., C HAZELLE , B., AND D OBKIN , D. 2002. Shape distributions. ACM ToG 21 (October), 807?832.\n          O VSJANIKOV , M., L I , W., G UIBAS , L., AND M ITRA , N. J. 2011. Exploration of continuous variability in collections of 3d shapes. ACM ToG 30, 4, 33:1?33:10.\n          R USINKIEWICZ , S., AND L EVOY , M. 2001. Efficient variants of the ICP algorithm. In 3DIM, 145?152.\n          S AXENA , A., S UN , M., AND N G , A. Y. 2009. Make3d: Learning 3d scene structure from a single still image. IEEE TPAMI 31, 5 (May), 824?840.\n          S HEN , C.-H., F U , H., C HEN , K., AND H U , S.-M. 2012. Structure recovery by part assembly. ACM ToG 31, 6, 180:1?180:11.\n          S ORKINE , O., C OHEN -O R , D., L IPMAN , Y., A LEXA , M., R OSSL  ? , C., AND S EIDEL , H.-P. 2004. Laplacian surface editing. CGF, 175?184.\n          S UMNER , R. W., S CHMID , J., AND P AULY , M. 2007. Embedded deformation for shape manipulation. ACM ToG 26, 3 (July).\n          S UN , M., K UMAR , S. S., B RADSKI , G., AND S AVARESE , S. 2011. Toward automatic 3d generic object modeling from one single image. In 3DIMPVT, IEEE.\n          S ZELISKI , R., Z ABIH , R., S CHARSTEIN , D., V EKSLER , O., K OLMOGOROV , V., A GARWALA , A., T APPEN , M., AND R OTHER , C. 2008. A comparative study of energy minimization methods for markov random fields with smoothness-based priors. IEEE TPAMI 30, 6 (June), 1068?1080.\n          W ANG , Y., G ONG , M., W ANG , T., C OHEN -O R , D., Z HANG , H., AND C HEN , B. 2013. Projective analysis for 3d shape segmentation. ACM ToG 32, 6 (Nov.), 192:1?192:12.\n          W U , T.-P., S UN , J., T ANG , C.-K., AND S HUM , H.-Y. 2008. Interactive normal reconstruction from a single image. ACM ToG, 119:1?119:9.\n          X U , K., Z HENG , H., Z HANG , H., C OHEN -O R , D., L IU , L., AND X IONG , Y. 2011. Photo-inspired model-driven 3d object modeling. ACM ToG 30, 4 (July), 80:1?80:10.\n          Z HENG , Y., C HEN , X., C HENG , M.-M., Z HOU , K., H U , S.-M., AND M ITRA , N. J. 2012. Interactive images: Cuboid proxies for smart image manipulation. ACM ToG 31, 4, 99:1?99:11.\n          Z IA , Z., S TARK , M., S CHIELE , B., AND S CHINDLER , K. 2013. Detailed 3d representations for object recognition and modeling. IEEE TPAMI 35, 11, 2608 ? 2623.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 37, Publication Date: July 2014\n      \n    \n  ",
  "resources" : [ ]
}