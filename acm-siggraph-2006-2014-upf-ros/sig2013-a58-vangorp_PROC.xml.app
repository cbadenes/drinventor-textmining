{
  "uri" : "sig2013-a58-vangorp_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a58-vangorp_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Perception of Perspective Distortions in Image-Based Rendering",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Peter-Vangorp",
      "name" : "Peter",
      "surname" : "Vangorp"
    }, {
      "uri" : "http://drinventor/Christian-Richardt",
      "name" : "Christian",
      "surname" : "Richardt"
    }, {
      "uri" : "http://drinventor/Emily A.-Cooper",
      "name" : "Emily A.",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Gaurav-Chaurasia",
      "name" : "Gaurav",
      "surname" : "Chaurasia"
    }, {
      "uri" : "http://drinventor/Martin S.-Banks",
      "name" : "Martin S.",
      "surname" : "Banks"
    }, {
      "uri" : "http://drinventor/George-Drettakis",
      "name" : "George",
      "surname" : "Drettakis"
    } ]
  },
  "bagOfWords" : [ "image-based", "rendering", "-lrb-", "ibr", "-rrb-", "create", "realistic", "image", "enrich", "simple", "geometry", "photograph", "e.g.", "map", "photograph", "building", "fa", "ade", "onto", "plane", "however", "soon", "viewer", "move", "away", "from", "correct", "viewpoint", "image", "retina", "become", "distorted", "sometimes", "lead", "gross", "misperception", "original", "geometry", "two", "hypothesis", "from", "vision", "science", "state", "how", "viewer", "perceive", "image", "distortion", "one", "claim", "can", "compensate", "they", "-lrb-", "therefore", "perceive", "scene", "geometry", "reasonably", "correctly", "-rrb-", "one", "claim", "can", "compensate", "-lrb-", "therefore", "can", "perceive", "rather", "significant", "distortion", "-rrb-", "we", "conduct", "rigorous", "experiment", "measure", "magnitude", "perceptual", "distortion", "occur", "ibr", "fa", "ade", "viewing", "we", "also", "conduct", "rating", "experiment", "assess", "acceptability", "distortion", "result", "two", "experiment", "be", "consistent", "one", "another", "from", "we", "experimental", "result", "we", "develop", "predictive", "model", "distortion", "street-level", "ibr", "which", "we", "use", "provide", "guideline", "acceptability", "virtual", "view", "capture", "camera", "density", "we", "perform", "confirmatory", "study", "validate", "we", "prediction", "illustrate", "use", "application", "guide", "user", "ibr", "navigation", "stay", "region", "where", "virtual", "view", "yield", "acceptable", "perceptual", "distortion", "keyword", "image-based", "rendering", "perception", "human", "vision", "dl", "pdf", "despite", "simplicity", "system", "create", "reasonably", "compelling", "3d", "experience", "we", "refer", "street-level", "ibr", "because", "system", "typically", "restrict", "viewer", "near", "one", "capture", "point", "image", "distortion", "refer", "retinal", "image", "-lrb-", "picture", "viewing", "-rrb-", "same", "image", "create", "when", "view", "original", "3d", "scene", "distortion", "can", "occur", "because", "display", "image", "distorted", "and/or", "because", "viewer", "position", "center", "projection", "-lrb-", "cop", "-rrb-", "key", "insight", "we", "work", "make", "link", "between", "distortion", "street-level", "ibr", "what", "study", "human", "vision", "tell", "we", "about", "result", "perceptual", "outcome", "vision", "science", "literature", "provide", "two", "useful", "hypothesis", "concern", "perception", "picture", "scene", "hypothesis", "retinal", "hypothesis", "scene", "hypothesis", "state", "viewer", "compensate", "incorrect", "view", "position", "so", "perceptual", "outcome", "much", "closer", "original", "3d", "scene", "than", "dictate", "distorted", "retinal", "image", "understand", "note", "viewer", "must", "position", "picture?s", "center", "projection", "retinal", "image", "faithful", "copy", "image", "would", "create", "view", "original", "3d", "scene", "retinal", "hypothesis", "other", "hand", "state", "viewer", "do", "compensate", "incorrect", "position", "rather", "perceptual", "outcome", "dictate", "distorted", "retinal", "image", "show", "each", "hypothesis", "valid", "different", "context", "system", "image", "first", "capture", "photograph", "fa", "ade", "from", "give", "position", "3d", "polygon", "turn", "view", "from", "new", "camera", "project", "2d", "form", "final", "image", "distortion", "occur", "because", "new", "camera", "have", "different", "cop", "than", "one", "associate", "original", "photograph", "situation", "have", "be", "study", "extensively", "vision", "science", "similar", "case", "arise", "picture", "within", "picture", "-lsb-", "Pirenne", "1970", "-rsb-", "when", "photograph", "within", "photograph", "slant", "two", "cop", "one", "photograph", "3d", "scene", "-lrb-", "cop", "usually", "central", "surface", "normal", "-rrb-", "one", "slant", "photograph", "within", "scene", "-lrb-", "cop", "central", "surface", "normal", "-rrb-", "viewer", "can", "compensate", "incorrect", "position", "respect", "first", "cop", "generally", "unable", "compensate", "incorrect", "position", "respect", "second", "cop", "thus", "compensation", "occur", "picture", "create", "one", "camera", "content", "create", "second", "camera", "street-level", "ibr", "create", "both", "scenario", "we", "use", "concept", "from", "vision", "science", "understand", "how", "image", "distortion", "create", "street-level", "ibr", "system", "perceive", "thereby", "determine", "how", "best", "create", "imagery", "without", "objectionable", "perceptual", "distortion", "case", "shape", "detail", "do", "change", "when", "simulated", "-lrb-", "virtual", "-rrb-", "viewpoint", "change", "would", "when", "change", "viewpoint", "original", "scene", "consequence", "image", "become", "distorted", "view-dependent", "texture", "mapping", "-lsb-", "Debevec", "et", "al.", "1998", "-rsb-", "unstructured", "lumigraph", "render", "-lsb-", "Buehler", "et", "al.", "2001", "-rsb-", "provide", "way", "choose", "among", "blend", "between", "multiple", "texture", "be", "capture", "from", "different", "viewpoint", "practice", "texture", "viewpoint", "sample", "densely", "so", "novel", "simulated", "viewpoint", "can", "still", "create", "image", "distortion", "angle", "distortion", "blend", "artifact", "quite", "objectionable", "occlusion", "boundary", "particularly", "synchronization", "camera", "motion", "key", "determinant", "perceive", "quality", "transition", "-lsb-", "Morvan", "O?Sullivan", "2009", "-rsb-", "transition", "more", "acceptable", "when", "exact", "edge", "correspondence", "coherent", "motion", "homogeneous", "region", "provide", "-lsb-", "Stich", "et", "al.", "2011", "-rsb-", "image", "simulated", "novel", "viewpoint", "more", "acceptable", "when", "transition", "faster", "fewer", "transition", "artifact", "distance", "viewpoint", "use", "texture", "small", "-lsb-", "Vangorp", "et", "al.", "2011", "-rsb-", "geometric", "distortion", "due", "view", "distance", "inconsistent", "field", "view", "image", "have", "be", "study", "context", "traditional", "render", "rather", "than", "ibr", "confirm", "observer", "make", "most", "accurate", "geometric", "estimate", "scene", "when", "close", "consistent", "viewpoint", "-lsb-", "Steinicke", "et", "al.", "2011", "-rsb-", "despite", "advance", "we", "understanding", "previous", "work", "have", "provide", "explanation", "why", "some", "image", "distortion", "acceptable", "while", "other", "we", "claim", "study", "human", "picture", "perception", "provide", "datum", "concept", "prove", "useful", "better", "implementation", "street-level", "ibr", "system", "instance", "when", "photograph", "take", "slant", "rectangle", "objectively", "parallel", "edge", "rectangle", "image", "non-parallel", "one", "extend", "line", "until", "intersect", "vanish", "point", "angle", "between", "line", "from", "viewer", "vanish", "point", "line", "from", "viewer", "rectangle", "specify", "slant", "tilt", "rectangle", "-lsb-", "Sedgwick", "1991", "-rsb-", "example", "when", "viewer", "leave", "right", "cop", "view", "picture", "its", "frame", "both", "eye", "compensate", "incorrect", "viewing", "position", "perceive", "3d", "structure", "reasonably", "accurately", "-lsb-", "Rosinski", "et", "al.", "1980", "Vishwanath", "et", "al.", "2005", "-rsb-", "some", "have", "claim", "compensation", "base", "use", "familiar", "shape", "cube", "-lsb-", "Perkins", "1972", "Yang", "Kubovy", "1999", "-rsb-", "while", "other", "have", "claim", "base", "measurement", "orientation", "surface", "picture", "-lsb-", "Wallach", "Marshall", "1986", "Vishwanath", "et", "al.", "2005", "-rsb-", "when", "slant", "picture", "object", "nearly", "perpendicular", "picture", "surface", "little", "compensation", "off-axis", "viewing", "occur", "-lsb-", "Goldstein", "1987", "Todorovi", "2008", "-rsb-", "we", "goal", "study", "how", "well", "hypothesis", "predict", "perceptual", "distortion", "street-level", "ibr", "street-level", "image-based", "Viewing", "another", "key", "goal", "use", "we", "finding", "provide", "practical", "guideline", "application", "setting", "we", "focus", "simplify", "image-based", "setup", "which", "akin", "exist", "visualization", "street-level", "imagery", "e.g.", "Google", "Maps", "Street", "ViewTM", "Bing", "Maps", "StreetsideTM", "Mappy", "Urban", "DiveTM", "while", "exact", "detail", "system", "always", "available", "use", "panoramic", "image", "capture", "discrete", "point", "along", "path", "render", "use", "equivalent", "view-dependent", "texture", "mapping", "-lsb-", "Debevec", "et", "al.", "1998", "-rsb-", "onto", "single", "planar", "proxy", "each", "fa", "ade", "similar", "technique", "most", "system", "transition", "between", "viewpoint", "occur", "fast", "blur", "mix", "between", "viewpoint", "possibly", "blend", "approach", "-lrb-", "e.g.", "akin", "Buehler", "et", "al.", "-lsb-", "2001", "-rsb-", "-rrb-", "paper", "we", "assume", "fa", "ade", "capture", "similar", "manner", "use", "panorama", "-lrb-", "wide-angle", "image", "-rrb-", "discrete", "point", "along", "path", "reproject", "onto", "planar", "proxy", "fa", "ade", "ground", "retinal", "hypothesis", "state", "perceptual", "outcome", "direct", "interpretation", "retinal", "image", "without", "compensation", "possibly", "incorrect", "position", "simulation", "camera", "viewer", "depend", "which", "corner", "we", "look", "eccentricity", "also", "affect", "distortion", "we", "maintain", "limit", "notation", "later", "differentiate", "case", "occur", "capture", "simulation", "projection", "we", "now", "derive", "capture-camera", "image-space", "coordinate", "vanish", "point", "two", "face", "note", "??", "fa", "ade", "frontoparallel", "capture", "camera", "transformation", "from", "project", "simulation", "coordinate", "view", "coordinate", "simply", "we", "now", "have", "all", "element", "need", "compute", "angle", "viewer", "perceive", "accord", "extended", "retinal", "hypothesis", "have", "same", "sign", "perceive", "angle", "total", "180", "front", "side", "analogous", "quantitative", "experiment", "shape", "perception", "would", "possible", "thus", "extended", "retinal", "hypothesis", "predict", "two-way", "interaction", "between", "simulation", "angle", "eccentricity", "angle", "we", "assume", "same", "formulum", "apply", "view", "display", "device", "so", "we", "use", "determine", "viewer?s", "distance", "from", "each", "device", "intended", "corner", "which", "always", "center", "image", "briefly", "indicate", "blinking", "dot", "do", "pose", "problem", "however", "because", "all", "participant", "encounter", "all", "condition", "so", "we", "could", "assess", "effect", "interest", "uncontaminate", "difference", "overall", "rating", "we", "do", "observe", "significant", "effect", "display", "device", "predict", "extend", "retinal", "hypothesis", "we", "use", "result", "we", "experiment", "determine", "amount", "perceptual", "distortion", "should", "consider", "acceptable", "real-world", "application", "simulation", "angle", "now", "always" ],
  "content" : "Image-based rendering (IBR) creates realistic images by enriching simple geometries with photographs, e.g., mapping the photograph of a building fa  ?ade onto a plane. However, as soon as the viewer moves away from the correct viewpoint, the image in the retina becomes distorted, sometimes leading to gross misperceptions of the original geometry. Two hypotheses from vision science state how viewers perceive such image distortions, one claiming that they can compensate for them (and therefore perceive scene geometry reasonably correctly), and one claiming that they cannot compensate (and therefore can perceive rather significant distortions). We then conducted a rigorous experiment that measured the magnitude of perceptual distortions that occur with IBR for fa  ?ade viewing. We also conducted a rating experiment that assessed the acceptability of the distortions. The results of the two experiments were consistent with one another. From our experimental results, we develop a predictive model of distortion for street-level IBR, which we use to provide guidelines for acceptability of virtual views and for capture camera density. We perform a confirmatory study to validate our predictions, and illustrate their use with an application that guides users in IBR navigation to stay in regions where virtual views yield acceptable perceptual distortions. Keywords: Image-based rendering, perception, human vision DL PDF W Despite their simplicity, such systems create reasonably compelling 3D experiences: we will refer to these as street-level IBR. Because of this, such systems typically restrict viewers to be near one of the capture points. Image distortion refers to retinal images (in picture viewing) that are not the same as the images created when viewing the original 3D scenes. Such distortions can occur because the displayed image is distorted and/or because the viewer is not positioned at the center of projection (COP). A key insight in our work is to make the link between distortions in street-level IBR and what studies of human vision tell us about the resulting perceptual outcomes. The vision science literature provides two useful hypotheses concerning the perception of pictures: the scene hypothesis and the retinal hypothesis. The scene hypothesis states that viewers compensate for incorrect viewing position, so the perceptual outcome is much closer to the original 3D scene than dictated by the distorted retinal image. To understand this, note that the viewer must be positioned at the picture?s center of projection for the retinal image to be a faithful copy of the image that would be created by viewing the original 3D scene. The retinal hypothesis, on the other hand, states that viewers do not compensate for incorrect position; rather the perceptual outcome is dictated by the distorted retinal image. 2, shows that each hypothesis is valid, but for different contexts. In these systems, an image is first captured by photographing a fa  ?ade from a given position. The 3D polygon is in turn viewed from a new camera, and projected into 2D to form the final image. Distortions occur because the new camera has a different COP than the one associated with the original photograph. Such situations have not been studied extensively in vision science, but a similar case arises in pictures within pictures [Pirenne 1970]. When a photograph within the photograph is slanted, there are two COPs, one for the photographed 3D scene (that COP is usually on the central surface normal) and one for the slanted photograph within the scene (that COP is not on the central surface normal). Viewers can compensate for their incorrect position with respect to the first COP, but are generally unable to compensate for incorrect position with respect to the second COP. Thus, compensation occurs for pictures created with one camera and not for content created by a second camera. Street-level IBR creates both of these scenarios. We will use concepts from vision science to understand how image distortions created in street-level IBR systems are perceived and thereby to determine how best to create such imagery without objectionable perceptual distortions. In such cases, the shape details do not change when the simulated (or virtual) viewpoint changes as they would when changing viewpoint in the original scene; as a consequence, the image becomes distorted. View-dependent texture mapping [Debevec et al. 1998] and unstructured Lumigraph rendering [Buehler et al. 2001] provide ways to choose among or blend between multiple textures that were captured from different viewpoints. In practice, the texture viewpoints are not sampled densely, so novel simulated viewpoints can still create image distortions ? angle distortions and blending artifacts ? that are quite objectionable. Occlusion boundaries, particularly their synchronization with camera motion, are key determinants of the perceived quality of transitions [Morvan and O?Sullivan 2009]. Transitions are more acceptable when exact edge correspondences, coherent motion and homogeneous regions are provided [Stich et al. 2011]. Images for simulated  novel viewpoints are more acceptable when the transitions are faster, there are fewer transition artifacts, and the distance to the viewpoint of the used texture is small [Vangorp et al. 2011]. Geometric distortions due to viewing distances inconsistent with the field of view of the image have been studied in the context of traditional rendering rather than IBR, confirming that observers make the most accurate geometric estimates of the scene when they are close to the consistent viewpoint [Steinicke et al. 2011]. Despite these advances in our understanding, previous work has not provided an explanation of why some image distortions are acceptable, while others are not. We claim that studies of human picture perception provide data and concepts that prove to be useful for a better implementation of street-level IBR systems. For instance, when a photograph is taken of a slanted rectangle, the objectively parallel edges of the rectangle are imaged as non-parallel. If one extends the lines until they intersect at the vanishing point, the angle between a line from the viewer to the vanishing point and a line from the viewer to the rectangle specifies the slant and tilt of the rectangle [Sedgwick 1991]. For example, when viewers are left or right of the COP and view the picture and its frame with both eyes, they compensate for their incorrect viewing position and perceive 3D structure reasonably accurately [Rosinski et al. 1980, Vishwanath et al. 2005]. Some have claimed that this compensation is based on the use of familiar shapes, such as cubes [Perkins 1972, Yang and Kubovy 1999] while others have claimed that it is based on measurement of the orientation of the surface of the picture [Wallach and Marshall 1986, Vishwanath et al. 2005]. When the slant of a pictured object is nearly perpendicular to the picture surface, little compensation for off-axis viewing occurs [Goldstein 1987, Todorovi? 2008]. Our goal is to study how well these hypotheses predict perceptual distortions in street-level IBR. Street-level Image-based Viewing Another key goal is to use our findings to provide practical guidelines in an application setting. We will focus on a simplified image-based setup which is akin to existing visualizations of street-level imagery, e.g., Google Maps Street ViewTM, Bing Maps StreetsideTM and Mappy Urban DiveTM. While exact details of these systems are not always available, they use panoramic images captured at discrete points along a path, and rendered using the equivalent of view-dependent texture mapping [Debevec et al. 1998] onto a single planar proxy for each fa  ?ade, or a similar technique. In most of these systems, transitions between viewpoints occur as a fast blurred mix between the viewpoints, possibly with a blending approach (e.g., akin to Buehler et al. [2001]). In this paper, we assume fa  ?ades are captured in a similar manner: using panoramas (or wide-angle images) at discrete points along a path and reprojected onto a planar proxy for the fa  ?ade and ground. 2, the retinal hypothesis states that the perceptual outcome is a direct interpretation of the retinal image without compensation for possibly incorrect positions of the simulation camera or viewer. Depending on which corner we look at, eccentricity also affects distortion. We maintain the limit notation to later differentiate cases occurring in the capture and simulation projections. We now derive the capture-camera image-space coordinates for the vanishing points of the two faces. ? Note that x c = ?? if the fa  ?ade is frontoparallel to the capture camera. The transformation from projected simulation coordinates to viewing coordinates is simply: We now have all the elements needed to compute the angle the viewer will perceive according to the extended retinal hypothesis. If ? e and ? s have the same sign, the perceived angle is ? total = 180? ? ? front + ? side . An analogous quantitative experiment for shape perception would not be possible. Thus, the extended retinal hypothesis predicts a two-way interaction between simulation angle and eccentricity angle. We assume the same formula applies for viewing display devices, so we used it to determine the viewer?s distance from each device. The intended corner, which was always in the center of the image, was briefly indicated by a blinking dot. This does not pose a problem, however, because all participants encountered all conditions, so we could assess the effects of interest uncontaminated by differences in overall ratings. We did not observe the significant effect of display device that was predicted by the extended retinal hypothesis. We use the results of our experiments to determine the amount of perceptual distortion that should be considered acceptable in real-world applications. The simulation angle ? s is now always 0?.",
  "resources" : [ ]
}