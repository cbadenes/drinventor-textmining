{
  "uri" : "sig2013a-a194-hu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013a/a194-hu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shi-Min-Hu",
      "name" : "Shi-Min",
      "surname" : "Hu"
    }, {
      "uri" : "http://drinventor/Kun-Xu",
      "name" : "Kun",
      "surname" : "Xu"
    }, {
      "uri" : "http://drinventor/Li-Qian-Ma",
      "name" : "Li-Qian",
      "surname" : "Ma"
    }, {
      "uri" : "http://drinventor/Bin-Liu",
      "name" : "Bin",
      "surname" : "Liu"
    }, {
      "uri" : "http://drinventor/Bi-Ye-Jiang",
      "name" : "Bi-Ye",
      "surname" : "Jiang"
    }, {
      "uri" : "http://drinventor/Jue-Wang",
      "name" : "Jue",
      "surname" : "Wang"
    } ]
  },
  "bagOfWords" : [ "1cf735ccd5433cbbcf092a98b50e263b1fb871957b3942d7a6508c77d93ce0ca", "p2t", "10.1145", "2508363.2508371", "name", "identification", "possible", "inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "Shi-Min", "Hu", "Kun", "Xu", "li-qian", "Ma", "Bin", "Liu", "Bi-Ye", "Jiang", "tnlist", "Tsinghua", "University", "Beijing", "Adobe", "Research", "Before-and-After", "image", "pair", "recover", "Editing", "history", "before", "after", "scale", "region", "adjust", "hue", "copy", "rotate", "Figure", "give", "source", "image", "edit", "copy", "-lrb-", "left", "-rrb-", "we", "system", "automatically", "recover", "semantic", "editing", "history", "-lrb-", "middle", "-rrb-", "which", "can", "use", "various", "application", "re-editing", "-lrb-", "right", "-rrb-", "case", "second", "editing", "step", "recovered", "history", "involve", "hue", "modification", "alter", "change", "berry", "difference", "color", "image", "courtesy", "Andrea", "Lein", "we", "study", "problem", "inverse", "image", "editing", "which", "recover", "semantically-meaningful", "editing", "history", "from", "source", "image", "edit", "copy", "we", "approach", "support", "wide", "range", "commonlyused", "editing", "operation", "crop", "object", "insertion", "removal", "linear", "non-linear", "color", "transformation", "spatiallyvary", "adjustment", "brush", "give", "input", "image", "pair", "we", "first", "apply", "dense", "correspondence", "method", "between", "they", "match", "edited", "image", "region", "source", "each", "edit", "region", "we", "determine", "geometric", "semantic", "appearance", "operation", "have", "be", "apply", "finally", "we", "compute", "optimal", "editing", "path", "from", "region-level", "editing", "operation", "base", "predefined", "semantic", "constraint", "recovered", "history", "can", "use", "various", "application", "image", "re-editing", "edit", "transfer", "image", "revision", "control", "user", "study", "suggest", "editing", "history", "generate", "from", "we", "system", "semantically", "comparable", "one", "generate", "artist", "cr", "category", "i.", "3.4", "-lsb-", "Computer", "Graphics", "-rsb-", "Graphics", "Utilities", "Graphics", "editor", "i.", "4.9", "-lsb-", "image", "processing", "computer", "Vision", "-rsb-", "application", "keyword", "image", "editing", "history", "inverse", "image", "editing", "history", "recovery", "region", "match", "Links", "dl", "pdf", "EB", "ACM", "Reference", "Format", "Hu", "S.", "Xu", "K.", "Ma", "L.", "Liu", "B.", "Jiang", "B.", "Wang", "J.", "2013", "inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "ACM", "Trans", "graph", "32", "Article", "194", "-lrb-", "November", "2013", "-rrb-", "11", "page", "dous", "10.1145", "2508363.2508371", "http://doi.acm.org/10.1145/2508363.2508371", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "author", "-lrb-", "-rrb-", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "2013", "copyright", "hold", "Owner/Author", "publication", "rights", "license", "ACM", "0730-0301/13", "11-art194", "15.00", "DOI", "http://dx.doi.org/10.1145/2508363.2508371", "jue", "Wang", "application", "re-edited", "image", "introduction", "image", "editing", "series", "operation", "often", "perform", "accomplish", "editing", "task", "instance", "user", "may", "first", "select", "object", "image", "apply", "geometric", "transform", "adjust", "its", "shape", "position", "apply", "various", "color", "adjustment", "enhance", "its", "appearance", "process", "repeat", "multiple", "image", "region", "need", "touch", "up", "achieve", "editing", "goal", "result", "long", "sometimes", "complicated", "editing", "history", "have", "clean", "complete", "editing", "history", "available", "require", "many", "graphic", "file", "management", "application", "image", "editing", "revision", "control", "-lsb-", "Chen", "et", "al.", "2011", "-rsb-", "automatic", "tutorial", "generation", "-lsb-", "grabler", "et", "al.", "2009", "-rsb-", "editing", "visualization", "-lsb-", "Heer", "et", "al.", "2008", "-rsb-", "retention", "editing", "history", "also", "lead", "new", "possibility", "image", "editing", "adjust", "history", "produce", "result", "different", "variation", "from", "source", "image", "transfer", "history", "another", "image", "etc.", "editing", "goal", "much", "harder", "achieve", "without", "have", "editing", "history", "unfortunately", "although", "exist", "software", "provide", "powerful", "editing", "tool", "universal", "efficient", "solution", "encode", "store", "transmit", "re-use", "editing", "history", "instance", "Adobe", "Photoshop", "only", "allow", "partial", "history", "save", "either", "image", "file", "command", "log", "file", "often", "case", "complete", "history", "available", "after", "editing", "task", "accomplish", "furthermore", "image", "editing", "trial", "error", "process", "especially", "amateur", "can", "easily", "take", "dozen", "operation", "explore", "different", "idea", "fix", "error", "fine", "tweak", "parameter", "before", "produce", "desire", "output", "result", "raw", "editing", "history", "produce", "novice", "user", "often", "long", "redundant", "less", "understandable", "may", "directly", "applicable", "some", "application", "require", "clean", "history", "work", "we", "study", "problem", "recover", "clean", "semantically", "meaningful", "editing", "history", "give", "source", "image", "edit", "version", "which", "we", "call", "inverse", "image", "editing", "task", "involve", "several", "technical", "challenge", "firstly", "we", "need", "discover", "which", "object", "region", "have", "be", "edit", "secondly", "we", "need", "determine", "how", "each", "object", "region", "have", "be", "edit", "finally", "semantically", "meaningful", "editing", "path", "need", "generate", "from", "recover", "local", "editing", "operation", "we", "system", "provide", "set", "solution", "problem", "particular", "we", "improve", "state-ofthe-art", "region", "match", "method", "handle", "large", "appearance", "difference", "between", "original", "object", "its", "edit", "version", "-lrb-", "see", "Sec", "-rrb-", "we", "propose", "new", "method", "recover", "semantic", "appearance", "operator", "each", "edit", "region", "-lrb-", "see", "Sec", "-rrb-", "merge", "they", "form", "meaningful", "editing", "path", "under", "predefined", "semantic", "constraint", "-lrb-", "see", "Sec", "-rrb-", "we", "also", "demonstrate", "how", "recovered", "history", "can", "apply", "various", "application", "-lrb-", "see", "Sec", "-rrb-", "impossible", "practice", "recover", "all", "editing", "operation", "may", "have", "be", "apply", "image", "we", "system", "support", "set", "commonly", "use", "linear", "non-linear", "geometric", "color", "adjustment", "arbitrary", "combination", "they", "specifically", "each", "edit", "step", "may", "contain", "-lrb-", "necessarily", "-rrb-", "follow", "operation", "-lrb-", "-rrb-", "select", "object", "-lrb-", "region", "-rrb-", "-lrb-", "-rrb-", "apply", "geometric", "transform", "may", "involve", "scaling", "rotation", "translation", "flip", "-lrb-", "-rrb-", "apply", "per-region", "color", "adjustment", "may", "involve", "brightness", "exposure", "hue", "saturation", "tone", "adjustment", "spatially-varying", "per-pixel", "strength", "map", "simulate", "local", "paint", "brush", "operator", "when", "stack", "together", "provide", "vast", "editing", "possibility", "cover", "many", "popular", "adjustment", "tool", "modern", "image", "editing", "software", "evaluate", "proposed", "system", "we", "construct", "test", "dataset", "contain", "before-and-after", "image", "pair", "along", "original", "editing", "step", "perform", "artist", "user", "study", "dataset", "reveal", "editing", "history", "generate", "we", "system", "general", "comparable", "original", "one", "term", "semantic", "meaningfulness", "-lrb-", "see", "Sec", "-rrb-", "http://www.photoshop.com/", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "194:2", "s.-m", "Hu", "et", "al.", "related", "work", "region", "matching", "between", "image", "core", "component", "we", "system", "build", "correspondence", "between", "region", "original", "edited", "image", "sparse", "matching", "method", "sift", "matching", "-lsb-", "Lowe", "2004", "-rsb-", "applicable", "dense", "correspondence", "require", "we", "application", "conventional", "optical", "flow", "method", "-lsb-", "Brox", "et", "al.", "2009", "Zimmer", "et", "al.", "2011", "-rsb-", "can", "apply", "either", "since", "edit", "region", "may", "undergo", "large", "appearance", "transformation", "violate", "apparent", "motion", "assumption", "optical", "flow", "recent", "sift", "flow", "method", "-lsb-", "Liu", "et", "al.", "2008", "-rsb-", "dual", "bootstrap", "method", "-lsb-", "Yang", "et", "al.", "2007", "-rsb-", "combine", "sparse", "feature", "dense", "match", "through", "seed-and-grow", "scheme", "still", "have", "difficulty", "match", "textureless", "object", "where", "feature", "hard", "extract", "recently", "family", "patch-based", "fast", "image", "correspondence", "method", "have", "be", "propose", "patchmatch", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "randomized", "algorithm", "find", "approximate", "nearest", "neighbor", "between", "image", "patch", "have", "be", "further", "extend", "include", "geometric", "color", "transformation", "during", "match", "-lsb-", "Barnes", "et", "al.", "2010", "-rsb-", "find", "exact", "nearest", "patch", "-lsb-", "Xiao", "et", "al.", "2011", "-rsb-", "build", "upon", "generalize", "PatchMatch", "algorithm", "-lsb-", "Barnes", "et", "al.", "2010", "-rsb-", "HaCohen", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "non-rigid", "dense", "correspondence", "-lrb-", "NRDC", "-rrb-", "algorithm", "match", "region", "between", "image", "shared", "content", "we", "adopt", "framework", "algorithm", "extend", "we", "application", "describe", "Sec", ".4", "editing", "process", "management", "Kurlander", "Feiner", "-lsb-", "1988", "-rsb-", "present", "early", "work", "generate", "editable", "graphical", "visualization", "long", "user", "session", "more", "recently", "Heer", "et", "al.", "-lsb-", "2008", "-rsb-", "propose", "visualization", "system", "interactively", "generate", "graphical", "history", "su", "et", "al.", "-lsb-", "2009", "-rsb-", "provide", "another", "interactive", "visualization", "approach", "manage", "operation", "history", "vector", "graphic", "Grabler", "et", "al.", "-lsb-", "2009", "-rsb-", "propose", "system", "automatically", "create", "stepby-step", "visually", "appeal", "tutorial", "complicated", "image", "editing", "process", "Chen", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "nonlinear", "revision", "control", "method", "image", "editing", "use", "direct", "acyclic", "graph", "managing", "visualize", "editing", "process", "Chen", "et", "al.", "-lsb-", "2012", "-rsb-", "suggest", "adaptive", "history", "which", "automatically", "segment", "group", "lengthy", "sequence", "editing", "command", "easy", "navigation", "Delta", "system", "-lsb-", "Kong", "et", "al.", "2012", "-rsb-", "can", "help", "user", "identify", "tradeoff", "between", "workflow", "use", "visual", "comparison", "all", "system", "require", "editing", "process", "available", "Fu", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "system", "estimate", "reasonable", "drawing", "order", "from", "static", "line", "drawing", "which", "spirit", "similar", "we", "proposed", "system", "limit", "line", "art", "rather", "than", "natural", "image", "edit", "transfer", "Berthouzoz", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "content", "adaptive", "macro", "transfer", "complex", "image", "manipulation", "apply", "source", "image", "new", "target", "image", "learn", "relationship", "between", "image", "feature", "parameter", "editing", "procedure", "similarly", "Bychkovsky", "et", "al.", "-lsb-", "2011", "-rsb-", "learn", "global", "tone", "adjustment", "model", "from", "training", "image", "which", "can", "automatically", "apply", "new", "image", "again", "system", "require", "editing", "procedure", "know", "image", "analogy", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "provide", "classic", "example", "edit", "transfer", "without", "recover", "editing", "process", "however", "can", "handle", "geometric", "object-level", "edit", "repfinder", "system", "-lsb-", "Cheng", "et", "al.", "2010", "-rsb-", "find", "repeated", "scene", "element", "image", "so", "edit", "make", "one", "element", "can", "transfer", "other", "one", "many", "application", "we", "recover", "history", "from", "one", "image", "pair", "can", "apply", "new", "image", "achieve", "semantic", "editing", "transfer", "imageadmixture", "system", "-lsb-", "Zhang", "et", "al.", "2012", "-rsb-", "find", "object-level", "group", "element", "image", "allow", "object", "mix", "appearance", "transfer", "between", "different", "image", "Y?cer", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "transfusive", "image", "manipulation", "automatic", "approach", "transfer", "edit", "make", "one", "image", "other", "contain", "same", "object", "scene", "Algorithm", "Overview", "pipeline", "we", "algorithm", "show", "fig.", "consist", "three", "main", "step", "-lrb-", "-rrb-", "region", "match", "-lrb-", "-rrb-", "recover", "semantic", "appearance", "operator", "each", "match", "region", "pair", "-lrb-", "-rrb-", "generate", "editing", "history", "specifically", "we", "first", "find", "all", "match", "region", "pair", "between", "source", "edit", "image", "-lrb-", "sec", "-rrb-", "achieve", "use", "matching", "method", "extend", "from", "non-rigid-densecorrespondence", "-lrb-", "NRDC", "-rrb-", "algorithm", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "accommodate", "wider", "range", "appearance", "difference", "between", "pair", "region", "secondly", "we", "recover", "semantic", "appearance", "operation", "each", "match", "region", "pair", "-lrb-", "sec", "-rrb-", "which", "may", "include", "both", "global", "linear", "color", "transform", "brightness", "exposure", "hue", "saturation", "adjustment", "well", "non-linear", "tone", "mapping", "local", "brush", "spatially-varying", "strength", "finally", "give", "match", "region", "pair", "recover", "editing", "operation", "we", "use", "optimization", "approach", "generate", "compact", "semanticallymeaningful", "editing", "history", "accord", "set", "predefined", "editing", "rule", "-lrb-", "sec", "-rrb-", "region", "match", "first", "step", "algorithm", "we", "seek", "reliably", "recover", "region", "pair", "between", "source", "edit", "image", "share", "same", "content", "compare", "two", "match", "region", "give", "we", "means", "discover", "whether", "source", "region", "have", "be", "edit", "so", "how", "we", "region", "match", "approach", "extend", "from", "NRDC", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "algorithm", "have", "better", "capability", "handle", "large", "appearance", "transform", "specifically", "we", "adopt", "coarse-to-fine", "framework", "original", "NRDC", "approach", "which", "initially", "downsample", "image", "low", "resolution", "find", "match", "use", "they", "constraint", "find", "match", "next", "finer", "resolution", "each", "level", "follow", "four", "step", "apply", "sequentially", "-lrb-", "-rrb-", "nearest", "neighbor", "search", "-lrb-", "-rrb-", "patch", "merge", "-lrb-", "-rrb-", "color", "transform", "estimation", "-lrb-", "-rrb-", "coarse-tofine", "propagation", "after", "above", "step", "finish", "finest", "level", "boundary", "refinement", "step", "apply", "produce", "accurate", "boundary", "each", "match", "region", "next", "we", "describe", "step", "difference", "from", "original", "NRDC", "approach", "more", "detail", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "Inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "194:3", "Input", "Matched", "Region", "Pairs", "appearance", "Operators", "Hue", "Sat", "Bri", "Exp", "Figure", "Pipeline", "propose", "inverse", "image", "editing", "system", "4.1", "nearest", "neighbor", "search", "give", "source", "image", "edit", "image", "each", "patch", "we", "goal", "find", "closest", "source", "patch", "minimize", "distance", "measure", "-lrb-", "-rrb-", "4d", "geometric", "transformation", "-lrb-", "i.e.", "represent", "2d", "translation", "rotation", "scaling", "flip", "-rrb-", "use", "represent", "geometric", "relationship", "between", "above", "problem", "can", "efficiently", "solve", "generalize", "patchmatch", "-lsb-", "Barnes", "et", "al.", "2010", "-rsb-", "which", "we", "now", "briefly", "review", "initially", "few", "random", "transformation", "assign", "each", "patch", "algorithm", "improve", "transformation", "iterate", "between", "propagation", "random", "search", "step", "propagation", "step", "proceeds", "scan-line", "order", "replace", "transform", "each", "patch", "its", "neighbor", "appropriate", "random", "search", "step", "each", "patch", "randomly", "find", "several", "transform", "further", "evaluation", "transform", "randomly", "choose", "from", "window", "exponentially", "decrease", "size", "we", "utilize", "generalize", "PatchMatch", "improve", "upon", "several", "way", "firstly", "like", "NRDC", "we", "use", "floating-point", "coordinate", "support", "sub-pixel", "precision", "secondly", "account", "crosschannel", "color", "change", "between", "patch", "we", "define", "match", "cost", "-lrb-", "-rrb-", "from", "patch", "patch", "4.3", "Color", "transform", "estimation", "-lrb-", "-rrb-", "min", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "represent", "color", "value", "pixel", "patch", "color", "transform", "matrix", "obtain", "least", "square", "minimization", "above", "equation", "account", "cross-channel", "color", "transform", "hue", "change", "measure", "however", "do", "work", "well", "degenerate", "case", "e.g.", "patch", "uniform", "color", "hence", "we", "define", "final", "distance", "between", "-lrb-", "-rrb-", "max", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "thirdly", "do", "PatchMatch", "Stereo", "method", "-lsb-", "Bleyer", "et", "al.", "2011", "-rsb-", "we", "add", "additional", "local", "refinement", "step", "each", "iteration", "step", "each", "patch", "we", "locally", "optimize", "store", "geometric", "transform", "use", "gradient", "descent", "method", "further", "reduce", "distance", "measure", "eqn", "because", "4d", "transformation", "space", "too", "large", "random", "search", "efficiently", "find", "optimal", "transform", "recover", "Editing", "history", "delete", "fish", "adjust", "hue", "Hue", "Tone", "sit", "Bri", "Exp", "adjust", "hue", "brightness", "tone", "adjust", "hue", "whole", "image", "move", "scale", "fish", "adjust", "brightness", "4.2", "Patch", "merge", "after", "best", "transform", "have", "be", "identify", "each", "patch", "adjacent", "patch", "which", "consistent", "merge", "larger", "region", "achieve", "we", "define", "consistency", "error", "between", "two", "patch", "edit", "image", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "represent", "two", "patch", "centered", "respectively", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "estimate", "geometric", "color", "transform", "they", "two", "term", "measure", "difference", "geometric", "color", "transform", "between", "respectively", "balancing", "weight", "which", "we", "fix", "0.1", "when", "use", "normalize", "color", "spatial", "coordinate", "-lsb-", "-rsb-", "note", "corresponding", "definition", "NRDC", "approach", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "do", "include", "color", "difference", "term", "which", "essential", "distinguish", "patch", "only", "color", "transformation", "next", "we", "adopt", "greedy", "scheme", "merge", "neighbor", "patch", "base", "propose", "consistency", "error", "specifically", "we", "randomly", "select", "patch", "greedily", "grow", "region", "merge", "its", "neighbor", "patch", "whose", "consistency", "error", "respect", "select", "patch", "low", "-lrb-", "threshold", "10", "-rrb-", "until", "more", "neighbor", "can", "include", "we", "select", "another", "patch", "another", "grow", "process", "follow", "NRDC", "we", "prune", "region", "too", "small", "-lrb-", "less", "than", "image", "size", "-rrb-", "process", "result", "set", "merged", "region", "each", "correspond", "match", "region", "a.", "each", "match", "region", "pair", "we", "assume", "per-region", "cross-channel", "cubic", "color", "transform", "between", "they", "along", "per-pixel", "smoothly-varying", "strength", "map", "allow", "we", "handle", "rich", "set", "possible", "color", "editing", "operation", "include", "hue", "contrast", "saturation", "tone", "brightness", "adjustment", "also", "important", "note", "introduce", "spatially-varying", "strength", "map", "allow", "system", "support", "various", "paint", "brush", "local", "editing", "mathematically", "color", "transform", "formulate", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "where", "denote", "pixel", "region", "corresponding", "pixel", "-lrb-", "-rrb-", "denote", "color", "value", "m-th", "channel", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "194:4", "s.-m", "Hu", "et", "al.", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "boundary", "refinement", "match", "region", "-lrb-", "-rrb-", "source", "edit", "image", "-lrb-", "-rrb-", "region", "matching", "result", "before", "after", "boundary", "refinement", "pixel", "coefficient", "per-region", "cross-channel", "cubic", "color", "transform", "-lrb-", "-rrb-", "per-pixel", "editing", "strength", "whose", "value", "between", "iterative", "optimization", "approach", "apply", "recover", "both", "per-region", "cross-channel", "cubic", "color", "transform", "perpixel", "edit", "strength", "map", "we", "constant", "strength", "map", "-lrb-", "-rrb-", "each", "iteration", "we", "first", "fix", "obtain", "use", "least", "square", "solver", "take", "account", "all", "pixel", "two", "match", "region", "we", "fix", "recover", "we", "assume", "vary", "very", "smoothly", "value", "roughly", "same", "small", "neighborhood", "window", "hence", "each", "pixel", "-lrb-", "-rrb-", "also", "obtain", "use", "least", "square", "solver", "take", "account", "neighbor", "pixel", "local", "window", "we", "implementation", "we", "find", "usually", "3-5", "iteration", "usually", "enough", "get", "good", "estimate", "both", "accurate", "enough", "we", "purpose", "4.4", "coarse-to-fine", "propagation", "similar", "NRDC", "approach", "we", "coarse-to-fine", "strategy", "we", "use", "result", "obtain", "current", "level", "constrain", "solution", "next", "level", "higher", "resolution", "specifically", "faster", "convergence", "store", "transform", "patch", "lead", "match", "region", "have", "its", "search", "space", "next", "level", "limit", "within", "small", "range", "-lrb-", "e.g.", "shift", "within", "pixel", "scale", "rotation", "within", "0.2", "-rrb-", "final", "transform", "current", "level", "besides", "estimate", "cross-channel", "cubic", "color", "transform", "region", "pair", "apply", "reduce", "appearance", "difference", "between", "two", "region", "before", "evaluate", "distance", "-lrb-", "eqn", "-rrb-", "between", "patch", "4.5", "boundary", "refinement", "because", "we", "use", "patch", "size", "region", "matching", "process", "boundary", "each", "match", "region", "typically", "located", "accurately", "show", "example", "fig.", "further", "improve", "accuracy", "region", "boundary", "we", "first", "over-segment", "both", "source", "edit", "image", "use", "mean", "shift", "algorithm", "-lsb-", "comaniciu", "Meer", "2002", "-rsb-", "assume", "each", "segment", "should", "belong", "single", "region", "base", "color", "consistency", "hence", "we", "check", "each", "segment", "conflict", "80", "segment", "inside", "match", "region", "unmatched", "region", "-lrb-", "i.e.", "pixels", "belong", "any", "match", "region", "-rrb-", "we", "assign", "whole", "segment", "same", "region", "next", "edit", "image", "we", "create", "refinement", "band", "around", "each", "match", "region", "expand", "shrink", "its", "boundary", "pixel", "calculate", "alpha", "matte", "refinement", "band", "use", "exist", "alpha", "matting", "technique", "-lsb-", "Levin", "et", "al.", "2008", "-rsb-", "once", "alpha", "matte", "compute", "we", "also", "refine", "transform", "pixel", "we", "use", "similar", "algorithm", "Sec", "4.1", "propagate", "transform", "from", "inside", "region", "refinement", "band", "only", "difference", "be", "we", "modify", "eqn", "use", "alpha", "value", "pixel", "-lrb-", "-rrb-", "weight", "calculate", "patch", "distance", "-lrb-", "-rrb-", "min", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "p?u", "where", "pixel", "corresponding", "pixel", "way", "we", "avoid", "influence", "background", "color", "when", "calculate", "match", "distance", "patch", "foreground", "object", "example", "boundary", "refinement", "show", "fig.", "finally", "we", "remove", "those", "region", "pair", "whose", "color", "geometric", "transform", "both", "identity", "transform", "-lrb-", "i.e.", "correspond", "edit", "-rrb-", "remain", "match", "region", "pair", "use", "subsequent", "step", "generate", "editing", "history", "fig.", "show", "several", "example", "match", "region", "pair", "Semantic", "appearance", "Operator", "Recovery", "recall", "we", "use", "per-region", "cross-channel", "cubic", "transform", "-lrb-", "eqn", "-rrb-", "describe", "appearance", "change", "between", "match", "region", "pair", "transformation", "powerful", "reconstruction", "lack", "semantic", "meaning", "section", "we", "describe", "how", "further", "recover", "semantic", "editing", "operation", "from", "wide", "variety", "global", "color", "appearance", "adjustment", "tool", "exist", "modern", "image", "editing", "software", "consult", "professional", "study", "representative", "software", "package", "gimp", "Adobe", "Lightroom", "Apple", "Aperture", "we", "have", "identify", "five", "most", "commonly", "use", "basic", "appearance", "operator", "brightness", "exposure", "hue", "saturation", "non-linear", "tone", "curve", "adjustment", "brightness", "hue", "operator", "shift", "value", "corresponding", "channel", "constant", "saturation", "exposure", "operator", "scale", "value", "each", "channel", "-lrb-", "-rrb-", "inspire", "exist", "tone", "curve", "adjustment", "interface", "we", "use", "cubic", "spline", "represent", "non-linear", "tone", "curve", "parameterize", "five", "control", "point", "-lrb-", "-rrb-", "-lrb-", "0.25", "-rrb-", "-lrb-", "0.5", "-rrb-", "-lrb-", "0.75", "-rrb-", "-lrb-", "-rrb-", "where", "three", "value", "-lsb-", "-rsb-", "total", "we", "have", "parameter", "appearance", "editing", "-lrb-", "i.e.", "each", "brightness", "exposure", "hue", "saturation", "tone", "curve", "-rrb-", "although", "short", "list", "basic", "operator", "wide", "range", "adjustment", "effect", "can", "achieve", "compose", "multiple", "operator", "instance", "commonly", "use", "contrast", "shadow", "highlight", "adjustment", "can", "all", "achieve", "non-linear", "tone", "curve", "manipulation", "important", "note", "besides", "per-region", "editing", "parameter", "each", "pixel", "inside", "region", "still", "maintain", "per-pixel", "editing", "strength", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "allow", "system", "support", "local", "adjustment", "brush", "spatially-varying", "strength", "similar", "Sec", "4.3", "we", "optimize", "per-region", "parameter", "per-pixel", "editing", "strength", "alternatively", "detail", "below", "parameter", "initialization", "initialize", "we", "use", "editing", "strength", "map", "obtain", "Sec", "4.3", "we", "obtain", "initial", "estimation", "per-region", "parameter", "apply", "follow", "step", "convert", "image", "hsv", "color", "space", "compute", "initial", "hue", "saturation", "parameter", "subtract", "mean", "hue", "value", "divide", "mean", "saturation", "value", "two", "region", "respectively", "convert", "image", "grayscale", "obtain", "initial", "brightness", "exposure", "parameter", "fitting", "linear", "model", "http://www.gimp.org/", "http://www.adobe.com/products/photoshop-lightroom.html", "http://www.apple.com/aperture/", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "Inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "194:5", "Figure", "match", "region", "pair", "leave", "right", "original", "edited", "image", "recover", "match", "region", "pair", "last", "example", "recover", "spatially", "vary", "strength", "map", "also", "show", "brightness", "exposure", "gamma", "source", "image", "ground", "truth", "edit", "image", "ground", "truth", "recover", "tone", "curve", "figure", "accuracy", "appearance", "operator", "recovery", "top", "ground", "truth", "edit", "image", "generate", "change", "brightness", "exposure", "gamma", "-lrb-", "tone", "-rrb-", "curve", "hue", "brightness", "hue", "saturation", "tone", "curve", "spatially", "vary", "adjustment", "brightness", "exposure", "respectively", "bottom", "result", "render", "use", "recover", "operator", "visually", "indistinguishable", "from", "ground", "truth", "error", "estimate", "parameter", "within", "bottom-left", "ground", "truth", "recover", "tone", "curve", "4-th", "hue", "saturation", "tone", "curve", "example", "Notice", "rightmost", "example", "spatially", "vary", "adjustment", "brightness", "exposure", "have", "be", "apply", "ground", "truth", "recover", "spatially-varying", "strength", "map", "show", "top", "left", "corner", "corresponding", "image", "respectively", "where", "intensity", "corresponding", "pixel", "two", "region", "after", "compensate", "brightness", "exposure", "difference", "use", "least", "square", "fitting", "obtain", "initial", "tone", "curve", "parameter", "parameter", "refinement", "after", "initialization", "we", "iteratively", "optimize", "per-pixel", "strength", "map", "per-region", "parameter", "we", "first", "fix", "use", "gradient", "descent", "method", "search", "optimal", "parameter", "value", "minimize", "color", "difference", "between", "two", "region", "after", "apply", "color", "adjustment", "since", "parameter", "also", "influence", "order", "operation", "we", "assume", "follow", "fix", "order", "color", "adjustment", "-lrb-", "-rrb-", "hue", "-lrb-", "-rrb-", "saturation", "-lrb-", "-rrb-", "brightness", "-lrb-", "-rrb-", "exposure", "-lrb-", "-rrb-", "tone", "curve", "next", "we", "fix", "per-region", "parameter", "adjust", "use", "same", "local", "constancy", "assumption", "least", "square", "solver", "describe", "Sec", "4.3", "we", "usually", "apply", "iteration", "process", "operation", "removal", "after", "above", "optimization", "process", "we", "examine", "parameter", "value", "see", "close", "default", "hue", "brightness", "hue", "saturation", "curve", "spatially", "vary", "adjustment", "recover", "edit", "image", "value", "correspond", "change", "some", "they", "we", "assume", "corresponding", "edit", "have", "be", "apply", "we", "fix", "parameter", "default", "value", "re-apply", "optimization", "process", "update", "other", "fig.", "show", "some", "example", "recovered", "appearance", "operator", "suggest", "recover", "operator", "accurate", "can", "generate", "high", "fidelity", "render", "result", "when", "compare", "ground", "truth", "edit", "image", "fig.", "give", "another", "example", "spatially", "vary", "adjustment", "from", "leave", "right", "we", "give", "source", "image", "edit", "image", "recover", "per-pixel", "editing", "strength", "map", "result", "suggest", "we", "method", "robust", "recover", "edit", "more", "thorough", "evaluation", "present", "Sec", "recover", "Editing", "history", "so", "far", "we", "have", "generate", "list", "match", "region", "pair", "appearance", "geometric", "transform", "between", "each", "pair", "we", "now", "explain", "how", "further", "process", "matching", "result", "generate", "compact", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "194:6", "s.-m", "Hu", "et", "al.", "source", "image", "edit", "image", "recover", "strength", "map", "figure", "spatially", "vary", "adjustment", "example", "editing", "history", "6.1", "layered", "editing", "before", "introduce", "propose", "editing", "path", "recovery", "method", "important", "note", "we", "system", "intrinsically", "do", "layered", "editing", "where", "each", "pair", "match", "region", "different", "layer", "note", "same", "object", "source", "image", "-lrb-", "denote", "-lrb-", "-rrb-", "-rrb-", "could", "match", "multiple", "object", "edit", "image", "-lrb-", "denote", "-lrb-", "-rrb-", "-rrb-", "often", "cause", "clone", "case", "multiple", "copy", "-lrb-", "-rrb-", "each", "one", "pair", "-lrb-", "-rrb-", "live", "different", "layer", "6.2", "optimal", "editing", "path", "problem", "generate", "editing", "path", "from", "match", "region", "do", "have", "unique", "solution", "many", "different", "path", "can", "lead", "same", "editing", "result", "find", "reasonable", "path", "we", "first", "introduce", "concept", "state", "formally", "let", "list", "match", "region", "pair", "-lcb-", "-rcb-", "-lrb-", "number", "match", "-rrb-", "where", "denote", "estimate", "per-region", "appearance", "geometric", "transform", "between", "each", "pair", "we", "denote", "state", "each", "region", "edit", "have", "already", "be", "apply", "hence", "begin", "state", "editing", "path", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "...", "-lrb-", "-rrb-", "-rcb-", "final", "state", "end", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "...", "-lrb-", "-rrb-", "-rcb-", "we", "goal", "find", "stack", "editing", "step", "change", "state", "from", "end", "framework", "single", "editing", "step", "involve", "select", "one", "multiple", "spatially", "connect", "region", "-lcb-", "-rcb-", "simultaneously", "modify", "state", "same", "way", "-lrb-", "-rrb-", "all", "where", "-lrb-", "-rrb-", "indicate", "particular", "operation", "apply", "-lrb-", "-rrb-", "allow", "both", "zero", "-lrb-", "i.e.", "edit", "-rrb-", "furthermore", "either", "should", "satisfy", "constraint", "now", "now", "least", "one", "select", "region", "-lrb-", "now", "now", "-rrb-", "denote", "current", "state", "region", "other", "word", "least", "one", "region", "reach", "its", "final", "color", "geometric", "transform", "single", "editing", "step", "give", "constraint", "any", "step", "number", "possible", "editing", "path", "take", "limit", "example", "give", "region", "whose", "current", "state", "-lrb-", "-rrb-", "allow", "editing", "step", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rcb-", "we", "also", "need", "define", "semantically", "what", "optimal", "path", "from", "end", "consult", "experienced", "artist", "we", "have", "determine", "four", "principle", "majority", "people", "agree", "layer", "edit", "coarse-to-fine", "order", "large", "visually", "dominant", "edit", "apply", "before", "fine-tuning", "appearance", "small", "object", "same", "edit", "apply", "multiple", "object", "apply", "either", "together", "once", "sequentially", "without", "interruption", "-lrb-", "temporal", "focus", "-rrb-", "nearby", "object", "edit", "sequentially", "before", "move", "far", "away", "object", "-lrb-", "spatial", "focus", "-rrb-", "downsample", "defer", "much", "possible", "specifically", "principle", "reflect", "spatio-temporal", "local", "editing", "focus", "mean", "user", "more", "likely", "focus", "same", "editing", "operation", "while", "before", "move", "other", "tool", "focus", "nearby", "object", "local", "region", "first", "before", "move", "far", "away", "one", "base", "above", "principle", "we", "define", "total", "editing", "cost", "+1", "i?m", "i?m", "where", "number", "editing", "step", "denote", "cost", "ith", "editing", "step", "-lrb-", "explain", "next", "-rrb-", "denote", "centroid", "select", "region", "-lrb-", "-rrb-", "i-th", "editing", "step", "before", "after", "apply", "edit", "respectively", "first", "term", "sum", "cost", "all", "editing", "step", "second", "term", "measure", "switching", "cost", "between", "adjacent", "editing", "step", "accord", "principle", "-lrb-", "i.e.", "+1", "approximate", "spatial", "movement", "from", "i-th", "step", "-lrb-", "-rrb-", "th", "step", "-rrb-", "control", "weight", "set", "0.5", "we", "experiment", "cost", "i-th", "editing", "step", "define", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lcb-", "-rcb-", "denote", "select", "region", "-lrb-", "-rrb-", "i-th", "editing", "step", "its", "size", "use", "approximate", "its", "visual", "dominance", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "measure", "amount", "color", "geometric", "transform", "apply", "step", "exact", "formulation", "give", "later", "constant", "set", "0.01", "we", "system", "term", "-lrb-", "-rrb-", "have", "two", "purpose", "-lrb-", "-rrb-", "penalize", "longer", "editing", "path", "shorter", "path", "prefer", "same", "result", "-lrb-", "ii", "-rrb-", "favor", "edit", "dominant", "object", "-lrb-", "i.e.", "large", "-rrb-", "be", "apply", "earlier", "-lrb-", "i.e.", "when", "smaller", "-rrb-", "thus", "satisfy", "Principle", "final", "term", "eqn", "penalize", "early", "down-sampling", "-lrb-", "satisfy", "Principle", "-rrb-", "define", "average", "scaling", "factor", "have", "already", "be", "apply", "where", "iterate", "over", "all", "select", "region", "editing", "step", "scale", "-lrb-", "-rrb-", "scale", "factor", "have", "already", "be", "apply", "region", "note", "we", "completely", "avoid", "enlarge", "after", "downsample", "same", "region", "set", "edit", "finally", "amount", "color", "geometric", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "eqn", "define", "where", "pixel", "its", "corresponding", "pixel", "control", "influence", "color", "transform", "-lrb-", "-rrb-", "rot", "scale", "denote", "average", "translation", "rotation", "angle", "scaling", "region", "respectively", "xy", "rot", "scale", "weight", "control", "influence", "each", "term", "we", "system", "weight", "empirically", "set", "xy", "rot", "scale", "ln", "above", "definition", "we", "seek", "editing", "path", "from", "end", "minimal", "total", "editing", "cost", "-lrb-", "define", "eqn", "-rrb-", "among", "all", "possible", "path", "can", "efficiently", "solve", "use", "dynamic", "programming", "min", "-lrb-", "1/t", "scale", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "p?s", "-lrb-", "-rrb-", "xy", "rot", "rot", "log", "scale", "scale", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "Inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "194:7", "source", "image", "edit", "image", "figure", "recover", "editing", "history", "from", "before-and-after", "image", "pair", "6.3", "handle", "no-match", "region", "may", "exist", "region", "both", "can", "find", "good", "match", "other", "image", "may", "cause", "operation", "crop", "object", "insertion", "removal", "we", "first", "examine", "edit", "image", "have", "be", "derive", "from", "original", "crop", "find", "two", "bound", "box", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "contain", "all", "match", "region", "respectively", "cropping", "identify", "-lrb-", "-rrb-", "cover", "entire", "image", "while", "-lrb-", "-rrb-", "cover", "only", "portion", "-lrb-", "-rrb-", "treat", "crop", "window", "we", "look", "object", "insertion", "removal", "unmatched", "region", "may", "result", "insert", "object", "remove", "one", "from", "fill", "hole", "use", "image", "completion", "technique", "region", "cause", "image", "completion", "should", "obvious", "seam", "between", "unmatched", "region", "its", "surrounding", "one", "otherwise", "region", "cause", "insert", "new", "object", "we", "expect", "region", "surround", "strong", "object", "boundary", "we", "thus", "check", "how", "smooth", "transition", "between", "region", "its", "surroundings", "specifically", "we", "check", "how", "well", "region", "boundary", "agree", "over-segmentation", "boundary", "obtain", "mean", "shift", "algorithm", "-lrb-", "see", "Sec", "4.5", "-rrb-", "agree", "each", "other", "well", "-lrb-", "e.g.", "more", "than", "50", "overlap", "-rrb-", "region", "consider", "newly", "insert", "object", "otherwise", "we", "further", "check", "same", "region", "source", "region", "also", "unmatched", "region", "we", "decide", "object", "source", "region", "have", "be", "remove", "hole", "have", "be", "fill", "image", "completion", "technique", "operation", "identify", "we", "incorporate", "they", "editing", "path", "compute", "Sec", "6.2", "specifically", "we", "add", "crop", "object", "removal", "beginning", "history", "add", "object", "insertion", "end", "create", "complete", "editing", "path", "application", "result", "we", "have", "implement", "we", "method", "pc", "Intel", "Xeon", "2.4", "GHz", "CPU", "8gb", "memory", "image", "size", "640", "480", "region", "match", "take", "minute", "recover", "appearance", "operator", "recover", "editing", "history", "use", "move", "source", "image", "use", "scale", "gate", "use", "color", "adjust", "tower", "edit", "image", "generate", "tutorial", "figure", "Generating", "tutorial", "from", "recover", "edit", "history", "take", "about", "10", "15", "seconds", "depend", "number", "region", "be", "edit", "find", "optimal", "editing", "path", "take", "about", "seconds", "fig.", "show", "number", "input", "before-and-after", "image", "pair", "recover", "editing", "history", "use", "we", "approach", "suggest", "we", "system", "work", "reliably", "even", "large", "geometric", "appearance", "transform", "appearance", "transform", "allow", "spatially", "vary", "-lrb-", "e.g.", "first", "example", "-rrb-", "furthermore", "recover", "editing", "step", "semantically-meaningful", "can", "easily", "use", "drive", "tool", "image", "editing", "software", "note", "how", "system", "generate", "compact", "reasonable", "editing", "history", "2nd", "3rd", "example", "from", "low-level", "region", "matching", "result", "show", "3rd", "4th", "row", "fig.", "next", "we", "illustrate", "how", "recovered", "history", "can", "use", "various", "application", "Automatic", "tutorial", "generation", "straightforward", "application", "use", "recover", "editing", "history", "automatically", "generate", "image", "editing", "tutorial", "fig.", "show", "simple", "example", "turn", "history", "step-by-step", "tutorial", "we", "method", "can", "combine", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "194:8", "s.-m", "Hu", "et", "al.", "source", "image", "edit", "image", "recover", "editing", "history", "figure", "10", "edit", "transfer", "give", "image", "pair", "we", "first", "recover", "editing", "history", "-lrb-", "middle", "-rrb-", "apply", "image", "generate", "result", "similar", "way", "object", "segmentation", "obtain", "grabcut", "source", "image", "edit", "image", "recover", "editing", "history", "Figure", "11", "merge", "multiple", "editing", "path", "from", "same", "source", "image", "image", "right", "render", "use", "merged", "editing", "history", "image", "courtesy", "Flickr", "user", "Hans", "E.", "Figure", "image", "re-editing", "result", "generate", "from", "recover", "editing", "history", "-lrb-", "see", "original", "example", "Fig.", "top", "right", "second", "row", "fig.", "-rrb-", "leave", "increase", "-lrb-", "instead", "decrease", "original", "edit", "-rrb-", "brightness", "swan", "right", "right", "first", "editing", "step", "-lrb-", "move", "scale", "-rrb-", "modify", "only", "include", "move", "fifth", "editing", "step", "-lrb-", "move", "-rrb-", "change", "removal", "more", "powerful", "tutorial", "generate", "system", "-lsb-", "Grabler", "et", "al.", "2009", "-rsb-", "produce", "more", "visually", "appealing", "result", "re-editing", "user", "can", "modify", "recovered", "history", "e.g.", "remove", "step", "change", "parameter", "some", "step", "re-apply", "modify", "history", "source", "image", "generate", "new", "edit", "image", "some", "example", "show", "fig.", "Fig.", "easier", "than", "directly", "editing", "original", "edited", "image", "achieve", "user", "may", "have", "carry", "out", "object", "selection", "use", "other", "editing", "tool", "again", "latter", "case", "user", "may", "also", "longer", "remember", "parameter", "part", "edit", "which", "remain", "unchanged", "edit", "transfer", "recover", "editing", "history", "from", "one", "pair", "image", "can", "transfer", "new", "image", "achieve", "edit", "transfer", "example", "show", "Fig.", "10", "where", "recover", "editing", "history", "new", "source", "image", "editing", "transfer", "result", "merge", "result", "from", "image", "apply", "image", "B.", "do", "we", "assume", "have", "similar", "composition", "so", "object", "roughly", "same", "location", "those", "a.", "automatically", "extract", "object", "mask", "we", "first", "identify", "rough", "bound", "box", "which", "1.5", "time", "larger", "than", "actual", "object", "bound", "box", "apply", "grabcut", "-lsb-", "Rother", "et", "al.", "2004", "-rsb-", "segmentation", "other", "dedicated", "approach", "have", "already", "be", "propose", "edit", "transfer", "image", "analogy", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "content-adaptive", "macro", "-lsb-", "Berthouzoz", "et", "al.", "2011", "-rsb-", "compare", "image", "analogy", "we", "method", "have", "more", "constraint", "input", "require", "similar", "composition", "image", "B.", "however", "other", "hand", "capable", "object-level", "editing", "also", "handle", "geometric", "editing", "crop", "which", "clear", "advantage", "over", "image", "analogy", "other", "appearance-based", "transfer", "approach", "content-adaptive", "macro", "require", "editing", "history", "know", "so", "we", "system", "can", "potentially", "use", "generate", "editing", "macro", "merge", "editing", "path", "we", "approach", "can", "combine", "image", "revision", "control", "system", "-lsb-", "Chen", "et", "al.", "2011", "-rsb-", "merge", "different", "editing", "path", "example", "show", "Fig.", "11", "where", "we", "system", "recover", "multiple", "different", "editing", "path", "merge", "they", "create", "single", "final", "render", "result", "evaluation", "objectively", "evaluate", "proposed", "system", "we", "create", "evaluation", "dataset", "contain", "21", "image", "editing", "example", "produce", "several", "artist", "produce", "example", "we", "explicitly", "explain", "range", "support", "operation", "artist", "ask", "they", "perform", "editing", "use", "support", "operation", "input", "image", "perform", "editing", "path", "be", "choose", "create", "artist", "without", "any", "supervision", "we", "select", "21", "successful", "moderately", "successful", "example", "out", "25", "we", "receive", "from", "artist", "user", "study", "original", "editing", "step", "record", "all", "example", "we", "generate", "editing", "history", "all", "example", "use", "we", "system", "compare", "they", "original", "one", "we", "first", "evaluate", "representation", "ability", "recover", "history", "measure", "psnr", "value", "reconstructed", "edited", "image", "average", "psnr", "value", "dataset", "26.1", "db", "standard", "deviation", "6.2", "db", "visually", "noticeable", "difference", "between", "reconstruct", "ground", "truth", "edit", "image", "most", "example", "result", "suggest", "we", "recover", "history", "can", "faithfully", "reproduce", "edit", "image", "high", "fidelity", "secondly", "we", "conduct", "qualitative", "evaluation", "semantic", "meaningfulness", "recover", "history", "we", "invite", "30", "participant", "who", "image", "editing", "enthusiast", "familiar", "Adobe", "Photoshop", "participate", "study", "each", "user", "session", "each", "editing", "example", "we", "first", "show", "before-and-after", "image", "pair", "subject", "follow", "two", "editing", "history", "original", "one", "recover", "one", "display", "order", "two", "history", "be", "randomly", "determine", "each", "history", "illustrate", "automatic", "slideshow", "subject", "allow", "switch", "back", "forth", "see", "each", "step", "more", "clearly", "time", "limit", "study", "after", "view", "pair", "editing", "history", "subject", "ask", "judge", "which", "one", "semantically", "more", "natural", "about", "equal", "result", "user", "study", "give", "Fig.", "12", "total", "we", "method", "achieve", "average", "score", "0.45", "-lrb-", "i.e.", "ours", "better", "about", "equal", "artist?s", "better", "score", "0.5", "respectively", "-rrb-", "one", "tail", "p-value", "0.039", "study", "demonstrate", "we", "recover", "history", "comparable", "original", "one", "produce", "artist", "term", "semantic", "meaningfulness", "17", "out", "21", "example", "-lrb-", "81.0", "-rrb-", "majority", "user", "rate", "we", "recover", "history", "equal", "better", "than", "original", "one", "all", "example", "use", "user", "study", "include", "supplemental", "material", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "Inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "194:9", "ours", "equal", "artist", "0.5", "11", "16", "21", "Figure", "12", "percentage", "favor", "we", "artist", "generate", "edit", "sequence", "Limitations", "Discussion", "9.1", "failure", "case", "better", "understand", "limitation", "proposed", "system", "we", "study", "low", "rating", "example", "-lrb-", "e.g.", "2nd", "6th", "example", "-rrb-", "above", "user", "study", "see", "why", "less", "successful", "2nd", "example", "-lrb-", "top", "row", "Fig.", "13", "-rrb-", "although", "we", "recover", "editing", "step", "same", "artist", "participant", "rate", "lower", "due", "its", "visually", "noticeable", "reconstruction", "error", "because", "saturated", "color", "value", "sun", "region", "lead", "error", "reconstructed", "color", "transform", "6th", "example", "-lrb-", "bottom", "row", "Fig.", "13", "-rrb-", "p-value", "measure", "average", "score", "all", "example", "i.e.", "compute", "average", "score", "each", "example", "measure", "p-value", "21", "score", "original", "editing", "patch", "produce", "artist", "have", "more", "natural", "spatial", "layout", "-lrb-", "i.e.", "change", "color", "leaf", "from", "bottom", "top", "give", "bottom", "one", "visually", "more", "dominant", "-rrb-", "while", "we", "method", "first", "edit", "leaf", "middle", "since", "region", "large", "-lrb-", "see", "Eqn", "-rrb-", "example", "suggest", "one", "could", "potentially", "use", "better", "algorithm", "rank", "visual", "dominance", "different", "object", "improve", "semantic", "meaningfulness", "recovered", "history", "we", "system", "can", "fail", "more", "dramatically", "when", "individual", "technical", "component", "incapable", "handle", "more", "difficult", "case", "firstly", "region", "match", "algorithm", "Sec", "may", "fail", "follow", "case", "-lrb-", "-rrb-", "when", "size", "match", "region", "too", "small", "classify", "reliable", "-lrb-", "-rrb-", "when", "apply", "geometry/color", "transformation", "too", "dramatic", "-lrb-", "-rrb-", "when", "some", "region", "purely", "textureless", "two", "example", "show", "Fig.", "14", "first", "row", "Fig.", "14", "matching", "head", "giraffe", "fail", "due", "large", "deformation", "second", "row", "Fig.", "14", "we", "region", "match", "miss", "left", "right", "mushroom", "stem", "since", "too", "thin", "secondly", "appearance", "operation", "can", "well", "approximate", "use", "one", "Sec", "lead", "large", "reconstruction", "error", "when", "re-apply", "history", "example", "show", "third", "row", "Fig.", "14", "where", "we", "method", "fail", "recover", "complex", "appearance", "change", "9.2", "support", "operation", "discuss", "earlier", "we", "system", "currently", "only", "support", "limited", "range", "operation", "we", "believe", "dedicated", "solution", "other", "editing", "operation", "could", "potentially", "support", "instance", "support", "gaussian", "blur", "we", "can", "optionally", "add", "new", "transform", "dimension", "region", "matching", "step", "allow", "blur", "region", "match", "against", "multiple", "version", "original", "image", "different", "amount", "blur", "we", "can", "match", "blur", "region", "its", "original", "sharp", "region", "same", "time", "produce", "estimation", "size", "blur", "appearance", "operator", "recovery", "step", "we", "can", "first", "blur", "source", "region", "use", "recover", "blur", "downsize", "both", "region", "lower", "resolution", "remove", "effect", "blur", "estimate", "other", "color", "operator", "we", "implement", "above", "procedure", "system", "Fig.", "15", "we", "provide", "example", "involve", "two", "operation", "foreground", "move", "background", "gaussian", "blur", "we", "method", "successfully", "recover", "both", "operation", "other", "more", "complicated", "operation", "support", "we", "current", "system", "bilateral", "filter", "alpha", "matting", "Poisson", "blend", "etc.", "general", "relatively", "easy", "support", "global", "operation", "however", "operation", "content", "aware", "i.e.", "color", "pixel", "change", "adaptively", "accord", "its", "local", "neighborhood", "appearance", "bilateral", "filter", "much", "harder", "recover", "nevertheless", "we", "have", "demonstrate", "we", "current", "system", "already", "widely", "useful", "support", "wide", "range", "commonly", "use", "editing", "operation", "10", "conclusion", "we", "present", "novel", "system", "recover", "semantically", "meaningful", "editing", "history", "from", "source", "image", "edit", "version", "achieve", "we", "use", "dense", "correspondence", "method", "which", "extend", "NRDC", "approach", "find", "all", "edit", "region", "recover", "appearance", "operation", "apply", "each", "region", "from", "all", "possible", "edit", "path", "we", "recover", "optimal", "one", "base", "semantic", "constraint", "experimental", "user", "study", "result", "show", "we", "system", "can", "recover", "clean", "meaningful", "editing", "history", "involve", "large", "geometric", "appearance", "transformation", "we", "further", "show", "recover", "history", "can", "useful", "wide", "range", "application", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "194:10", "s.-m", "Hu", "et", "al.", "source", "image", "edit", "image", "source", "image", "edit", "image", "original", "history", "artist", "figure", "13", "less", "successful", "example", "top", "noticeable", "color", "reconstruction", "error", "due", "saturated", "sun", "region", "bottom", "recovered", "history", "have", "less", "ideal", "operation", "order", "compare", "original", "one", "image", "courtesy", "Flickr", "user", "quinoktium", "-lrb-", "top", "-rrb-", "future", "work", "we", "would", "like", "improve", "robustness", "proposed", "system", "replace", "some", "technical", "component", "newly", "develop", "more", "advanced", "method", "example", "we", "could", "potentially", "improve", "robustness", "region", "matching", "step", "use", "recently", "propose", "higher-order", "deformation", "model", "-lsb-", "y?cer", "et", "al.", "2012", "-rsb-", "after", "obtain", "initial", "matching", "we", "propose", "method", "another", "way", "improve", "system", "robustness", "combine", "we", "method", "technique", "propose", "previous", "photo", "manipulation", "detection", "system", "-lsb-", "o?brien", "Farid", "2012", "Kee", "et", "al.", "2013", "-rsb-", "we", "also", "interested", "combine", "we", "system", "work", "-lsb-", "Ma", "et", "al.", "2013", "-rsb-", "automatically", "generate", "change", "blindness", "image", "extend", "we", "method", "handle", "vector", "image", "-lsb-", "Lai", "et", "al.", "2009", "Liao", "et", "al.", "2012", "-rsb-", "another", "potential", "extension", "apply", "we", "method", "large", "image", "library", "-lsb-", "Hu", "et", "al.", "2013", "-rsb-", "analyze", "image", "correlation", "and/or", "dependency", "within", "large", "dataset", "Acknowlegements", "we", "thank", "anonymous", "reviewer", "valuable", "comment", "work", "support", "National", "Basic", "Research", "Project", "China", "-lrb-", "2011CB302205", "-rrb-", "natural", "Science", "Foundation", "China", "-lrb-", "61120106007", "61170153", "-rrb-", "National", "High", "Technology", "Research", "Development", "Program", "China", "-lrb-", "2012aa011802", "-rrb-", "pcsirt", "Tsinghua", "University", "Initiative", "Scientific", "Research", "Program", "reference", "arne", "C.", "hechtman", "E.", "inkelstein", "a.", "old", "man", "D.", "B.", "2009", "Patchmatch", "randomize", "correspondence", "algorithm", "structural", "image", "editing", "ACM", "Trans", "graph", "28", "24:1", "24:11", "arne", "C.", "hechtman", "E.", "OLDMAN", "D.", "B.", "inkel", "stein", "a.", "2010", "generalize", "patchmatch", "correspondence", "algorithm", "Proc", "eccv", "29", "43", "erthouzoz", "F.", "W.", "ONTCHEVA", "M.", "GRAWALA", "M.", "2011", "framework", "content-adaptive", "photo", "manipulation", "macro", "application", "face", "landscape", "global", "manipulation", "ACM", "Trans", "graph", "30", "120:1", "120:14", "leyer", "M.", "HEMANN", "C.", "OTHER", "C.", "2011", "Patchmatch", "stereo", "stereo", "match", "slant", "support", "window", "Proceedings", "british", "machine", "Vision", "Conference", "14.1", "14.11", "rox", "T.", "REGLER", "C.", "ALIK", "J.", "2009", "large", "displacement", "optical", "flow", "Proc", "CVPR", "41", "48", "we", "recover", "history", "reconstruct", "image", "we", "recovered", "history", "ychkovsky", "V.", "ari", "S.", "HAN", "E.", "URAND", "F.", "2011", "Learning", "photographic", "global", "tonal", "adjustment", "database", "input", "output", "image", "pair", "Proc", "CVPR", "97", "104", "hen", "h.-t.", "eus", "l.-y.", "hang", "c.-f", "2011", "nonlinear", "revision", "control", "image", "ACM", "Trans", "graph", "30", "105:1", "105:10", "hen", "h.-t.", "EI", "L.-Y.", "ARTMANN", "B.", "GRAWALA", "M.", "2012", "data-driven", "adaptive", "history", "image", "editing", "Technical", "Report", "heng", "m.-m.", "hang", "f.-l.", "itra", "N.", "J.", "UANG", "X.", "s.-m", "2010", "Repfinder", "find", "approximately", "repeat", "scene", "element", "image", "editing", "ACM", "Trans", "graph", "29", "-lrb-", "July", "-rrb-", "83:1", "83:8", "omaniciu", "D.", "eer", "P.", "2002", "mean", "shift", "robust", "approach", "toward", "feature", "space", "analysis", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "24", "603", "619", "H.", "HOU", "S.", "IU", "L.", "itra", "N.", "J.", "2011", "animated", "construction", "line", "drawing", "ACM", "Trans", "graph", "30", "-lrb-", "Dec.", "-rrb-", "133:1", "133:10", "rabler", "F.", "GRAWALA", "M.", "W.", "ONTCHEVA", "M.", "GARASHI", "T.", "2009", "Generating", "photo", "manipulation", "tutorial", "demonstration", "ACM", "Trans", "graph", "28", "-lrb-", "July", "-rrb-", "66:1", "66:9", "ohen", "Y.", "hechtman", "E.", "OLDMAN", "D.", "B.", "ISCHINSKI", "D.", "2011", "non-rigid", "dense", "correspondence", "application", "image", "enhancement", "ACM", "Trans", "graph", "30", "70:1", "70:10", "eer", "J.", "ACKINLAY", "J.", "tolte", "C.", "GRAWALA", "M.", "2008", "graphical", "history", "visualization", "support", "analysis", "communication", "evaluation", "IEEE", "transaction", "visualization", "computer", "graphic", "14", "1189", "1196", "ertzmann", "a.", "ACOBS", "C.", "E.", "liver", "N.", "URLESS", "B.", "ALESIN", "D.", "H.", "2001", "image", "analogy", "Proc", "Siggraph", "327", "340", "s.-m.", "hen", "T.", "K.", "HENG", "M.-M.", "artin", "R.", "R.", "2013", "Internet", "visual", "media", "processing", "survey", "graphic", "vision", "application", "Visual", "Computer", "29", "393", "405", "source", "image", "edit", "image", "reconstructed", "image", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013", "Inverse", "image", "editing", "recover", "Semantic", "Editing", "history", "from", "Before-and-After", "image", "pair", "194:11", "Figure", "14", "failure", "example", "due", "large", "geometric", "deformation", "-lrb-", "top", "-rrb-", "small", "image", "structure", "-lrb-", "middle", "-rrb-", "complex", "color", "change", "-lrb-", "bottom", "-rrb-", "image", "courtesy", "Flickr", "user", "Michael", "Li", "-lrb-", "third", "row", "-rrb-", "source", "image", "edit", "image", "recover", "history", "figure", "15", "gaussian", "blur", "example", "image", "courtesy", "Flickr", "user", "larrybiker", "EE", "E.", "BRIEN", "J.", "arid", "H.", "2013", "expose", "photo", "manipulation", "inconsistent", "shadow", "ACM", "transaction", "Graphics", "32", "28:1", "28:12", "ONG", "N.", "ROSSMAN", "T.", "ARTMANN", "B.", "GRAWALA", "M.", "itzmaurice", "G.", "W.", "2012", "delta", "tool", "represent", "compare", "workflow", "Proc", "CHI", "1027", "1036", "URLANDER", "D.", "einer", "S.", "1988", "editable", "graphical", "history", "IEEE", "Workshop", "Visual", "Languages", "127", "134", "aus", "y.-k.", "s.-m.", "artin", "R.", "R.", "2009", "Automatic", "topology-preserving", "gradient", "mesh", "generation", "image", "vectorization", "ACM", "Trans", "graph", "28", "85:1", "85:8", "evin", "a.", "ischinskus", "D.", "EISS", "Y.", "2008", "closedform", "solution", "natural", "image", "matting", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "30", "228", "242", "iao", "Z.", "OPPE", "H.", "orsyth", "D.", "Y.", "2012", "subdivision-based", "representation", "vector", "image", "editing", "IEEE", "transaction", "visualization", "computer", "graphic", "18", "11", "1858", "1867", "iu", "C.", "uen", "J.", "orralba", "a.", "ivic", "J.", "reeman", "W.", "T.", "2008", "sift", "flow", "dense", "correspondence", "across", "different", "scene", "Proc", "eccv", "28", "42", "owe", "D.", "G.", "2004", "distinctive", "image", "feature", "from", "scale-invariant", "keypoint", "int", "J.", "Comput", "Vision", "60", "91", "110", "l.-q.", "K.", "ONG", "T.-T.", "IANG", "B.-Y.", "S.M.", "2013", "Change", "blindness", "image", "IEEE", "transaction", "visualization", "computer", "graphic", "appear", "O?B", "RIEN", "J.", "F.", "arid", "H.", "2012", "expose", "photo", "manipulation", "inconsistent", "reflection", "ACM", "transaction", "Graphics", "31", "4:1", "4:11", "other", "C.", "OLMOGOROV", "V.", "lake", "a.", "2004", "grabcut", "interactive", "foreground", "extraction", "use", "iterated", "graph", "cut", "ACM", "Trans", "graph", "23", "309", "314", "S.", "L.", "ari", "S.", "LIAGA", "F.", "cull", "C.", "OHNSON", "S.", "URAND", "F.", "2009", "interactive", "visual", "history", "vector", "graphic", "Tech", "Report", "mit-csail-tr-2009-031", "iao", "C.", "IU", "M.", "ongweus", "N.", "ONG", "Z.", "2011", "fast", "exact", "nearest", "patch", "match", "patch-based", "image", "editing", "processing", "IEEE", "transaction", "visualization", "computer", "graphic", "17", "1122", "1134", "ang", "G.", "TEWART", "C.", "OFKA", "M.", "saus", "c.-l", "2007", "registration", "challenging", "image", "pair", "initialization", "estimation", "decision", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "29", "11", "1973", "1989", "ucer", "K.", "ACOBSON", "a.", "ornung", "a.", "orkine", "O.", "2012", "transfusive", "image", "manipulation", "ACM", "Trans", "graph", "31", "176:1", "176:9", "hang", "F.-L.", "HENG", "M.-M.", "IA", "J.", "s.-m", "2012", "Imageadmixture", "put", "together", "dissimilar", "object", "from", "group", "IEEE", "transaction", "visualization", "computer", "graphic", "18", "11", "1849", "1857", "immer", "H.", "RUHN", "A.", "EICKERT", "J.", "2011", "optic", "flow", "harmony", "int", "J.", "Comput", "Vision", "93", "368", "388", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "194", "publication", "date", "November", "2013" ],
  "content" : "\n  \n    1cf735ccd5433cbbcf092a98b50e263b1fb871957b3942d7a6508c77d93ce0ca\n    p2t\n    10.1145/2508363.2508371\n    Name identification was not possible. \n  \n  \n    \n      \n        Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n      \n      Shi-Min Hu 1 Kun Xu 1 Li-Qian Ma 1 Bin Liu 1 Bi-Ye Jiang 1 1 TNList, Tsinghua University, Beijing 2 Adobe Research Before-and-After Image Pair Recovered Editing History before after scale the region adjust hue copy, rotate and\n      \n        \n        \n        Figure 1: Given a source image and an edited copy (left), our system automatically recovers a semantic editing history (middle), which can be used for various applications, such as re-editing (right). In this case, the second editing step of the recovered history, involving hue modification, is altered to change the berries to a difference color. Image courtesy of Andrea Lein.\n      \n      We study the problem of inverse image editing, which recovers a semantically-meaningful editing history from a source image and an edited copy. Our approach supports a wide range of commonlyused editing operations such as cropping, object insertion and removal, linear and non-linear color transformations, and spatiallyvarying adjustment brushes. Given an input image pair, we first apply a dense correspondence method between them to match edited image regions with their sources. For each edited region, we determine geometric and semantic appearance operations that have been applied. Finally, we compute an optimal editing path from the region-level editing operations, based on predefined semantic constraints. The recovered history can be used in various applications such as image re-editing, edit transfer, and image revision control. A user study suggests that the editing histories generated from our system are semantically comparable to the ones generated by artists.  CR Categories: I.3.4 [Computer Graphics]: Graphics Utilities? Graphics editors; I.4.9 [Image Processing and Computer Vision]: Applications Keywords: image editing history, inverse image editing, history recovery, region matching\n      Links:\n      \n        \n      \n      DL PDF W\n      \n        \n        \n      \n      EB\n    \n    \n      \n        ACM Reference Format\n        Hu, S., Xu, K., Ma, L., Liu, B., Jiang, B., Wang, J. 2013. Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair. ACM Trans. Graph. 32, 6, Article 194 (November 2013), 11 pages. DOI = 10.1145/2508363.2508371 http://doi.acm.org/10.1145/2508363.2508371.\n      \n      \n        Copyright Notice\n        Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . 2013 Copyright held by the Owner/Author. Publication rights licensed to ACM. 0730-0301/13/11-ART194 $15.00. DOI: http://dx.doi.org/10.1145/2508363.2508371\n        Jue Wang 2 Applications re-edited image\n        \n          \n        \n      \n      \n        1 Introduction\n      \n      In image editing, a series of operations are often performed to accomplish an editing task. For instance, the user may first select an object in the image, apply geometric transforms to adjust its shape and position, and then apply various color adjustments to enhance its appearance. This process is repeated if multiple image regions need to be touched up to achieve an editing goal, resulting in a long and sometimes complicated editing history. Having a clean and complete editing history available is required in many graphics and file management applications, such as image editing revision control [Chen et al. 2011], automatic tutorial generation [Grabler et al. 2009] and editing visualization [Heer et al. 2008]. Retention of an editing history also leads to new possibilities in image editing, such as: adjusting the history to produce results with different variations from the source image; transferring the history to another image, etc. These editing goals are much harder to achieve without having the editing history. Unfortunately, although existing software provides powerful editing tools, there is no universal, efficient solution to encode, store, transmit and re-use an editing history. For instance, Adobe Photoshop 1 only allows a partial history to be saved in either the image file or the command log file. It is often the case that a complete history is not available after the editing task is accomplished. Furthermore, image editing is a trial and error process especially for amateurs. It can easily take dozens of operations to explore different ideas, fix errors and fine tweak parameters before producing a desired output. As a result, the raw editing history produced by a novice user is often long, redundant, less understandable, and may not be directly applicable in some applications that requires a clean history. In this work, we study the problem of recovering a clean and semantically meaningful editing history given a source image and an edited version, which we call inverse image editing. This task involves several technical challenges. Firstly, we need to discover which objects or regions have been edited. Secondly, we need to determine how each object or region has been edited. Finally, a semantically meaningful editing path needs to be generated from  recovered local editing operations. Our system provides a set of solutions to these problems. In particular, we improve the state-ofthe-art region matching methods to handle large appearance differences between an original object and its edited version (see Sec. 4). We propose new methods to recover semantic appearance operators for each edited region (see Sec. 5), and then merge them to form a meaningful editing path under predefined semantic constraints (see Sec. 6). We also demonstrate how the recovered history can be applied to various applications (see Sec. 7). It is impossible in practice to recover all editing operations that may have been applied to an image. Our system supports a set of commonly used linear and non-linear geometric and color adjustments, and arbitrary combinations of them. Specifically, each edit step may contain (but not necessarily) the following operations: (1) select an object (region); (2) apply a geometric transform that may involve scaling, rotation, translation, and flipping; and (3) apply per-region color adjustments that may involve brightness, exposure, hue, saturation, and tone adjustments, with a spatially-varying per-pixel strength map for simulating local paint brushes. These operators, when stacked together, provide vast editing possibilities, and cover many popular adjustment tools in modern image editing software. To evaluate the proposed system, we construct a test dataset that contains before-and-after image pairs, along with the original editing steps performed by artists. A user study on this dataset revealed that the editing histories generated by our system are in general comparable to the original ones in terms of semantic meaningfulness (see Sec. 8).\n      1 http://www.photoshop.com/\n      ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n      194:2\n      ?\n      S.-M. Hu et al.\n      \n        2 Related Work\n        Region matching between images. A core component of our system is to build correspondences between regions in the original and edited images. Sparse matching methods such as SIFT matching [Lowe 2004] are not applicable, as dense correspondences are required in our application. Conventional optical flow methods [Brox et al. 2009; Zimmer et al. 2011] cannot be applied either, since an edited region may undergo large appearance transformations that violate the apparent motion assumption of optical flow. The recent SIFT flow method [Liu et al. 2008] and the dual bootstrap method [Yang et al. 2007] combine sparse features with dense matching through a seed-and-grow scheme, but they still have difficulty in matching textureless objects where features are hard to extract. Recently, a family of patch-based fast image correspondence methods have been proposed. PatchMatch [Barnes et al. 2009] is a randomized algorithm for finding approximate nearest neighbors between image patches; it has been further extended to include geometric and color transformations during matching [Barnes et al. 2010], or to find exact nearest patches [Xiao et al. 2011]. Built upon the generalized PatchMatch algorithm [Barnes et al. 2010], HaCohen et al. [2011] propose a non-rigid dense correspondence (NRDC) algorithm for matching regions between images with shared content. We adopt the framework of this algorithm and extend it for our application, as described in Sec.4. Editing process management. Kurlander and Feiner [1988] present an early work on generating editable graphical visualizations for long user sessions. More recently, Heer et al. [2008] propose a visualization system for interactively generating graphical histories. Su et al. [2009] provide another interactive visualization approach for managing operation histories for vector graphics. Grabler et al. [2009] propose a system to automatically create stepby-step, visually appealing tutorials of complicated image editing processes. Chen et al. [2011] propose a nonlinear revision control  method for image editing, using a directed acyclic graph for managing and visualizing the editing process. Chen et al. [2012] suggest an adaptive history, which automatically segments and groups a lengthy sequence of editing commands for easy navigation. The Delta system [Kong et al. 2012] can help users identify the tradeoffs between workflows using visual comparisons. All these systems require the editing process to be available. Fu et al. [2011] propose a system to estimate a reasonable drawing order from a static line drawing, which is in spirit similar to our proposed system, but is limited to line art rather than natural images. Edit transfer. Berthouzoz et al. [2011] propose content adaptive macros to transfer complex image manipulations applied on source images to new target images, by learning the relationships between image features and the parameters of editing procedures. Similarly, Bychkovsky et al. [2011] learn global tone adjustment models from training images, which can then be automatically applied to new images. Again, these systems require the editing procedure to be known. Image Analogies [Hertzmann et al. 2001] provides a classic example of edit transfer without recovering the editing process. However, it cannot handle geometric or object-level edits. The RepFinder system [Cheng et al. 2010] finds repeated scene elements in an image so that edits made to one element can be transferred to others. As one of many applications, our recovered history from one image pair can be applied to new images to achieve semantic editing transfer. The ImageAdmixture system [Zhang et al. 2012] finds object-level grouped elements in images, and allows object mixing and appearance transferring between different images. Y?cer et al. [2012] propose transfusive image manipulation, an automatic approach to transfer edits made to one image to others containing the same object or scene.\n      \n      \n        3 Algorithm Overview\n        The pipeline of our algorithm is shown in Fig. 2 . It consists of three main steps: (1) region matching; (2) recovering semantic appearance operators for each matched region pair; and (3) generating the editing history. Specifically, we first find all matched region pairs between the source and edited images (Sec. 4). This is achieved by using a matching method extended from the non-rigid-densecorrespondence (NRDC) algorithm [HaCohen et al. 2011] to accommodate a wider range of appearance difference between a pair of regions. Secondly, we recover semantic appearance operations for each matched region pair (Sec. 5), which may include both global linear color transforms such as brightness, exposure, hue and saturation adjustments, as well as non-linear tone mapping and local brushes with spatially-varying strength. Finally, given the matched region pairs and their recovered editing operations, we use an optimization approach to generate a compact, semanticallymeaningful editing history, according to a set of predefined editing rules (Sec. 6).\n      \n      \n        4 Region Matching\n        In the first step of the algorithm, we seek to reliably recover region pairs between the source and edited images that share the same content. Comparing two matched regions gives us the means to discover whether the source region has been edited, and if so how. Our region matching approach is extended from the NRDC [HaCohen et al. 2011] algorithm to have better capabilities of handling large appearance transforms. Specifically, we adopt the coarse-to-fine framework in the original  NRDC approach, which initially downsamples the image to a low resolution, finds matches, and uses them as constraints for finding matches at the next finer resolution. At each level, the following four steps are applied sequentially: (1)nearest neighbor search; (2) patch merging; (3) color transform estimation; and (4) coarse-tofine propagation. After the above steps finish at the finest level, a boundary refinement step is applied to produce an accurate boundary for each matched region. Next we will describe these steps and their differences from the original NRDC approach in more detail.\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n        ?\n        194:3\n        Input Matched Region Pairs Appearance Operators\n        \n          \n        \n        Hue Sat Bri Exp\n        \n          \n          \n          Figure 2: Pipeline of the proposed inverse image editing system.\n        \n        \n          4.1 Nearest neighbor search\n          Given a source image A and an edited image A , for each patch u in A , our goal is to find the closest source patch u in A that minimize a distance measure D(u, u ). A 4D geometric transformation G (i.e. representing 2D translation, rotation, scaling, and flipping) is used to represent the geometric relationship between u and u . The above problem can be efficiently solved by generalized PatchMatch [Barnes et al. 2010], which we now briefly review. Initially, a few random transformations are assigned to each patch. The algorithm improves the transformations by iterating between a propagation and a random search step. The propagation step proceeds in scan-line order, replacing the transforms of each patch by that of its neighbor if appropriate. In the random search step, each patch randomly finds several transforms for further evaluation. The transforms are randomly chosen from windows of exponentially decreasing sizes. We utilize generalized PatchMatch, but improve upon it in several ways. Firstly, like NRDC, we use floating-point coordinates to support sub-pixel precision. Secondly, to account for crosschannel color changes between patches, we define the matching cost D m (u ? u ) from patch u to patch u as:\n          \n            1\n            m\n          \n        \n        \n          4.3 Color transform estimation D (u ? u ) = min C C u I(u) ? I(u ) 2 , (1)\n          u\n          where I(?) represents the color values of pixels in a patch. C u is a 3 ? 3 color transform matrix, obtained by least square minimization of the above equation. It accounts for cross-channel color transforms such as a hue change. This measure, however, do not work well with degenerated cases, e.g., a patch u with a uniform color. Hence, we define the final distance between u and u to be:\n          \n            2\n            D(u, u ) = max(D m (u ? u ), D m (u ? u)).\n          \n          Thirdly, as done in the PatchMatch Stereo method [Bleyer et al. 2011], we add an additional local refinement step in each iteration. In this step, for each patch, we locally optimize the stored geometric transforms using the gradient descent method to further reduce  the distance measure in Eqn. 2. This is because the 4D transformation space is too large for random search to efficiently find optimal transforms.\n          Recovered Editing History delete a fish\n          adjust hue Hue Tone Sat Bri Exp adjust hue and brightness Tone\n          \n            \n          \n          adjust hue of the whole image\n          \n            \n          \n          move and scale a fish and adjust brightness\n        \n        \n          4.2 Patch merging\n          After the best transform has been identified for each patch u , adjacent patches which are consistent are merged into larger regions. To achieve this we define the consistency error between two patches in the edited image as: ?(u , v ) = G G u v (u (v c c ) ) ? ? G G u u (v (v c c ) ) 2 2 + ? (C u ? C v ) ? I(u ) 2 , where u , v represent two patches in A centered at u c , v c , respectively, and (G u , C u ) and (G v , C v ) are estimated geometric and color transforms of them. The two terms measure differences in geometric and color transforms between u and v respectively, and ? is a balancing weight which we fix at 0.1 when using normalized color and spatial coordinates in [0, 1]. Note that the corresponding definition in the NRDC approach [HaCohen et al. 2011] does not include a color difference term, which is essential to distinguish patches with only color transformation. Next, we adopt a greedy scheme to merge neighboring patches based on the proposed consistency error. Specifically, we randomly select a patch, and greedily grow the region by merging it with its neighboring patches whose consistency error with respect to the selected patch is low (the threshold is 10), until no more neighbors can be included. We then select another patch to start another growing process. Following NRDC, we prune regions that are too small (less than 1% image size). This process results in a set of merged regions in A , each corresponds to a matched region in A.  For each matched region pair R and R , we assume a per-region cross-channel cubic color transform C R between them, along with a per-pixel smoothly-varying strength map w. This allows us to handle a rich set of possible color editing operations, including hue, contrast, saturation, tone, and brightness adjustments. It is also important to note that introducing a spatially-varying strength map w allows the system to support various paint brushes for local editing. Mathematically the color transform is formulated as:\n          \n            3\n            I m (p ) =w(p) ? a i,j,k,m (I 0 (p)) i (I 1 (p)) j (I 2 (p)) k i,j,k?0;i+j+k?3 + (1 ? w(p)) ? I m (p), m = 0, 1, 2; p ? R\n          \n          where p denotes a pixel in region R, and p is the corresponding pixel in R , I m (?) denotes the color value of the m-th channel of\n          ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n          194:4\n          ?\n          S.-M. Hu et al.\n          \n            \n          \n          (a) (b) (c) (d)\n          \n            Figure 3: Boundary refinement for matched regions. (a,b) Source and edited images. (c,d) Region matching results before and after boundary refinement.\n          \n          a pixel; a i,j,k,m are the coefficients of the per-region cross-channel cubic color transform C R , and w(p) is the per-pixel editing strength whose value is between 0 and 1. An iterative optimization approach is applied to recover both the per-region cross-channel cubic color transform C R , and the perpixel edit strength map w. We start with a constant strength map w(p) = 1. At each iteration, we first fix w and obtain C R using a least square solver, by taking into account all pixels in two matched regions. We then fix C R to recover w. We assume that it varies very smoothly, and the values are roughly the same in a small neighborhood such as a 7 ? 7 window. Hence, for each pixel p, w(p) is also obtained using a least square solver, by taking into account the neighboring pixels in a local 7 ? 7 window. In our implementation, we find usually 3-5 iterations is usually enough to get good estimates of both C R and w that are accurate enough for our purposes.\n        \n        \n          4.4 Coarse-to-fine propagation\n          Similar to the NRDC approach, in our coarse-to-fine strategy, we use the results obtained at the current level to constrain the solution at the next level with a higher resolution. Specifically, for faster convergence, the stored transform of a patch leading to a matched region have its search space at the next level limited to be within a small range (e.g. shift within 5 pixels; scale, rotation within 0.2) of the final transform at the current level. Besides, the estimated cross-channel cubic color transform C R of a region pair is applied to reduce the appearance difference between the two regions, before evaluating the distance (Eqn. 2) between patches.\n        \n        \n          4.5 Boundary refinement ?\n          Because we use patches of size 7 7 in the region matching process, the boundary of each matched region is typically not located accurately, as shown in the examples in Fig. 3 . To further improve the accuracy of region boundaries, we first over-segment both the source and the edited images using the Mean Shift algorithm [Comaniciu and Meer 2002], and assume that each segment should belong to a single region based on color consistency. Hence, we check each segment for conflict. If 80% of a segment is inside a matched region or the unmatched region U (i.e. pixels not belonging to any matched region), we assign the whole segment to the same region. Next, in the edited image, we create a refinement band B around each matched region R by expanding and shrinking its boundary by 5 pixels, and then calculate an alpha matte in the refinement band using an existing alpha matting technique [Levin et al. 2008]. Once the alpha matte is computed, we also refine the transforms for pixels in B . We use a similar algorithm to that in Sec. 4.1 to propagate the transforms from inside the region R to the refinement band B , the only difference being that we modify Eqn. 1 to use the alpha values of pixels (? p ) as weights in calculating patch distance:\n          \n            4\n            2 D m (u ? u ) = min ? p C u I(p) ? I(p ) , C u p?u\n          \n          where p is a pixel in u, and p is the corresponding pixel in u . In this way we avoid the influence of background colors when calculating matching distances for patches in the foreground objects. Examples of boundary refinement are shown in Fig. 3 . Finally, we remove those region pairs whose color and geometric transforms are both identity transforms (i.e. correspond to no edit). The remaining matched region pairs will be used in subsequent steps for generating the editing history. Fig. 4 shows several examples of the matched region pairs.\n        \n      \n      \n        5 Semantic Appearance Operator Recovery\n        Recall that we use a per-region cross-channel cubic transform (Eqn. 3) to describe appearance changes between a matched region pair. Such a transformation is powerful for reconstruction, but lacks semantic meanings. In this section we describe how to further recover semantic editing operations from it. A wide variety of global color and appearance adjustment tools exist in modern image editing software. By consulting professionals and studying representative software packages such as GIMP 2 , Adobe Lightroom 3 and Apple Aperture 4 , we have identified five of the most commonly used basic appearance operators: brightness, exposure, hue, saturation, and non-linear tone curve adjustments. Brightness and hue operators shift the values x of their corresponding channels by a constant f , to x + f . Saturation and exposure operators scale the value of each channel x to x ? (1 + f ). Inspired by existing tone curve adjustment interfaces, we use a cubic spline to represent a non-linear tone curve, parameterized by five control points: (0, 0), (0.25, f 1 ), (0.5, f 2 ), (0.75, f 3 ), (1, 1), where f 1 , f 2 , f 3 are three values in [0, 1]. In total, we have 7 parameters for appearance editing (i.e. 1 each for brightness, exposure, hue and saturation, 3 for the tone curve). Although this is a short list of basic operators, a wide range of adjustment effects can be achieved by composing multiple operators. For instance, commonly used contrast, shadow and highlight adjustments can all be achieved by non-linear tone curve manipulation. It is important to note that, besides the 7 per-region editing parameters, each pixel inside the region still maintains a per-pixel editing strength w, such that: x = w ? f (x) + (1 ? w) ? x. This allows the system to support local adjustment brushes with spatially-varying strength. Similar to Sec. 4.3, we optimize the per-region parameters and the per-pixel editing strength alternatively, as detailed below. Parameter initialization. For initializing w, we use the editing strength map obtained in Sec. 4.3. We then obtain the initial estimations of per-region parameters by applying the following steps: 1. Convert images to HSV color space, and compute initial hue and saturation parameters by subtracting the mean hue values and dividing the mean saturation values of the two regions, respectively. 2. Convert images to grayscale, and obtain initial brightness and exposure parameters f b and f e by fitting a linear model x =\n        2 http://www.gimp.org/ 3 http://www.adobe.com/products/photoshop-lightroom.html 4 http://www.apple.com/aperture/\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n        ?\n        194:5\n        \n          \n          Figure 4: Matched region pairs. Left to right: original and edited images, and recovered matched region pairs. In the last example, the recovered spatially varying strength maps are also shown.\n        \n        brightness & exposure gamma\n        \n          \n        \n        source image\n        ground truth edited images\n        ground truth recovered\n        \n          \n        \n        tone curve Figure 5 : The accuracy of the appearance operator recovery. Top: ground truth edited images generated by changing: brightness and exposure, gamma (tone) curve, hue and brightness, hue and saturation and tone curve, spatially varying adjustment of brightness and exposure, respectively. Bottom: results rendered using recovered operators are visually indistinguishable from the ground truth. Errors in estimated parameters are within 1%. Bottom-left: ground truth and recovered tone curves for the 4-th ?hue and saturation and tone curve? example. Notice that in the rightmost example, spatially varying adjustments of brightness and exposure have been applied, and the ground truth and recovered spatially-varying strength maps are shown in the top left corner of corresponding images, respectively. f e ? x + f b , where x and x are intensities of corresponding pixels in the two regions. 3. After compensating for brightness and exposure differences, use least square fitting to obtain initial tone curve parameters.\n        Parameter refinement. After initialization, we iteratively optimize the per-pixel strength map w and the per-region parameters. we first fix w and use a gradient descent method to search for optimal parameter values, by minimizing the L 2 color difference between the two regions after applying the color adjustments. Since the parameters are also influenced by the order of operations, we assume the following fixed order for color adjustments: (1) hue, (2) saturation, (3) brightness, (4) exposure, (5) tone curve. Next, we fix the per-region parameters and adjust w, using the same local constancy assumption and least square solvers as described in Sec. 4.3. We usually apply 5 iterations in this process. Operation removal. After the above optimization process, we examine the parameter values to see if they are close to the default\n        hue & brightness hue & saturation & curve spatially varying adjustment\n        recovered edited images\n        values that correspond to no change. If some of them are, we then assume that the corresponding edits have not been applied. We then fix these parameters to their default values and re-apply the optimization process to update others. Fig. 5 shows some examples of recovered appearance operators. It suggests that the recovered operators are accurate and can generate high fidelity rendered results when compared with the ground truth edited images. Fig. 6 gives another example on spatially varying adjustment. From left to right, we give the source image, edited image, and recovered per-pixel editing strength map. The result suggests our method is robust to recover such edits. A more thorough evaluation is presented in Sec. 8.\n      \n      \n        6 Recovering the Editing History\n        So far we have generated a list of matched region pairs, and appearance and geometric transforms between each pair. We now explain how to further process the matching result to generate a compact\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        194:6\n        ?\n        S.-M. Hu et al.\n        \n          \n        \n        source image edited image recovered strength map\n        \n          Figure 6: Spatially varying adjustment example.\n        \n        editing history. 6.1 Layered editing\n        Before introducing the proposed editing path recovery method, it is important to note that our system intrinsically does layered editing, where each pair of matched region is on a different layer. Note that the same object in the source image A (denoted as O(A)) could match to multiple objects in the edited image A (denoted as O i (A )), often caused by cloning. In this case there will be multiple copies of O(A) and each one pairs with an O i (A ) and lives in a different layer.\n        \n          6.2 The optimal editing path\n          The problem of generating an editing path from matched regions does not have a unique solution, as many different paths can lead to the same editing result. To find a reasonable path, we first introduce the concept of state. Formally, let the list of matched region pairs be {R k ? c ? k ? ,g ? k R k } (1 ? k ? n, n is the number of matches), where c k , g k denote the estimated per-region appearance and geometric transforms between each pair. We denote the state of each region R k by the edits that have already been applied. Hence, the beginning state of an editing path is: ? start = {? 1 = (0, 0), ? 2 = (0, 0), ..., ? n = (0, 0)}, and the final state is: ? end = {? 1 = (c 1 , g 1 ), ? 2 = (c 2 , g 2 ), ..., ? n = (c n , g n )}. Our goal is to find a stack of editing steps that change the state from ? start to ? end . In this framework, a single editing step involves selecting one, or multiple spatially connected regions {R j } simultaneously, and modifying their states in the same way: ? j = ? j + (c e , g e ) for all j, where (c e , g e ) indicates the particular operation applied. (c e , g e ) are not allowed to be both zero (i.e., no edit). Furthermore, either c e or g e should satisfy the constraint that c e = c j ? c j now or g e = g j ? g j now for at least one selected region R j ; (c j now , g j now ) denotes the current state of region R j . In other words, at least one region reaches its final color or geometric transform in a single editing step. Given these constraints, at any step, the number of possible editing paths to be taken is limited. For example, given a region R 1 whose current state is ? 1 = (0, 0), the allowed editing steps are {(0, g 1 ), (c 1 , 0), (c 1 , g 1 )}. We also need to define semantically what is the optimal path from ? start to ? end . By consulting with experienced artists, we have determined four principles that the majority of people agree: 1. Layers are edited in the coarse-to-fine order. Large, visually dominant edits are applied before fine-tuning the appearance of small objects; 2. If the same edits are applied on multiple objects, then they are applied either together at once, or sequentially without interruption (temporal focus); 3. Nearby objects are edited sequentially before moving to far away objects (spatial focus);  4. Downsampling is deferred as much as possible. Specifically, Principle 2 and 3 reflects the spatio-temporal local editing focus, meaning that the users are more likely to focus on the same editing operations for a while before moving to other tools, and focus on nearby objects in a local region first before moving to far away ones. Based on the above principles, we define the total editing cost as:\n          \n            5\n            E i + ? p |t i+1 ? t i |, 1?i?m 1?i?m?1\n          \n          where m is the number of editing steps, E i denotes the cost of the ith editing step (to be explained next), and t i , t i denote the centroids of the selected region(s) of the i-th editing step before and after applying the edits, respectively. The first term is the summed cost of all editing steps, and the second term measures the switching costs between adjacent editing steps according to Principle 3 (i.e. |t i+1 ? t i | approximates the spatial movement from the i-th step to the (i + 1)-th step). ? p is a controlling weight set at 0.5 in our experiments. The cost of the i-th editing step E i is defined as:\n          \n            6\n            E i = S i ? (d c (i) + d g (i)) ? (1 + ? s i) ? P i ,\n          \n          where S i = {R j } denotes the selected region(s) of the i-th editing step, S i is its size and S i is used to approximate its visual dominance. d c (i) and d g (i) measure the amount of color and geometric transforms applied in this step, their exact formulation will be given later. ? s is a constant set to 0.01 in our system. The term (1 + ? s i) has two purposes: (i) it penalizes longer editing paths: a shorter path is preferred for the same result, and (ii) it favors edits on dominant objects (i.e. S i is large) being applied earlier (i.e. when i is smaller), thus satisfying Principle 1. The final term P i in Eqn. 6 penalizes for early down-sampling (satisfying Principle 4). It is defined as the average scaling factor that has already been applied:  where j iterates over all selected regions in this editing step, t scale (R j ) is the scale factor that has already been applied to that region R j . Note that we completely avoid enlarging after downsampling the same region, by setting P i = +? for such edits. Finally, the amount of color and geometric transforms d c (i) and d g (i) in Eqn. 6 are defined as: where p is a pixel in S i and p is its corresponding pixel, and ? c controls the influence of the color transform. (T x , T y ), T rot and T scale denote the average translation, rotation angle and scaling of the region, respectively. ? xy , ? rot and ? scale are weights controlling the influence of each term. In our system, the weights are empirically set to ? c = 1, ? xy = 1, ? rot = ?, ? scale = ln 3. With above definitions, we seek the editing path from ? start to ? end with minimal total editing cost (as defined in Eqn. 5) among all possible paths. This can be efficiently solved using dynamic programming.\n          j R j min(1/t scale (R j ), 1) P i = , j R j\n          d c (i) = I(p ) ? I(p) /(? c R s ), p?S i d g (i) = T x 2 + T y 2 /? xy + T rot /? rot + | log T scale |/? scale ,\n          ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n          Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n          ?\n          194:7\n          \n            \n          \n          source image edited image Figure 7 : Recovered editing histories from before-and-after image pairs.\n          6.3 Handling no-match regions\n          There may exist regions in both A and A that cannot find good matches on the other image. This may be caused by operations such as cropping, object insertion or removal. We first examine if the edited image A has been derived from the original A by cropping, by finding two bounding boxes B(A) an B(A ) containing all the matched regions in A and A , respectively. Cropping is identified if B(A ) covers the entire image while B(A) covers only a portion of it, and B(A) is treated as the cropping window. We then look for object insertion and removal. An unmatched region in A may be the result of inserting an object into A, or removing one from A and filling the hole using image completion techniques. If the region is caused by image completion, then there should be no obvious seams between the unmatched region and its surrounding ones. Otherwise if the region is caused by inserting a new object, then we expect the region to be surrounded by a strong object boundary. We thus check how smooth the transition is between the region and its surroundings. Specifically, we check how well the region boundary agrees with the over-segmentation boundaries obtained by the mean shift algorithm (see Sec. 4.5). If they agree with each other well (e.g. there is more than 50% overlap), this region is considered to be a newly inserted object. Otherwise we further check the same region in A, and if the source region is also an unmatched region, then we decide that the object in the source region has been removed and the hole has been filled by image completion techniques. If such operations are identified, we incorporate them into the editing path computed in Sec. 6.2. Specifically, we add cropping and object removal at the beginning of the history, and add object insertion at the end of it to create a completed editing path.\n        \n      \n      \n        7 Applications and Results\n        We have implemented our method on a PC with an Intel Xeon 2.4GHz CPU and 8GB memory. For an image of size 640?480, region matching takes 2?3 minutes, recovering appearance operators\n        recovered editing history\n        1. Use ?move? to the\n        \n          \n        \n        source image\n        2. Use ?scale? to the gate.\n        \n          \n        \n        3. Use ?color? to adjust and L of tower\n        \n          \n        \n        edited image generated tutorial\n        \n          Figure 8: Generating a tutorial from the recovered edit history.\n        \n        takes about 10?15 seconds, depending on the number of regions being edited, and finding the optimal editing path takes about 2?5 seconds.   Fig. 7 shows a number of input before-and-after image pairs and the recovered editing histories using our approach. It suggests that our system works reliably even for large geometric and appearance transforms, and the appearance transform is allowed to be spatially varying (e.g. the first example). Furthermore, the recovered editing steps are semantically-meaningful, and can easily be used to drive tools in image editing software. Note how the system generates a compact and reasonable editing history for the 2nd and 3rd examples, from the low-level region matching results shown in the 3rd and 4th rows of Fig. 4 . Next, we illustrate how the recovered history can be used in various applications. Automatic tutorial generation. A straightforward application is to use the recovered editing history to automatically generate an image editing tutorial. Fig. 8 shows a simple example of turning the history into a step-by-step tutorial. Our method can be combined with\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        194:8\n        ?\n        S.-M. Hu et al.\n        \n          \n        \n        source image A edited image A recovered editing history\n        \n          Figure 10: Edit transfer. Given the image pair A and A , we first recover the editing history (middle), then apply it to image B to generate a result B in a similar way. Object segmentation in B is obtained by GrabCut.\n        \n        source image edited images\n        \n          \n        \n        recovered editing histories\n        \n          Figure 11: Merging multiple editing paths from the same source image. The image on the right is rendered using the merged editing history. Image courtesy of Flickr user Hans J E.\n          \n        \n        \n          Figure 9: Image re-editing results generated from recovered editing histories (see original examples in Fig. 4 top right and the second row of Fig. 7 ). Left: increasing (instead of decreasing in the original edit) the brightness of the swan on the right. Right: the first editing step (move and scale) is modified to only include move, and the fifth editing step (move) is changed to removal.\n        \n        more powerful tutorial generating systems [Grabler et al. 2009] to produce more visually appealing results. Re-editing. Users can modify the recovered history, e.g. to remove steps, or change parameters of some steps, and re-apply the modified history to the source image A to generate a new edited image A . Some examples are shown in Fig. 1 and Fig. 9 . This is easier than directly editing the original edited image A to achieve A , as the user may have to carry out object selection and use other editing tools again in the latter case; the user may also no longer remember the parameters for the parts of the edit which are to remain unchanged. Edit transfer. The recovered editing history from one pair of images can be transferred to a new image to achieve edit transfer. An example is shown in Fig. 10 , where the recovered editing history\n        \n          \n        \n        new source image B editing transfer result B\n        merged result\n        from images A and A is applied to image B. To do this, we assume that A and B have similar composition, so that objects in B are roughly at the same locations as those in A. To automatically extract the object mask in B, we first identify a rough bounding box in B which is 1.5 times larger than the actual object bounding box in A, then apply GrabCut [Rother et al. 2004] for segmentation. Other dedicated approaches have already been proposed for edit transfer, such as image analogies [Hertzmann et al. 2001] and content-adaptive macros [Berthouzoz et al. 2011]. Compared to image analogies, our method has more constraints on the input, requiring similar composition of images A and B. However, on the other hand, it is capable of object-level editing, and also handles geometric editing such as cropping, which are clear advantages over image analogies and other appearance-based transfer approaches. Content-adaptive macros requires the editing history to be known, so our system can be potentially used to generate such editing macros. Merging editing paths. Our approach can be combined with the image revision control system [Chen et al. 2011] to merge different editing paths. An example is shown in Fig. 11 , where our system recovers multiple different editing paths and merges them to create a single final rendering result.\n      \n      \n        8 Evaluation\n        To objectively evaluate the proposed system, we create an evaluation dataset that contains 21 image editing examples produced by several artists. To produce these examples, we explicitly explained the range of supported operations to the artists and asked them to perform editing using supported operations. The input images and the performed editing paths were chosen or created by the artists  without any supervision. We then selected 21 successful or moderately successful examples out of 25 that we received from the artists for user study. The original editing steps are recorded for all examples. We generate editing histories for all these examples using our system, and compare them to the original ones. We first evaluate the representation ability of the recover histories, by measuring the PSNR values of the reconstructed edited images. The average PSNR value of this dataset is 26.1dB, and the standard deviation is 6.2dB. Visually, there is no noticeable difference between the reconstructed and the ground truth edited images for most examples. These results suggest that our recovered histories can faithfully reproduce the edited images in high fidelity. Secondly, we conducted a qualitative evaluation on the semantic meaningfulness of the recovered histories. We invited 30 participants who are image editing enthusiasts and are familiar with Adobe Photoshop to participate in the study. In each user session, for each editing example, we first showed the before-and-after image pair to the subject, followed by the two editing histories: the original one and the recovered one. The display orders of the two histories were randomly determined. Each history was illustrated as an automatic slideshow, and the subject was allowed to switch back and forth to see each step more clearly, and there is no time limit in the study. After viewing a pair of editing histories, the subject was then asked to judge which one is semantically more natural, or they are about equal. The results of the user study are given in Fig. 12 . In total, our method achieved an averaged score of 0.45 (i.e. ?ours is better?, ?about equal?, ?artist?s is better? are scored at 1, 0.5, and 0, respectively), and the one tail p-value is 0.039 5 . This study demonstrates that our recovered histories are comparable to the original ones produced by artists in terms of semantic meaningfulness. In 17 out of 21 examples (81.0%), a majority of users rated that our recovered histories are equal or better than the original ones. All examples used in the user study are included in the supplemental material.\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n        ?\n        194:9\n        1 ours equal artist 0.5 0 1 6 11 16 21\n        \n          Figure 12: Percentage in favor of our and artist generated edit sequences.\n        \n      \n      \n        9 Limitations and Discussion\n        \n          9.1 Failure cases\n          To better understand the limitation of the proposed system, we studied the low rating examples (e.g. the 2nd and 6th examples) in the above user study to see why they are less successful. For the 2nd example (top row in Fig. 13 ), although our recovered editing steps are the same as that of the artist, participants rated it lower due to its visually noticeable reconstruction error. This is because the saturated color values in the sun region lead to errors in the reconstructed color transforms. For the 6th example (bottom row in Fig. 13 ), the\n          5 This p-value measures the average scores of all examples, i.e. compute averaged scores for each example, and measure the p-value of 21 scores.\n          original editing patch produced by the artist has a more natural spatial layout (i.e. change the color of the leaves from bottom to top, given the bottom one is visually more dominant), while our method first edits the leaves in the middle since their regions are large (see Eqn. 6). This example suggests that one could potentially use a better algorithm to rank the visual dominance of different objects to improve the semantic meaningfulness of the recovered history.  Our system can fail more dramatically when the individual technical components are incapable of handling more difficult cases. Firstly, the region matching algorithm in Sec. 4 may fail in the following cases: (1) when the size of the matched regions is too small to be classified as reliable; (2) when the applied geometry/color transformations are too dramatic; and (3) when some regions are purely textureless. Two such examples are shown in Fig. 14. In the first row of Fig. 14, the matching of the head of giraffe failed due to the large deformation. In the second row of Fig. 14, our region matching missed the left and right mushroom stems since they are too thin. Secondly, appearance operations that cannot be well approximated using the ones in Sec. 5 will lead to large reconstruction error when re-applying the history. Such an example is shown in the third row of Fig. 14, where our method failed to recover the complex appearance changes.\n        \n        \n          9.2 Supported operations\n          As discussed earlier, our system currently only supports a limited range of operations. We believe that with dedicated solutions, other editing operations could potentially be supported. For instance, to support Gaussian blur, we can optionally add it as a new transform dimension in the region matching step. By allowing a blurred region to match against multiple versions of the original image with different amount of blur, we can match the blurred region to its original sharp region, and at the same time produce an estimation of the size of the blur. In the appearance operator recovery step, we can first blur the source region using the recovered blur, then downsize both regions to a lower resolution to remove the effect of blurring for estimating other color operators. We implemented the above procedure in the system and in Fig. 15, we provide an example involving two operations: foreground move, and background Gaussian blur. Our method successfully recovered both operations.  Other more complicated operations are not supported in our current system, such as bilateral filtering, alpha matting, Poisson blending, etc. In general, it is relatively easy to support global operations. However, if the operation is ?content aware?, i.e., the color of a pixel is changed adaptively according to its local neighborhood appearance, such as bilateral filtering, then it is much harder to recover. Nevertheless, we have demonstrated that our current system is already widely useful as it supports a wide range of commonly used editing operations.\n        \n      \n      \n        10 Conclusion\n        We present a novel system for recovering a semantically meaningful editing history from a source image and an edited version of it. To achieve this, we use a dense correspondence method which extends the NRDC approach to find all edited regions, and recovers appearance operations applied to each region. From all possible edit paths, we recover an optimal one based on semantic constraints. Experimental and user study results show that our system can recover clean and meaningful editing histories involving large geometric and appearance transformations. We further show that the recovered histories can be useful in a wide range of applications.\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        194:10\n        ?\n        S.-M. Hu et al.\n        \n          \n        \n        source image edited image\n        \n          \n        \n        source image edited image original history by artist\n        \n          Figure 13: Less successful examples. Top: noticeable color reconstruction error due to the saturated sun region. Bottom: the recovered history has less ideal operation order compared with the original one. Image courtesy of Flickr user ?quinoktium (top).\n        \n        As future work, we would like to improve the robustness of the proposed system, by replacing some technical components with newly developed, more advanced methods. For example, we could potentially improve the robustness of the region matching step by using recently proposed higher-order deformation models [Y?cer et al. 2012], after obtaining the initial matching by our proposed method. Another way to improve system robustness is to combine our method with techniques proposed in previous photo manipulation detection systems [O?Brien and Farid 2012; Kee et al. 2013]. We are also interested in combining our system with the work of [Ma et al. 2013] to automatically generate change blindness images, and in extending our method to handle vector images [Lai et al. 2009; Liao et al. 2012]. Another potential extension is to apply our method to large image libraries [Hu et al. 2013], for analyzing image correlations and/or dependencies within a large dataset. Acknowlegements. We thank the anonymous reviewers for their valuable comments. This work was supported by National Basic Research Project of China (2011CB302205), Natural Science Foundation of China (61120106007 and 61170153), National High Technology Research and Development Program of China (2012AA011802), PCSIRT and Tsinghua University Initiative Scientific Research Program.\n      \n      \n        References\n        \n          B ARNES , C., S HECHTMAN , E., F INKELSTEIN , A., AND G OLD MAN , D. B. 2009. Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28, 3, 24:1?24:11.\n          B ARNES , C., S HECHTMAN , E., G OLDMAN , D. B., AND F INKEL STEIN , A. 2010. The generalized patchmatch correspondence algorithm. In Proc. of ECCV, 29?43.\n          B ERTHOUZOZ , F., L I , W., D ONTCHEVA , M., AND A GRAWALA , M. 2011. A framework for content-adaptive photo manipulation macros: Application to face, landscape, and global manipulations. ACM Trans. Graph. 30, 5, 120:1?120:14.\n          B LEYER , M., R HEMANN , C., AND R OTHER , C. 2011. Patchmatch stereo stereo matching with slanted support windows. In Proceedings of the British Machine Vision Conference, 14.1 ? 14.11.\n          B ROX , T., B REGLER , C., AND M ALIK , J. 2009. Large displacement optical flow. In Proc. of CVPR, 41 ?48.\n          our recovered history  reconstructed image our recovered history B YCHKOVSKY , V., P ARIS , S., C HAN , E., AND D URAND , F. 2011. Learning photographic global tonal adjustment with a database of input / output image pairs. In Proc. of CVPR, 97?104.\n          C HEN , H.-T., W EI , L.-Y., AND C HANG , C.-F. 2011. Nonlinear revision control for images. ACM Trans. Graph. 30, 4, 105:1? 105:10.\n          C HEN , H.-T., W EI , L.-Y., H ARTMANN , B., AND A GRAWALA , M. 2012. Data-driven adaptive history for image editing. Technical Report.\n          C HENG , M.-M., Z HANG , F.-L., M ITRA , N. J., H UANG , X., AND H U , S.-M. 2010. Repfinder: finding approximately repeated scene elements for image editing. ACM Trans. Graph. 29 (July), 83:1?83:8.\n          C OMANICIU , D., AND M EER , P. 2002. Mean shift: A robust approach toward feature space analysis. IEEE Trans. Pattern Anal. Mach. Intell. 24, 5, 603?619.\n          F U , H., Z HOU , S., L IU , L., AND M ITRA , N. J. 2011. Animated construction of line drawings. ACM Trans. Graph. 30, 6 (Dec.), 133:1?133:10.\n          G RABLER , F., A GRAWALA , M., L I , W., D ONTCHEVA , M., AND I GARASHI , T. 2009. Generating photo manipulation tutorials by demonstration. ACM Trans. Graph. 28, 3 (July), 66:1?66:9.\n          H A C OHEN , Y., S HECHTMAN , E., G OLDMAN , D. B., AND L ISCHINSKI , D. 2011. Non-rigid dense correspondence with applications for image enhancement. ACM Trans. Graph. 30, 4, 70:1?70:10.\n          H EER , J., M ACKINLAY , J., S TOLTE , C., AND A GRAWALA , M. 2008. Graphical histories for visualization: Supporting analysis, communication, and evaluation. IEEE Transactions on Visualization and Computer Graphics 14, 6, 1189?1196.\n          H ERTZMANN , A., J ACOBS , C. E., O LIVER , N., C URLESS , B., AND S ALESIN , D. H. 2001. Image analogies. In Proc. of Siggraph, 327?340.\n          H U , S.-M., C HEN , T., X U , K., C HENG , M.-M., AND M ARTIN , R. R. 2013. Internet visual media processing: a survey with graphics and vision applications. The Visual Computer 29, 5, 393?405.\n          source image  edited image reconstructed image\n        \n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n        Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair\n        ?\n        194:11\n        \n          \n        \n        \n          Figure 14: Failure examples due to large geometric deformation (top), small image structures (middle) and complex color change (bottom). Image courtesy of Flickr user Michael Li (third row).\n          \n        \n        source image edited image recovered history\n        \n          Figure 15: A gaussian blur example. Image courtesy of Flickr user LarryBiker.\n        \n        K EE , E., O? BRIEN , J., AND F ARID , H. 2013. Exposing photo manipulation with inconsistent shadows. ACM Transactions on Graphics 32, 3, 28:1?28:12.\n        K ONG , N., G ROSSMAN , T., H ARTMANN , B., A GRAWALA , M., AND F ITZMAURICE , G. W. 2012. Delta: a tool for representing and comparing workflows. In Proc. of CHI, 1027?1036.\n        K URLANDER , D., AND F EINER , S. 1988. Editable graphical histories. In IEEE Workshop on Visual Languages, 127?134.\n        L AI , Y.-K., H U , S.-M., AND M ARTIN , R. R. 2009. Automatic and topology-preserving gradient mesh generation for image vectorization. ACM Trans. Graph. 28, 3, 85:1?85:8.\n        L EVIN , A., L ISCHINSKI , D., AND W EISS , Y. 2008. A closedform solution to natural image matting. IEEE Trans. Pattern Anal. Mach. Intell. 30, 2, 228?242. L IAO , Z., H OPPE , H., F ORSYTH , D., AND Y U , Y. 2012. A subdivision-based representation for vector image editing. IEEE\n        Transactions on Visualization and Computer Graphics 18, 11, 1858?1867. L IU , C., Y UEN , J., T ORRALBA , A., S IVIC , J., AND F REEMAN , W. T. 2008. Sift flow: Dense correspondence across different scenes. In Proc. of ECCV, 28?42. L OWE , D. G. 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision 60, 2, 91?110. M A , L.-Q., X U , K., W ONG , T.-T., J IANG , B.-Y., AND H U , S.M. 2013. Change blindness images. IEEE Transactions on Visualization and Computer Graphics, to appear. O?B RIEN , J. F., AND F ARID , H. 2012. Exposing photo manipulation with inconsistent reflections. ACM Transactions on Graphics 31, 1, 4:1?4:11. R OTHER , C., K OLMOGOROV , V., AND B LAKE , A. 2004. ?grabcut?: interactive foreground extraction using iterated graph cuts. ACM Trans. Graph. 23, 3, 309?314. S U , S. L., P ARIS , S., A LIAGA , F., S CULL , C., J OHNSON , S., AND D URAND , F. 2009. Interactive visual histories for vector graphics. Tech Report, MIT-CSAIL-TR-2009-031. X IAO , C., L IU , M., Y ONGWEI , N., AND D ONG , Z. 2011. Fast exact nearest patch matching for patch-based image editing and processing. IEEE Transactions on Visualization and Computer Graphics 17, 8, 1122?1134. Y ANG , G., S TEWART , C., S OFKA , M., AND T SAI , C.-L. 2007. Registration of challenging image pairs: Initialization, estimation, and decision. IEEE Transactions on Pattern Analysis and Machine Intelligence 29, 11, 1973?1989. Y UCER  ? , K., J ACOBSON , A., H ORNUNG , A., AND S ORKINE , O. 2012. Transfusive image manipulation. ACM Trans. Graph. 31, 6, 176:1?176:9. Z HANG , F.-L., C HENG , M.-M., J IA , J., AND H U , S.-M. 2012. Imageadmixture: Putting together dissimilar objects from groups. IEEE Transactions on Visualization and Computer Graphics 18, 11, 1849?1857. Z IMMER , H., B RUHN , A., AND W EICKERT , J. 2011. Optic flow in harmony. Int. J. Comput. Vision 93, 3, 368?388.\n        ACM Transactions on Graphics, Vol. 32, No. 6, Article 194, Publication Date: November 2013\n      \n    \n  ",
  "resources" : [ ]
}