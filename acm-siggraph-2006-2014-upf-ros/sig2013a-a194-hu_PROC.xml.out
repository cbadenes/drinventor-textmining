{
  "uri" : "sig2013a-a194-hu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013a/a194-hu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Inverse Image Editing: Recovering a Semantic Editing History from a Before-and-After Image Pair",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shi-Min-Hu",
      "name" : "Shi-Min",
      "surname" : "Hu"
    }, {
      "uri" : "http://drinventor/Kun-Xu",
      "name" : "Kun",
      "surname" : "Xu"
    }, {
      "uri" : "http://drinventor/Li-Qian-Ma",
      "name" : "Li-Qian",
      "surname" : "Ma"
    }, {
      "uri" : "http://drinventor/Bin-Liu",
      "name" : "Bin",
      "surname" : "Liu"
    }, {
      "uri" : "http://drinventor/Bi-Ye-Jiang",
      "name" : "Bi-Ye",
      "surname" : "Jiang"
    }, {
      "uri" : "http://drinventor/Jue-Wang",
      "name" : "Jue",
      "surname" : "Wang"
    } ]
  },
  "bagOfWords" : [ "can", "easily", "take", "dozen", "operation", "explore", "different", "idea", "fix", "error", "fine", "tweak", "parameter", "before", "produce", "desire", "output", "we", "system", "provide", "set", "solution", "problem", "particular", "we", "improve", "state-ofthe-art", "region", "match", "method", "handle", "large", "appearance", "difference", "between", "original", "object", "its", "edit", "version", "-lrb-", "see", "Sec", "we", "also", "demonstrate", "how", "recovered", "history", "can", "apply", "various", "application", "-lrb-", "see", "Sec", "impossible", "practice", "recover", "all", "editing", "operation", "may", "have", "be", "apply", "image", "evaluate", "proposed", "system", "we", "construct", "test", "dataset", "contain", "before-and-after", "image", "pair", "along", "original", "editing", "step", "perform", "artist", "user", "study", "dataset", "reveal", "editing", "history", "generate", "we", "system", "general", "comparable", "original", "one", "term", "semantic", "meaningfulness", "-lrb-", "see", "Sec", "sparse", "matching", "method", "sift", "matching", "-lsb-", "Lowe", "2004", "-rsb-", "applicable", "dense", "correspondence", "require", "we", "application", "we", "adopt", "framework", "algorithm", "extend", "we", "application", "describe", "Sec", ".4", "Delta", "system", "-lsb-", "Kong", "et", "al.", "2012", "-rsb-", "can", "help", "user", "identify", "tradeoff", "between", "workflow", "use", "visual", "comparison", "edit", "transfer", "image", "analogy", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "provide", "classic", "example", "edit", "transfer", "without", "recover", "editing", "process", "repfinder", "system", "-lsb-", "Cheng", "et", "al.", "2010", "-rsb-", "find", "repeated", "scene", "element", "image", "so", "edit", "make", "one", "element", "can", "transfer", "other", "pipeline", "we", "algorithm", "show", "fig.", "consist", "three", "main", "step", "-lrb-", "-rrb-", "region", "match", "-lrb-", "-rrb-", "recover", "semantic", "appearance", "operator", "each", "match", "region", "pair", "-lrb-", "-rrb-", "generate", "editing", "history", "specifically", "we", "first", "find", "all", "match", "region", "pair", "between", "source", "edit", "image", "-lrb-", "sec", "achieve", "use", "matching", "method", "extend", "from", "non-rigid-densecorrespondence", "-lrb-", "NRDC", "-rrb-", "algorithm", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "accommodate", "wider", "range", "appearance", "difference", "between", "pair", "region", "secondly", "we", "recover", "semantic", "appearance", "operation", "each", "match", "region", "pair", "-lrb-", "sec", "-rrb-", "which", "may", "include", "both", "global", "linear", "color", "transform", "brightness", "exposure", "hue", "saturation", "adjustment", "well", "non-linear", "tone", "mapping", "local", "brush", "spatially-varying", "strength", "finally", "give", "match", "region", "pair", "recover", "editing", "operation", "we", "use", "optimization", "approach", "generate", "compact", "semanticallymeaningful", "editing", "history", "accord", "set", "predefined", "editing", "rule", "-lrb-", "sec", "first", "step", "algorithm", "we", "seek", "reliably", "recover", "region", "pair", "between", "source", "edit", "image", "share", "same", "content", "compare", "two", "match", "region", "give", "we", "means", "discover", "whether", "source", "region", "have", "be", "edit", "so", "how", "we", "region", "match", "approach", "extend", "from", "NRDC", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "algorithm", "have", "better", "capability", "handle", "large", "appearance", "transform", "specifically", "we", "adopt", "coarse-to-fine", "framework", "original", "NRDC", "approach", "which", "initially", "downsample", "image", "low", "resolution", "find", "match", "use", "they", "constraint", "find", "match", "next", "finer", "resolution", "each", "level", "follow", "four", "step", "apply", "sequentially", "-lrb-", "-rrb-", "nearest", "neighbor", "search", "-lrb-", "-rrb-", "patch", "merge", "-lrb-", "-rrb-", "color", "transform", "estimation", "-lrb-", "-rrb-", "coarse-tofine", "propagation", "after", "above", "step", "finish", "finest", "level", "boundary", "refinement", "step", "apply", "produce", "accurate", "boundary", "each", "match", "region", "4d", "geometric", "transformation", "-lrb-", "i.e.", "represent", "2d", "translation", "rotation", "scaling", "flip", "-rrb-", "use", "represent", "geometric", "relationship", "between", "algorithm", "improve", "transformation", "iterate", "between", "propagation", "random", "search", "step", "propagation", "step", "proceeds", "scan-line", "order", "replace", "transform", "each", "patch", "its", "neighbor", "appropriate", "random", "search", "step", "each", "patch", "randomly", "find", "several", "transform", "further", "evaluation", "transform", "randomly", "choose", "from", "window", "exponentially", "decrease", "size", "we", "utilize", "generalize", "PatchMatch", "improve", "upon", "several", "way", "firstly", "like", "NRDC", "we", "use", "floating-point", "coordinate", "support", "sub-pixel", "precision", "secondly", "account", "crosschannel", "color", "change", "between", "patch", "we", "define", "match", "cost", "-lrb-", "-rrb-", "from", "patch", "patch", "where", "-lrb-", "-rrb-", "represent", "color", "value", "pixel", "patch", "account", "cross-channel", "color", "transform", "hue", "change", "measure", "however", "do", "work", "well", "degenerate", "case", "e.g.", "patch", "uniform", "color", "hence", "we", "define", "final", "distance", "between", "thirdly", "do", "PatchMatch", "Stereo", "method", "-lsb-", "Bleyer", "et", "al.", "2011", "-rsb-", "we", "add", "additional", "local", "refinement", "step", "each", "iteration", "step", "each", "patch", "we", "locally", "optimize", "store", "geometric", "transform", "use", "gradient", "descent", "method", "further", "reduce", "distance", "measure", "eqn", "after", "best", "transform", "have", "be", "identify", "each", "patch", "adjacent", "patch", "which", "consistent", "merge", "larger", "region", "achieve", "we", "define", "consistency", "error", "between", "two", "patch", "edit", "image", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "represent", "two", "patch", "centered", "respectively", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "estimate", "geometric", "color", "transform", "they", "note", "corresponding", "definition", "NRDC", "approach", "-lsb-", "HaCohen", "et", "al.", "2011", "-rsb-", "do", "include", "color", "difference", "term", "which", "essential", "distinguish", "patch", "only", "color", "transformation", "next", "we", "adopt", "greedy", "scheme", "merge", "neighbor", "patch", "base", "propose", "consistency", "error", "specifically", "we", "randomly", "select", "patch", "greedily", "grow", "region", "merge", "its", "neighbor", "patch", "whose", "consistency", "error", "respect", "select", "patch", "low", "-lrb-", "threshold", "10", "-rrb-", "until", "more", "neighbor", "can", "include", "we", "select", "another", "patch", "another", "grow", "process", "follow", "NRDC", "we", "prune", "region", "too", "small", "-lrb-", "less", "than", "image", "size", "-rrb-", "process", "result", "set", "merged", "region", "each", "correspond", "match", "region", "a.", "each", "match", "region", "pair", "we", "assume", "per-region", "cross-channel", "cubic", "color", "transform", "between", "they", "along", "per-pixel", "smoothly-varying", "strength", "map", "allow", "we", "handle", "rich", "set", "possible", "color", "editing", "operation", "include", "hue", "contrast", "saturation", "tone", "brightness", "adjustment", "also", "important", "note", "introduce", "spatially-varying", "strength", "map", "allow", "system", "support", "various", "paint", "brush", "local", "editing", "mathematically", "color", "transform", "formulate", "where", "denote", "pixel", "region", "corresponding", "pixel", "-lrb-", "-rrb-", "denote", "color", "value", "m-th", "channel", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "pixel", "coefficient", "per-region", "cross-channel", "cubic", "color", "transform", "-lrb-", "-rrb-", "per-pixel", "editing", "strength", "whose", "value", "between", "iterative", "optimization", "approach", "apply", "recover", "both", "per-region", "cross-channel", "cubic", "color", "transform", "perpixel", "edit", "strength", "map", "we", "constant", "strength", "map", "-lrb-", "-rrb-", "each", "iteration", "we", "first", "fix", "obtain", "use", "least", "square", "solver", "take", "account", "all", "pixel", "two", "match", "region", "we", "fix", "recover", "we", "assume", "vary", "very", "smoothly", "value", "roughly", "same", "small", "neighborhood", "window", "hence", "each", "pixel", "-lrb-", "-rrb-", "also", "obtain", "use", "least", "square", "solver", "take", "account", "neighbor", "pixel", "local", "window", "we", "implementation", "we", "find", "usually", "3-5", "iteration", "usually", "enough", "get", "good", "estimate", "both", "accurate", "enough", "we", "purpose", "similar", "NRDC", "approach", "we", "coarse-to-fine", "strategy", "we", "use", "result", "obtain", "current", "level", "constrain", "solution", "next", "level", "higher", "resolution", "specifically", "faster", "convergence", "store", "transform", "patch", "lead", "match", "region", "have", "its", "search", "space", "next", "level", "limit", "within", "small", "range", "-lrb-", "e.g.", "shift", "within", "pixel", "scale", "rotation", "within", "0.2", "-rrb-", "final", "transform", "current", "level", "besides", "estimate", "cross-channel", "cubic", "color", "transform", "region", "pair", "apply", "reduce", "appearance", "difference", "between", "two", "region", "before", "evaluate", "distance", "-lrb-", "eqn", "-rrb-", "between", "patch", "further", "improve", "accuracy", "region", "boundary", "we", "first", "over-segment", "both", "source", "edit", "image", "use", "mean", "shift", "algorithm", "-lsb-", "comaniciu", "Meer", "2002", "-rsb-", "assume", "each", "segment", "should", "belong", "single", "region", "base", "color", "consistency", "hence", "we", "check", "each", "segment", "conflict", "80", "segment", "inside", "match", "region", "unmatched", "region", "-lrb-", "i.e.", "pixels", "belong", "any", "match", "region", "-rrb-", "we", "assign", "whole", "segment", "same", "region", "next", "edit", "image", "we", "create", "refinement", "band", "around", "each", "match", "region", "expand", "shrink", "its", "boundary", "pixel", "calculate", "alpha", "matte", "refinement", "band", "use", "exist", "alpha", "matting", "technique", "-lsb-", "Levin", "et", "al.", "2008", "-rsb-", "once", "alpha", "matte", "compute", "we", "also", "refine", "transform", "pixel", "we", "use", "similar", "algorithm", "Sec", "4.1", "propagate", "transform", "from", "inside", "region", "refinement", "band", "only", "difference", "be", "we", "modify", "eqn", "use", "alpha", "value", "pixel", "-lrb-", "-rrb-", "weight", "calculate", "patch", "distance", "where", "pixel", "corresponding", "pixel", "way", "we", "avoid", "influence", "background", "color", "when", "calculate", "match", "distance", "patch", "foreground", "object", "example", "boundary", "refinement", "show", "fig.", "finally", "we", "remove", "those", "region", "pair", "whose", "color", "geometric", "transform", "both", "identity", "transform", "-lrb-", "i.e.", "correspond", "edit", "-rrb-", "remain", "match", "region", "pair", "use", "subsequent", "step", "generate", "editing", "history", "fig.", "show", "several", "example", "match", "region", "pair", "recall", "we", "use", "per-region", "cross-channel", "cubic", "transform", "-lrb-", "eqn", "-rrb-", "describe", "appearance", "change", "between", "match", "region", "pair", "section", "we", "describe", "how", "further", "recover", "semantic", "editing", "operation", "from", "brightness", "hue", "operator", "shift", "value", "corresponding", "channel", "constant", "saturation", "exposure", "operator", "scale", "value", "each", "channel", "-lrb-", "-rrb-", "inspire", "exist", "tone", "curve", "adjustment", "interface", "we", "use", "cubic", "spline", "represent", "non-linear", "tone", "curve", "parameterize", "five", "control", "point", "-lrb-", "-rrb-", "-lrb-", "0.25", "-rrb-", "-lrb-", "0.5", "-rrb-", "-lrb-", "0.75", "-rrb-", "-lrb-", "-rrb-", "where", "three", "value", "-lsb-", "-rsb-", "total", "we", "have", "parameter", "appearance", "editing", "-lrb-", "i.e.", "each", "brightness", "exposure", "hue", "saturation", "tone", "curve", "-rrb-", "instance", "commonly", "use", "contrast", "shadow", "highlight", "adjustment", "can", "all", "achieve", "non-linear", "tone", "curve", "manipulation", "important", "note", "besides", "per-region", "editing", "parameter", "each", "pixel", "inside", "region", "still", "maintain", "per-pixel", "editing", "strength", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "allow", "system", "support", "local", "adjustment", "brush", "spatially-varying", "strength", "similar", "Sec", "4.3", "we", "optimize", "per-region", "parameter", "per-pixel", "editing", "strength", "alternatively", "detail", "below", "parameter", "initialization", "initialize", "we", "use", "editing", "strength", "map", "obtain", "Sec", "we", "obtain", "initial", "estimation", "per-region", "parameter", "apply", "follow", "step", "convert", "image", "hsv", "color", "space", "compute", "initial", "hue", "saturation", "parameter", "subtract", "mean", "hue", "value", "divide", "mean", "saturation", "value", "two", "region", "respectively", "source", "image", "ground", "truth", "edit", "image", "tone", "curve", "figure", "accuracy", "appearance", "operator", "recovery", "top", "ground", "truth", "edit", "image", "generate", "change", "brightness", "exposure", "gamma", "-lrb-", "tone", "-rrb-", "curve", "hue", "brightness", "hue", "saturation", "tone", "curve", "spatially", "vary", "adjustment", "brightness", "exposure", "respectively", "bottom", "result", "render", "use", "recover", "operator", "visually", "indistinguishable", "from", "ground", "truth", "error", "estimate", "parameter", "within", "bottom-left", "ground", "truth", "recover", "tone", "curve", "4-th", "hue", "saturation", "tone", "curve", "example", "Notice", "rightmost", "example", "spatially", "vary", "adjustment", "brightness", "exposure", "have", "be", "apply", "ground", "truth", "recover", "spatially-varying", "strength", "map", "show", "top", "left", "corner", "corresponding", "image", "respectively", "where", "intensity", "corresponding", "pixel", "two", "region", "after", "compensate", "brightness", "exposure", "difference", "use", "least", "square", "fitting", "obtain", "initial", "tone", "curve", "parameter", "parameter", "refinement", "after", "initialization", "we", "iteratively", "optimize", "per-pixel", "strength", "map", "per-region", "parameter", "we", "first", "fix", "use", "gradient", "descent", "method", "search", "optimal", "parameter", "value", "minimize", "color", "difference", "between", "two", "region", "after", "apply", "color", "adjustment", "since", "parameter", "also", "influence", "order", "operation", "we", "assume", "follow", "fix", "order", "color", "adjustment", "-lrb-", "-rrb-", "hue", "-lrb-", "-rrb-", "saturation", "-lrb-", "-rrb-", "brightness", "-lrb-", "-rrb-", "exposure", "-lrb-", "-rrb-", "tone", "curve", "next", "we", "fix", "per-region", "parameter", "adjust", "use", "same", "local", "constancy", "assumption", "least", "square", "solver", "describe", "Sec", "we", "usually", "apply", "iteration", "process", "operation", "removal", "recover", "edited", "image", "value", "correspond", "change", "some", "they", "we", "assume", "corresponding", "edit", "have", "be", "apply", "we", "fix", "parameter", "default", "value", "re-apply", "optimization", "process", "update", "other", "fig.", "show", "some", "example", "recovered", "appearance", "operator", "suggest", "recover", "operator", "accurate", "can", "generate", "high", "fidelity", "render", "result", "when", "compare", "ground", "truth", "edit", "image", "fig.", "give", "another", "example", "spatially", "vary", "adjustment", "from", "leave", "right", "we", "give", "source", "image", "edit", "image", "recover", "per-pixel", "editing", "strength", "map", "result", "suggest", "we", "method", "robust", "recover", "edit", "more", "thorough", "evaluation", "present", "Sec", "so", "far", "we", "have", "generate", "list", "match", "region", "pair", "appearance", "geometric", "transform", "between", "each", "pair", "we", "now", "explain", "how", "further", "process", "matching", "result", "generate", "compact", "source", "image", "edit", "image", "recover", "strength", "map", "editing", "history", "6.1", "layered", "editing", "before", "introduce", "propose", "editing", "path", "recovery", "method", "important", "note", "we", "system", "intrinsically", "do", "layered", "editing", "where", "each", "pair", "match", "region", "different", "layer", "note", "same", "object", "source", "image", "-lrb-", "denote", "-lrb-", "-rrb-", "-rrb-", "could", "match", "multiple", "object", "edit", "image", "-lrb-", "denote", "-lrb-", "-rrb-", "-rrb-", "often", "cause", "clone", "case", "multiple", "copy", "-lrb-", "-rrb-", "each", "one", "pair", "-lrb-", "-rrb-", "live", "different", "layer", "find", "reasonable", "path", "we", "first", "introduce", "concept", "state", "formally", "let", "list", "match", "region", "pair", "-lcb-", "-rcb-", "-lrb-", "number", "match", "-rrb-", "where", "denote", "estimate", "per-region", "appearance", "geometric", "transform", "between", "each", "pair", "-lrb-", "-rrb-", "-rcb-", "final", "state", "end", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "...", "-lrb-", "-rrb-", "-rcb-", "framework", "single", "editing", "step", "involve", "select", "one", "multiple", "spatially", "connect", "region", "-lcb-", "-rcb-", "simultaneously", "modify", "state", "same", "way", "-lrb-", "-rrb-", "all", "where", "-lrb-", "-rrb-", "indicate", "particular", "operation", "apply", "-lrb-", "-rrb-", "allow", "both", "zero", "-lrb-", "i.e.", "edit", "-rrb-", "other", "word", "least", "one", "region", "reach", "its", "final", "color", "geometric", "transform", "single", "editing", "step", "give", "constraint", "any", "step", "number", "possible", "editing", "path", "take", "limit", "we", "also", "need", "define", "semantically", "what", "optimal", "path", "from", "end", "consult", "experienced", "artist", "we", "have", "determine", "four", "principle", "majority", "people", "agree", "layer", "edit", "coarse-to-fine", "order", "large", "visually", "dominant", "edit", "apply", "before", "fine-tuning", "appearance", "small", "object", "same", "edit", "apply", "multiple", "object", "apply", "either", "together", "once", "sequentially", "without", "interruption", "-lrb-", "temporal", "focus", "-rrb-", "downsample", "defer", "much", "possible", "base", "above", "principle", "we", "define", "total", "editing", "cost", "where", "number", "editing", "step", "denote", "cost", "ith", "editing", "step", "-lrb-", "explain", "next", "-rrb-", "denote", "centroid", "select", "region", "-lrb-", "-rrb-", "i-th", "editing", "step", "before", "after", "apply", "edit", "respectively", "first", "term", "sum", "cost", "all", "editing", "step", "second", "term", "measure", "switching", "cost", "between", "adjacent", "editing", "step", "accord", "principle", "-lrb-", "i.e.", "+1", "approximate", "spatial", "movement", "from", "i-th", "step", "-lrb-", "-rrb-", "th", "step", "-rrb-", "control", "weight", "set", "0.5", "we", "experiment", "cost", "i-th", "editing", "step", "define", "where", "-lcb-", "-rcb-", "denote", "select", "region", "-lrb-", "-rrb-", "i-th", "editing", "step", "its", "size", "use", "approximate", "its", "visual", "dominance", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "measure", "amount", "color", "geometric", "transform", "apply", "step", "exact", "formulation", "give", "later", "constant", "set", "0.01", "we", "system", "term", "-lrb-", "-rrb-", "have", "two", "purpose", "-lrb-", "-rrb-", "penalize", "longer", "editing", "path", "shorter", "path", "prefer", "same", "result", "-lrb-", "ii", "-rrb-", "favor", "edit", "dominant", "object", "-lrb-", "i.e.", "large", "-rrb-", "be", "apply", "earlier", "-lrb-", "i.e.", "when", "smaller", "-rrb-", "thus", "satisfy", "Principle", "final", "term", "eqn", "penalize", "early", "down-sampling", "-lrb-", "satisfy", "Principle", "-rrb-", "define", "average", "scaling", "factor", "have", "already", "be", "apply", "where", "iterate", "over", "all", "select", "region", "editing", "step", "scale", "-lrb-", "-rrb-", "scale", "factor", "have", "already", "be", "apply", "region", "note", "we", "completely", "avoid", "enlarge", "after", "downsample", "same", "region", "set", "edit", "finally", "amount", "color", "geometric", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "eqn", "-lrb-", "-rrb-", "rot", "scale", "denote", "average", "translation", "rotation", "angle", "scaling", "region", "respectively", "we", "system", "weight", "empirically", "set", "xy", "rot", "scale", "ln", "above", "definition", "we", "seek", "editing", "path", "from", "end", "minimal", "total", "editing", "cost", "-lrb-", "define", "eqn", "-rrb-", "among", "all", "possible", "path", "can", "efficiently", "solve", "use", "dynamic", "programming", "source", "image", "edit", "image", "figure", "recover", "editing", "history", "from", "before-and-after", "image", "pair", "may", "exist", "region", "both", "can", "find", "good", "match", "other", "image", "may", "cause", "operation", "crop", "object", "insertion", "removal", "we", "first", "examine", "edit", "image", "have", "be", "derive", "from", "original", "crop", "find", "two", "bound", "box", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "contain", "all", "match", "region", "respectively", "cropping", "identify", "-lrb-", "-rrb-", "cover", "entire", "image", "while", "-lrb-", "-rrb-", "cover", "only", "portion", "-lrb-", "-rrb-", "treat", "crop", "window", "we", "look", "object", "insertion", "removal", "unmatched", "region", "may", "result", "insert", "object", "remove", "one", "from", "fill", "hole", "use", "image", "completion", "technique", "region", "cause", "image", "completion", "should", "obvious", "seam", "between", "unmatched", "region", "its", "surrounding", "one", "otherwise", "region", "cause", "insert", "new", "object", "we", "expect", "region", "surround", "strong", "object", "boundary", "we", "thus", "check", "how", "smooth", "transition", "between", "region", "its", "surroundings", "specifically", "we", "check", "how", "well", "region", "boundary", "agree", "over-segmentation", "boundary", "obtain", "mean", "shift", "algorithm", "-lrb-", "see", "Sec", "agree", "each", "other", "well", "-lrb-", "e.g.", "more", "than", "50", "overlap", "-rrb-", "region", "consider", "newly", "insert", "object", "otherwise", "we", "further", "check", "same", "region", "source", "region", "also", "unmatched", "region", "we", "decide", "object", "source", "region", "have", "be", "remove", "hole", "have", "be", "fill", "image", "completion", "technique", "operation", "identify", "we", "incorporate", "they", "editing", "path", "compute", "Sec", "specifically", "we", "add", "crop", "object", "removal", "beginning", "history", "add", "object", "insertion", "end", "create", "complete", "editing", "path", "we", "have", "implement", "we", "method", "pc", "Intel", "Xeon", "2.4", "GHz", "CPU", "8gb", "memory", "image", "size", "640", "480", "region", "match", "take", "minute", "recover", "appearance", "operator", "recover", "editing", "history", "source", "image", "edit", "image", "generate", "tutorial", "take", "about", "10", "15", "seconds", "depend", "number", "region", "be", "edit", "find", "optimal", "editing", "path", "take", "about", "seconds", "fig.", "show", "number", "input", "before-and-after", "image", "pair", "recover", "editing", "history", "use", "we", "approach", "suggest", "we", "system", "work", "reliably", "even", "large", "geometric", "appearance", "transform", "appearance", "transform", "allow", "spatially", "vary", "-lrb-", "e.g.", "first", "example", "-rrb-", "furthermore", "recover", "editing", "step", "semantically-meaningful", "can", "easily", "use", "drive", "tool", "image", "editing", "software", "note", "how", "system", "generate", "compact", "reasonable", "editing", "history", "2nd", "3rd", "example", "from", "low-level", "region", "matching", "result", "show", "3rd", "4th", "row", "fig.", "next", "we", "illustrate", "how", "recovered", "history", "can", "use", "various", "application", "Automatic", "tutorial", "generation", "straightforward", "application", "use", "recover", "editing", "history", "automatically", "generate", "image", "editing", "tutorial", "fig.", "show", "simple", "example", "turn", "history", "step-by-step", "tutorial", "we", "method", "can", "combine", "source", "image", "edit", "image", "recover", "editing", "history", "more", "powerful", "tutorial", "generate", "system", "-lsb-", "Grabler", "et", "al.", "2009", "-rsb-", "produce", "more", "visually", "appealing", "result", "re-editing", "user", "can", "modify", "recovered", "history", "e.g.", "remove", "step", "change", "parameter", "some", "step", "re-apply", "modify", "history", "source", "image", "generate", "new", "edit", "image", "some", "example", "show", "fig.", "Fig.", "easier", "than", "directly", "editing", "original", "edited", "image", "achieve", "user", "may", "have", "carry", "out", "object", "selection", "use", "other", "editing", "tool", "again", "latter", "case", "user", "may", "also", "longer", "remember", "parameter", "part", "edit", "which", "remain", "unchanged", "edit", "transfer", "recover", "editing", "history", "from", "one", "pair", "image", "can", "transfer", "new", "image", "achieve", "edit", "transfer", "example", "show", "Fig.", "10", "where", "recover", "editing", "history", "from", "image", "apply", "image", "B.", "do", "we", "assume", "have", "similar", "composition", "so", "object", "roughly", "same", "location", "those", "a.", "automatically", "extract", "object", "mask", "we", "first", "identify", "rough", "bound", "box", "which", "1.5", "time", "larger", "than", "actual", "object", "bound", "box", "apply", "grabcut", "-lsb-", "Rother", "et", "al.", "2004", "-rsb-", "segmentation", "other", "dedicated", "approach", "have", "already", "be", "propose", "edit", "transfer", "image", "analogy", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "content-adaptive", "macro", "-lsb-", "Berthouzoz", "et", "al.", "2011", "-rsb-", "compare", "image", "analogy", "we", "method", "have", "more", "constraint", "input", "require", "similar", "composition", "image", "B.", "however", "other", "hand", "capable", "object-level", "editing", "also", "handle", "geometric", "editing", "crop", "which", "clear", "advantage", "over", "image", "analogy", "other", "appearance-based", "transfer", "approach", "content-adaptive", "macro", "require", "editing", "history", "know", "so", "we", "system", "can", "potentially", "use", "generate", "editing", "macro", "merge", "editing", "path", "we", "approach", "can", "combine", "image", "revision", "control", "system", "-lsb-", "Chen", "et", "al.", "2011", "-rsb-", "merge", "different", "editing", "path", "example", "show", "Fig.", "11", "where", "we", "system", "recover", "multiple", "different", "editing", "path", "merge", "they", "create", "single", "final", "render", "result", "objectively", "evaluate", "proposed", "system", "we", "create", "evaluation", "dataset", "contain", "21", "image", "editing", "example", "produce", "several", "artist", "produce", "example", "we", "explicitly", "explain", "range", "support", "operation", "artist", "ask", "they", "perform", "editing", "use", "support", "operation", "input", "image", "perform", "editing", "path", "be", "choose", "create", "artist", "without", "any", "supervision", "we", "select", "21", "successful", "moderately", "successful", "example", "out", "25", "we", "receive", "from", "artist", "user", "study", "original", "editing", "step", "record", "all", "example", "we", "generate", "editing", "history", "all", "example", "use", "we", "system", "compare", "they", "original", "one", "we", "first", "evaluate", "representation", "ability", "recover", "history", "measure", "psnr", "value", "reconstructed", "edited", "image", "average", "psnr", "value", "dataset", "26.1", "db", "standard", "deviation", "6.2", "db", "visually", "noticeable", "difference", "between", "reconstruct", "ground", "truth", "edit", "image", "most", "example", "result", "suggest", "we", "recover", "history", "can", "faithfully", "reproduce", "edit", "image", "high", "fidelity", "secondly", "we", "conduct", "qualitative", "evaluation", "semantic", "meaningfulness", "recover", "history", "we", "invite", "30", "participant", "who", "image", "editing", "enthusiast", "familiar", "Adobe", "Photoshop", "participate", "study", "each", "user", "session", "each", "editing", "example", "we", "first", "show", "before-and-after", "image", "pair", "subject", "follow", "two", "editing", "history", "original", "one", "recover", "one", "display", "order", "two", "history", "be", "randomly", "determine", "each", "history", "illustrate", "automatic", "slideshow", "subject", "allow", "switch", "back", "forth", "see", "each", "step", "more", "clearly", "time", "limit", "study", "after", "view", "pair", "editing", "history", "subject", "ask", "judge", "which", "one", "semantically", "more", "natural", "about", "equal", "result", "user", "study", "give", "Fig.", "12", "total", "we", "method", "achieve", "average", "score", "0.45", "-lrb-", "i.e.", "ours", "better", "about", "equal", "artist?s", "better", "score", "0.5", "respectively", "-rrb-", "one", "tail", "p-value", "0.039", "study", "demonstrate", "we", "recover", "history", "comparable", "original", "one", "produce", "artist", "term", "semantic", "meaningfulness", "17", "out", "21", "example", "-lrb-", "81.0", "-rrb-", "majority", "user", "rate", "we", "recover", "history", "equal", "better", "than", "original", "one", "all", "example", "use", "user", "study", "include", "supplemental", "material", "better", "understand", "limitation", "proposed", "system", "we", "study", "low", "rating", "example", "-lrb-", "e.g.", "2nd", "6th", "example", "-rrb-", "above", "user", "study", "see", "why", "less", "successful", "2nd", "example", "-lrb-", "top", "row", "Fig.", "13", "-rrb-", "although", "we", "recover", "editing", "step", "same", "artist", "participant", "rate", "lower", "due", "its", "visually", "noticeable", "reconstruction", "error", "because", "saturated", "color", "value", "sun", "region", "lead", "error", "reconstructed", "color", "transform", "6th", "example", "-lrb-", "bottom", "row", "Fig.", "13", "-rrb-", "p-value", "measure", "average", "score", "all", "example", "i.e.", "compute", "average", "score", "each", "example", "measure", "p-value", "21", "score", "original", "editing", "patch", "produce", "artist", "have", "more", "natural", "spatial", "layout", "-lrb-", "i.e.", "change", "color", "leaf", "from", "bottom", "top", "give", "bottom", "one", "visually", "more", "dominant", "-rrb-", "while", "we", "method", "first", "edit", "leaf", "middle", "since", "region", "large", "-lrb-", "see", "Eqn", "example", "suggest", "one", "could", "potentially", "use", "better", "algorithm", "rank", "visual", "dominance", "different", "object", "improve", "semantic", "meaningfulness", "recovered", "history", "we", "system", "can", "fail", "more", "dramatically", "when", "individual", "technical", "component", "incapable", "handle", "more", "difficult", "case", "firstly", "region", "match", "algorithm", "Sec", "may", "fail", "follow", "case", "-lrb-", "-rrb-", "when", "size", "match", "region", "too", "small", "classify", "reliable", "-lrb-", "-rrb-", "when", "apply", "geometry/color", "transformation", "too", "dramatic", "-lrb-", "-rrb-", "when", "some", "region", "purely", "textureless", "two", "example", "show", "Fig.", "14", "first", "row", "Fig.", "14", "matching", "head", "giraffe", "fail", "due", "large", "deformation", "second", "row", "Fig.", "14", "we", "region", "match", "miss", "left", "right", "mushroom", "stem", "since", "too", "thin", "secondly", "appearance", "operation", "can", "well", "approximate", "use", "one", "Sec", "lead", "large", "reconstruction", "error", "when", "re-apply", "history", "example", "show", "third", "row", "Fig.", "14", "where", "we", "method", "fail", "recover", "complex", "appearance", "change", "discuss", "earlier", "we", "system", "currently", "only", "support", "limited", "range", "operation", "we", "believe", "dedicated", "solution", "other", "editing", "operation", "could", "potentially", "support", "instance", "support", "gaussian", "blur", "we", "can", "optionally", "add", "new", "transform", "dimension", "region", "matching", "step", "allow", "blur", "region", "match", "against", "multiple", "version", "original", "image", "different", "amount", "blur", "we", "can", "match", "blur", "region", "its", "original", "sharp", "region", "same", "time", "produce", "estimation", "size", "blur", "appearance", "operator", "recovery", "step", "we", "can", "first", "blur", "source", "region", "use", "recover", "blur", "downsize", "both", "region", "lower", "resolution", "remove", "effect", "blur", "estimate", "other", "color", "operator", "we", "implement", "above", "procedure", "system", "Fig.", "15", "we", "provide", "example", "involve", "two", "operation", "foreground", "move", "background", "gaussian", "blur", "we", "method", "successfully", "recover", "both", "operation", "other", "more", "complicated", "operation", "support", "we", "current", "system", "bilateral", "filter", "alpha", "matting", "Poisson", "blend", "etc.", "general", "relatively", "easy", "support", "global", "operation", "however", "operation", "content", "aware", "i.e.", "color", "pixel", "change", "adaptively", "accord", "its", "local", "neighborhood", "appearance", "bilateral", "filter", "much", "harder", "recover", "nevertheless", "we", "have", "demonstrate", "we", "current", "system", "already", "widely", "useful", "support", "wide", "range", "commonly", "use", "editing", "operation", "we", "present", "novel", "system", "recover", "semantically", "meaningful", "editing", "history", "from", "source", "image", "edit", "version", "achieve", "we", "use", "dense", "correspondence", "method", "which", "extend", "NRDC", "approach", "find", "all", "edit", "region", "recover", "appearance", "operation", "apply", "each", "region", "from", "all", "possible", "edit", "path", "we", "recover", "optimal", "one", "base", "semantic", "constraint", "experimental", "user", "study", "result", "show", "we", "system", "can", "recover", "clean", "meaningful", "editing", "history", "involve", "large", "geometric", "appearance", "transformation", "we", "further", "show", "recover", "history", "can", "useful", "wide", "range", "application", "source", "image", "edit", "image", "source", "image", "edit", "image", "original", "history", "artist", "example", "we", "could", "potentially", "improve", "robustness", "region", "matching", "step", "use", "recently", "propose", "higher-order", "deformation", "model", "-lsb-", "y?cer", "et", "al.", "2012", "-rsb-", "after", "obtain", "initial", "matching", "we", "propose", "method", "another", "way", "improve", "system", "robustness", "combine", "we", "method", "technique", "propose", "previous", "photo", "manipulation", "detection", "system", "-lsb-", "o?brien", "Farid", "2012", "Kee", "et", "al.", "2013", "-rsb-", "another", "potential", "extension", "apply", "we", "method", "large", "image", "library", "-lsb-", "Hu", "et", "al.", "2013", "-rsb-", "analyze", "image", "correlation", "and/or", "dependency", "within", "large", "dataset", "Acknowlegements", "we", "thank", "anonymous", "reviewer", "valuable", "comment", "work", "support", "National", "Basic", "Research", "Project", "China", "-lrb-", "2011CB302205", "-rrb-", "natural", "Science", "Foundation", "China", "-lrb-", "61120106007", "61170153", "-rrb-", "National", "High", "Technology", "Research", "Development", "Program", "China", "-lrb-", "2012aa011802", "-rrb-", "pcsirt", "Tsinghua", "University", "Initiative", "Scientific", "Research", "Program", "source", "image", "edit", "image", "recover", "history", "ee", "E.", "BRIEN", "J.", "arid", "H.", "2013", "expose", "photo", "manipulation", "inconsistent", "shadow", "ACM", "transaction", "Graphics", "32", "28:1", "28:12", "ONG", "N.", "ROSSMAN", "T.", "ARTMANN", "B.", "GRAWALA", "M.", "itzmaurice", "G.", "W.", "2012", "delta", "tool", "represent", "compare", "workflow", "CHI", "1027", "1036", "URLANDER", "D.", "einer", "S.", "1988", "editable", "graphical", "history", "IEEE", "Workshop", "Visual", "Languages", "127", "134", "aus", "y.-k.", "s.-m.", "artin", "R.", "R.", "2009", "Automatic", "topology-preserving", "gradient", "mesh", "generation", "image", "vectorization", "ACM", "Trans", "28", "85:1", "85:8", "evin", "a.", "ischinskus", "D.", "EISS", "Y.", "2008", "closedform", "solution", "natural", "image", "matting", "IEEE", "Trans", "pattern", "Anal", "30", "228", "242", "iao", "Z.", "OPPE", "H.", "orsyth", "D.", "Y.", "2012", "subdivision-based", "representation", "vector", "image", "editing", "IEEE", "transaction", "visualization", "computer", "graphic", "18", "11", "1858", "1867", "iu", "C.", "uen", "J.", "orralba", "a.", "ivic", "J.", "reeman", "W.", "T.", "2008", "sift", "flow", "dense", "correspondence", "across", "different", "scene", "eccv", "28", "42", "owe", "D.", "G.", "2004", "distinctive", "image", "feature", "from", "scale-invariant", "keypoint", "J.", "Comput", "Vision", "60", "91", "110", "l.-q.", "K.", "ONG", "T.-T.", "IANG", "B.-Y.", "S.M.", "2013", "Change", "blindness", "image", "IEEE", "transaction", "visualization", "computer", "graphic", "appear", "O?B", "RIEN", "J.", "F.", "arid", "H.", "2012", "expose", "photo", "manipulation", "inconsistent", "reflection", "ACM", "transaction", "Graphics", "31", "4:1", "4:11", "other", "C.", "OLMOGOROV", "V.", "lake", "a.", "2004", "grabcut", "interactive", "foreground", "extraction", "use", "iterated", "graph", "cut", "ACM", "Trans", "23", "309", "314", "S.", "L.", "ari", "S.", "LIAGA", "F.", "cull", "C.", "OHNSON", "S.", "URAND", "F.", "2009", "interactive", "visual", "history", "vector", "graphic", "Tech", "Report", "mit-csail-tr-2009-031", "iao", "C.", "IU", "M.", "ongweus", "N.", "ONG", "Z.", "2011", "fast", "exact", "nearest", "patch", "match", "patch-based", "image", "editing", "processing", "IEEE", "transaction", "visualization", "computer", "graphic", "17", "1122", "1134", "ang", "G.", "TEWART", "C.", "OFKA", "M.", "saus", "c.-l", "2007", "registration", "challenging", "image", "pair", "initialization", "estimation", "decision", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "29", "11", "1973", "1989", "ucer", "K.", "ACOBSON", "a.", "ornung", "a.", "orkine", "O.", "2012", "transfusive", "image", "manipulation", "ACM", "Trans", "31", "176:1", "176:9", "hang", "F.-L.", "HENG", "M.-M.", "IA", "J.", "s.-m", "2012", "Imageadmixture", "put", "together", "dissimilar", "object", "from", "group", "IEEE", "transaction", "visualization", "computer", "graphic", "18", "11", "1849", "1857", "immer", "H.", "RUHN", "A.", "EICKERT", "J.", "2011", "optic", "flow", "harmony", "J.", "Comput", "Vision", "93", "368", "388" ],
  "content" : "It can easily take dozens of operations to explore different ideas, fix errors and fine tweak parameters before producing a desired output. Our system provides a set of solutions to these problems. In particular, we improve the state-ofthe-art region matching methods to handle large appearance differences between an original object and its edited version (see Sec. We also demonstrate how the recovered history can be applied to various applications (see Sec. It is impossible in practice to recover all editing operations that may have been applied to an image. To evaluate the proposed system, we construct a test dataset that contains before-and-after image pairs, along with the original editing steps performed by artists. A user study on this dataset revealed that the editing histories generated by our system are in general comparable to the original ones in terms of semantic meaningfulness (see Sec. Sparse matching methods such as SIFT matching [Lowe 2004] are not applicable, as dense correspondences are required in our application. We adopt the framework of this algorithm and extend it for our application, as described in Sec.4. The Delta system [Kong et al. 2012] can help users identify the tradeoffs between workflows using visual comparisons. Edit transfer. Image Analogies [Hertzmann et al. 2001] provides a classic example of edit transfer without recovering the editing process. The RepFinder system [Cheng et al. 2010] finds repeated scene elements in an image so that edits made to one element can be transferred to others. The pipeline of our algorithm is shown in Fig. 2 . It consists of three main steps: (1) region matching; (2) recovering semantic appearance operators for each matched region pair; and (3) generating the editing history. Specifically, we first find all matched region pairs between the source and edited images (Sec. This is achieved by using a matching method extended from the non-rigid-densecorrespondence (NRDC) algorithm [HaCohen et al. 2011] to accommodate a wider range of appearance difference between a pair of regions. Secondly, we recover semantic appearance operations for each matched region pair (Sec. 5), which may include both global linear color transforms such as brightness, exposure, hue and saturation adjustments, as well as non-linear tone mapping and local brushes with spatially-varying strength. Finally, given the matched region pairs and their recovered editing operations, we use an optimization approach to generate a compact, semanticallymeaningful editing history, according to a set of predefined editing rules (Sec. In the first step of the algorithm, we seek to reliably recover region pairs between the source and edited images that share the same content. Comparing two matched regions gives us the means to discover whether the source region has been edited, and if so how. Our region matching approach is extended from the NRDC [HaCohen et al. 2011] algorithm to have better capabilities of handling large appearance transforms. Specifically, we adopt the coarse-to-fine framework in the original  NRDC approach, which initially downsamples the image to a low resolution, finds matches, and uses them as constraints for finding matches at the next finer resolution. At each level, the following four steps are applied sequentially: (1)nearest neighbor search; (2) patch merging; (3) color transform estimation; and (4) coarse-tofine propagation. After the above steps finish at the finest level, a boundary refinement step is applied to produce an accurate boundary for each matched region. A 4D geometric transformation G (i.e. representing 2D translation, rotation, scaling, and flipping) is used to represent the geometric relationship between u and u . The algorithm improves the transformations by iterating between a propagation and a random search step. The propagation step proceeds in scan-line order, replacing the transforms of each patch by that of its neighbor if appropriate. In the random search step, each patch randomly finds several transforms for further evaluation. The transforms are randomly chosen from windows of exponentially decreasing sizes. We utilize generalized PatchMatch, but improve upon it in several ways. Firstly, like NRDC, we use floating-point coordinates to support sub-pixel precision. Secondly, to account for crosschannel color changes between patches, we define the matching cost D m (u ? u ) from patch u to patch u as: where I(?) represents the color values of pixels in a patch. It accounts for cross-channel color transforms such as a hue change. This measure, however, do not work well with degenerated cases, e.g., a patch u with a uniform color. Hence, we define the final distance between u and u to be: Thirdly, as done in the PatchMatch Stereo method [Bleyer et al. 2011], we add an additional local refinement step in each iteration. In this step, for each patch, we locally optimize the stored geometric transforms using the gradient descent method to further reduce  the distance measure in Eqn. After the best transform has been identified for each patch u , adjacent patches which are consistent are merged into larger regions. To achieve this we define the consistency error between two patches in the edited image as: ?(u , v ) = G G u v (u (v c c ) ) ? ? G G u u (v (v c c ) ) 2 2 + ? (C u ? C v ) ? I(u ) 2 , where u , v represent two patches in A centered at u c , v c , respectively, and (G u , C u ) and (G v , C v ) are estimated geometric and color transforms of them. Note that the corresponding definition in the NRDC approach [HaCohen et al. 2011] does not include a color difference term, which is essential to distinguish patches with only color transformation. Next, we adopt a greedy scheme to merge neighboring patches based on the proposed consistency error. Specifically, we randomly select a patch, and greedily grow the region by merging it with its neighboring patches whose consistency error with respect to the selected patch is low (the threshold is 10), until no more neighbors can be included. We then select another patch to start another growing process. Following NRDC, we prune regions that are too small (less than 1% image size). This process results in a set of merged regions in A , each corresponds to a matched region in A.  For each matched region pair R and R , we assume a per-region cross-channel cubic color transform C R between them, along with a per-pixel smoothly-varying strength map w. This allows us to handle a rich set of possible color editing operations, including hue, contrast, saturation, tone, and brightness adjustments. It is also important to note that introducing a spatially-varying strength map w allows the system to support various paint brushes for local editing. Mathematically the color transform is formulated as: where p denotes a pixel in region R, and p is the corresponding pixel in R , I m (?) denotes the color value of the m-th channel of (a) (b) (c) (d) a pixel; a i,j,k,m are the coefficients of the per-region cross-channel cubic color transform C R , and w(p) is the per-pixel editing strength whose value is between 0 and 1. An iterative optimization approach is applied to recover both the per-region cross-channel cubic color transform C R , and the perpixel edit strength map w. We start with a constant strength map w(p) = 1. At each iteration, we first fix w and obtain C R using a least square solver, by taking into account all pixels in two matched regions. We then fix C R to recover w. We assume that it varies very smoothly, and the values are roughly the same in a small neighborhood such as a 7 ? 7 window. Hence, for each pixel p, w(p) is also obtained using a least square solver, by taking into account the neighboring pixels in a local 7 ? 7 window. In our implementation, we find usually 3-5 iterations is usually enough to get good estimates of both C R and w that are accurate enough for our purposes. Similar to the NRDC approach, in our coarse-to-fine strategy, we use the results obtained at the current level to constrain the solution at the next level with a higher resolution. Specifically, for faster convergence, the stored transform of a patch leading to a matched region have its search space at the next level limited to be within a small range (e.g. shift within 5 pixels; scale, rotation within 0.2) of the final transform at the current level. Besides, the estimated cross-channel cubic color transform C R of a region pair is applied to reduce the appearance difference between the two regions, before evaluating the distance (Eqn. 2) between patches. To further improve the accuracy of region boundaries, we first over-segment both the source and the edited images using the Mean Shift algorithm [Comaniciu and Meer 2002], and assume that each segment should belong to a single region based on color consistency. Hence, we check each segment for conflict. If 80% of a segment is inside a matched region or the unmatched region U (i.e. pixels not belonging to any matched region), we assign the whole segment to the same region. Next, in the edited image, we create a refinement band B around each matched region R by expanding and shrinking its boundary by 5 pixels, and then calculate an alpha matte in the refinement band using an existing alpha matting technique [Levin et al. 2008]. Once the alpha matte is computed, we also refine the transforms for pixels in B . We use a similar algorithm to that in Sec. 4.1 to propagate the transforms from inside the region R to the refinement band B , the only difference being that we modify Eqn. 1 to use the alpha values of pixels (? p ) as weights in calculating patch distance: where p is a pixel in u, and p is the corresponding pixel in u . In this way we avoid the influence of background colors when calculating matching distances for patches in the foreground objects. Examples of boundary refinement are shown in Fig. 3 . Finally, we remove those region pairs whose color and geometric transforms are both identity transforms (i.e. correspond to no edit). The remaining matched region pairs will be used in subsequent steps for generating the editing history. Fig. 4 shows several examples of the matched region pairs. Recall that we use a per-region cross-channel cubic transform (Eqn. 3) to describe appearance changes between a matched region pair. In this section we describe how to further recover semantic editing operations from it. Brightness and hue operators shift the values x of their corresponding channels by a constant f , to x + f . Saturation and exposure operators scale the value of each channel x to x ? (1 + f ). Inspired by existing tone curve adjustment interfaces, we use a cubic spline to represent a non-linear tone curve, parameterized by five control points: (0, 0), (0.25, f 1 ), (0.5, f 2 ), (0.75, f 3 ), (1, 1), where f 1 , f 2 , f 3 are three values in [0, 1]. In total, we have 7 parameters for appearance editing (i.e. 1 each for brightness, exposure, hue and saturation, 3 for the tone curve). For instance, commonly used contrast, shadow and highlight adjustments can all be achieved by non-linear tone curve manipulation. It is important to note that, besides the 7 per-region editing parameters, each pixel inside the region still maintains a per-pixel editing strength w, such that: x = w ? f (x) + (1 ? w) ? x. This allows the system to support local adjustment brushes with spatially-varying strength. Similar to Sec. 4.3, we optimize the per-region parameters and the per-pixel editing strength alternatively, as detailed below. Parameter initialization. For initializing w, we use the editing strength map obtained in Sec. We then obtain the initial estimations of per-region parameters by applying the following steps: 1. Convert images to HSV color space, and compute initial hue and saturation parameters by subtracting the mean hue values and dividing the mean saturation values of the two regions, respectively. source image\n        ground truth edited images tone curve Figure 5 : The accuracy of the appearance operator recovery. Top: ground truth edited images generated by changing: brightness and exposure, gamma (tone) curve, hue and brightness, hue and saturation and tone curve, spatially varying adjustment of brightness and exposure, respectively. Bottom: results rendered using recovered operators are visually indistinguishable from the ground truth. Errors in estimated parameters are within 1%. Bottom-left: ground truth and recovered tone curves for the 4-th ?hue and saturation and tone curve? example. Notice that in the rightmost example, spatially varying adjustments of brightness and exposure have been applied, and the ground truth and recovered spatially-varying strength maps are shown in the top left corner of corresponding images, respectively. f e ? x + f b , where x and x are intensities of corresponding pixels in the two regions. After compensating for brightness and exposure differences, use least square fitting to obtain initial tone curve parameters. Parameter refinement. After initialization, we iteratively optimize the per-pixel strength map w and the per-region parameters. we first fix w and use a gradient descent method to search for optimal parameter values, by minimizing the L 2 color difference between the two regions after applying the color adjustments. Since the parameters are also influenced by the order of operations, we assume the following fixed order for color adjustments: (1) hue, (2) saturation, (3) brightness, (4) exposure, (5) tone curve. Next, we fix the per-region parameters and adjust w, using the same local constancy assumption and least square solvers as described in Sec. We usually apply 5 iterations in this process. Operation removal. recovered edited images\n        values that correspond to no change. If some of them are, we then assume that the corresponding edits have not been applied. We then fix these parameters to their default values and re-apply the optimization process to update others. Fig. 5 shows some examples of recovered appearance operators. It suggests that the recovered operators are accurate and can generate high fidelity rendered results when compared with the ground truth edited images. Fig. 6 gives another example on spatially varying adjustment. From left to right, we give the source image, edited image, and recovered per-pixel editing strength map. The result suggests our method is robust to recover such edits. A more thorough evaluation is presented in Sec. So far we have generated a list of matched region pairs, and appearance and geometric transforms between each pair. We now explain how to further process the matching result to generate a compact source image edited image recovered strength map editing history. 6.1 Layered editing\n        Before introducing the proposed editing path recovery method, it is important to note that our system intrinsically does layered editing, where each pair of matched region is on a different layer. Note that the same object in the source image A (denoted as O(A)) could match to multiple objects in the edited image A (denoted as O i (A )), often caused by cloning. In this case there will be multiple copies of O(A) and each one pairs with an O i (A ) and lives in a different layer. To find a reasonable path, we first introduce the concept of state. Formally, let the list of matched region pairs be {R k ? c ? k ? ,g ? k R k } (1 ? k ? n, n is the number of matches), where c k , g k denote the estimated per-region appearance and geometric transforms between each pair. , ? n = (0, 0)}, and the final state is: ? end = {? 1 = (c 1 , g 1 ), ? 2 = (c 2 , g 2 ), ... , ? n = (c n , g n )}. In this framework, a single editing step involves selecting one, or multiple spatially connected regions {R j } simultaneously, and modifying their states in the same way: ? j = ? j + (c e , g e ) for all j, where (c e , g e ) indicates the particular operation applied. (c e , g e ) are not allowed to be both zero (i.e., no edit). In other words, at least one region reaches its final color or geometric transform in a single editing step. Given these constraints, at any step, the number of possible editing paths to be taken is limited. We also need to define semantically what is the optimal path from ? start to ? end . By consulting with experienced artists, we have determined four principles that the majority of people agree: 1. Layers are edited in the coarse-to-fine order. Large, visually dominant edits are applied before fine-tuning the appearance of small objects; 2. If the same edits are applied on multiple objects, then they are applied either together at once, or sequentially without interruption (temporal focus); 3. Downsampling is deferred as much as possible. Based on the above principles, we define the total editing cost as: where m is the number of editing steps, E i denotes the cost of the ith editing step (to be explained next), and t i , t i denote the centroids of the selected region(s) of the i-th editing step before and after applying the edits, respectively. The first term is the summed cost of all editing steps, and the second term measures the switching costs between adjacent editing steps according to Principle 3 (i.e. |t i+1 ? t i | approximates the spatial movement from the i-th step to the (i + 1)-th step). ? p is a controlling weight set at 0.5 in our experiments. The cost of the i-th editing step E i is defined as: where S i = {R j } denotes the selected region(s) of the i-th editing step, S i is its size and S i is used to approximate its visual dominance. d c (i) and d g (i) measure the amount of color and geometric transforms applied in this step, their exact formulation will be given later. ? s is a constant set to 0.01 in our system. The term (1 + ? s i) has two purposes: (i) it penalizes longer editing paths: a shorter path is preferred for the same result, and (ii) it favors edits on dominant objects (i.e. S i is large) being applied earlier (i.e. when i is smaller), thus satisfying Principle 1. The final term P i in Eqn. 6 penalizes for early down-sampling (satisfying Principle 4). It is defined as the average scaling factor that has already been applied:  where j iterates over all selected regions in this editing step, t scale (R j ) is the scale factor that has already been applied to that region R j . Note that we completely avoid enlarging after downsampling the same region, by setting P i = +? for such edits. Finally, the amount of color and geometric transforms d c (i) and d g (i) in Eqn. (T x , T y ), T rot and T scale denote the average translation, rotation angle and scaling of the region, respectively. In our system, the weights are empirically set to ? c = 1, ? xy = 1, ? rot = ?, ? scale = ln 3. With above definitions, we seek the editing path from ? start to ? end with minimal total editing cost (as defined in Eqn. 5) among all possible paths. This can be efficiently solved using dynamic programming. source image edited image Figure 7 : Recovered editing histories from before-and-after image pairs. There may exist regions in both A and A that cannot find good matches on the other image. This may be caused by operations such as cropping, object insertion or removal. We first examine if the edited image A has been derived from the original A by cropping, by finding two bounding boxes B(A) an B(A ) containing all the matched regions in A and A , respectively. Cropping is identified if B(A ) covers the entire image while B(A) covers only a portion of it, and B(A) is treated as the cropping window. We then look for object insertion and removal. An unmatched region in A may be the result of inserting an object into A, or removing one from A and filling the hole using image completion techniques. If the region is caused by image completion, then there should be no obvious seams between the unmatched region and its surrounding ones. Otherwise if the region is caused by inserting a new object, then we expect the region to be surrounded by a strong object boundary. We thus check how smooth the transition is between the region and its surroundings. Specifically, we check how well the region boundary agrees with the over-segmentation boundaries obtained by the mean shift algorithm (see Sec. If they agree with each other well (e.g. there is more than 50% overlap), this region is considered to be a newly inserted object. Otherwise we further check the same region in A, and if the source region is also an unmatched region, then we decide that the object in the source region has been removed and the hole has been filled by image completion techniques. If such operations are identified, we incorporate them into the editing path computed in Sec. Specifically, we add cropping and object removal at the beginning of the history, and add object insertion at the end of it to create a completed editing path. We have implemented our method on a PC with an Intel Xeon 2.4GHz CPU and 8GB memory. For an image of size 640?480, region matching takes 2?3 minutes, recovering appearance operators\n        recovered editing history source image edited image generated tutorial takes about 10?15 seconds, depending on the number of regions being edited, and finding the optimal editing path takes about 2?5 seconds. Fig. 7 shows a number of input before-and-after image pairs and the recovered editing histories using our approach. It suggests that our system works reliably even for large geometric and appearance transforms, and the appearance transform is allowed to be spatially varying (e.g. the first example). Furthermore, the recovered editing steps are semantically-meaningful, and can easily be used to drive tools in image editing software. Note how the system generates a compact and reasonable editing history for the 2nd and 3rd examples, from the low-level region matching results shown in the 3rd and 4th rows of Fig. 4 . Next, we illustrate how the recovered history can be used in various applications. Automatic tutorial generation. A straightforward application is to use the recovered editing history to automatically generate an image editing tutorial. Fig. 8 shows a simple example of turning the history into a step-by-step tutorial. Our method can be combined with source image A edited image A recovered editing history more powerful tutorial generating systems [Grabler et al. 2009] to produce more visually appealing results. Re-editing. Users can modify the recovered history, e.g. to remove steps, or change parameters of some steps, and re-apply the modified history to the source image A to generate a new edited image A . Some examples are shown in Fig. 1 and Fig. 9 . This is easier than directly editing the original edited image A to achieve A , as the user may have to carry out object selection and use other editing tools again in the latter case; the user may also no longer remember the parameters for the parts of the edit which are to remain unchanged. Edit transfer. The recovered editing history from one pair of images can be transferred to a new image to achieve edit transfer. An example is shown in Fig. 10 , where the recovered editing history from images A and A is applied to image B. To do this, we assume that A and B have similar composition, so that objects in B are roughly at the same locations as those in A. To automatically extract the object mask in B, we first identify a rough bounding box in B which is 1.5 times larger than the actual object bounding box in A, then apply GrabCut [Rother et al. 2004] for segmentation. Other dedicated approaches have already been proposed for edit transfer, such as image analogies [Hertzmann et al. 2001] and content-adaptive macros [Berthouzoz et al. 2011]. Compared to image analogies, our method has more constraints on the input, requiring similar composition of images A and B. However, on the other hand, it is capable of object-level editing, and also handles geometric editing such as cropping, which are clear advantages over image analogies and other appearance-based transfer approaches. Content-adaptive macros requires the editing history to be known, so our system can be potentially used to generate such editing macros. Merging editing paths. Our approach can be combined with the image revision control system [Chen et al. 2011] to merge different editing paths. An example is shown in Fig. 11 , where our system recovers multiple different editing paths and merges them to create a single final rendering result. To objectively evaluate the proposed system, we create an evaluation dataset that contains 21 image editing examples produced by several artists. To produce these examples, we explicitly explained the range of supported operations to the artists and asked them to perform editing using supported operations. The input images and the performed editing paths were chosen or created by the artists  without any supervision. We then selected 21 successful or moderately successful examples out of 25 that we received from the artists for user study. The original editing steps are recorded for all examples. We generate editing histories for all these examples using our system, and compare them to the original ones. We first evaluate the representation ability of the recover histories, by measuring the PSNR values of the reconstructed edited images. The average PSNR value of this dataset is 26.1dB, and the standard deviation is 6.2dB. Visually, there is no noticeable difference between the reconstructed and the ground truth edited images for most examples. These results suggest that our recovered histories can faithfully reproduce the edited images in high fidelity. Secondly, we conducted a qualitative evaluation on the semantic meaningfulness of the recovered histories. We invited 30 participants who are image editing enthusiasts and are familiar with Adobe Photoshop to participate in the study. In each user session, for each editing example, we first showed the before-and-after image pair to the subject, followed by the two editing histories: the original one and the recovered one. The display orders of the two histories were randomly determined. Each history was illustrated as an automatic slideshow, and the subject was allowed to switch back and forth to see each step more clearly, and there is no time limit in the study. After viewing a pair of editing histories, the subject was then asked to judge which one is semantically more natural, or they are about equal. The results of the user study are given in Fig. 12 . In total, our method achieved an averaged score of 0.45 (i.e. ?ours is better?, ?about equal?, ?artist?s is better? are scored at 1, 0.5, and 0, respectively), and the one tail p-value is 0.039 5 . This study demonstrates that our recovered histories are comparable to the original ones produced by artists in terms of semantic meaningfulness. In 17 out of 21 examples (81.0%), a majority of users rated that our recovered histories are equal or better than the original ones. All examples used in the user study are included in the supplemental material. To better understand the limitation of the proposed system, we studied the low rating examples (e.g. the 2nd and 6th examples) in the above user study to see why they are less successful. For the 2nd example (top row in Fig. 13 ), although our recovered editing steps are the same as that of the artist, participants rated it lower due to its visually noticeable reconstruction error. This is because the saturated color values in the sun region lead to errors in the reconstructed color transforms. For the 6th example (bottom row in Fig. 13 ), the\n          5 This p-value measures the average scores of all examples, i.e. compute averaged scores for each example, and measure the p-value of 21 scores. original editing patch produced by the artist has a more natural spatial layout (i.e. change the color of the leaves from bottom to top, given the bottom one is visually more dominant), while our method first edits the leaves in the middle since their regions are large (see Eqn. This example suggests that one could potentially use a better algorithm to rank the visual dominance of different objects to improve the semantic meaningfulness of the recovered history. Our system can fail more dramatically when the individual technical components are incapable of handling more difficult cases. Firstly, the region matching algorithm in Sec. 4 may fail in the following cases: (1) when the size of the matched regions is too small to be classified as reliable; (2) when the applied geometry/color transformations are too dramatic; and (3) when some regions are purely textureless. Two such examples are shown in Fig. 14. In the first row of Fig. 14, the matching of the head of giraffe failed due to the large deformation. In the second row of Fig. 14, our region matching missed the left and right mushroom stems since they are too thin. Secondly, appearance operations that cannot be well approximated using the ones in Sec. 5 will lead to large reconstruction error when re-applying the history. Such an example is shown in the third row of Fig. 14, where our method failed to recover the complex appearance changes. As discussed earlier, our system currently only supports a limited range of operations. We believe that with dedicated solutions, other editing operations could potentially be supported. For instance, to support Gaussian blur, we can optionally add it as a new transform dimension in the region matching step. By allowing a blurred region to match against multiple versions of the original image with different amount of blur, we can match the blurred region to its original sharp region, and at the same time produce an estimation of the size of the blur. In the appearance operator recovery step, we can first blur the source region using the recovered blur, then downsize both regions to a lower resolution to remove the effect of blurring for estimating other color operators. We implemented the above procedure in the system and in Fig. 15, we provide an example involving two operations: foreground move, and background Gaussian blur. Our method successfully recovered both operations. Other more complicated operations are not supported in our current system, such as bilateral filtering, alpha matting, Poisson blending, etc. In general, it is relatively easy to support global operations. However, if the operation is ?content aware?, i.e., the color of a pixel is changed adaptively according to its local neighborhood appearance, such as bilateral filtering, then it is much harder to recover. Nevertheless, we have demonstrated that our current system is already widely useful as it supports a wide range of commonly used editing operations. We present a novel system for recovering a semantically meaningful editing history from a source image and an edited version of it. To achieve this, we use a dense correspondence method which extends the NRDC approach to find all edited regions, and recovers appearance operations applied to each region. From all possible edit paths, we recover an optimal one based on semantic constraints. Experimental and user study results show that our system can recover clean and meaningful editing histories involving large geometric and appearance transformations. We further show that the recovered histories can be useful in a wide range of applications. source image edited image source image edited image original history by artist For example, we could potentially improve the robustness of the region matching step by using recently proposed higher-order deformation models [Y?cer et al. 2012], after obtaining the initial matching by our proposed method. Another way to improve system robustness is to combine our method with techniques proposed in previous photo manipulation detection systems [O?Brien and Farid 2012; Kee et al. 2013]. Another potential extension is to apply our method to large image libraries [Hu et al. 2013], for analyzing image correlations and/or dependencies within a large dataset. Acknowlegements. We thank the anonymous reviewers for their valuable comments. This work was supported by National Basic Research Project of China (2011CB302205), Natural Science Foundation of China (61120106007 and 61170153), National High Technology Research and Development Program of China (2012AA011802), PCSIRT and Tsinghua University Initiative Scientific Research Program. source image edited image recovered history K EE , E., O? BRIEN , J., AND F ARID , H. 2013. Exposing photo manipulation with inconsistent shadows. ACM Transactions on Graphics 32, 3, 28:1?28:12. K ONG , N., G ROSSMAN , T., H ARTMANN , B., A GRAWALA , M., AND F ITZMAURICE , G. W. 2012. Delta: a tool for representing and comparing workflows. of CHI, 1027?1036. K URLANDER , D., AND F EINER , S. 1988. Editable graphical histories. In IEEE Workshop on Visual Languages, 127?134. L AI , Y.-K., H U , S.-M., AND M ARTIN , R. R. 2009. Automatic and topology-preserving gradient mesh generation for image vectorization. ACM Trans. 28, 3, 85:1?85:8. L EVIN , A., L ISCHINSKI , D., AND W EISS , Y. 2008. A closedform solution to natural image matting. IEEE Trans. Pattern Anal. 30, 2, 228?242. L IAO , Z., H OPPE , H., F ORSYTH , D., AND Y U , Y. 2012. A subdivision-based representation for vector image editing. IEEE\n        Transactions on Visualization and Computer Graphics 18, 11, 1858?1867. L IU , C., Y UEN , J., T ORRALBA , A., S IVIC , J., AND F REEMAN , W. T. 2008. Sift flow: Dense correspondence across different scenes. of ECCV, 28?42. L OWE , D. G. 2004. Distinctive image features from scale-invariant keypoints. J. Comput. Vision 60, 2, 91?110. M A , L.-Q., X U , K., W ONG , T.-T., J IANG , B.-Y., AND H U , S.M. 2013. Change blindness images. IEEE Transactions on Visualization and Computer Graphics, to appear. O?B RIEN , J. F., AND F ARID , H. 2012. Exposing photo manipulation with inconsistent reflections. ACM Transactions on Graphics 31, 1, 4:1?4:11. R OTHER , C., K OLMOGOROV , V., AND B LAKE , A. 2004. ?grabcut?: interactive foreground extraction using iterated graph cuts. ACM Trans. 23, 3, 309?314. S U , S. L., P ARIS , S., A LIAGA , F., S CULL , C., J OHNSON , S., AND D URAND , F. 2009. Interactive visual histories for vector graphics. Tech Report, MIT-CSAIL-TR-2009-031. X IAO , C., L IU , M., Y ONGWEI , N., AND D ONG , Z. 2011. Fast exact nearest patch matching for patch-based image editing and processing. IEEE Transactions on Visualization and Computer Graphics 17, 8, 1122?1134. Y ANG , G., S TEWART , C., S OFKA , M., AND T SAI , C.-L. 2007. Registration of challenging image pairs: Initialization, estimation, and decision. IEEE Transactions on Pattern Analysis and Machine Intelligence 29, 11, 1973?1989. Y UCER  ? , K., J ACOBSON , A., H ORNUNG , A., AND S ORKINE , O. 2012. Transfusive image manipulation. ACM Trans. 31, 6, 176:1?176:9. Z HANG , F.-L., C HENG , M.-M., J IA , J., AND H U , S.-M. 2012. Imageadmixture: Putting together dissimilar objects from groups. IEEE Transactions on Visualization and Computer Graphics 18, 11, 1849?1857. Z IMMER , H., B RUHN , A., AND W EICKERT , J. 2011. Optic flow in harmony. J. Comput. Vision 93, 3, 368?388.",
  "resources" : [ ]
}