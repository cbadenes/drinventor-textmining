{
  "uri" : "sig2012a-a157-rivers_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012a/a157-rivers_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Sculpting by Numbers",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Alec R.-Rivers",
      "name" : "Alec R.",
      "surname" : "Rivers"
    }, {
      "uri" : "http://drinventor/Andrew-Adams",
      "name" : "Andrew",
      "surname" : "Adams"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "759b051b6bd12fd99eee6f1975558d92d1b1bc343f0a63c96257cfcf0cdb1d62", "p2n", "10.1145", "2366145.2366176", "name", "identification", "possible", "sculpt", "Numbers", "Alec", "Rivers", "Andrew", "Adams", "MIT", "CSAIL", "MIT", "CSAIL", "-lrb-", "-rrb-", "target", "3d", "model", "-lrb-", "-rrb-", "guidance", "project", "onto", "material", "figure", "we", "assist", "user", "create", "physical", "object", "match", "digital", "3d", "model", "give", "target", "3d", "model", "-lrb-", "-rrb-", "we", "project", "different", "form", "guidance", "onto", "work", "progress", "-lrb-", "-rrb-", "indicate", "how", "must", "deform", "match", "target", "model", "user", "follow", "guidance", "physical", "object?s", "shape", "approach", "target", "-lrb-", "-rrb-", "we", "system", "unskilled", "user", "able", "produce", "accurate", "physical", "replica", "complex", "3d", "model", "here", "we", "recreate", "Stanford", "bunny", "model", "-lrb-", "courtesy", "Stanford", "Computer", "Graphics", "Laboratory", "-rrb-", "out", "polymer", "clay", "we", "propose", "method", "allow", "unskilled", "user", "create", "accurate", "physical", "replica", "digital", "3d", "model", "we", "use", "projector/camera", "pair", "scan", "work", "progress", "project", "multiple", "form", "guidance", "onto", "object", "itself", "indicate", "which", "area", "need", "more", "material", "which", "need", "less", "where", "any", "ridge", "valley", "depth", "discontinuity", "user", "adjust", "model", "use", "guidance", "iterate", "make", "shape", "physical", "object", "approach", "target", "3d", "model", "over", "time", "we", "show", "how", "approach", "can", "use", "create", "duplicate", "exist", "object", "scan", "object", "use", "scan", "target", "shape", "user", "free", "make", "reproduction", "different", "scale", "out", "different", "material", "we", "turn", "toy", "car", "cake", "we", "extend", "technique", "support", "replicate", "sequence", "model", "create", "stop-motion", "video", "we", "demonstrate", "end-to-end", "system", "which", "real-world", "performance", "capture", "datum", "retarget", "claymation", "we", "approach", "allow", "user", "easily", "accurately", "create", "complex", "shape", "naturally", "support", "large", "range", "material", "model", "size", "keyword", "personal", "digital", "fabrication", "spatially", "augment", "reality", "sculpt", "Links", "dl", "pdf", "acm", "Reference", "Format", "Rivers", "a.", "Adams", "A.", "Durand", "F.", "2012", "sculpt", "number", "ACM", "Trans", "graph", "31", "Article", "157", "-lrb-", "November", "2012", "-rrb-", "page", "dous", "10.1145", "2366145.2366176", "http://doi.acm.org/10.1145/2366145.2366176", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "212", "869-0481", "permissions@acm.org", "2012", "ACM", "0730-0301/2012", "11-art157", "15.00", "DOI", "10.1145", "2366145.2366176", "http://doi.acm.org/10.1145/2366145.2366176", "Fr?do", "Durand", "MIT", "CSAIL", "-lrb-", "-rrb-", "sculpted", "physical", "replica", "introduction", "most", "people", "find", "challenge", "sculpt", "carve", "manually", "form", "precise", "shape", "we", "argue", "usually", "because", "lack", "manual", "dexterity", "average", "person", "able", "perform", "very", "precise", "manipulation", "rather", "because", "lack", "precise", "3d", "information", "can", "figure", "out", "what", "need", "do", "modify", "work", "progress", "order", "reach", "goal", "shape", "analogy", "can", "make", "task", "reproduce", "2d", "painting", "when", "give", "outline", "need", "only", "fill", "child?s", "color", "book", "paint-by-numbers", "kit", "even", "unskilled", "user", "can", "accurately", "reproduce", "complex", "painting", "challenge", "lie", "place", "paint", "canvas", "know", "where", "place", "motivate", "observation", "we", "present", "sculpt", "number", "method", "provide", "analogous", "guidance", "creation", "3d", "object", "which", "assist", "user", "make", "object", "precisely", "match", "shape", "target", "3d", "model", "we", "employ", "spatially-augmented", "reality", "approach", "-lrb-", "see", "e.g.", "raskar", "et", "al.", "-lsb-", "1998", "-rsb-", "Bimber", "Raskar", "-lsb-", "2005", "-rsb-", "overview", "spatially-augmented", "reality", "-rrb-", "which", "visual", "feedback", "illustrate", "discrepancy", "between", "work", "progress", "target", "3d", "shape", "approach", "first", "propose", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "approach", "projector-camera", "pair", "use", "scan", "object", "be", "create", "use", "structured", "light", "scan", "shape", "compare", "target", "3d", "model", "projector", "annotate", "object", "color", "indicate", "how", "object", "ought", "change", "match", "target", "user", "follow", "guidance", "adjust", "object", "rescan", "when", "necessary", "bring", "object", "closer", "target", "shape", "over", "time", "we", "propose", "method", "provide", "guidance", "illustrate", "depth", "disparity", "between", "current", "work", "target", "similar", "approach", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "well", "additional", "form", "guidance", "which", "we", "call", "edge", "guidance", "which", "aid", "reproduce", "high-frequency", "surface", "detail", "we", "demonstrate", "application", "reproduce", "exist", "physical", "object", "different", "scale", "use", "different", "material", "we", "further", "show", "how", "scene", "can", "repeatedly", "deform", "match", "sequence", "target", "3d", "model", "purpose", "stop-motion", "animation", "start", "from", "depth", "video", "record", "Kinect", "we", "produce", "claymation", "physically", "correct", "dynamics", "finally", "we", "present", "result", "user", "testing", "novice", "sculptor", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "157:2", "a.", "Rivers", "et", "al.", "related", "work", "advent", "digital", "fabrication", "technology", "3d", "printer", "laser", "cutter", "CNC", "machine", "have", "make", "possible", "automatically", "produce", "2d", "3d", "object", "main", "tool", "use", "3d", "digital", "fabrication", "3d", "printer", "5-axis", "cnc", "machine", "Devices", "both", "kind", "typically", "expensive", "though", "recent", "effort", "DIY", "community", "have", "make", "low-priced", "entry-level", "tool", "available", "-lsb-", "Hokanson", "Reilly", "Kelly", "MakerBot", "Industries", "Drumm", "2011", "Sells", "et", "al.", "2009", "-rsb-", "3d", "printing", "cnc", "method", "highly", "accurate", "suffer", "from", "fabrication", "artifact", "limit", "size", "material", "object", "can", "produce", "multi-material", "fabrication", "device", "particularly", "challenging", "recent", "work", "computer", "graphic", "have", "therefore", "focus", "software", "technique", "enable", "alternative", "method", "fabrication", "we", "technique", "fall", "category", "two", "recent", "work", "propose", "method", "generate", "model", "slide", "planar", "slice", "approximate", "target", "3d", "model", "-lsb-", "Hildebrand", "et", "al.", "2012", "McCrae", "et", "al.", "2011", "-rsb-", "Lau", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "method", "convert", "3d", "furniture", "model", "fabricatable", "planar", "part", "other", "incorporate", "fabrication", "consideration", "directly", "modeling", "process", "so", "fabrication", "straightforward", "design", "Kilian", "et", "al.", "-lsb-", "2008", "-rsb-", "propose", "method", "design", "shape", "can", "produce", "folding", "single", "sheet", "material", "while", "Mori", "Igarashi", "-lsb-", "2007", "-rsb-", "propose", "interface", "design", "stuff", "animal", "we", "approach", "aim", "address", "limitation", "both", "manual", "craft", "current", "digital", "fabrication", "technology", "combine", "element", "each", "hybrid", "approach", "hybrid", "human-computer", "interface", "have", "be", "use", "before", "especially", "medical", "application", "example", "combine", "human", "surgeon?s", "ability", "plan", "motion", "react", "change", "condition", "robotic", "arm?s", "precision", "-lsb-", "kragic", "et", "al.", "2005", "Mako", "Surgical", "-rsb-", "interestingly", "approach", "typically", "human", "do", "planning", "machine", "do", "execution", "while", "we", "approach", "reverse", "Rivers", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "hybrid", "approach", "digital", "fabrication", "2d", "shape", "which", "user", "coarsely", "position", "tool", "can", "automatically", "adjust", "its", "own", "position", "keep", "exactly", "digital", "plan", "we", "approach", "share", "philosophical", "goal", "enable", "digitallyguided", "manual", "fabrication", "apply", "3d", "shape", "work", "just", "guide", "user", "without", "automatic", "error", "correction", "hybrid", "manual-automatic", "method", "have", "also", "be", "use", "2d", "design", "painter", "have", "use", "photograph", "camera", "obscura", "reference", "-lsb-", "Coke", "1964", "Pennsylvania", "Academy", "fine", "art", "et", "al.", "1994", "-rsb-", "Flagg", "Rehg", "-lsb-", "Flagg", "Rehg", "2006", "-rsb-", "present", "ProjectorGuided", "Painting", "system", "digitally", "guide", "artist", "engage", "reproduce", "2d", "painting", "support", "artist", "build", "painting", "out", "layer", "include", "show", "how", "orient", "brush", "certain", "stage", "explicitly", "help", "task", "mix", "paint", "achieve", "desire", "color", "Dixon", "et", "al.", "-lsb-", "2010", "-rsb-", "propose", "hybrid", "method", "task", "sketch", "human", "face", "from", "reference", "photograph", "which", "user", "guide", "automatic", "feedback", "guidance", "include", "high-level", "sketch", "principle", "each", "part", "face", "well", "specific", "feedback", "how", "adjust", "freehand", "drawing", "more", "closely", "match", "shape", "reference", "image", "shadowdraw", "-lsb-", "Lee", "et", "al.", "2011", "-rsb-", "propose", "more", "open-ended", "system", "which", "user", "allow", "draw", "freely", "while", "shadow", "image", "suggestive", "contour", "update", "display", "behind", "sketch", "illustrate", "possible", "sketch", "completion", "while", "we", "interested", "challenge", "create", "physical", "sculpture", "from", "virtual", "model", "complementary", "challenge", "create", "virtual", "model", "use", "physical", "object", "input", "method", "have", "also", "be", "explore", "Sheng", "et", "al.", "-lsb-", "2006", "-rsb-", "present", "system", "which", "user", "whose", "finger", "be", "be", "track", "could", "manipulate", "deformable", "physical", "prop", "perform", "sculpt", "operation", "virtual", "3d", "model", "goal", "provide", "more", "natural", "hands-on", "modeling", "experience", "we", "approach", "guide", "sculpting", "involve", "scanning", "work", "progress", "project", "guidance", "annotate", "physical", "object", "3d", "scanning", "have", "be", "active", "area", "research", "many", "year", "variety", "optical", "method", "be", "propose", "see", "Chen", "et", "al.", "-lsb-", "2000", "-rsb-", "survey", "one", "oldest", "most", "robust", "method", "involve", "project", "sequence", "Gray", "code", "imaging", "result", "retrieve", "depth", "information", "-lsb-", "Inokuchi", "et", "al.", "1984", "Caspi", "et", "al.", "1998", "Rusinkiewicz", "et", "al.", "2002", "-rsb-", "method", "simple", "implement", "robust", "able", "capture", "full", "depth", "field", "quickly", "system", "design", "core", "operation", "guide", "sculpting", "create", "physical", "object", "match", "target", "3d", "model", "do", "project", "different", "form", "guidance", "onto", "object", "illustrate", "how", "must", "change", "match", "target", "shape", "all", "adjustment", "model", "do", "manually", "which", "naturally", "support", "wide", "range", "material", "shape", "guidance", "make", "possible", "achieve", "complex", "precise", "shape", "approach", "require", "two", "main", "technical", "component", "method", "scanning", "3d", "shape", "high", "spatial", "depth", "resolution", "projector", "register", "same", "coordinate", "system", "project", "guidance", "onto", "material", "we", "achieve", "both", "goal", "same", "hardware", "use", "projector-camera", "pair", "perform", "structured", "light", "scanning", "Gray", "code", "-lrb-", "e.g.", "-lsb-", "Caspi", "et", "al.", "1998", "-rsb-", "-rrb-", "while", "faster", "scanning", "method", "exist", "e.g.", "Primesense/Microsoft", "Kinect", "typically", "have", "much", "lower", "spatial", "resolution", "which", "more", "issue", "we", "than", "refresh", "rate", "we", "require", "user", "explicitly", "request", "each", "new", "scan", "which", "have", "two", "advantage", "guidance", "do", "change", "unexpectedly", "user?s", "hand", "scan", "confuse", "target", "object", "3.1", "scan", "input", "scanning", "process", "target", "3d", "model", "which", "may", "mesh", "point", "cloud", "virtually", "position", "within", "work", "volume", "block", "raw", "material", "-lrb-", "e.g.", "clay", "-rrb-", "place", "roughly", "same", "real", "position", "Whenever", "user", "initiate", "scan", "we", "project", "sequence", "Gray", "code", "onto", "scene", "photograph", "scene", "illuminate", "each", "code", "image", "give", "we", "correspondence", "from", "camera", "coordinate", "projector", "coordinate", "we", "filter", "out", "invalid", "implausible", "correspondence", "invert", "map", "compute", "correspondence", "from", "projector", "coordinate", "camera", "coordinate", "from", "we", "triangulate", "compute", "depth", "map", "from", "projector?s", "perspective", "we", "also", "compute", "depth", "map", "target", "model", "render", "from", "same", "perspective", "finally", "we", "compare", "depth", "map", "compute", "guidance", "we", "project", "onto", "object", "we", "describe", "each", "operation", "detail", "below", "obtain", "pixel", "correspondence", "gray-code-based", "structured", "light", "scanning", "operate", "project", "sequence", "black", "white", "horizontal", "vertical", "bar", "onto", "scene", "record", "result", "image", "examine", "illumination", "single", "camera", "pixel", "across", "all", "project", "pattern", "one", "can", "determine", "coordinate", "projector", "pixel", "image", "camera", "pixel", "while", "basic", "approach", "straightforward", "number", "factor", "can", "create", "incorrect", "correspondence", "first", "some", "camera", "pixel", "may", "image", "part", "scene", "illuminate", "projector", "all", "can", "detect", "exclude", "ignore", "pixel", "do", "vary", "substantially", "between", "image", "which", "projector", "display", "entirely", "white", "entirely", "black", "more", "difficult", "detect", "case", "which", "camera", "pixel", "record", "specular", "highlight", "reflection", "light", "cast", "projector", "camera", "pixel", "vary", "appropriately", "between", "white", "black", "projector", "image", "corresponding", "projector", "pixel", "can", "compute", "they", "however", "invalid", "correspondence", "corrupt", "depth", "map", "projector", "pixel", "we", "able", "reject", "majority", "correspondence", "consistency", "check", "give", "homography", "camera", "projector", "we", "can", "determine", "world-space", "ray", "correspond", "each", "camera", "projector", "pixel", "ray", "ought", "intersect", "within", "work", "volume", "case", "true", "correspondence", "ray", "do", "approximately", "intersect", "may", "case", "camera", "see", "reflection", "we", "reject", "correspondence", "put", "another", "way", "scan", "scan", "epipolar", "constraint", "combine", "overconstrain", "pixel", "correspondence", "which", "let", "we", "reject", "invalid", "pixel", "constraint", "also", "help", "eliminate", "spurious", "correspondence", "detect", "due", "noise", "final", "source", "inaccuracy", "can", "arise", "when", "narrow", "alternate", "black", "white", "column", "fine-scale", "Gray", "code", "pattern", "blur", "average", "gray", "due", "combination", "camera", "projector", "defocus", "we", "case", "work", "volume", "often", "close", "camera", "projector", "so", "depths", "field", "relatively", "shallow", "loss", "finest", "Gray", "code", "pattern", "result", "lose", "least-significant", "bit", "projector-camera", "correspondence", "when", "coarsely", "shape", "object", "loss", "resolution", "do", "matter", "towards", "end", "sculpt", "process", "become", "important", "we", "therefore", "allow", "user", "perform", "slightly", "more", "time-consuming", "precise", "scan", "inspire", "laser", "plane", "sweep", "scanning", "-lrb-", "e.g.", "-lsb-", "Curless", "Levoy", "1995", "-rsb-", "-rrb-", "simple", "implementation", "we", "could", "project", "single", "white", "row", "column", "sweep", "across", "entire", "projector", "image", "every", "camera", "pixel", "simply", "record", "which", "code", "illumination", "highest", "method", "robust", "defocus", "slow", "we", "instead", "project", "Gray", "code", "pattern", "usual", "compute", "correspondence", "may", "incorrect", "least", "significant", "bit", "we", "sweep", "one-pixel-wide", "vertical", "hor", "izontal", "line", "recur", "every", "row", "column", "across", "all", "position", "record", "position", "maximum", "illumination", "every", "camera", "pixel", "correct", "three", "least-significant", "bit", "correspondence", "precise", "scan", "take", "about", "seconds", "give", "we", "depth", "resolution", "under", "1mm", "center", "work", "volume", "Computing", "depth", "map", "we", "display", "guidance", "use", "projector", "we", "need", "compute", "depth", "map", "from", "projector?s", "point", "view", "rather", "than", "camera?s", "achieve", "we", "must", "resample", "correspondence", "projector?s", "point", "view", "we", "do", "so", "splat", "each", "valid", "camera", "pixel", "contribute", "kernel", "exponential", "falloff", "about", "corresponding", "projector", "pixel", "contribution", "sum", "homogeneous", "coordinate", "normalize", "produce", "camera", "coordinate", "some", "subset", "projector", "pixel", "kernel", "size", "choose", "enough", "interpolate", "across", "small", "hole", "depth", "map", "which", "arise", "from", "projector", "pixel", "see", "any", "camera", "pixel", "give", "resampled", "correspondence", "homography", "both", "camera", "projector", "we", "can", "triangulate", "compute", "depth", "value", "each", "projector", "pixel", "meanwhile", "depth", "map", "target", "model", "compute", "render", "from", "projector?s", "point", "view", "use", "projector?s", "calibrate", "homography", "opengl", "projection", "matrix", "depth", "map", "compare", "compute", "guidance", "we", "project", "onto", "material", "hardware", "setup", "we", "use", "3M", "MP160", "projector", "run", "800", "600", "60hz", "Nokia", "n9", "camera-phone", "use", "FCam", "API", "-lsb-", "Adams", "et", "al.", "2010", "-rsb-", "generate", "raw", "datum", "1632", "1232", "13hz", "-lrb-", "75m", "exposure", "time", "-rrb-", "FCam", "API", "make", "easier", "we", "synchronize", "camera", "projector", "raw", "stream", "prevent", "we", "from", "lose", "resolution", "demosaicing", "Gray", "code", "do", "rely", "color", "spatial", "coherence", "so", "we", "can", "treat", "each", "pixel", "independently", "ignore", "Bayer", "mosaic", "separation", "between", "camera", "projector", "place", "roughly", "from", "center", "work", "area", "we", "compute", "world-space", "homography", "camera", "projector", "scan", "known", "planar", "calibration", "target", "place", "two", "known", "depths", "work", "volume", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "Sculpting", "Numbers", "157:3", "-lrb-", "-rrb-", "target", "model", "-lrb-", "-rrb-", "scan", "depth", "map", "-lrb-", "-rrb-", "depth", "guidance", "figure", "sculpt", "guidance", "user", "give", "feedback", "during", "sculpting", "two", "form", "guidance", "project", "onto", "model", "which", "can", "toggle", "between", "guidance", "compute", "compare", "target", "depth", "map", "-lrb-", "-rrb-", "scan", "depth", "map", "current", "shape", "-lrb-", "-rrb-", "depth", "guidance", "-lrb-", "-rrb-", "illustrate", "difference", "between", "two", "map", "helpful", "match", "position", "surface", "absolute", "3d", "coordinate", "Edge", "guidance", "-lrb-", "-rrb-", "illustrate", "second", "derivative", "target", "model", "-lrb-", "i.e.", "ridge", "valley", "-rrb-", "useful", "recreate", "fine", "detail", "once", "model", "close", "target", "shape", "smoothly-varying", "range", "color", "use", "depth", "mode", "green", "indicate", "where", "surface", "should", "move", "closer", "scanner", "red", "farther", "while", "edge", "mode", "green", "indicate", "positive", "second", "derivative", "-lrb-", "ridge", "-rrb-", "red", "negative", "-lrb-", "valley", "-rrb-", "either", "mode", "blue", "indicate", "background", "so", "any", "part", "material", "color", "blue", "should", "remove", "entirely", "closer", "ridge", "farther", "valley", "-lrb-", "-rrb-", "Edge", "guidance", "3.2", "guidance", "we", "provide", "two", "form", "guidance", "which", "user", "may", "toggle", "between", "depend", "context", "which", "project", "directly", "onto", "object", "be", "create", "each", "form", "useful", "recreate", "different", "aspect", "model?s", "appearance", "user", "can", "also", "view", "rendering", "target", "3d", "model", "screen", "computer", "run", "system", "depth", "guidance", "use", "achieve", "correct", "surface", "position", "absolute", "3d", "space", "mode", "projector", "pixel", "color", "base", "difference", "between", "scan", "depth", "pixel", "target", "depth", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "Green", "indicate", "area", "where", "user", "must", "bring", "surface", "closer", "projector", "red", "indicate", "area", "where", "user", "must", "move", "surface", "farther", "away", "pixel", "do", "intersect", "target", "model", "color", "blue", "range", "scale", "user-tunable", "user", "can", "also", "probe", "exact", "depth", "error", "millimeter", "particular", "point", "mouse", "over", "pixel", "laptop", "control", "projector", "Edge", "guidance", "depend", "only", "target", "3d", "model", "use", "help", "user", "match", "high-frequency", "surface", "detail", "target", "3d", "model", "mode", "projector", "pixel", "color", "accord", "second", "derivative", "target", "depth", "map", "-lrb-", "i.e.", "ridge", "valley", "show", "figure", "-lrb-", "-rrb-", "-rrb-", "color", "follow", "same", "scale", "depth", "feedback", "mode", "its", "range", "can", "similarly", "tune", "Rotation", "method", "describe", "so", "far", "assist", "user", "achieve", "correct", "shape", "from", "single", "point", "view", "however", "complete", "model", "must", "correct", "from", "all", "side", "challenge", "rotate", "target", "model", "partial", "sculpture", "same", "amount", "about", "same", "axis", "rotation", "so", "user", "can", "continue", "without", "invalidate", "work", "already", "do", "while", "automatic", "method", "-lrb-", "e.g.", "icp", "-lsb-", "Zhang", "1994", "-rsb-", "-rrb-", "could", "employ", "deal", "we", "handle", "more", "simply", "sculpt", "atop", "Lego", "stage", "which", "can", "accurately", "rotate", "90", "degree", "around", "known", "axis", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "157:4", "a.", "Rivers", "et", "al.", "application", "use", "we", "system", "practice", "involve", "interplay", "computer", "guidance", "human", "intuition", "which", "both", "precision", "computer", "intuition", "artist", "require", "create", "object", "correct", "proportion", "aesthetic", "quality", "process", "computer?s", "ability", "accurately", "detect", "absolute", "3d", "shape", "world", "coordinate", "mean", "guidance", "can", "rely", "upon", "achieve", "correct", "relative", "position", "size", "feature", "model", "which", "can", "otherwise", "extremely", "challenge", "novice", "sculptor", "however", "process", "also", "rely", "user?s", "understanding", "shape", "one", "simple", "reason", "guidance", "must", "interpret", "internalize", "both", "block", "user?s", "hand", "tool", "slowly", "invalidate", "user", "work", "model", "between", "scan", "user", "become", "familiar", "system", "quickly", "become", "able", "perceive", "how", "shape", "must", "deform", "judge", "when", "initiate", "rescan", "less", "obvious", "important", "way", "which", "we", "benefit", "from", "have", "artist", "-lrb-", "even", "novice", "one", "-rrb-", "loop", "user", "can", "make", "unprompted", "change", "model", "achieve", "desire", "aesthetic", "appearance", "thus", "user", "may", "interpret", "project", "guidance", "liberally", "perform", "freehand", "adjustment", "recreate", "salient", "detail", "can", "both", "speed", "creation", "major", "feature", "enable", "reproduction", "fine-detail", "surface", "feature", "too", "small", "individually", "highlight", "reproduce", "use", "system", "we", "describe", "how", "process", "play", "out", "basic", "case", "make", "shape", "match", "target", "3d", "model", "go", "explore", "additional", "application", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "head", "Michaelangelo?s", "David", "original", "marble", "-lrb-", "photograph", "Steve", "Hanna", "-rrb-", "-lrb-", "-rrb-", "tall", "version", "make", "modeling", "clay", "we", "system", "-lrb-", "-rrb-", "use", "datum", "from", "digital", "Michelanglo", "project", "-lsb-", "Levoy", "et", "al.", "2000", "-rsb-", "clay", "model", "create", "novice", "sculptor", "4.1", "create", "object", "match", "3d", "model", "we", "find", "use", "we", "system", "fall", "fairly", "consistent", "pattern", "typical", "modeling", "session", "begin", "depth", "guidance", "mode", "use", "user", "establish", "coarse", "shape", "silhouette", "model", "model", "approach", "target", "shape", "user", "make", "successively", "smaller", "adjustment", "object", "shorten", "red-to-green", "guidance", "range", "necessary", "highlight", "necessary", "adjustment", "eventually", "user", "have", "shape", "match", "target", "3d", "model", "absolute", "coordinate", "limit", "user?s", "ability", "model", "system?s", "ability", "scan", "even", "after", "model", "have", "be", "make", "match", "target", "3d", "shape", "closely", "possible", "depth", "guidance", "mode", "surface", "may", "still", "lack", "high-frequency", "surface", "detail", "wrinkle", "rough", "surface", "hair", "detail", "often", "essential", "aesthetic", "appearance", "model", "difficult", "replicate", "simple", "depth", "feedback", "may", "finer", "than", "scan", "can", "resolve", "may", "hide", "low-frequency", "depth", "discrepancy", "although", "larger", "can", "ignore", "therefore", "user", "typically", "finish", "model", "switch", "edge", "guidance", "mode", "mode", "can", "see", "location", "ridge", "valley", "other", "high-frequency", "detail", "project", "onto", "surface", "use", "those", "start", "point", "artistic", "pass", "surface", "user", "usually", "want", "shape", "object", "from", "all", "side", "rather", "than", "form", "model", "perfectly", "from", "one", "side", "rotate", "generally", "easier", "form", "material", "match", "target", "model", "roughly", "from", "all", "side", "first", "make", "successively", "higherdetail", "pass", "from", "each", "direction", "minimize", "problem", "alteration", "one", "side", "affect", "already-completed", "region", "another", "result", "novice", "sculptor", "use", "we", "system", "can", "see", "figure", "we", "find", "we", "system", "enable", "unskilled", "sculptor", "achieve", "far", "better", "result", "than", "would", "otherwise", "capable", "while", "give", "they", "sense", "ownership", "result", "creation", "would", "absent", "automatically-produced", "object", "one", "unexpected", "possibility", "allow", "we", "approach", "collaborative", "sculpting", "because", "computer", "guidance", "provide", "common", "goal", "work", "towards", "possible", "multiple", "user", "work", "sculpture", "same", "time", "both", "time-saving", "highly", "enjoyable", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "Sculpting", "Numbers", "157:5", "Figure", "sculpt", "from", "physical", "example", "simple", "extension", "we", "can", "use", "scan", "exist", "physical", "object", "target", "sculpt", "example", "without", "any", "prior", "cake-decorating", "experience", "we", "remake", "toy", "car", "cake", "we", "free", "make", "replica", "different", "scale", "out", "different", "material", "car", "have", "be", "scaled-up", "make", "out", "madeira", "cake", "frosting", "marzipan", "cookie", "candy", "4.2", "sculpt", "from", "physical", "example", "we", "approach", "can", "also", "use", "simple", "extension", "allow", "user", "create", "object", "just", "from", "digital", "3d", "mesh", "also", "from", "already-existing", "physical", "object", "do", "user", "place", "object", "front", "projector-camera", "pair", "scan", "use", "method", "above", "rotate", "scanning", "from", "all", "four", "direction", "recover", "point", "cloud", "save", "model", "remove", "store", "point", "cloud", "use", "target", "shape", "modeling", "above", "because", "we", "use", "point", "cloud", "recover", "scanning", "algorithm", "directly", "we", "able", "sidestep", "difficulty", "edge", "case", "attempt", "reconstruct", "full", "3d", "mesh", "from", "scan", "addition", "we", "allow", "user", "change", "scale", "material", "reproduction", "relative", "original", "illustrate", "flexibility", "process", "we", "scan", "toy", "car", "use", "we", "system", "recreate", "scaling", "factor", "2.5", "cake", "consist", "mixture", "edible", "material", "-lrb-", "figure", "-rrb-", "timelapse", "process", "can", "see", "video", "accompany", "paper", "complete", "cake", "eat", "enjoy", "4.3", "stop-motion", "animation", "we", "approach", "also", "adapt", "well", "be", "use", "guide", "stopmotion", "animation", "load", "series", "target", "instead", "just", "one", "allow", "user", "flip", "between", "target", "become", "possible", "generate", "series", "snapshot", "physical", "object", "correspond", "desire", "3d", "animation", "while", "skilled", "sculptor", "can", "already", "achieve", "what", "we", "system", "allow", "its", "basic", "form", "application", "provide", "capability", "otherwise", "extremely", "difficult", "even", "expert", "artist", "ability", "deform", "physical", "object", "sequence", "shape", "animation", "form", "concatenate", "image", "those", "shape", "appear", "physically", "correct", "put", "another", "way", "while", "skilled", "artist", "may", "able", "make", "plausible", "reproduction", "single", "pose", "human", "act", "run", "even", "skilled", "artist", "may", "have", "trouble", "determine", "how", "much", "adjust", "model", "make", "correspond", "pose", "runner", "exactly", "1/30th", "second", "later", "test", "we", "build", "end-to-end", "system", "which", "performance", "capture", "kinect", "use", "make", "claymation", "sequence", "-lrb-", "figure", "-rrb-", "reconstruct", "exist", "physical", "object", "we", "have", "advantage", "point", "cloud", "derive", "from", "Kinect", "can", "use", "directly", "modeling", "without", "need", "generate", "explicit", "3d", "mesh", "each", "frame", "model", "proceeds", "normal", "addition", "ability", "toggle", "between", "frame", "take", "snapshot", "scene", "save", "frame", "animation", "we", "show", "thumbnail", "from", "result", "animation", "Figure", "full", "animation", "can", "view", "we", "accompany", "video", "Figure", "stop-motion", "animation", "we", "approach", "can", "use", "help", "create", "stop-motion", "animation", "we", "demonstrate", "end-to-end", "system", "which", "2.5", "video", "-lrb-", "left", "center", "column", "-rrb-", "real", "person", "walking", "capture", "kinect", "use", "create", "claymation", "sequence", "-lrb-", "right", "column", "-rrb-", "4.4", "user", "testing", "limitation", "we", "feel", "we", "approach", "make", "possible", "user", "prior", "experience", "sculpt", "create", "aesthetically-pleasing", "3d", "sculpture", "test", "we", "ask", "six", "first-time", "user", "reproduce", "3d", "model", "Stanford", "bunny", "out", "modeling", "clay", "each", "user", "ask", "make", "one", "attempt", "freehand", "refer", "rendering", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "one", "attempt", "use", "we", "system", "after", "10", "minute", "training", "user", "be", "give", "30", "minute", "each", "attempt", "save", "time", "we", "request", "only", "front", "bunny", "need", "look", "correct", "half", "user", "start", "freehand", "attempt", "half", "we", "method", "result", "show", "Figure", "user", "comment", "system", "greatly", "improve", "ability", "achieve", "correct", "proportion", "dynamic", "sculpting", "process", "also", "change", "when", "sculpt", "freehand", "user", "typically", "feel", "finish", "after", "30", "minute", "acknowledge", "proportion", "weren?t", "quite", "right", "also", "weren?t", "sure", "what", "do", "fix", "they", "use", "we", "method", "slower", "always", "provide", "clear", "direction", "which", "proceed", "after", "30", "minute", "we", "system", "user", "feel", "could", "have", "continue", "productively", "improve", "model", "sculpture", "make", "we", "method", "be", "generally", "better", "proportion", "than", "freehand", "sculpture", "only", "one", "user", "do", "benefit", "from", "system", "-lrb-", "show", "Figure", "column", "-rrb-", "user?s", "freehand", "model", "very", "good", "while", "able", "make", "continual", "progress", "use", "we", "method", "user", "able", "achieve", "same", "quality", "same", "amount", "time", "when", "sculptor", "have", "enough", "skill", "innately", "know", "how", "adjust", "model", "automatic", "guidance", "just", "slow", "they", "down", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "157:6", "a.", "Rivers", "et", "al.", "Figure", "user", "testing", "result", "after", "10", "minute", "training", "practice", "user", "be", "ask", "sculpt", "Stanford", "bunny", "out", "polymer", "clay", "both", "freehand", "-lrb-", "top", "row", "-rrb-", "also", "use", "we", "system", "-lrb-", "bottom", "row", "-rrb-", "30", "minute", "be", "allot", "each", "task", "odd", "column", "first", "use", "system", "sculpt", "freehand", "while", "even", "column", "sculpted", "freehand", "use", "we", "system", "all", "user", "artistically-inclined", "column", "system", "greatly", "improve", "ability", "create", "correctly-proportioned", "bunny", "conclusion", "we", "have", "demonstrate", "new", "approach", "make", "physical", "object", "match", "target", "3d", "shape", "which", "unskilled", "user", "assist", "guidance", "project", "onto", "object", "be", "make", "we", "have", "find", "we", "system", "greatly", "increase", "what", "novice", "sculptor", "capable", "-lrb-", "e.g.", "figure", "-rrb-", "addition", "produce", "physical", "replica", "3d", "model", "we", "approach", "can", "use", "enable", "reproduction", "exist", "physical", "object", "potentially", "different", "scale", "out", "different", "material", "we", "further", "demonstrate", "how", "we", "approach", "can", "use", "retarget", "real-world", "performance", "capture", "datum", "generate", "stop-motion", "animation", "physically", "correct", "dynamics", "we", "believe", "combination", "human", "actuation", "situate", "feedback", "from", "computer-accurate", "measurement", "have", "great", "potential", "future", "work", "we", "particularly", "wish", "explore", "application", "task", "assemble", "rigid", "part", "accurately", "match", "target", "configuration", "which", "may", "useful", "industrial", "context", "acknowledgment", "author", "wish", "thank", "Abe", "Davis", "Elena", "Adams", "user", "study", "participant", "work", "partially", "fund", "NSF", "grant", "0964004", "Foxconn", "gift", "from", "Cognex", "reference", "dam", "a.", "orowitz", "M.", "ark", "S.", "H.", "ELFAND", "N.", "AEK", "J.", "atusik", "W.", "EVOY", "M.", "ACOBS", "D.", "E.", "OL", "SON", "J.", "ico", "M.", "ULLI", "K.", "alvalum", "e.-v.", "jdin", "B.", "AQUERO", "D.", "ENSCH", "H.", "P.", "A.", "2010", "Frankencamera", "ACM", "Trans", "graph", "29", "-lrb-", "July", "-rrb-", "imber", "O.", "askar", "R.", "2005", "spatial", "augmented", "reality", "merge", "real", "virtual", "world", "a.", "K.", "Peters", "Ltd.", "Natick", "MA", "USA", "aspus", "D.", "IRYATI", "N.", "HAMIR", "J.", "1998", "Range", "imaging", "adaptive", "color", "structure", "light", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "20", "470", "480", "hen", "F.", "ROWN", "G.", "M.", "ONG", "M.", "2000", "overview", "three-dimensional", "shape", "measurement", "use", "optical", "method", "Optical", "Engineering", "39", "10", "oke", "V.", "D.", "1964", "painter", "photograph", "from", "Delacroix", "Warhol", "University", "New", "Mexico", "Press", "urless", "B.", "EVOY", "M.", "1995", "better", "optical", "triangulation", "through", "spacetime", "analysis", "Proceedings", "Fifth", "International", "Conference", "Computer", "Vision", "-lrb-", "June", "-rrb-", "987", "994", "ixon", "D.", "rasad", "M.", "AMMOND", "T.", "2010", "iCanDraw", "use", "sketch", "recognition", "corrective", "feedback", "assist", "user", "draw", "human", "face", "Proceedings", "28th", "international", "conference", "human", "factor", "compute", "system", "ACM", "New", "York", "NY", "USA", "CHI", "10", "897", "906", "rumm", "B.", "2011", "Printrbot", "http://www.printrbot", "com", "lagg", "m.", "ehg", "J.", "M.", "2006", "projector-guided", "painting", "Proceedings", "19th", "annual", "acm", "symposium", "user", "interface", "software", "technology", "ACM", "New", "York", "NY", "USA", "UIST", "06", "235", "244", "ildebrand", "K.", "ICKEL", "B.", "LEXA", "M.", "2012", "crdbrd", "shape", "fabrication", "slide", "Planar", "Slices", "Computer", "Graphics", "Forum", "-lrb-", "Eurographics", "2012", "-rrb-", "vol", "31", "okanson", "T.", "EILLY", "C.", "DIYLILCNC", "http", "diylilcnc.org", "nokuchi", "S.", "ato", "K.", "atsuda", "F.", "1984", "rangeimage", "3d", "object", "recognition", "icpr", "806", "808", "ELLY", "S.", "Bluumax", "CNC", "http://www.bluumaxcnc.com/", "gantry-router.html", "ILIAN", "M.", "ory", "S.", "HEN", "Z.", "ITRA", "N.", "J.", "heffer", "a.", "ottmann", "H.", "2008", "curved", "folding", "ACM", "Trans", "graph", "27", "-lrb-", "Aug.", "-rrb-", "75:1", "75:9", "RAGIC", "D.", "RAGIC", "D.", "ARAYONG", "P.", "M.", "KAMURA", "A.", "M.", "ager", "G.", "D.", "2005", "human-machine", "collaborative", "system", "microsurgical", "application", "International", "Journal", "Robotics", "Research", "24", "731", "741", "au", "m.", "hgawara", "a.", "ITANI", "J.", "garashi", "t.", "2011", "convert", "3d", "furniture", "model", "fabricatable", "part", "connector", "ACM", "Trans", "graph", "30", "-lrb-", "Aug.", "-rrb-", "85:1", "85:6", "ee", "Y.", "J.", "ITNICK", "C.", "L.", "OHEN", "M.", "F.", "2011", "ShadowDraw", "ACM", "Trans", "graph", "30", "-lrb-", "July", "-rrb-", "27:1", "27:10", "evoy", "M.", "INSBERG", "J.", "hade", "J.", "ulk", "D.", "ULLI", "K.", "URLESS", "B.", "USINKIEWICZ", "S.", "OLLER", "D.", "ereira", "L.", "INZTON", "M.", "NDERSON", "S.", "AVIS", "J.", "2000", "digital", "Michelangelo", "project", "Proceedings", "27th", "annual", "conference", "computer", "graphic", "interactive", "technique", "ACM", "Press", "New", "York", "New", "York", "USA", "131", "144", "aker", "ot", "ndustry", "MakerBot", "http://www", "makerbot.com", "ako", "urgical", "RIO", "Robotic", "arm", "Interactive", "System", "ra", "J.", "ingh", "K.", "itra", "N.", "J.", "2011", "slice", "shape-proxy", "base", "planar", "section", "ACM", "transaction", "graphic", "-lrb-", "tog", "-rrb-", "30", "-lrb-", "Dec.", "-rrb-", "168", "orus", "Y.", "GARASHI", "T.", "2007", "Plushie", "interactive", "design", "system", "plush", "toy", "ACM", "Trans", "graph", "26", "-lrb-", "Aug.", "-rrb-", "45:1", "45:7", "ennsylvanium", "cademy", "ine", "RTS", "ANLY", "S.", "EIBOLD", "C.", "1994", "eakin", "photograph", "work", "Thomas", "Eakins", "he", "circle", "collection", "Pennsylvania", "Academy", "fine", "art", "publish", "Pennsylvania", "Academy", "fine", "art", "Smithsonian", "Institution", "Press", "askar", "R.", "ELCH", "G.", "uch", "H.", "1998", "spatially", "augmented", "reality", "First", "IEEE", "Workshop", "augmented", "reality", "-lrb-", "iwar98", "11", "20", "iver", "a.", "oyer", "i.", "E.", "URAND", "F.", "2012", "positioncorrect", "tool", "2D", "Digital", "Fabrication", "ACM", "Trans", "graph", "31", "usinkiewicz", "S.", "all", "olt", "O.", "EVOY", "M.", "2002", "real-time", "3d", "model", "acquisition", "ACM", "Trans", "graph", "21", "-lrb-", "July", "-rrb-", "438", "446", "ell", "E.", "mith", "Z.", "AILARD", "S.", "OWYER", "a.", "liver", "V.", "2009", "RepRap", "replicate", "rapid", "prototypermaximize", "customizability", "breed", "means", "production", "handbook", "research", "mass", "customization", "personalization", "568", "580", "heng", "J.", "ALAKRISHNAN", "R.", "INGH", "K.", "2006", "interface", "virtual", "3d", "sculpt", "via", "physical", "proxy", "Proceedings", "4th", "international", "conference", "computer", "graphic", "interactive", "technique", "Australasia", "Southeast", "Asia", "ACM", "New", "York", "NY", "USA", "GRAPHITE", "06", "213", "220", "keel", "C.", "ehg", "J.", "2007", "ShapeShift", "projector-guided", "sculpture", "system", "-lrb-", "poster", "-rrb-", "UIST", "2007", "Poster", "session", "hang", "Z.", "1994", "iterative", "point", "match", "registration", "freeform", "curve", "surface", "International", "Journal", "Computer", "Vision", "13", "-lrb-", "Oct.", "-rrb-", "119", "152", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012", "Sculpting", "Numbers", "157:7", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "157", "publication", "date", "November", "2012" ],
  "content" : "\n  \n    759b051b6bd12fd99eee6f1975558d92d1b1bc343f0a63c96257cfcf0cdb1d62\n    p2n\n    10.1145/2366145.2366176\n    Name identification was not possible. \n  \n  \n    \n      \n        Sculpting by Numbers\n      \n      Alec Rivers Andrew Adams MIT CSAIL MIT CSAIL\n      \n        \n        \n      \n      (a) Target 3D model (b) Guidance projected onto material\n      \n        Figure 1: We assist users in creating physical objects that match digital 3D models. Given a target 3D model (a), we project different forms of guidance onto a work in progress (b) that indicate how it must be deformed to match the target model. As the user follows this guidance, the physical object?s shape approaches that of the target (c). With our system, unskilled users are able to produce accurate physical replicas of complex 3D models. Here, we recreate the Stanford bunny model (courtesy of the Stanford Computer Graphics Laboratory) out of polymer clay.\n      \n      We propose a method that allows an unskilled user to create an accurate physical replica of a digital 3D model. We use a projector/camera pair to scan a work in progress, and project multiple forms of guidance onto the object itself that indicate which areas need more material, which need less, and where any ridges, valleys or depth discontinuities are. The user adjusts the model using the guidance and iterates, making the shape of the physical object approach that of the target 3D model over time. We show how this approach can be used to create a duplicate of an existing object, by scanning the object and using that scan as the target shape. The user is free to make the reproduction at a different scale and out of different materials: we turn a toy car into cake. We extend the technique to support replicating a sequence of models to create stop-motion video. We demonstrate an end-to-end system in which real-world performance capture data is retargeted to claymation. Our approach allows users to easily and accurately create complex shapes, and naturally supports a large range of materials and model sizes. Keywords: personal digital fabrication, spatially augmented reality, sculpting\n      Links:\n      \n        \n      \n      DL PDF\n      \n        \n      \n    \n    \n      \n        ACM Reference Format\n        Rivers, A., Adams, A., Durand, F. 2012. Sculpting by Numbers. ACM Trans. Graph. 31 6, Article 157 (November 2012), 7 pages. DOI = 10.1145/2366145.2366176 http://doi.acm.org/10.1145/2366145.2366176.\n      \n      \n        Copyright Notice\n        Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1\n        \n          212\n          869-0481, or permissions@acm.org.\n        \n        ? 2012 ACM 0730-0301/2012/11-ART157 $15.00 DOI 10.1145/2366145.2366176 http://doi.acm.org/10.1145/2366145.2366176\n        Fr?do Durand MIT CSAIL\n        \n          \n        \n        (c) Sculpted physical replica\n      \n      \n        1 Introduction\n      \n      Most people find it challenging to sculpt, carve or manually form a precise shape. We argue that this is usually not because they lack manual dexterity ? the average person is able to perform very precise manipulations ? but rather because they lack precise 3D information, and cannot figure out what needs to be done to modify a work in progress in order to reach a goal shape. An analogy can be made to the task of reproducing a 2D painting: when given outlines that need only be filled in, as in a child?s coloring book or a paint-by-numbers kit, even an unskilled user can accurately reproduce a complex painting; the challenge lies not in placing paint on the canvas but in knowing where to place it. Motivated by this observation, we present Sculpting by Numbers, a method to provide analogous guidance for the creation of 3D objects, which assists a user in making an object that precisely matches the shape of a target 3D model.  We employ a spatially-augmented reality approach (see e.g. Raskar et al. [1998] or Bimber and Raskar [2005] for an overview of spatially-augmented reality), in which visual feedback illustrates the discrepancy between a work in progress and a target 3D shape. This approach was first proposed by Skeels and Rehg [2007]. In this approach, a projector-camera pair is used to scan the object being created using structured light. The scanned shape is compared with the target 3D model, and the projector then annotates the object with colors that indicate how the object ought to be changed to match the target. The user follows this guidance to adjust the object and rescans when necessary, bringing the object closer to the target shape over time. Our proposed method provides guidance that illustrates depth disparities between the current work and the target, similar to the approach of Skeels and Rehg [2007], as well as an additional form of guidance, which we call edge guidance, which aids in reproducing high-frequency surface details. We demonstrate an application to reproducing an existing physical object at a different scale or using different materials. We further show how a scene can be repeatedly deformed to match a sequence of target 3D models for the purpose of stop-motion animation. Starting from a depth video recorded with a Kinect, we produce claymation with physically correct dynamics. Finally, we present results of user testing with novice sculptors.\n      ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n      157:2\n      ?\n      A. Rivers et al.\n      \n        2 Related work\n        The advent of digital fabrication technologies, such as 3D printers, laser cutters, and CNC machines, has made it possible to automatically produce 2D and 3D objects. The main tools used in 3D digital fabrication are 3D printers and 5-axis CNC machines. Devices of both kinds are typically expensive, though recent efforts in the DIY community have made low-priced entry-level tools available [Hokanson and Reilly ; Kelly ; MakerBot Industries ; Drumm 2011 ; Sells et al. 2009]. 3D printing and CNC methods are highly accurate, but suffer from fabrication artifacts, and are limited in the size and materials of the object they can produce. Multi-material fabrication with these devices is particularly challenging. Recent work in computer graphics has therefore focused on software techniques that enable alternative methods for fabrication, and our technique falls into this category. Two recent works propose methods to generate models of sliding planar slices that approximate target 3D models [Hildebrand et al. 2012; McCrae et al. 2011]. Lau et al. [2011] propose a method to convert 3D furniture models into fabricatable planar parts. Others incorporate fabrication considerations directly into the modeling process, so that fabrication is straightforward by design: Kilian et al. [2008] propose a method for designing shapes that can be produced by folding a single sheet of material, while Mori and Igarashi [2007] propose an interface for designing stuffed animals. Our approach aims to address the limitations of both manual crafting and current digital fabrication technologies by combining elements of each in a hybrid approach. Hybrid human-computer interfaces have been used before, especially in medical applications, for example to combine a human surgeon?s ability to plan motions and react to changing conditions with a robotic arm?s precision [Kragic et al. 2005; Mako Surgical ]. Interestingly, in these approaches, it is typically the human that does the planning and the machine that does the execution, while in our approach it is the reverse. Rivers et al. [2012] propose a hybrid approach to digital fabrication of 2D shapes in which a user coarsely positions a tool that can then automatically adjust its own position to keep it exactly on a digital plan; our approach shares the philosophical goal of enabling digitallyguided manual fabrication, but applies to 3D shapes, and works by just guiding a user without automatic error correction. Hybrid manual-automatic methods have also been used for 2D design. Painters have used photographs or a camera obscura as references [Coke 1964; Pennsylvania Academy of the Fine Arts et al. 1994]. Flagg and Rehg [Flagg and Rehg 2006] presented ProjectorGuided Painting, a system for digitally guiding an artist engaged in reproducing a 2D painting. They support the artist in building a painting out of layers, including showing how to orient the brush at certain stages, and explicitly help with the task of mixing paint to achieve a desired color. Dixon et al. [2010] proposed a hybrid method for the task of sketching a human face from a reference photograph in which the user is guided by automatic feedback. The guidance included high-level sketching principles for each part of the face as well as specific feedback on how to adjust the freehand drawings to more closely match the shape of the reference image. ShadowDraw [Lee et al. 2011] proposed a more open-ended system in which a user is allowed to draw freely while a ?shadow image? of suggestive contours is updated and displayed behind the sketch to illustrate possible sketch completions. While we are interested in the challenge of creating physical sculptures from a virtual model, the complementary challenge of creating virtual models using physical objects as an input method has also been explored. Sheng et al. [2006] presented a system in which a user whose fingers were being tracked could manipulate deformable physical props to perform sculpting operations on a virtual 3D model, with the goal of providing a more natural, hands-on modeling experience. Our approach to guided sculpting involves scanning the work in progress and projecting guidance that annotates the physical object. 3D scanning has been an active area of research for many years, with a variety of optical methods being proposed; see Chen et al. [2000] for a survey. One of the oldest and most robust methods involves projecting a sequence of Gray codes and imaging the result to retrieve depth information [Inokuchi et al. 1984; Caspi et al. 1998; Rusinkiewicz et al. 2002]. This method is simple to implement, robust, and able to capture a full depth field quickly.\n      \n      \n        3 System design\n        The core operation in guided sculpting is creating a physical object that matches a target 3D model. This is done by projecting different forms of guidance onto the object that illustrate how it must be changed to match a target shape. All adjustments to the model are done manually, which naturally supports a wide range of materials and shapes. The guidance makes it possible to achieve complex and precise shapes. This approach requires two main technical components: a method for scanning a 3D shape at high spatial and depth resolution, and a projector registered to the same coordinate system to project guidance onto the material. We achieve both goals with the same hardware by using a projector-camera pair to perform structured light scanning with Gray codes (e.g. [Caspi et al. 1998]). While faster scanning methods exist, e.g. the Primesense/Microsoft Kinect, they typically have much lower spatial resolution, which is more of an issue to us than refresh rate. We require the user to explicitly request each new scan, which has two advantages: the guidance does not change unexpectedly, and the user?s hands are not scanned and confused with the target object.\n        \n          3.1 Scanning\n          The input to the scanning process is a target 3D model, which may be a mesh or a point cloud. It is virtually positioned within the working volume, and a block of raw material (e.g. clay) is placed at roughly the same real position. Whenever the user initiates a scan, we project a sequence of Gray codes onto the scene and photograph the scene illuminated by each code. These images give us a correspondence from camera coordinates to projector coordinates. We filter out invalid or implausible correspondences, and then invert this map to compute a correspondence from projector coordinates to camera coordinates. From this we triangulate to compute a depth map from the projector?s perspective. We also compute a depth map of the target model rendered from the same perspective. Finally, we compare the depth maps to compute the guidance that we project onto the object. We describe each of these operations in detail below.  Obtaining pixel correspondences Gray-code-based structured light scanning operates by projecting a sequence of black and white horizontal and vertical bars onto a scene and recording the resulting images. By examining the illumination of a single camera pixel across all projected patterns, one can determine the X and Y coordinate of the projector pixel that was imaged by that camera pixel. While the basic approach is straightforward, a number of factors can create incorrect correspondences. First, some camera pixels may image parts of the scene that are not illuminated by the projector at all. These can be detected and excluded by ignoring pixels that do not vary substantially between images in which the projector displays entirely white and entirely black. More difficult to detect are cases in which a camera pixel records specular highlights or reflections of light cast by the projector. These camera pixels will vary appropriately between white and black projector images, and a corresponding projector pixel can be computed for them; however, this invalid correspondence will corrupt the depth map at that projector pixel. We are able to reject the majority of these correspondences with a consistency check. Given the homographies for the camera and projector, we can determine the world-space ray that corresponds to each camera and projector pixel. These rays ought to intersect within the working volume in the case of a true correspondence. If the rays do not approximately intersect, as may be the case if the camera is seeing a reflection, we reject the correspondence. Put another way, the scan in X, the scan in Y, and the epipolar constraint combine to overconstrain the pixel correspondence, which lets us reject invalid pixels. This constraint also helps eliminate spurious correspondences detected due to noise. A final source of inaccuracy can arise when narrow alternating black and white columns in a fine-scale Gray code pattern are blurred into an average gray due to the combination of camera and projector defocus. In our case, the working volume is often close to the camera and projector, so their depths of field are relatively shallow. Loss of the finest Gray code patterns results in losing the least-significant bits of the projector-camera correspondence. When coarsely shaping an object this loss of resolution does not matter, but towards the end of the sculpting process it becomes important. We therefore allow the user to perform a slightly more time-consuming ?precise? scan inspired by laser plane sweep scanning (e.g. [Curless and Levoy 1995]). In a simple implementation, we could project a single white row or column that sweeps across the entire projector image, and for every camera pixel simply record for which code the illumination was highest. This method is robust to defocus, but slow. We instead project the Gray code patterns as usual to compute a correspondence that may be incorrect in the least significant bits. We then sweep one-pixel-wide vertical or hor- izontal lines that recur every 8 rows or columns across all 8 positions, and record the position of maximum illumination for every camera pixel to correct the three least-significant bits of the correspondence. This precise scan takes about 7 seconds, and gives us a depth resolution of under 1mm at the center of the working volume. Computing depth maps As we will be displaying guidance using the projector, we need to compute the depth map from the projector?s point of view rather than the camera?s. To achieve this, we must resample the correspondence into the projector?s point of view. We do so by splatting: each valid camera pixel contributes to a 7 ? 7 kernel with exponential falloff about the corresponding projector pixel. The contributions are summed in homogeneous coordinates and then normalized to produce a camera coordinate for some subset of projector pixels. The kernel size chosen is enough to interpolate across small holes in the depth map, which arise from projector pixels not seen by any camera pixel. Given these resampled correspondences and the homographies for both camera and projector, we can triangulate to compute depth values for each projector pixel. Meanwhile, the depth map for the target model is computed by rendering it from the projector?s point of view using the projector?s calibrated homography as the OpenGL projection matrix. These depth maps are then compared to compute the guidance that we project onto the material. Hardware setup We use a 3M MP160 projector running at 800?600 at 60Hz and a Nokia N9 camera-phone using the FCam API [Adams et al. 2010] generating raw data at 1632?1232 at 13Hz (75ms exposure time). The FCam API makes it easier for us to synchronize the camera with the projector, and the raw stream prevents us from losing resolution to demosaicing. Gray codes do not rely on color or spatial coherence, so we can treat each pixel independently and ignore the Bayer mosaic. The separation between camera and projector is 5?, and they placed roughly 2? from the center of the work area. We compute world-space homographies for the camera and projector by scanning a known planar calibration target placed at two known depths in the work volume.\n          ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n          Sculpting by Numbers\n          ?\n          157:3\n          \n            \n          \n          (a) Target model (b) Scanned depth map\n          (c) Depth guidance\n          \n            Figure 2: Sculpting guidance: A user is given feedback during sculpting by two forms of guidance projected onto the model, which they can toggle between. Guidance is computed by comparing the target depth map (a) to the scanned depth map of the current shape (b). Depth guidance (c) illustrates the difference between the two maps and is helpful for matching the position of the surface in absolute 3D coordinates. Edge guidance (d) illustrates the second derivative of the target model (i.e. ridges and valleys), and is useful for recreating fine details once the model is close to the target shape. A smoothly-varying range of colors is used: in depth mode green indicates where the surface should be moved closer to the scanner and red farther, while in edge mode green indicates a positive second derivative (a ridge) and red negative (a valley). In either mode, blue indicates background, so any part of the material colored blue should be removed entirely.\n          \n          + closer\n          + ridge\n          farther\n          \n            \n          \n          valley (d) Edge guidance\n        \n        \n          3.2 Guidance\n          We provide two forms of guidance, which the user may toggle between depending on the context, and which are projected directly  onto the object being created. Each form is useful for recreating a different aspect of the model?s appearance. The user can also view a rendering of the target 3D model on the screen of the computer running the system. Depth guidance is used to achieve the correct surface position in absolute 3D space. In the this mode, projector pixels are colored based on the difference between the scanned depth at that pixel and the target depth ( Figure 2 (c)). Green indicates areas where the user must bring the surface closer to the projector and red indicates areas where the user must move the surface farther away. Pixels that do not intersect the target model are colored blue. The range of the scale is user-tunable. The user can also probe the exact depth error in millimeters at a particular point by mousing over that pixel on the laptop controlling the projector. Edge guidance depends only on the target 3D model, and is used to help the user match the high-frequency surface details of the target 3D model. In this mode, projector pixels are colored according to the second derivative of the target depth map (i.e. ridges and valleys, as shown in Figure 2 (d)). This color follows the same scale as the depth feedback mode, and its range can be similarly tuned. Rotation. The method described so far assists a user in achieving the correct shape from a single point of view. However, a complete model must be correct from all sides. The challenge is to rotate the target model and the partial sculpture by the same amount about the same axis of rotation so the user can continue without invalidating work already done. While automatic methods (e.g. ICP [Zhang 1994]) could be employed to deal with this, we handle this more simply by sculpting atop a Lego stage, which can be accurately rotated by 90 degrees around a known axis.\n          ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n          157:4\n          ?\n          A. Rivers et al.\n        \n      \n      \n        4 Applications\n        Using our system in practice involves an interplay of computer guidance and human intuition, in which both the precision of the computer and the intuition of the artist are required to create an object with the correct proportions and aesthetic qualities.  In this process, the computer?s ability to accurately detect absolute 3D shape in world coordinates means that the guidance can be relied upon to achieve the correct relative positions and sizes of features of the model, which can otherwise be extremely challenging for a novice sculptor. However, the process also relies on the user?s understanding of the shape. One simple reason is that the guidance must be interpreted and internalized, as it will both be blocked by the user?s hands and tools, and is slowly invalidated as the user works on the model between scans. As users become familiar with the system, they quickly become able to perceive how a shape must be deformed and judge when to initiate a rescan. A less obvious but important way in which we benefit from having an artist (even if a novice one) ?in the loop? is that the user can make unprompted changes to the model to achieve a desired aesthetic appearance. Thus, a user may interpret the projected guidance liberally, and perform freehand adjustments to recreate salient details. This can both speed the creation of major features and enable the reproduction of fine-detail surface features that are too small to be individually highlighted and reproduced using the system. We describe how this process plays out for the basic case of making a shape that matches a target 3D model, and go on to explore additional applications.\n        \n          \n        \n        (a) (b)\n        \n          Figure 3: The head of Michaelangelo?s David, in the original marble (photograph by Steve Hanna) (a) and in a 6? tall version made in modeling clay with our system (b) using data from the digital Michelanglo project [Levoy et al. 2000]. The clay model was created by novice sculptors.\n        \n        \n          4.1 Creating an object that matches a 3D model\n          We found that use of our system fell into a fairly consistent pattern. A typical modeling session will begin in depth guidance mode. Using this the user will establish the coarse shape and silhouette of the model. As the model approaches the target shape, the user will make successively smaller adjustments to the object, shortening the red-to-green guidance range as necessary to highlight the necessary adjustments. Eventually, the user will have a shape that matches the target 3D model in absolute coordinates to the limit of the user?s ability to model and the system?s ability to scan.  Even after the model has been made to match the target 3D shape as closely as possible in depth guidance mode, the surface may still lack high-frequency surface details, such as wrinkles or the rough surface of hair. These details are often essential to the aesthetic appearance of a model, but are difficult to replicate with simple depth feedback: they may be finer than the scan can resolve, or may be hidden by low-frequency depth discrepancies that, although larger, can be ignored. Therefore, users will typically finish a model by switching to edge guidance mode. In this mode, they can see the locations of ridges, valleys, and other high-frequency details projected onto the surface, and use those as a starting point for an artistic pass on the surface. The user will usually want to shape an object from all sides. Rather than forming the model perfectly from one side and then rotating, it is generally easier to form the material to match the target model roughly from all sides first, and then make successively higherdetail passes from each direction. This minimizes the problem of alterations to one side affecting already-completed regions of another. Results of novice sculptors using our system can be seen in Figures 1 and 3. We found that our system enabled unskilled sculptors to achieve far better results than they would otherwise be capable of, while giving them a sense of ownership of the resulting creations that would be absent with an automatically-produced object. One unexpected possibility allowed by our approach is collaborative sculpting. Because the computer guidance provides a common goal to work towards, it is possible for multiple users to work on a sculpture at the same time. This is both time-saving and highly enjoyable.\n          ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n          Sculpting by Numbers\n          ?\n          157:5\n          \n            \n            Figure 4: Sculpting from a physical example: With a simple extension, we can use a scan of an existing physical object as a target for sculpting. For example, without any prior cake-decorating experience, we remade this toy car as a cake. We are free to make a replica at a different scale and out of different materials: the car has been scaled-up and made out of madeira cake, frosting, marzipan, cookies, and candy.\n          \n        \n        \n          4.2 Sculpting from a physical example\n          Our approach can also be used with a simple extension to allow the user to create an object not just from a digital 3D mesh, but also from an already-existing physical object. To do this, the user places an object in front of the projector-camera pair and scans it using the method above, rotating it and scanning it from all four directions. The recovered point clouds are saved. The model is then removed and the stored point clouds are used as the target shape for modeling as above. Because we use the point cloud recovered by the scanning algorithm directly, we are able to sidestep the difficulties and edge cases of attempting to reconstruct a full 3D mesh from a scan. In addition, we allow the user to change the scale or the materials of the reproduction relative to the original. To illustrate the flexibility of this process, we scanned a toy car and used our system to recreate it at a scaling factor of 2.5? as a cake consisting of a mixture of edible materials ( Figure 4 ). A timelapse of the process can be seen in the video that accompanies this paper. The completed cake was eaten and enjoyed.\n        \n        \n          4.3 Stop-motion animation\n          Our approach also adapts well to being used as a guide for stopmotion animation. By loading a series of targets, instead of just one, and allowing the user to flip between these targets, it becomes  possible to generate a series of snapshots of a physical object that correspond to a desired 3D animation. While a skilled sculptor can already achieve what our system allows in its basic form, this application provides a capability that is otherwise extremely difficult even for expert artists. This is the ability to deform a physical object into a sequence of shapes such that the animation formed by concatenating images of those shapes appears physically correct. Put another way, while a skilled artist may be able to make a plausible reproduction of a single pose of a human in the act of running, even that skilled artist may have trouble determining how much to adjust the model to make it correspond to the pose of that runner exactly 1/30th of a second later. To test this, we built an end-to-end system in which a performance is captured by a Kinect and used to make a claymation sequence ( Figure 5 ). As with reconstructing an existing physical object, we have the advantage that the point cloud derived from the Kinect can be used directly for modeling, without needing to generate an explicit 3D mesh for each frame. Modeling proceeds as normal, with the addition of the ability to toggle between frames and to take a snapshot of the scene to save as a frame of the animation. We show thumbnails from the resulting animation in Figure 5 , and the full animation can be viewed in our accompanying video.\n          \n            \n            Figure 5: Stop-motion animation: Our approach can be used to help create stop-motion animation. We demonstrate an end-to-end system, in which a 2.5D video (left and center columns) of a real person walking was captured with a Kinect and used to create a claymation sequence (right column).\n          \n        \n        \n          4.4 User testing and limitations\n          We felt that our approach made it possible for users with no prior experience sculpting to create aesthetically-pleasing 3D sculptures. To test this, we asked six first-time users to reproduce a 3D model  of the Stanford bunny out of modeling clay. Each user was asked to make one attempt freehand, referring to a rendering ( Figure 1(a) ), and one attempt using our system, after a 10 minute training period. Users were given 30 minutes for each attempt. To save time, we requested that only the front of the bunny need look correct. Half of the users started with the freehand attempt, and half with our method. The results are shown in Figure 6 . Users commented that the system greatly improved their ability to achieve the correct proportions. The dynamic of the sculpting process was also changed. When sculpting freehand, users typically felt finished after 30 minutes; they acknowledged that the proportions weren?t quite right, but also that they weren?t sure what to do to fix them. Using our method was slower, but always provided a clear direction in which to proceed; after 30 minutes with our system, users felt that they could have continued to productively improve the model. Sculptures made with our method were generally better proportioned than the freehand sculptures. Only one user did not benefit from the system (shown in Figure 6 column 3). This user?s freehand model was very good. While able to make continual progress using our method, this user was not able to achieve the same quality in the same amount of time. When a sculptor has enough skill to innately know how to adjust a model, automatic guidance just slows them down.\n          ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n          157:6\n          ?\n          A. Rivers et al.\n          \n            \n            Figure 6: User testing results: After 10 minutes of training and practice, users were asked to sculpt the Stanford bunny out of polymer clay both freehand (top row), and also using our system (bottom row). 30 minutes were allotted for each task. The odd columns first used the system and then sculpted freehand, while the even columns sculpted freehand and then used our system. For all users but the artistically-inclined column 3, the system greatly improved their ability to create a correctly-proportioned bunny.\n          \n        \n      \n      \n        5 Conclusion\n        We have demonstrated a new approach to making a physical object that matches a target 3D shape in which an unskilled user is assisted by guidance projected onto the object as it is being made. We have found that our system greatly increases what novice sculptors are capable of (e.g. Figures 1, 3, 4, and 5). In addition to producing physical replicas of a 3D model, our approach can be used to enable reproduction of existing physical objects, potentially at a different scale and out of different materials. We further demonstrate how our approach can be used to retarget real-world performance capture data to generate stop-motion animation with physically correct dynamics.  We believe that the combination of human actuation and situated feedback from computer-accurate measurements has great potential. In future work, we particularly wish to explore applications to the task of assembling rigid parts to accurately match a target configuration, which may be useful in industrial contexts.\n      \n      \n        6 Acknowledgments\n        The authors wish to thank Abe Davis, Elena Adams, and the user study participants. The work was partially funded by NSF grant 0964004, Foxconn, and a gift from Cognex.\n      \n      \n        References\n        \n          A DAMS , A., H OROWITZ , M., P ARK , S. H., G ELFAND , N., B AEK , J., M ATUSIK , W., L EVOY , M., J ACOBS , D. E., D OL SON , J., T ICO , M., P ULLI , K., T ALVALA , E.-V., A JDIN , B., V AQUERO , D., AND L ENSCH , H. P. A. 2010. The Frankencamera. ACM Trans. Graph. 29, 4 (July), 1.\n          B IMBER , O., AND R ASKAR , R. 2005. Spatial augmented reality: merging real and virtual worlds. A. K. Peters, Ltd., Natick, MA, USA.\n          C ASPI , D., K IRYATI , N., AND S HAMIR , J. 1998. Range imaging with adaptive color structured light. IEEE Transactions on Pattern Analysis and Machine Intelligence 20, 470 ? 480.\n          C HEN , F., B ROWN , G. M., AND S ONG , M. 2000. Overview of three-dimensional shape measurement using optical methods. Optical Engineering 39, 10.\n          C OKE , V. D. 1964. The painter and the photograph: from Delacroix to Warhol. University of New Mexico Press.\n          C URLESS , B., AND L EVOY , M. 1995. Better optical triangulation through spacetime analysis. Proceedings of the Fifth International Conference on Computer Vision (June), 987?994.\n          D IXON , D., P RASAD , M., AND H AMMOND , T. 2010. iCanDraw: using sketch recognition and corrective feedback to assist a user in drawing human faces. In Proceedings of the 28th international conference on Human factors in computing systems, ACM, New York, NY, USA, CHI ?10, 897?906.\n          D RUMM , B., 2011. Printrbot. http://www.printrbot . com/.\n          F LAGG , M., AND R EHG , J. M. 2006. Projector-guided painting. In Proceedings of the 19th annual ACM symposium on User interface software and technology, ACM, New York, NY, USA, UIST ?06, 235?244.\n          H ILDEBRAND , K., B ICKEL , B., AND A LEXA , M. 2012. crdbrd: Shape Fabrication by Sliding Planar Slices. In Computer Graphics Forum (Eurographics 2012), vol. 31.\n          H OKANSON , T., AND R EILLY , C. DIYLILCNC. http:// diylilcnc.org/. I NOKUCHI , S., S ATO , K., AND M ATSUDA , F. 1984. Rangeimaging for 3D object recognition. ICPR, 806?808.\n          K ELLY , S. Bluumax CNC. http://www.bluumaxcnc.com/ Gantry-Router.html. K ILIAN , M., F L ORY  ? , S., C HEN , Z., M ITRA , N. J., S HEFFER , A., AND P OTTMANN , H. 2008. Curved folding. ACM Trans. Graph. 27, 3 (Aug.), 75:1?75:9.\n          K RAGIC , D., K RAGIC , D., M ARAYONG , P., L I , M., O KAMURA , A. M., AND H AGER , G. D. 2005. Human-machine collaborative systems for microsurgical applications. International Journal of Robotics Research 24, 731?741.\n          L AU , M., O HGAWARA , A., M ITANI , J., AND I GARASHI , T. 2011. Converting 3D furniture models to fabricatable parts and connectors. ACM Trans. Graph. 30, 4 (Aug.), 85:1?85:6.\n          L EE , Y. J., Z ITNICK , C. L., AND C OHEN , M. F. 2011. ShadowDraw. ACM Trans. Graph. 30, 4 (July), 27:1?27:10.\n          L EVOY , M., G INSBERG , J., S HADE , J., F ULK , D., P ULLI , K., C URLESS , B., R USINKIEWICZ , S., K OLLER , D., P EREIRA , L., G INZTON , M., A NDERSON , S., AND D AVIS , J. 2000. The digital Michelangelo project. In Proceedings of the 27th annual conference on computer graphics and interactive techniques, ACM Press, New York, New York, USA, 131?144.\n          M AKER B OT I NDUSTRIES . MakerBot. http://www . makerbot.com/. M AKO S URGICAL . RIO Robotic Arm Interactive System. M C C RAE , J., S INGH , K., AND M ITRA , N. J. 2011. Slices: a shape-proxy based on planar sections. ACM Transactions on Graphics (TOG) 30, 6 (Dec.), 168.\n          M ORI , Y., AND I GARASHI , T. 2007. Plushie: an interactive design system for plush toys. ACM Trans. Graph. 26, 3 (Aug.), 45:1? 45:7.\n          P ENNSYLVANIA A CADEMY OF THE F INE A RTS , D ANLY , S., AND L EIBOLD , C. 1994. Eakins and the photograph: works by Thomas Eakins and his circle in the collection of the Pennsylvania Academy of the Fine Arts. Published for the Pennsylvania Academy of the Fine Arts by the Smithsonian Institution Press.\n          R ASKAR , R., W ELCH , G., AND F UCHS , H. 1998. Spatially augmented reality. In In First IEEE Workshop on Augmented Reality (IWAR98, 11?20.\n          R IVERS , A., M OYER , I. E., AND D URAND , F. 2012. PositionCorrecting Tools for 2D Digital Fabrication. ACM Trans. Graph. 31, 4.\n          R USINKIEWICZ , S., H ALL -H OLT , O., AND L EVOY , M. 2002. Real-time 3D model acquisition. ACM Trans. Graph. 21, 3 (July), 438?446.\n          S ELLS , E., S MITH , Z., B AILARD , S., B OWYER , A., AND O L LIVER , V. 2009. RepRap: The Replicating Rapid Prototypermaximizing customizability by breeding the means of production. Handbook of Research in Mass Customization and Personalization 1, 568?580.\n          S HENG , J., B ALAKRISHNAN , R., AND S INGH , K. 2006. An interface for virtual 3D sculpting via physical proxy. In Proceedings of the 4th international conference on Computer graphics and interactive techniques in Australasia and Southeast Asia, ACM, New York, NY, USA, GRAPHITE ?06, 213?220.\n          S KEELS , C., AND R EHG , J. 2007. ShapeShift: a projector-guided sculpture system (Poster). UIST 2007 Poster Session.\n          Z HANG , Z. 1994. Iterative point matching for registration of freeform curves and surfaces. International Journal of Computer Vision 13, 2 (Oct.), 119?152.\n        \n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n        Sculpting by Numbers\n        ?\n        157:7\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 157, Publication Date: November 2012\n      \n    \n  ",
  "resources" : [ ]
}