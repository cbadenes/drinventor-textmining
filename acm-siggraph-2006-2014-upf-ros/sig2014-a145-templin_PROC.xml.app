{
  "uri" : "sig2014-a145-templin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a145-templin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Krzysztof-Templin",
      "name" : "Krzysztof",
      "surname" : "Templin"
    }, {
      "uri" : "http://drinventor/Piotr-Didyk",
      "name" : "Piotr",
      "surname" : "Didyk"
    }, {
      "uri" : "http://drinventor/Karol-Myszkowski",
      "name" : "Karol",
      "surname" : "Myszkowski"
    }, {
      "uri" : "http://drinventor/Mohamed-Hefeeda",
      "name" : "Mohamed",
      "surname" : "Hefeeda"
    }, {
      "uri" : "http://drinventor/Hans-Peter-Seidel",
      "name" : "Hans-Peter",
      "surname" : "Seidel"
    }, {
      "uri" : "http://drinventor/Wojciech-Matusik",
      "name" : "Wojciech",
      "surname" : "Matusik"
    } ]
  },
  "bagOfWords" : [ "sudden", "temporal", "depth", "change", "cut", "introduce", "video", "edit", "can", "significantly", "degrade", "quality", "stereoscopic", "content", "since", "usually", "encounter", "real", "world", "very", "challenging", "audience", "because", "eye", "vergence", "have", "constantly", "adapt", "new", "disparity", "spite", "conflict", "accommodation", "requirement", "rapid", "disparity", "change", "may", "lead", "confusion", "reduce", "understanding", "scene", "overall", "attractiveness", "content", "most", "case", "problem", "can", "solve", "simply", "match", "depth", "around", "transition", "would", "require", "flatten", "scene", "completely", "better", "understand", "limitation", "human", "visual", "system", "we", "conduct", "series", "eye-tracking", "experiment", "besides", "compute", "user-specific", "model", "we", "also", "estimate", "parameter", "average", "observer", "model", "enable", "range", "strategy", "minimize", "adaptation", "time", "audience", "however", "despite", "significant", "improvement", "only", "display", "device", "also", "image", "generation", "capture", "post-processing", "technique", "many", "consumer", "still", "skeptical", "about", "quality", "current", "s3d", "content", "future", "technology", "itself", "concern", "usually", "relate", "naturalness", "effortlessness", "overall", "appearance", "s3d", "effect", "should", "distraction", "difficulty", "s3d", "production", "sufficient", "produce", "two", "good", "image", "place", "one", "arrive", "good", "stereoscopic", "effect", "-lsb-", "Zilly", "et", "al.", "2011", "-rsb-", "s3d", "strong", "illusion", "since", "isolate", "only", "one", "real-world", "phenomenon", "fail", "reproduce", "many", "other", "prominent", "example", "be", "accommodation", "cue", "impose", "numerous", "restriction", "production", "process", "depth", "range", "variation", "must", "too", "large", "view-dependent", "effect", "need", "handle", "correctly", "image", "carefully", "register", "so", "work", "we", "concern", "rapid", "temporal", "change", "disparity", "although", "less", "problematic", "2d", "can", "challenge", "stereoscopic", "3d", "moreover", "vergence", "system", "need", "quickly", "adapt", "new", "condition", "spite", "conflict", "goal", "interconnected", "accommodation", "system", "clearly", "short", "shot", "increase", "viewer", "engagement", "force", "eye", "quickly", "follow", "newly", "appear", "content", "however", "accumulation", "sharp", "cut", "challenge", "visual", "system", "require", "seamless", "adjustment", "vergence", "between", "many", "shot", "over", "possibly", "wide", "range", "depths", "ultra-short", "mtvstyle", "shot", "need", "replace", "more", "slow-paced", "edit", "nevertheless", "modern", "movie", "often", "simultaneously", "release", "2d", "s3d", "one", "should", "expect", "director", "cinematographer", "editor", "entirely", "give", "up", "artistic", "vision", "style", "merely", "sake", "s3d", "medium", "limitation", "manipulation", "range", "from", "simple", "depth", "manipulation", "cross-dissolve", "type", "cut", "more", "sophisticated", "transition", "where", "multiple", "sequence", "gradually", "change", "depth", "combine", "-lsb-", "Owens", "2013", "-rsb-", "all", "manipulation", "time-consuming", "expensive", "perform", "manually", "example", "Owens", "-lsb-", "2013", "-rsb-", "point", "out", "editing", "transition", "one", "most", "challenging", "step", "post-production", "u2", "concert", "record", "stereoscopic", "3d", "abrupt", "depth", "change", "well", "beyond", "real-world", "experience", "should", "also", "expect", "action", "computer", "game", "address", "problem", "rapid", "depth", "change", "we", "propose", "relate", "transition", "quality", "vergence", "adaptation", "time", "instead", "simpler", "disparity", "difference", "we", "present", "series", "experiment", "human", "observer", "which", "vergence", "response", "be", "measure", "use", "consumer", "s3d", "equipment", "high-framerate", "eye-tracker", "here", "we", "overview", "basic", "finding", "eye", "vergence", "mechanism", "main", "focus", "s3d", "display", "condition", "vergence", "Dynamic", "process", "eye", "vergence", "drive", "depth", "change", "target", "object", "can", "perform", "high", "accuracy", "-lrb-", "error", "below", "10", "arcmin", "-rrb-", "both", "real", "world", "s3d", "display", "observation", "condition", "-lsb-", "Okuyama", "1998", "-rsb-", "other", "factor", "blur", "proximity", "target", "size", "luminance", "might", "affect", "vergence", "lesser", "extent", "-lsb-", "Campbell", "Westheimer", "1959", "-rsb-", "phasic", "-rrb-", "mechanism", "-lrb-", "react", "even", "brief", "200", "m", "flash", "-rrb-", "bring", "vergence", "proximity", "target", "depth", "slower", "sustained", "-lrb-", "a.k.a.", "small", "depth", "change", "within", "panum?s", "fusional", "area", "motoric", "vergence", "activate", "sensoric", "fusion", "image", "retina", "sufficient", "since", "range", "accommodation", "while", "view", "s3d", "display", "determine", "distance", "screen", "unnatural", "decoupling", "vergence", "accommodation", "require", "which", "may", "cause", "visual", "discomfort", "increase", "binocular", "fusion", "time", "-lsb-", "Hoffman", "et", "al.", "2008", "Lambooij", "et", "al.", "2009", "-rsb-", "when", "screen", "disparity", "increase", "beyond", "panum?s", "fusional", "area", "vergence", "eye", "movement", "bring", "disparity", "back", "area", "which", "shift", "accommodation", "away", "from", "screen", "when", "shift", "beyond", "depth", "focus", "-lrb-", "dof", "-rrb-", "zone", "accommodative-vergence", "feedback", "activate", "counteract", "loss", "sharp", "vision", "which", "turn", "direct", "vergence", "back", "towards", "display", "-lsb-", "Lambooij", "et", "al.", "2009", "-rsb-", "range", "vergence", "angle", "assure", "clear", "single", "binocular", "vision", "know", "comfort", "zone", "-lsb-", "Shibata", "et", "al.", "2011", "Zilly", "et", "al.", "2011", "-rsb-", "since", "rather", "conservative", "bind", "work", "we", "assume", "wider", "range", "2.5", "deg", "achieve", "extreme", "disparity", "would", "possible", "through", "sudden", "jump", "case", "scene", "cut", "we", "step-like", "change", "most", "relevant", "work", "we", "focus", "disparity", "range", "2.5", "deg", "lower", "disparity", "step", "amplitude", "which", "important", "comfortable", "experience", "while", "view", "s3d", "display", "use", "off-the-shelf", "s3d", "display", "we", "experiment", "deal", "real-world", "image", "validation", "step", "we", "ensure", "condition", "possibly", "similar", "one", "expect", "application", "where", "accommodation", "pictorial", "cue", "may", "affect", "vergence", "furthermore", "viewing", "condition", "do", "force", "decoupling", "accommodation", "vergence", "while", "s3d", "display", "have", "be", "consider", "some", "computational", "model", "main", "goal", "artificially", "alter", "link", "between", "accommodation", "vergence", "system", "study", "change", "pre-task", "post-task", "measure", "ac/a", "ca/c", "-lsb-", "Eadie", "et", "al.", "2000", "-rsb-", "investigate", "developmental", "plasticity", "child", "expose", "s3d", "game", "-lsb-", "Rushton", "Riddell", "1999", "-rsb-", "work", "we", "propose", "simple", "data-driven", "model", "eye", "vergence", "tune", "step-like", "disparity", "change", "we", "emphasize", "here", "vergence", "dynamics", "function", "initial", "target", "disparity", "we", "goal", "minimization", "vergence", "adaptation", "time", "scene", "cut", "through", "disparity", "editing", "solve", "problem", "disparity", "adjustment", "technique", "require", "however", "only", "few", "technique", "can", "deal", "temporal", "effect", "improvement", "we", "case", "come", "from", "more", "informed", "choice", "initial", "target", "disparity", "artist?s", "involvement", "still", "need", "though", "design", "transition", "manually", "without", "any", "feedback", "human", "ability", "adapt", "rapid", "disparity", "change", "however", "primary", "goal", "maintain", "scene", "disparity", "range", "within", "give", "limit", "equivalent", "minimize", "vergence", "adaptation", "time", "which", "depend", "only", "disparity", "difference", "also", "initial", "disparity", "value", "order", "measure", "vergence", "response", "both", "eye", "be", "track", "use", "EyeLink", "1000", "plus", "eye", "tracker", "desktop", "mount", "Vergence", "calculate", "difference", "between", "x-coordinate", "two", "gaze", "position", "express", "pixel", "blink", "saccade", "track", "error", "-rrb-", "be", "interpolate", "linearly", "segment", "require", "interpolation", "more", "than", "50", "sample", "be", "exclude", "next", "each", "type", "transition", "time", "reach", "95", "require", "vergence", "change", "determine", "two", "surface", "be", "fit", "obtain", "datum", "point", "since", "we", "be", "interested", "relative", "gaze", "position", "significance", "drift", "low", "moreover", "adaptation", "time", "be", "determine", "95", "of-change", "position", "which", "very", "sensitive", "shift", "scaling", "etc.", "order", "gain", "insight", "relation", "vergence", "response", "initial", "end", "disparity", "well", "estimate", "number", "trial", "necessary", "response", "curve", "converge", "we", "conduct", "pilot", "study", "other", "hand", "unclear", "how", "well", "model", "predict", "response", "cut", "between", "natural", "image", "presence", "complex", "luminance", "pattern", "high-level", "process", "relate", "scene", "understanding", "may", "very", "well", "influence", "transition", "time", "therefore", "we", "conduct", "validation", "experiment", "test", "model", "can", "generalize", "also", "possible", "transition", "facilitate", "some", "extent", "learning", "effect", "order", "evaluate", "stereoscopic", "transition", "estimate", "transition", "time", "we", "first", "need", "determine", "pair", "disparity", "value", "between", "which", "transition", "occur", "can", "obtain", "directly", "from", "render", "pipeline", "animated", "movie", "use", "user", "input", "case", "2d-to-3d", "conversion", "use", "disparity", "estimation", "technique", "natural", "scene", "when", "depth", "map", "available", "from", "cut", "cut", "-rrb-", "selectively", "around", "cut", "smooth", "blend", "back", "original", "parameter", "-lsb-", "Lang", "et", "al.", "2010", "Koppal", "et", "al.", "2011", "-rsb-", "all", "manipulation", "can", "easily", "integrate", "use", "we", "model", "disparity", "difference", "between", "circle", "px", "order", "determine", "arrangement", "recognition", "time", "all", "14", "step", "we", "perform", "14", "independent", "quest", "threshold", "estimation", "procedure", "-lsb-", "Watson", "Pelli", "1983", "-rsb-", "each", "estimate", "time", "75", "correctness", "we", "knowledge", "first", "work", "propose", "automatically", "edit", "stereoscopic", "cut", "take", "account", "vary", "performance", "human", "visual", "system", "adapt", "rapid", "disparity", "change", "we", "would", "like", "thank", "Aude", "Oliva", "Lavanya", "Sharan", "Zoya", "Bylinskii", "Sylvain", "Paris", "YiChang", "Shih", "Tobias", "Ritschel", "Katarina", "Struckmann", "David", "Levin", "anonymous", "subject", "who", "take", "part", "we", "perceptual", "study" ],
  "content" : "Sudden temporal depth changes, such as cuts that are introduced by video edits, can significantly degrade the quality of stereoscopic content. Since usually not encountered in the real world, they are very challenging for the audience. This is because the eye vergence has to constantly adapt to new disparities in spite of conflicting accommodation requirements. Such rapid disparity changes may lead to confusion, reduced understanding of the scene, and overall attractiveness of the content. In most cases the problem cannot be solved by simply matching the depth around the transition, as this would require flattening the scene completely. To better understand this limitation of the human visual system, we conducted a series of eye-tracking experiments. Besides computing user-specific models, we also estimated parameters of an average observer model. This enables a range of strategies for minimizing the adaptation time in the audience. However, despite the significant improvements, not only in display devices, but also in image generation, capture and post-processing techniques, many consumers are still skeptical about the quality of current S3D content and the future of the technology itself. These concerns are usually related to naturalness, effortlessness, and overall appearance: S3D effect should not be a distraction. The difficulty in S3D production is that it is not sufficient to produce two good images in place of one to arrive at a good stereoscopic effect [Zilly et al. 2011]. S3D is a strong illusion, since it isolates only one real-world phenomenon, failing to reproduce many others, a prominent example being the accommodation cue. This imposes numerous restrictions on the production process: the depth range and variation must not be too large, view-dependent effects need to be handled correctly, images carefully registered, and so on. In this work, we are concerned with rapid temporal changes of disparity. Although less problematic in 2D, this can be challenging in stereoscopic 3D. Moreover, the vergence system needs to quickly adapt to new conditions, in spite of the conflicting goal of the interconnected accommodation system. Clearly, short shots increase the viewer engagement by forcing eyes to quickly follow newly appearing content. However, such accumulation of sharp cuts challenges the visual system by requiring seamless adjustment of vergence between many shots over a possibly wide range of depths. , ultra-short ?MTVstyle? shots need to be replaced by more slow-paced edits. Nevertheless, modern movies are often simultaneously released in 2D and S3D, and one should not expect that directors, cinematographers, and editors will entirely give up on their artistic visions and style merely for the sake of S3D medium limitations. Such manipulations range from simple depth manipulations and cross-dissolve types of cuts to more sophisticated transitions, where multiple sequences with gradually changing depth are combined [Owens 2013]. All these manipulations are time-consuming and expensive, as they are performed manually. For example, Owens [2013] pointed out that the editing of transitions was one of the most challenging steps in the post-production of the U2 concert recorded in stereoscopic 3D. Abrupt depth changes, well beyond the real-world experience, should be also expected in action computer games. To address the problem of rapid depth changes, we propose to relate the transition quality to vergence adaptation time, instead of simpler disparity difference. We present a series of experiments with human observers, in which vergence responses were measured using consumer S3D equipment and a high-framerate eye-tracker. Here we overview basic findings on the eye vergence mechanisms, with the main focus on S3D display conditions. Vergence as a Dynamic Process The eye vergence is driven by the depth changes of a target object, and can be performed with high accuracy (error below 10 arcmin) both in the real world and S3D display observation conditions [Okuyama 1998]. Other factors, such as blur, proximity, target size, and luminance might affect vergence, but to a lesser extent [Campbell and Westheimer 1959]. phasic) mechanism (reacts even for brief 200 ms flashes) brings the vergence in the proximity of the target depth, and then the slower sustained (a.k.a. For small depth changes within Panum?s fusional area, the motoric vergence is not activated, and sensoric fusion of images on the retina is sufficient. Since the range of accommodation while viewing the S3D display is determined by the distance to the screen, unnatural decoupling of vergence and accommodation is required, which may cause visual discomfort and increase binocular fusion times [Hoffman et al. 2008; Lambooij et al. 2009]. When the screen disparity increases beyond Panum?s fusional area, vergence eye movements bring the disparity back to this area, which shifts accommodation away from the screen. When such a shift is beyond the depth of focus (DOF) zone, the accommodative-vergence feedback is activated to counteract the loss of sharp vision, which in turn directs vergence back towards the display [Lambooij et al. 2009]. The range of vergence angles that assure clear and single binocular vision is known as the ?comfort zone? [Shibata et al. 2011; Zilly et al. 2011]. Since it is a rather conservative bound, in this work we assume a wider range of ?2.5 deg. Achieving such extreme disparities would not be possible through sudden jumps as in the case of scene cuts. For us, the step-like changes are the most relevant. In this work we focus on the disparity range ?2.5 deg and lower disparity step amplitudes, which are important for comfortable experience while viewing S3D displays. By using an off-the-shelf S3D display in our experiments, and dealing with real-world images in the validation step, we ensure that the conditions are possibly similar to the ones in expected applications, where accommodation and pictorial cues may affect the vergence. Furthermore, the viewing conditions did not force decoupling of accommodation and vergence. While S3D displays have been considered in some computational models, the main goal was to artificially alter the link between the accommodation and vergence systems to study the change in pre-task and post-task measures of AC/A and CA/C [Eadie et al. 2000], or to investigate developmental plasticity in children exposed to S3D games [Rushton and Riddell 1999]. In this work, we propose a simple data-driven model of eye vergence that is tuned to step-like disparity changes. We emphasize here on vergence dynamics as a function of the initial and target disparities, and our goal is minimization of the vergence adaptation time at scene cuts through disparity editing. To solve this problem, disparity adjustment techniques are required. However, only few techniques can deal with temporal effects. The improvement in our case comes from more informed choice of the initial and target disparities. The artist?s involvement is still needed, though, to design the transitions manually, without any feedback on human abilities to adapt to rapid disparity changes. However, their primary goal was to maintain the scene disparity range within given limits. This is not equivalent to minimizing the vergence adaptation time, which depends not only on disparity difference but also the initial disparity value. In order to measure the vergence responses, both eyes were tracked using an EyeLink 1000 Plus eye tracker with a desktop mount. Vergence was calculated as the difference between the x-coordinates of the two gaze positions expressed in pixels. , blinks, saccades, or tracking errors) were interpolated linearly, and the segments that required interpolation of more than 50% samples were excluded. Next, for each type of a transition, the time to reach 95% of the required vergence change was determined, and two surfaces were fitted to the obtained data points. Since we were interested in relative gaze positions, the significance of drift was low. Moreover, adaptation times were determined by the 95%-of-change position, which is not very sensitive to shifts, scaling, etc. In order to gain insight into the relation of vergence response to the initial and end disparities, as well as to estimate the number of trials m necessary for the response curves to converge, we conducted a pilot study. On the other hand, it is unclear how well the model predicts response to cuts between natural images: the presence of complex luminance patterns or high-level processes related to scene understanding may very well influence the transition times. Therefore, we conducted a validation experiment, to test if the model can be generalized. It is also possible that the transition was facilitated to some extent by the learning effect. In order to evaluate stereoscopic transition and estimate transition time, we first need to determine the pairs of disparity values between which the transitions occur. This can be obtained directly from the rendering pipeline for animated movies, using user input in the case of 2D-to-3D conversion, or using disparity estimation techniques for natural scenes when the depth map is not available. , from cut to cut), or selectively around the cuts, with smooth blending back to original parameters [Lang et al. 2010; Koppal et al. 2011]. All such manipulations can be easily integrated and used with our model. The disparity difference between the circles was 2 px. In order to determine the arrangement recognition time for all 14 steps, we performed 14 independent QUEST threshold estimation procedures [Watson and Pelli 1983], each estimating time of 75% correctness. To our knowledge, this is the first work that proposes to automatically edit stereoscopic cuts taking into account varying performance of the human visual system in adapting to rapid disparity changes. We would like to thank Aude Oliva, Lavanya Sharan, Zoya Bylinskii, Sylvain Paris, YiChang Shih, Tobias Ritschel, Katarina Struckmann, David Levin, and the Anonymous Subjects who took part in our perceptual studies.",
  "resources" : [ ]
}