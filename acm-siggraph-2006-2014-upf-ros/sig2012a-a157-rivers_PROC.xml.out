{
  "uri" : "sig2012a-a157-rivers_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012a/a157-rivers_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Sculpting by Numbers",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Alec R.-Rivers",
      "name" : "Alec R.",
      "surname" : "Rivers"
    }, {
      "uri" : "http://drinventor/Andrew-Adams",
      "name" : "Andrew",
      "surname" : "Adams"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "we", "propose", "method", "allow", "unskilled", "user", "create", "accurate", "physical", "replica", "digital", "3d", "model", "analogy", "can", "make", "task", "reproduce", "2d", "painting", "when", "give", "outline", "need", "only", "fill", "child?s", "color", "book", "paint-by-numbers", "kit", "even", "unskilled", "user", "can", "accurately", "reproduce", "complex", "painting", "challenge", "lie", "place", "paint", "canvas", "know", "where", "place", "motivate", "observation", "we", "present", "sculpt", "number", "method", "provide", "analogous", "guidance", "creation", "3d", "object", "which", "assist", "user", "make", "object", "precisely", "match", "shape", "target", "3d", "model", "scan", "shape", "compare", "target", "3d", "model", "projector", "annotate", "object", "color", "indicate", "how", "object", "ought", "change", "match", "target", "we", "further", "show", "how", "scene", "can", "repeatedly", "deform", "match", "sequence", "target", "3d", "model", "purpose", "stop-motion", "animation", "finally", "we", "present", "result", "user", "testing", "novice", "sculptor", "advent", "digital", "fabrication", "technology", "3d", "printer", "laser", "cutter", "CNC", "machine", "have", "make", "possible", "automatically", "produce", "2d", "3d", "object", "main", "tool", "use", "3d", "digital", "fabrication", "3d", "printer", "5-axis", "cnc", "machine", "Devices", "both", "kind", "typically", "expensive", "though", "recent", "effort", "DIY", "community", "have", "make", "low-priced", "entry-level", "tool", "available", "-lsb-", "Hokanson", "Reilly", "Kelly", "MakerBot", "Industries", "Drumm", "2011", "Sells", "et", "al.", "2009", "-rsb-", "other", "incorporate", "fabrication", "consideration", "directly", "modeling", "process", "so", "fabrication", "straightforward", "design", "Kilian", "et", "al.", "-lsb-", "2008", "-rsb-", "propose", "method", "design", "shape", "can", "produce", "folding", "single", "sheet", "material", "while", "Mori", "Igarashi", "-lsb-", "2007", "-rsb-", "propose", "interface", "design", "stuff", "animal", "we", "approach", "aim", "address", "limitation", "both", "manual", "craft", "current", "digital", "fabrication", "technology", "combine", "element", "each", "hybrid", "approach", "interestingly", "approach", "typically", "human", "do", "planning", "machine", "do", "execution", "while", "we", "approach", "reverse", "Rivers", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "hybrid", "approach", "digital", "fabrication", "2d", "shape", "which", "user", "coarsely", "position", "tool", "can", "automatically", "adjust", "its", "own", "position", "keep", "exactly", "digital", "plan", "we", "approach", "share", "philosophical", "goal", "enable", "digitallyguided", "manual", "fabrication", "apply", "3d", "shape", "work", "just", "guide", "user", "without", "automatic", "error", "correction", "guidance", "include", "high-level", "sketch", "principle", "each", "part", "face", "well", "specific", "feedback", "how", "adjust", "freehand", "drawing", "more", "closely", "match", "shape", "reference", "image", "Sheng", "et", "al.", "-lsb-", "2006", "-rsb-", "present", "system", "which", "user", "whose", "finger", "be", "be", "track", "could", "manipulate", "deformable", "physical", "prop", "perform", "sculpt", "operation", "virtual", "3d", "model", "goal", "provide", "more", "natural", "hands-on", "modeling", "experience", "we", "approach", "guide", "sculpting", "involve", "scanning", "work", "progress", "project", "guidance", "annotate", "physical", "object", "one", "oldest", "most", "robust", "method", "involve", "project", "sequence", "Gray", "code", "imaging", "result", "retrieve", "depth", "information", "-lsb-", "Inokuchi", "et", "al.", "1984", "Caspi", "et", "al.", "1998", "Rusinkiewicz", "et", "al.", "2002", "-rsb-", "method", "simple", "implement", "robust", "able", "capture", "full", "depth", "field", "quickly", "core", "operation", "guide", "sculpting", "create", "physical", "object", "match", "target", "3d", "model", "do", "project", "different", "form", "guidance", "onto", "object", "illustrate", "how", "must", "change", "match", "target", "shape", "all", "adjustment", "model", "do", "manually", "which", "naturally", "support", "wide", "range", "material", "shape", "guidance", "make", "possible", "achieve", "complex", "precise", "shape", "we", "achieve", "both", "goal", "same", "hardware", "use", "projector-camera", "pair", "perform", "structured", "light", "scanning", "Gray", "code", "-lrb-", "e.g.", "-lsb-", "Caspi", "et", "al.", "1998", "-rsb-", "-rrb-", "we", "require", "user", "explicitly", "request", "each", "new", "scan", "which", "have", "two", "advantage", "guidance", "do", "change", "unexpectedly", "user?s", "hand", "scan", "confuse", "target", "object", "input", "scanning", "process", "target", "3d", "model", "which", "may", "mesh", "point", "cloud", "virtually", "position", "within", "work", "volume", "block", "raw", "material", "-lrb-", "e.g.", "clay", "-rrb-", "place", "roughly", "same", "real", "position", "Whenever", "user", "initiate", "scan", "we", "project", "sequence", "Gray", "code", "onto", "scene", "photograph", "scene", "illuminate", "each", "code", "image", "give", "we", "correspondence", "from", "camera", "coordinate", "projector", "coordinate", "we", "filter", "out", "invalid", "implausible", "correspondence", "invert", "map", "compute", "correspondence", "from", "projector", "coordinate", "camera", "coordinate", "from", "we", "triangulate", "compute", "depth", "map", "from", "projector?s", "perspective", "we", "also", "compute", "depth", "map", "target", "model", "render", "from", "same", "perspective", "finally", "we", "compare", "depth", "map", "compute", "guidance", "we", "project", "onto", "object", "we", "describe", "each", "operation", "detail", "below", "obtain", "pixel", "correspondence", "gray-code-based", "structured", "light", "scanning", "operate", "project", "sequence", "black", "white", "horizontal", "vertical", "bar", "onto", "scene", "record", "result", "image", "more", "difficult", "detect", "case", "which", "camera", "pixel", "record", "specular", "highlight", "reflection", "light", "cast", "projector", "we", "able", "reject", "majority", "correspondence", "consistency", "check", "give", "homography", "camera", "projector", "we", "can", "determine", "world-space", "ray", "correspond", "each", "camera", "projector", "pixel", "ray", "ought", "intersect", "within", "work", "volume", "case", "true", "correspondence", "put", "another", "way", "scan", "scan", "epipolar", "constraint", "combine", "overconstrain", "pixel", "correspondence", "which", "let", "we", "reject", "invalid", "pixel", "constraint", "also", "help", "eliminate", "spurious", "correspondence", "detect", "due", "noise", "final", "source", "inaccuracy", "can", "arise", "when", "narrow", "alternate", "black", "white", "column", "fine-scale", "Gray", "code", "pattern", "blur", "average", "gray", "due", "combination", "camera", "projector", "defocus", "we", "case", "work", "volume", "often", "close", "camera", "projector", "so", "depths", "field", "relatively", "shallow", "loss", "finest", "Gray", "code", "pattern", "result", "lose", "least-significant", "bit", "projector-camera", "correspondence", "when", "coarsely", "shape", "object", "loss", "resolution", "do", "matter", "towards", "end", "sculpt", "process", "become", "important", "we", "therefore", "allow", "user", "perform", "slightly", "more", "time-consuming", "precise", "scan", "inspire", "laser", "plane", "sweep", "scanning", "-lrb-", "e.g.", "-lsb-", "Curless", "Levoy", "1995", "-rsb-", "-rrb-", "simple", "implementation", "we", "could", "project", "single", "white", "row", "column", "sweep", "across", "entire", "projector", "image", "every", "camera", "pixel", "simply", "record", "which", "code", "illumination", "highest", "method", "robust", "defocus", "slow", "we", "sweep", "one-pixel-wide", "vertical", "hor", "izontal", "line", "recur", "every", "row", "column", "across", "all", "position", "record", "position", "maximum", "illumination", "every", "camera", "pixel", "correct", "three", "least-significant", "bit", "correspondence", "precise", "scan", "take", "about", "seconds", "give", "we", "depth", "resolution", "under", "1mm", "center", "work", "volume", "Computing", "depth", "map", "we", "display", "guidance", "use", "projector", "we", "need", "compute", "depth", "map", "from", "projector?s", "point", "view", "rather", "than", "camera?s", "we", "do", "so", "splat", "each", "valid", "camera", "pixel", "contribute", "kernel", "exponential", "falloff", "about", "corresponding", "projector", "pixel", "contribution", "sum", "homogeneous", "coordinate", "normalize", "produce", "camera", "coordinate", "some", "subset", "projector", "pixel", "kernel", "size", "choose", "enough", "interpolate", "across", "small", "hole", "depth", "map", "which", "arise", "from", "projector", "pixel", "see", "any", "camera", "pixel", "give", "resampled", "correspondence", "homography", "both", "camera", "projector", "we", "can", "triangulate", "compute", "depth", "value", "each", "projector", "pixel", "meanwhile", "depth", "map", "target", "model", "compute", "render", "from", "projector?s", "point", "view", "use", "projector?s", "calibrate", "homography", "opengl", "projection", "matrix", "depth", "map", "compare", "compute", "guidance", "we", "project", "onto", "material", "hardware", "setup", "we", "use", "3M", "MP160", "projector", "run", "800", "600", "60hz", "Nokia", "n9", "camera-phone", "use", "FCam", "API", "-lsb-", "Adams", "et", "al.", "2010", "-rsb-", "generate", "raw", "datum", "1632", "1232", "13hz", "-lrb-", "75m", "exposure", "time", "-rrb-", "FCam", "API", "make", "easier", "we", "synchronize", "camera", "projector", "raw", "stream", "prevent", "we", "from", "lose", "resolution", "demosaicing", "Gray", "code", "do", "rely", "color", "spatial", "coherence", "so", "we", "can", "treat", "each", "pixel", "independently", "ignore", "Bayer", "mosaic", "separation", "between", "camera", "projector", "place", "roughly", "from", "center", "work", "area", "we", "compute", "world-space", "homography", "camera", "projector", "scan", "known", "planar", "calibration", "target", "place", "two", "known", "depths", "work", "volume", "each", "form", "useful", "recreate", "different", "aspect", "model?s", "appearance", "user", "can", "also", "view", "rendering", "target", "3d", "model", "screen", "computer", "run", "system", "depth", "guidance", "use", "achieve", "correct", "surface", "position", "absolute", "3d", "space", "mode", "projector", "pixel", "color", "base", "difference", "between", "scan", "depth", "pixel", "target", "depth", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "Green", "indicate", "area", "where", "user", "must", "bring", "surface", "closer", "projector", "red", "indicate", "area", "where", "user", "must", "move", "surface", "farther", "away", "pixel", "do", "intersect", "target", "model", "color", "blue", "range", "scale", "user-tunable", "user", "can", "also", "probe", "exact", "depth", "error", "millimeter", "particular", "point", "mouse", "over", "pixel", "laptop", "control", "projector", "Edge", "guidance", "depend", "only", "target", "3d", "model", "use", "help", "user", "match", "high-frequency", "surface", "detail", "target", "3d", "model", "mode", "projector", "pixel", "color", "accord", "second", "derivative", "target", "depth", "map", "-lrb-", "i.e.", "ridge", "valley", "show", "figure", "-lrb-", "-rrb-", "-rrb-", "color", "follow", "same", "scale", "depth", "feedback", "mode", "its", "range", "can", "similarly", "tune", "method", "describe", "so", "far", "assist", "user", "achieve", "correct", "shape", "from", "single", "point", "view", "while", "automatic", "method", "-lrb-", "e.g.", "icp", "-lsb-", "Zhang", "1994", "-rsb-", "-rrb-", "could", "employ", "deal", "we", "handle", "more", "simply", "sculpt", "atop", "Lego", "stage", "which", "can", "accurately", "rotate", "90", "degree", "around", "known", "axis", "use", "we", "system", "practice", "involve", "interplay", "computer", "guidance", "human", "intuition", "which", "both", "precision", "computer", "intuition", "artist", "require", "create", "object", "correct", "proportion", "aesthetic", "quality", "however", "process", "also", "rely", "user?s", "understanding", "shape", "one", "simple", "reason", "guidance", "must", "interpret", "internalize", "both", "block", "user?s", "hand", "tool", "slowly", "invalidate", "user", "work", "model", "between", "scan", "thus", "user", "may", "interpret", "project", "guidance", "liberally", "perform", "freehand", "adjustment", "recreate", "salient", "detail", "can", "both", "speed", "creation", "major", "feature", "enable", "reproduction", "fine-detail", "surface", "feature", "too", "small", "individually", "highlight", "reproduce", "use", "system", "we", "describe", "how", "process", "play", "out", "basic", "case", "make", "shape", "match", "target", "3d", "model", "go", "explore", "additional", "application", "we", "find", "use", "we", "system", "fall", "fairly", "consistent", "pattern", "typical", "modeling", "session", "begin", "depth", "guidance", "mode", "use", "user", "establish", "coarse", "shape", "silhouette", "model", "model", "approach", "target", "shape", "user", "make", "successively", "smaller", "adjustment", "object", "shorten", "red-to-green", "guidance", "range", "necessary", "highlight", "necessary", "adjustment", "eventually", "user", "have", "shape", "match", "target", "3d", "model", "absolute", "coordinate", "limit", "user?s", "ability", "model", "system?s", "ability", "scan", "even", "after", "model", "have", "be", "make", "match", "target", "3d", "shape", "closely", "possible", "depth", "guidance", "mode", "surface", "may", "still", "lack", "high-frequency", "surface", "detail", "wrinkle", "rough", "surface", "hair", "mode", "can", "see", "location", "ridge", "valley", "other", "high-frequency", "detail", "project", "onto", "surface", "use", "those", "start", "point", "artistic", "pass", "surface", "user", "usually", "want", "shape", "object", "from", "all", "side", "rather", "than", "form", "model", "perfectly", "from", "one", "side", "rotate", "generally", "easier", "form", "material", "match", "target", "model", "roughly", "from", "all", "side", "first", "make", "successively", "higherdetail", "pass", "from", "each", "direction", "result", "novice", "sculptor", "use", "we", "system", "can", "see", "figure", "one", "unexpected", "possibility", "allow", "we", "approach", "collaborative", "sculpting", "because", "computer", "guidance", "provide", "common", "goal", "work", "towards", "possible", "multiple", "user", "work", "sculpture", "same", "time", "both", "time-saving", "highly", "enjoyable", "we", "approach", "can", "also", "use", "simple", "extension", "allow", "user", "create", "object", "just", "from", "digital", "3d", "mesh", "also", "from", "already-existing", "physical", "object", "do", "user", "place", "object", "front", "projector-camera", "pair", "scan", "use", "method", "above", "rotate", "scanning", "from", "all", "four", "direction", "recover", "point", "cloud", "save", "addition", "we", "allow", "user", "change", "scale", "material", "reproduction", "relative", "original", "illustrate", "flexibility", "process", "we", "scan", "toy", "car", "use", "we", "system", "recreate", "scaling", "factor", "2.5", "cake", "consist", "mixture", "edible", "material", "-lrb-", "figure", "-rrb-", "timelapse", "process", "can", "see", "video", "accompany", "paper", "complete", "cake", "eat", "enjoy", "we", "approach", "also", "adapt", "well", "be", "use", "guide", "stopmotion", "animation", "load", "series", "target", "instead", "just", "one", "allow", "user", "flip", "between", "target", "become", "possible", "generate", "series", "snapshot", "physical", "object", "correspond", "desire", "3d", "animation", "while", "skilled", "sculptor", "can", "already", "achieve", "what", "we", "system", "allow", "its", "basic", "form", "application", "provide", "capability", "otherwise", "extremely", "difficult", "even", "expert", "artist", "ability", "deform", "physical", "object", "sequence", "shape", "animation", "form", "concatenate", "image", "those", "shape", "appear", "physically", "correct", "we", "show", "thumbnail", "from", "result", "animation", "Figure", "full", "animation", "can", "view", "we", "accompany", "video", "we", "feel", "we", "approach", "make", "possible", "user", "prior", "experience", "sculpt", "create", "aesthetically-pleasing", "3d", "sculpture", "test", "we", "ask", "six", "first-time", "user", "reproduce", "3d", "model", "Stanford", "bunny", "out", "modeling", "clay", "each", "user", "ask", "make", "one", "attempt", "freehand", "refer", "rendering", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "one", "attempt", "use", "we", "system", "after", "10", "minute", "training", "user", "be", "give", "30", "minute", "each", "attempt", "save", "time", "we", "request", "only", "front", "bunny", "need", "look", "correct", "half", "user", "start", "freehand", "attempt", "half", "we", "method", "result", "show", "Figure", "user", "comment", "system", "greatly", "improve", "ability", "achieve", "correct", "proportion", "dynamic", "sculpting", "process", "also", "change", "when", "sculpt", "freehand", "user", "typically", "feel", "finish", "after", "30", "minute", "acknowledge", "proportion", "weren?t", "quite", "right", "also", "weren?t", "sure", "what", "do", "fix", "they", "use", "we", "method", "slower", "always", "provide", "clear", "direction", "which", "proceed", "after", "30", "minute", "we", "system", "user", "feel", "could", "have", "continue", "productively", "improve", "model", "sculpture", "make", "we", "method", "be", "generally", "better", "proportion", "than", "freehand", "sculpture", "only", "one", "user", "do", "benefit", "from", "system", "-lrb-", "show", "Figure", "column", "-rrb-", "user?s", "freehand", "model", "very", "good", "while", "able", "make", "continual", "progress", "use", "we", "method", "user", "able", "achieve", "same", "quality", "same", "amount", "time", "when", "sculptor", "have", "enough", "skill", "innately", "know", "how", "adjust", "model", "automatic", "guidance", "just", "slow", "they", "down", "we", "have", "demonstrate", "new", "approach", "make", "physical", "object", "match", "target", "3d", "shape", "which", "unskilled", "user", "assist", "guidance", "project", "onto", "object", "be", "make", "we", "have", "find", "we", "system", "greatly", "increase", "what", "novice", "sculptor", "capable", "-lrb-", "e.g.", "figure", "-rrb-", "addition", "produce", "physical", "replica", "3d", "model", "we", "approach", "can", "use", "enable", "reproduction", "exist", "physical", "object", "potentially", "different", "scale", "out", "different", "material", "we", "further", "demonstrate", "how", "we", "approach", "can", "use", "retarget", "real-world", "performance", "capture", "datum", "generate", "stop-motion", "animation", "physically", "correct", "dynamics", "we", "believe", "combination", "human", "actuation", "situate", "feedback", "from", "computer-accurate", "measurement", "have", "great", "potential", "author", "wish", "thank", "Abe", "Davis", "Elena", "Adams", "user", "study", "participant", "work", "partially", "fund", "NSF", "grant", "0964004", "Foxconn", "gift", "from", "Cognex" ],
  "content" : "We propose a method that allows an unskilled user to create an accurate physical replica of a digital 3D model. An analogy can be made to the task of reproducing a 2D painting: when given outlines that need only be filled in, as in a child?s coloring book or a paint-by-numbers kit, even an unskilled user can accurately reproduce a complex painting; the challenge lies not in placing paint on the canvas but in knowing where to place it. Motivated by this observation, we present Sculpting by Numbers, a method to provide analogous guidance for the creation of 3D objects, which assists a user in making an object that precisely matches the shape of a target 3D model. The scanned shape is compared with the target 3D model, and the projector then annotates the object with colors that indicate how the object ought to be changed to match the target. We further show how a scene can be repeatedly deformed to match a sequence of target 3D models for the purpose of stop-motion animation. Finally, we present results of user testing with novice sculptors. The advent of digital fabrication technologies, such as 3D printers, laser cutters, and CNC machines, has made it possible to automatically produce 2D and 3D objects. The main tools used in 3D digital fabrication are 3D printers and 5-axis CNC machines. Devices of both kinds are typically expensive, though recent efforts in the DIY community have made low-priced entry-level tools available [Hokanson and Reilly ; Kelly ; MakerBot Industries ; Drumm 2011 ; Sells et al. 2009]. Others incorporate fabrication considerations directly into the modeling process, so that fabrication is straightforward by design: Kilian et al. [2008] propose a method for designing shapes that can be produced by folding a single sheet of material, while Mori and Igarashi [2007] propose an interface for designing stuffed animals. Our approach aims to address the limitations of both manual crafting and current digital fabrication technologies by combining elements of each in a hybrid approach. Interestingly, in these approaches, it is typically the human that does the planning and the machine that does the execution, while in our approach it is the reverse. Rivers et al. [2012] propose a hybrid approach to digital fabrication of 2D shapes in which a user coarsely positions a tool that can then automatically adjust its own position to keep it exactly on a digital plan; our approach shares the philosophical goal of enabling digitallyguided manual fabrication, but applies to 3D shapes, and works by just guiding a user without automatic error correction. The guidance included high-level sketching principles for each part of the face as well as specific feedback on how to adjust the freehand drawings to more closely match the shape of the reference image. Sheng et al. [2006] presented a system in which a user whose fingers were being tracked could manipulate deformable physical props to perform sculpting operations on a virtual 3D model, with the goal of providing a more natural, hands-on modeling experience. Our approach to guided sculpting involves scanning the work in progress and projecting guidance that annotates the physical object. One of the oldest and most robust methods involves projecting a sequence of Gray codes and imaging the result to retrieve depth information [Inokuchi et al. 1984; Caspi et al. 1998; Rusinkiewicz et al. 2002]. This method is simple to implement, robust, and able to capture a full depth field quickly. The core operation in guided sculpting is creating a physical object that matches a target 3D model. This is done by projecting different forms of guidance onto the object that illustrate how it must be changed to match a target shape. All adjustments to the model are done manually, which naturally supports a wide range of materials and shapes. The guidance makes it possible to achieve complex and precise shapes. We achieve both goals with the same hardware by using a projector-camera pair to perform structured light scanning with Gray codes (e.g. [Caspi et al. 1998]). We require the user to explicitly request each new scan, which has two advantages: the guidance does not change unexpectedly, and the user?s hands are not scanned and confused with the target object. The input to the scanning process is a target 3D model, which may be a mesh or a point cloud. It is virtually positioned within the working volume, and a block of raw material (e.g. clay) is placed at roughly the same real position. Whenever the user initiates a scan, we project a sequence of Gray codes onto the scene and photograph the scene illuminated by each code. These images give us a correspondence from camera coordinates to projector coordinates. We filter out invalid or implausible correspondences, and then invert this map to compute a correspondence from projector coordinates to camera coordinates. From this we triangulate to compute a depth map from the projector?s perspective. We also compute a depth map of the target model rendered from the same perspective. Finally, we compare the depth maps to compute the guidance that we project onto the object. We describe each of these operations in detail below. Obtaining pixel correspondences Gray-code-based structured light scanning operates by projecting a sequence of black and white horizontal and vertical bars onto a scene and recording the resulting images. More difficult to detect are cases in which a camera pixel records specular highlights or reflections of light cast by the projector. We are able to reject the majority of these correspondences with a consistency check. Given the homographies for the camera and projector, we can determine the world-space ray that corresponds to each camera and projector pixel. These rays ought to intersect within the working volume in the case of a true correspondence. Put another way, the scan in X, the scan in Y, and the epipolar constraint combine to overconstrain the pixel correspondence, which lets us reject invalid pixels. This constraint also helps eliminate spurious correspondences detected due to noise. A final source of inaccuracy can arise when narrow alternating black and white columns in a fine-scale Gray code pattern are blurred into an average gray due to the combination of camera and projector defocus. In our case, the working volume is often close to the camera and projector, so their depths of field are relatively shallow. Loss of the finest Gray code patterns results in losing the least-significant bits of the projector-camera correspondence. When coarsely shaping an object this loss of resolution does not matter, but towards the end of the sculpting process it becomes important. We therefore allow the user to perform a slightly more time-consuming ?precise? scan inspired by laser plane sweep scanning (e.g. [Curless and Levoy 1995]). In a simple implementation, we could project a single white row or column that sweeps across the entire projector image, and for every camera pixel simply record for which code the illumination was highest. This method is robust to defocus, but slow. We then sweep one-pixel-wide vertical or hor- izontal lines that recur every 8 rows or columns across all 8 positions, and record the position of maximum illumination for every camera pixel to correct the three least-significant bits of the correspondence. This precise scan takes about 7 seconds, and gives us a depth resolution of under 1mm at the center of the working volume. Computing depth maps As we will be displaying guidance using the projector, we need to compute the depth map from the projector?s point of view rather than the camera?s. We do so by splatting: each valid camera pixel contributes to a 7 ? 7 kernel with exponential falloff about the corresponding projector pixel. The contributions are summed in homogeneous coordinates and then normalized to produce a camera coordinate for some subset of projector pixels. The kernel size chosen is enough to interpolate across small holes in the depth map, which arise from projector pixels not seen by any camera pixel. Given these resampled correspondences and the homographies for both camera and projector, we can triangulate to compute depth values for each projector pixel. Meanwhile, the depth map for the target model is computed by rendering it from the projector?s point of view using the projector?s calibrated homography as the OpenGL projection matrix. These depth maps are then compared to compute the guidance that we project onto the material. Hardware setup We use a 3M MP160 projector running at 800?600 at 60Hz and a Nokia N9 camera-phone using the FCam API [Adams et al. 2010] generating raw data at 1632?1232 at 13Hz (75ms exposure time). The FCam API makes it easier for us to synchronize the camera with the projector, and the raw stream prevents us from losing resolution to demosaicing. Gray codes do not rely on color or spatial coherence, so we can treat each pixel independently and ignore the Bayer mosaic. The separation between camera and projector is 5?, and they placed roughly 2? from the center of the work area. We compute world-space homographies for the camera and projector by scanning a known planar calibration target placed at two known depths in the work volume. Each form is useful for recreating a different aspect of the model?s appearance. The user can also view a rendering of the target 3D model on the screen of the computer running the system. Depth guidance is used to achieve the correct surface position in absolute 3D space. In the this mode, projector pixels are colored based on the difference between the scanned depth at that pixel and the target depth ( Figure 2 (c)). Green indicates areas where the user must bring the surface closer to the projector and red indicates areas where the user must move the surface farther away. Pixels that do not intersect the target model are colored blue. The range of the scale is user-tunable. The user can also probe the exact depth error in millimeters at a particular point by mousing over that pixel on the laptop controlling the projector. Edge guidance depends only on the target 3D model, and is used to help the user match the high-frequency surface details of the target 3D model. In this mode, projector pixels are colored according to the second derivative of the target depth map (i.e. ridges and valleys, as shown in Figure 2 (d)). This color follows the same scale as the depth feedback mode, and its range can be similarly tuned. The method described so far assists a user in achieving the correct shape from a single point of view. While automatic methods (e.g. ICP [Zhang 1994]) could be employed to deal with this, we handle this more simply by sculpting atop a Lego stage, which can be accurately rotated by 90 degrees around a known axis. Using our system in practice involves an interplay of computer guidance and human intuition, in which both the precision of the computer and the intuition of the artist are required to create an object with the correct proportions and aesthetic qualities. However, the process also relies on the user?s understanding of the shape. One simple reason is that the guidance must be interpreted and internalized, as it will both be blocked by the user?s hands and tools, and is slowly invalidated as the user works on the model between scans. Thus, a user may interpret the projected guidance liberally, and perform freehand adjustments to recreate salient details. This can both speed the creation of major features and enable the reproduction of fine-detail surface features that are too small to be individually highlighted and reproduced using the system. We describe how this process plays out for the basic case of making a shape that matches a target 3D model, and go on to explore additional applications. We found that use of our system fell into a fairly consistent pattern. A typical modeling session will begin in depth guidance mode. Using this the user will establish the coarse shape and silhouette of the model. As the model approaches the target shape, the user will make successively smaller adjustments to the object, shortening the red-to-green guidance range as necessary to highlight the necessary adjustments. Eventually, the user will have a shape that matches the target 3D model in absolute coordinates to the limit of the user?s ability to model and the system?s ability to scan. Even after the model has been made to match the target 3D shape as closely as possible in depth guidance mode, the surface may still lack high-frequency surface details, such as wrinkles or the rough surface of hair. In this mode, they can see the locations of ridges, valleys, and other high-frequency details projected onto the surface, and use those as a starting point for an artistic pass on the surface. The user will usually want to shape an object from all sides. Rather than forming the model perfectly from one side and then rotating, it is generally easier to form the material to match the target model roughly from all sides first, and then make successively higherdetail passes from each direction. Results of novice sculptors using our system can be seen in Figures 1 and 3. One unexpected possibility allowed by our approach is collaborative sculpting. Because the computer guidance provides a common goal to work towards, it is possible for multiple users to work on a sculpture at the same time. This is both time-saving and highly enjoyable. Our approach can also be used with a simple extension to allow the user to create an object not just from a digital 3D mesh, but also from an already-existing physical object. To do this, the user places an object in front of the projector-camera pair and scans it using the method above, rotating it and scanning it from all four directions. The recovered point clouds are saved. In addition, we allow the user to change the scale or the materials of the reproduction relative to the original. To illustrate the flexibility of this process, we scanned a toy car and used our system to recreate it at a scaling factor of 2.5? as a cake consisting of a mixture of edible materials ( Figure 4 ). A timelapse of the process can be seen in the video that accompanies this paper. The completed cake was eaten and enjoyed. Our approach also adapts well to being used as a guide for stopmotion animation. By loading a series of targets, instead of just one, and allowing the user to flip between these targets, it becomes  possible to generate a series of snapshots of a physical object that correspond to a desired 3D animation. While a skilled sculptor can already achieve what our system allows in its basic form, this application provides a capability that is otherwise extremely difficult even for expert artists. This is the ability to deform a physical object into a sequence of shapes such that the animation formed by concatenating images of those shapes appears physically correct. We show thumbnails from the resulting animation in Figure 5 , and the full animation can be viewed in our accompanying video. We felt that our approach made it possible for users with no prior experience sculpting to create aesthetically-pleasing 3D sculptures. To test this, we asked six first-time users to reproduce a 3D model  of the Stanford bunny out of modeling clay. Each user was asked to make one attempt freehand, referring to a rendering ( Figure 1(a) ), and one attempt using our system, after a 10 minute training period. Users were given 30 minutes for each attempt. To save time, we requested that only the front of the bunny need look correct. Half of the users started with the freehand attempt, and half with our method. The results are shown in Figure 6 . Users commented that the system greatly improved their ability to achieve the correct proportions. The dynamic of the sculpting process was also changed. When sculpting freehand, users typically felt finished after 30 minutes; they acknowledged that the proportions weren?t quite right, but also that they weren?t sure what to do to fix them. Using our method was slower, but always provided a clear direction in which to proceed; after 30 minutes with our system, users felt that they could have continued to productively improve the model. Sculptures made with our method were generally better proportioned than the freehand sculptures. Only one user did not benefit from the system (shown in Figure 6 column 3). This user?s freehand model was very good. While able to make continual progress using our method, this user was not able to achieve the same quality in the same amount of time. When a sculptor has enough skill to innately know how to adjust a model, automatic guidance just slows them down. We have demonstrated a new approach to making a physical object that matches a target 3D shape in which an unskilled user is assisted by guidance projected onto the object as it is being made. We have found that our system greatly increases what novice sculptors are capable of (e.g. Figures 1, 3, 4, and 5). In addition to producing physical replicas of a 3D model, our approach can be used to enable reproduction of existing physical objects, potentially at a different scale and out of different materials. We further demonstrate how our approach can be used to retarget real-world performance capture data to generate stop-motion animation with physically correct dynamics. We believe that the combination of human actuation and situated feedback from computer-accurate measurements has great potential. The authors wish to thank Abe Davis, Elena Adams, and the user study participants. The work was partially funded by NSF grant 0964004, Foxconn, and a gift from Cognex.",
  "resources" : [ ]
}