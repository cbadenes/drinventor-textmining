{
  "uri" : "sig2009-a25-vergne_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2009/a25-vergne_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Light Warping for Enhanced Surface Depiction",
    "published" : "2009",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Romain-Vergne",
      "name" : "Romain",
      "surname" : "Vergne"
    }, {
      "uri" : "http://drinventor/Romain-Pacanowski",
      "name" : "Romain",
      "surname" : "Pacanowski"
    }, {
      "uri" : "http://drinventor/Pascal-Barla",
      "name" : "Pascal",
      "surname" : "Barla"
    }, {
      "uri" : "http://drinventor/Xavier-Granier",
      "name" : "Xavier",
      "surname" : "Granier"
    }, {
      "uri" : "http://drinventor/Christophe-Schlick",
      "name" : "Christophe",
      "surname" : "Schlick"
    } ]
  },
  "bagOfWords" : [ "recent", "research", "human", "visual", "system", "show", "we", "perception", "object", "shape", "rely", "part", "compression", "stretch", "reflect", "lighting", "environment", "onto", "its", "surface", "second", "we", "present", "new", "light", "warping", "approach", "locally", "deform", "lighting", "pattern", "reveal", "important", "surface", "feature", "practical", "side", "we", "approach", "also", "more", "flexible", "than", "previous", "technique", "require", "pre-process", "work", "arbitrary", "static", "dynamic", "input", "easily", "incorporate", "direct", "global", "illumination", "renderer", "add", "relatively", "small", "performance", "overhead", "enable", "real-time", "rendering", "direct", "illumination", "scenario", "abstraction", "provide", "line-based", "method", "interesting", "many", "respects", "because", "create", "legible", "picture", "economy", "means", "focus", "we", "paper", "precisely", "find", "alternative", "preserve", "material", "illumination", "information", "while", "still", "efficiently", "depict", "shape", "effect", "illustrate", "figure", "where", "curved", "surface", "contract", "wider", "region", "environment", "lighting", "than", "flat", "surface", "hence", "produce", "more", "compress", "pattern", "from", "particular", "point", "view", "we", "contribution", "reside", "viewcentered", "warping", "function", "deform", "incoming", "light", "direction", "around", "salient", "surface", "feature", "every", "stage", "we", "system", "perform", "real-time", "modern", "graphic", "hardware", "exception", "-lrb-", "optional", "-rrb-", "global", "illumination", "routine", "execute", "off-line", "make", "we", "system", "very", "flexible", "normal", "may", "either", "sample", "from", "3d", "surface", "-lrb-", "implicit", "surface", "mesh", "etc", "-rrb-", "read", "from", "image-based", "representation", "-lrb-", "e.g.", "rgbn", "image", "-lsb-", "Toler-Franklin", "et", "al.", "2007", "-rsb-", "normal", "map", "-rrb-", "explain", "process", "let", "we", "first", "study", "simple", "1d", "normal", "field", "show", "top", "row", "Figure", "middle", "bottom", "row", "show", "first", "second", "derivative", "depth", "field", "respectively", "foremost", "important", "feature", "identify", "silhouette", "crease", "represent", "discontinuity", "zerothand", "first-order", "derivative", "respectively", "second", "most", "important", "feature", "inflection", "point", "separate", "convex", "from", "concave", "region", "inflection", "point", "correspond", "extrema", "first-order", "derivative", "zero-crossing", "second-order", "derivative", "between", "silhouette", "crease", "inflection", "magnitude", "second-order", "derivative", "give", "information", "about", "surface", "curvature", "note", "we", "approach", "concavity", "correspond", "positive", "curvature", "convexity", "correspond", "negative", "curvature", "we", "use", "warm", "hue", "concavity", "cold", "hue", "convexity", "remainder", "paper", "previous", "work", "essentially", "make", "use", "object-centered", "measure", "main", "advantage", "view-centered", "curvature", "properly", "reflect", "surface", "foreshortening", "well", "size", "project", "feature", "cue", "likely", "take", "account", "hv", "note", "section", "addition", "view-centered", "approach", "have", "important", "number", "practical", "advantage", "because", "compute", "dynamically", "from", "current", "viewpoint", "analysis", "perform", "similarly", "2d", "over", "whole", "image", "follow", "we", "denote", "axis", "image", "space", "we", "only", "interested", "curvature", "-lrb-", "i.e.", "second-order", "information", "-rrb-", "require", "explicitly", "compute", "depth", "field", "indeed", "we", "denote", "normal", "point", "image", "space", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "relative", "depth", "-lrb-", "-rrb-", "direct", "relationship", "between", "gradient", "give", "depth", "gradient", "first-order", "derivative", "direction", "other", "word", "obtain", "directly", "from", "surface", "normal", "without", "have", "differentiate", "silhouette", "thus", "undefined", "forbid", "any", "differentiation", "across", "they", "make", "sense", "image", "neighborhood", "should", "restricted", "connected", "surface", "neighborhood", "hessian", "depth", "field", "compute", "differentiate", "gradient", "where", "first-order", "derivative", "direction", "other", "word", "obtain", "differentiate", "component", "crease", "where", "differentiable", "undefined", "hence", "differentiation", "must", "restricted", "accross", "crease", "well", "however", "we", "mostly", "consider", "smooth", "surface", "example", "give", "paper", "supplemental", "material", "curvature", "tensor", "symmetric", "matrix", "can", "easily", "rewrite", "follow", "dq", "where", "principal", "curvature", "correspond", "principal", "direction", "we", "local", "shape", "descriptor", "consist", "union", "visible", "silhouette", "crease", "surface", "point", "from", "which", "we", "get", "curvature", "information", "via", "h.", "all", "example", "give", "paper", "supplemental", "material", "we", "display", "silhouette", "crease", "black", "well", "concave", "convex", "region", "warm", "cold", "hue", "respectively", "major", "advantage", "use", "view-centered", "curvature", "tensor", "we", "descriptor", "confer", "automatic", "simplification", "behavior", "demonstrate", "Figure", "leave", "observe", "how", "surface", "feature", "naturally", "agglomerate", "together", "when", "object", "get", "away", "from", "viewpoint", "note", "behavior", "would", "lot", "more", "difficult", "obtain", "object-centered", "description", "instance", "require", "on-the-fly", "view-dependent", "mesh", "simplification", "more", "detailed", "comparison", "between", "objectand", "view-centered", "curvature", "find", "supplemental", "material", "another", "important", "advantage", "view-centered", "curvature", "tensor", "easily", "modify", "dynamically", "extract", "surface", "feature", "multiple", "scale", "do", "integrate", "-lrb-", "i.e.", "smoothing", "-rrb-", "over", "extended", "neighborhood", "image", "space", "reason", "we", "perform", "integration", "via", "anisotropic", "diffusion", "-lsb-", "Perona", "Malik", "1990", "-rsb-", "where", "refer", "scale", "-lrb-", "-rrb-", "conductance", "function", "equal", "silhouette", "crease", "otherwise", "produce", "blur", "gradient", "preserve", "silhouette", "crease", "blur", "curvature", "tensor", "obtain", "before", "-lrb-", "-rrb-", "do", "let", "user", "choose", "importance", "function", "-lrb-", "-rrb-", "control", "number", "iteration", "diffusion", "process", "few", "iteration", "lead", "fine", "detail", "important", "picture", "region", "any", "importance", "function", "could", "use", "we", "show", "example", "Figure", "right", "supplemental", "video", "practice", "we", "local", "shape", "descriptor", "compute", "per-pixel", "entirely", "GPU", "use", "multiple", "pass", "we", "take", "normal", "depth", "buffer", "input", "output", "we", "descriptor", "another", "buffer", "consist", "pixel-wise", "multi-scale", "hessian", "silhouette", "crease", "weight", "follow", "pseudo-code", "denote", "current", "pixel", "its", "pixel", "neighborhood", "algorithm", "multi-scale", "descriptor", "GPU", "-lrb-", "-rrb-", "Sobel", "Filter", "-lrb-", "Depth", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "Dihedral", "Angle", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "depth", "gradient", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lsb-", "-lrb-", "-rrb-", "-rsb-", "do", "-lrb-", "-rrb-", "anisotropic", "diffusion", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "end", "-lrb-", "-rrb-", "Sobel", "Filter", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "essentially", "five", "step", "algorithm", "silhouette", "-lrb-", "-rrb-", "compute", "per-pixel", "weight", "use", "Sobel", "filter", "depth", "we", "find", "approach", "more", "accurate", "coherent", "detect", "locius", "image", "point", "where", "practice", "crease", "-lrb-", "-rrb-", "compute", "per-pixel", "weight", "well", "use", "dihedral", "angle", "between", "neighbor", "normal", "multi-scale", "depth", "gradient", "obtain", "-lrb-", "-rrb-", "computing", "-lrb-", "4-6", "-rrb-", "discretize", "anisotropic", "diffusion", "equation", "iterative", "solver", "explain", "-lsb-", "Perona", "Malik", "1990", "-rsb-", "we", "use", "-lrb-", "-rrb-", "max", "-lrb-", "-rrb-", "conductance", "function", "finally", "hessian", "-lrb-", "-rrb-", "compute", "differentiate", "multi-scale", "gradient", "Sobel", "filter", "from", "practical", "point", "view", "we", "solution", "offer", "important", "advantage", "since", "only", "require", "per-pixel", "normal", "depths", "require", "pre-process", "work", "dynamic", "3d", "scene", "term", "performance", "output-sensitive", "its", "complexity", "linear", "number", "diffusion", "iteration", "next", "section", "we", "show", "how", "we", "make", "use", "information", "make", "available", "we", "local", "shape", "analysis", "novel", "light", "warp", "approach", "animated", "example", "glossy", "object", "show", "supplemental", "video", "we", "require", "warping", "function", "bijective", "mapping", "sphere", "direction", "so", "inverse", "warping", "function", "analytically", "define", "moreover", "note", "-lsb-", "Fleming", "et", "al.", "2009", "-rsb-", "compression", "reflect", "light", "pattern", "reflect", "anisotropy", "curvature", "define", "ratio", "principal", "curvature", "likely", "salient", "shape", "cue", "hv", "we", "design", "warping", "function", "so", "deform", "incoming", "illumination", "different", "way", "along", "principal", "curvature", "direction", "since", "we", "descriptor", "provide", "curvature", "information", "form", "symmetric", "tensor", "image", "space", "we", "also", "require", "we", "warping", "function", "symmetric", "respect", "leave", "invariant", "every", "light", "direction", "thus", "transform", "-lcb-", "-rcb-", "reference", "frame", "prior", "warping", "transform", "back", "afterwards", "Curvature", "light", "direction", "express", "same", "coordinate", "system", "though", "order", "establish", "correspondence", "between", "cartesian", "angular", "space", "we", "use", "stereographic", "projection", "image", "plane", "process", "illustrate", "figure", "-lrb-", "-rrb-", "light", "direction", "stereographically", "project", "plane", "give", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "warped", "accord", "curvature", "information", "yield", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "map", "back", "sphere", "direction", "give", "warped", "light", "direction", "-lrb-", "-rrb-", "give", "light", "direction", "-lrb-", "-rrb-", "stereographic", "projection", "define", "2l", "2l", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "direction", "project", "origin", "stereographic", "plane", "Intermediate", "light", "direction", "project", "further", "from", "origin", "get", "closer", "-lrb-", "-rrb-", "direction", "which", "project", "infinity", "warping", "function", "simply", "define", "curvature-dependent", "non-linear", "scaling", "stereographic", "plane", "-lrb-", "see", "Figure", "right", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "scaling", "factor", "compute", "map", "local", "curvature", "angular", "deviation", "sphere", "we", "implementation", "we", "use", "tan", "-lrb-", "arctan", "-lrb-", "??", "-rrb-", "-rrb-", "guarantee", "most", "one", "half", "lighting", "energy", "find", "one", "side", "hemisphere", "direction", "warped", "other", "one", "formulation", "user-defined", "parameter", "control", "amount", "warp", "perform", "accord", "curvature", "while", "anisotropy", "curvature", "naturally", "take", "account", "inverse", "stereographic", "projection", "give", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "2t", "-rrb-", "where", "-lrb-", "-rrb-", "describe", "parametric", "location", "intersection", "between", "sphere", "projection", "direction", "we", "concatenate", "operation", "single", "warping", "function", "yielding", "2t", "2t", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "2t", "-lrb-", "-rrb-", "note", "inverse", "warping", "function", "simply", "obtain", "use", "inverse", "-lrb-", "i.e.", "replace", "-rrb-", "final", "stage", "we", "system", "render", "3d", "object", "arbitrary", "material", "illumination", "while", "take", "account", "way", "environment", "lighting", "must", "warped", "each", "surface", "point", "we", "illustrate", "approach", "photorealistic", "well", "nonphotorealistic", "scenario", "both", "real-time", "off-line", "renderer", "we", "first", "reformulate", "reflect", "radiance", "equation", "take", "light", "warping", "account", "where", "surface", "point", "viewpoint", "direction", "incoming", "lighting", "direction", "sphere", "direction", "brdf", "warping", "function", "define", "section", "we", "clamp", "light", "direction", "-lrb-", "both", "original", "warped", "-rrb-", "hemisphere", "direction", "around", "way", "similar", "clamp", "do", "when", "use", "bump", "normal", "map", "discretization", "equation", "may", "raise", "performance", "quality", "issue", "though", "indeed", "common", "sample", "light", "source", "pre-process", "reduce", "noise", "result", "-lrb-", "e.g.", "kriv?nek", "Colbert", "-lsb-", "2008", "-rsb-", "-rrb-", "however", "since", "light", "warping", "different", "every", "point", "approach", "become", "intractable", "equation", "enable", "pre-sampling", "light", "source", "we", "re-write", "substitute", "-lrb-", "-rrb-", "where", "jacobian", "-lrb-", "see", "supplemental", "material", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "we", "implement", "light", "warp", "approach", "different", "renderer", "both", "case", "we", "use", "ashikmin?s", "brdf", "model", "-lsb-", "ashikhmin", "et", "al.", "2000", "-rsb-", "we", "real-time", "rendering", "system", "evaluate", "equation", "use", "pre-sampled", "environment", "light", "however", "avoid", "compute", "visibility", "information", "ignore", "indirect", "illumination", "Figure", "show", "how", "shape", "input", "3d", "object", "enhance", "two", "different", "illumination", "setting", "system", "note", "how", "enhancement", "remain", "coherent", "while", "pattern", "reflect", "lighting", "completely", "different", "each", "image", "indeed", "only", "cue", "we", "provide", "here", "deformation", "pattern", "additional", "real-time", "capture", "use", "render", "system", "show", "supplemental", "video", "we", "off-line", "rendering", "system", "we", "compute", "full", "global", "illumination", "result", "equation", "indirect", "lighting", "equation", "direct", "lighting", "use", "we", "path", "tracer", "-lsb-", "dutr", "et", "al.", "2006", "-rsb-", "apply", "light", "warp", "only", "first", "ray", "bounce", "we", "also", "implement", "warped", "ambient", "occlusion", "use", "diffuse", "material", "after", "be", "warped", "light", "ray", "have", "different", "visibility", "change", "visibility", "enhance", "shape", "best", "see", "supplemental", "result", "rendering", "show", "Figure", "where", "shape", "same", "input", "3d", "object", "enhance", "each", "configuration", "again", "surface", "feature", "enhance", "matter", "material", "characteristic", "we", "also", "experiment", "purely", "reflective", "refractive", "material", "show", "supplemental", "material", "complexity", "we", "light", "warp", "approach", "linear", "number", "sample", "light", "direction", "practice", "apply", "warping", "function", "negligible", "global", "illumination", "decrease", "frame", "rate", "50", "direct", "illumination", "however", "we", "still", "get", "real-time", "frame", "rate", "practice", "instance", "result", "Figure", "obtain", "37", "fp", "800", "600", "use", "54", "light", "finally", "we", "experiment", "non-photorealistic", "rendering", "technique", "use", "we", "light", "warp", "approach", "order", "exaggerate", "enhancement", "obtain", "light", "warping", "we", "incorporate", "curvature-dependent", "contrast", "enhancement", "exaggerated", "reflect", "radiance", "give", "where", "-lsb-", "-rsb-", "contrast", "parameter", "when", "both", "other", "case", "contrast", "increase", "depend", "curvature", "warping", "magnitude", "when", "apply", "object", "diffuse", "material", "minimal", "illumination", "method", "come", "close", "mean", "curvature", "shade", "technique", "-lsb-", "Kindlmann", "et", "al.", "2003", "-rsb-", "Figure", "show", "effect", "use", "equation", "we", "real-time", "renderer", "both", "natural", "minimal", "illumination", "we", "also", "apply", "stylized", "quantization", "algorithm", "-lsb-", "winnem?ller", "et", "al.", "2006", "-rsb-", "both", "rendering", "show", "how", "very", "same", "warped", "lighting", "able", "enhance", "stylized", "shading", "end", "result", "compelling", "cartoon", "style", "work", "arbitrary", "material", "illumination", "we", "local", "shape", "descriptor", "bear", "some", "similarity", "work", "Judd", "et", "al.", "-lsb-", "2007", "-rsb-", "Vergne", "et", "al.", "-lsb-", "2008", "-rsb-", "who", "have", "investigate", "view-dependent", "approach", "shape", "depiction", "past", "however", "provide", "only", "partial", "curvature", "information", "either", "maximum", "principal", "curvature", "-lsb-", "Judd", "et", "al.", "2007", "-rsb-", "blend", "between", "objectand", "view-centered", "curvature", "-lsb-", "Vergne", "et", "al.", "2008", "-rsb-", "former", "case", "limit", "method", "line-based", "rendering", "while", "latter", "case", "result", "objectionable", "artifact", "around", "silhouette", "we", "method", "considerably", "simpler", "require", "pre-process", "object", "space", "basically", "consist", "apply", "filter", "operation", "normal", "depth", "buffer", "picture", "plane", "when", "get", "close", "surface", "mesh", "we", "descriptor", "enhance", "geometry", "tesselation", "show", "inset", "image", "-lrb-", "zoom", "from", "Figure", "-rrb-", "may", "see", "limitation", "surprise", "normal", "only", "continuous", "across", "triangle", "edge", "due", "Phong", "interpolation", "simplest", "way", "address", "issue", "use", "dynamically", "subdivide", "mesh", "implicit", "surface", "instance", "we", "also", "believe", "adapt", "descriptor?s", "scale", "base", "surface", "depth", "could", "smooth", "area", "coarse", "tesselation", "light", "warp", "approach", "we", "introduce", "enhance", "surface", "feature", "arbitrary", "material", "illuminations", "style", "may", "produce", "result", "similar", "exaggerated", "shading", "3d", "unsharp", "masking", "show", "Figure", "10", "compare", "-lsb-", "Rusinkiewicz", "et", "al.", "2006", "-rsb-", "enhance", "surface", "shape", "much", "wider", "range", "material", "furthermore", "exaggerated", "shade", "suffer", "from", "light", "direction", "sensitivity", "tend", "flatten", "overall", "shape", "perception", "show", "supplemental", "video", "besides", "require", "time-consuming", "pre-process", "yet", "do", "incorporate", "automatic", "simplification", "behavior", "oppose", "we", "approach", "compare", "-lsb-", "Ritschel", "et", "al.", "2008", "-rsb-", "we", "system", "offer", "greater", "control", "enhance", "surface", "feature", "uniformly", "while", "3d", "unsharp", "masking", "increase", "indiscriminately", "radiance", "contrast", "result", "irregular", "enhancement", "alteration", "material", "property", "we", "approach", "also", "simpler", "control", "compare", "previous", "work", "offer", "intuitive", "parameter", "warp", "magnitude", "light", "contrast", "feature", "scale", "light", "warping", "technique", "show", "some", "limitation", "though", "first", "depend", "existence", "lighting", "variation", "scene", "appear", "relate", "statistics", "natural", "environment", "-lsb-", "Fleming", "et", "al.", "2009", "-rsb-", "practice", "always", "possible", "enhance", "surface", "shape", "use", "equation", "case", "where", "environment", "lighting", "have", "few", "variation", "second", "reach", "its", "limit", "pure", "reflection", "refraction", "object", "exhibit", "many", "surface", "detail", "because", "tend", "make", "picture", "less", "legible", "whole", "overall", "shape", "cast", "shadow", "may", "also", "distort", "favor", "depiction", "sur", "face", "feature", "better", "balance", "between", "surface", "shadow", "shape", "depiction", "might", "thus", "need", "moreover", "we", "warping", "function", "tend", "sharpen", "shade", "transition", "when", "push", "high", "value", "hence", "affect", "material", "perception", "finally", "warp", "increase", "noise", "off-line", "rendering", "add", "relatively", "small", "overhead", "real-time", "rendering", "extend", "we", "approach", "pre-computed", "warped", "radiance", "transfer", "would", "interesting", "solution", "increase", "performance", "we", "have", "present", "new", "approach", "surface", "shape", "enhancement", "call", "light", "warping", "preserve", "material", "illumination", "characteristic", "well", "stylistic", "choice", "also", "have", "number", "practical", "advantage", "over", "previous", "method", "flexibility", "respect", "input", "datum", "representation", "automatic", "well", "controllable", "levels-of-detail", "real-time", "rendering", "GPU", "moreover", "we", "present", "one", "way", "perform", "light", "warping", "stereographic", "space", "we", "would", "like", "investigate", "other", "potential", "function", "particular", "we", "could", "imagine", "make", "use", "additional", "information", "explicit", "description", "environment", "illumination", "we", "thank", "member", "IPARLA", "team", "Roland", "Fleming", "useful", "feedback", "Ma?tena", "Vives", "mention", "work", "Norman", "Rockwell", "work", "have", "be", "sponsor", "anr-08-jcjc-0078-01", "project" ],
  "content" : "Recent research on the human visual system shows that our perception of object shape relies in part on compression and stretching of the reflected lighting environment onto its surface. Second, we present a new light warping approach that locally deforms lighting patterns to reveal important surface features. On the practical side, our approach is also more flexible than previous techniques: it requires no pre-process and works on arbitrary static or dynamic inputs; it is easily incorporated into direct or global illumination renderers, and it adds a relatively small performance overhead, enabling real-time rendering in direct illumination scenarios. The abstraction provided by line-based methods is interesting in many respects, because it creates legible pictures with an economy of means. , and the focus of our paper is precisely on finding alternatives that preserve material and illumination information, while still efficiently depicting shape. This effect is illustrated in Figure 2 , where curved surfaces contract a wider region of the environment lighting than flat surfaces, hence producing more compressed patterns from a particular point of view. Our contribution resides in a viewcentered warping function that deforms incoming light directions around salient surface features. Every stage of our system is performed in real-time on modern graphics hardware, with the exception of (optional) global illumination routines that are executed off-line. This makes our system very flexible, as normals may either be sampled from 3D surfaces (implicit surfaces, meshes, etc), or read from image-based representations (e.g., RGBN images [Toler-Franklin et al. 2007], normal maps). To explain the process, let us first study a simple 1D normal field, as shown in the top row of Figure 4 . The middle and bottom rows show the first and second derivatives of this depth field respectively. The foremost important features to identify are silhouettes and creases as they represent discontinuities in zerothand first-order derivatives respectively. The second most important features are inflection points, as they separate convex from concave regions. Inflection points correspond to extrema of the first-order derivative, and zero-crossings of the second-order derivative. In between silhouettes, creases and inflections, the magnitude of the second-order derivative gives information about the surface curvature. Note that in our approach, concavities correspond to positive curvature and convexities correspond to negative curvature. We use warm hues for concavities and cold hues for convexities in the remainder of this paper. Previous work essentially made use of object-centered measures. The main advantage of view-centered curvature is that it properly reflects surface foreshortening, as well as the size of projected features. These cues are likely to be taken into account by the HVS, as noted in Section 1. In addition, the view-centered approach has an important number of practical advantages because it is computed dynamically from the current viewpoint. The analysis is performed similarly in 2D over the whole image. In the following, we denote by x, y and z the axes of image space. As we are only interested in curvature (i.e., second-order information), it is not required to explicitly compute the depth field. Indeed, if we denote the normal at a point p in image space by n(p) = (n x , n y , n z ) and the relative depth by d(p), then there is a direct relationship between the gradient of d and n, given by:  with g the depth gradient, and d x and d y the first-order derivatives of d in the x and y directions. In other words, g is obtained directly from surface normals without having to differentiate d. At silhouettes, n z = 0 and thus g is undefined, forbidding any differentiation across them. This makes sense as image neighborhoods should be restricted to connected surface neighborhoods. The Hessian of the depth field is then computed by differentiating the gradient: where g x and g y are the first-order derivatives of g in the x and y directions. In other words, H is obtained by differentiating the components of g. At creases, where g is not differentiable, H is undefined, hence differentiation must be restricted accross creases as well. However, we mostly consider smooth surfaces in the examples given in the paper and supplemental materials. H is a curvature tensor, a symmetric 2 ? 2 matrix that can be easily rewritten as follows: H = Q T DQ = u v ? u 0 u v T 0 ? v where ? u and ? v are the principal curvatures, and u and v correspond to the principal directions. Our local shape descriptor consists of the union of visible silhouettes and creases, and surface points from which we get curvature information via H. For all examples given in the paper and supplemental materials, we display silhouettes and creases in black, as well as concave and convex regions in warm and cold hues respectively. A major advantage of using a view-centered curvature tensor for our descriptor is that it confers automatic simplification behaviors, as demonstrated in Figure 5 -left. Observe how surface features are naturally agglomerated together when the object gets away from the viewpoint. Note that this behavior would be a lot more difficult to obtain with an object-centered description, for instance requiring on-the-fly view-dependent mesh simplification. A more detailed comparison between objectand view-centered curvatures is found in supplemental material. Another important advantage of a view-centered curvature tensor is that it is easily modified to dynamically extract surface features at multiple scales. This is done by integrating (i.e., smoothing) g over extended neighborhoods in image space. For this reason, we perform this integration via anisotropic diffusion [Perona and Malik 1990]: where s refers to the scale and c(p) is the conductance function that is equal to 0 on silhouettes and creases, and 1 otherwise. It produces a blurred gradient g s that preserves silhouettes and creases. The blurred curvature tensor is obtained as before: H s (p) = ? T g s . This is done by letting the user choose an importance function I(p) that controls the number of iterations of the diffusion process: few iterations lead to fine details in important picture regions. Any importance function could be used, and we show an example in Figure 5 -right and in the supplemental video. In practice, our local shape descriptor is computed per-pixel entirely on the GPU using multiple passes. We take normal and depth buffers as input, and output our descriptor in another buffer. It consists of a pixel-wise multi-scale Hessian H s , with silhouette and crease weights w s and w c . In the following pseudo-code, p denotes the current pixel and p i its 3 ? 3 pixel neighborhood. Algorithm 1 Multi-scale descriptor on the GPU 1: w s (p) ? Sobel Filter ( Depth(p i ) ) 2: w c (p) ? Dihedral Angle ( n(p i ) ) 3: g 0 (p) ? Depth Gradient ( n(p) ) 4: for s ? [1.. I(p)] do 5: g s (p) ? Anisotropic Diffusion ( g s?1 (p i ), w s (p), w c (p)) 6: end for 7: H s (p) ? Sobel Filter ( g s (p i ) )\n        There are essentially five steps in the algorithm. Silhouettes (1) are computed as a per-pixel weight w s using a Sobel filter on depth. We found this approach more accurate and coherent that detecting the locii of image points where n z = 0 in practice. Creases (2) are computed as per-pixel weights w c as well using the dihedral angle between neighboring normals. The multi-scale depth gradient g s is obtained by (3) computing g 0 and (4-6) discretizing the anisotropic diffusion equation with an iterative solver as explained in [Perona and Malik 1990]. We use c(p) = 1 ? max(w s , w c ) for the conductance function. Finally, the Hessian (7) is computed by differentiating the multi-scale gradient with a Sobel filter. From a practical point of view, our solution offers important advantages. Since it only requires per-pixel normals and depths, it requires no pre-process and works with dynamic 3D scenes. In terms of performance, it is output-sensitive, and its complexity is linear in the number of diffusion iterations. In the next section, we show how we make use of the information made available by our local shape analysis with a novel light warping approach. An animated example with a glossy object is shown in the supplemental video. We require the warping function to be a bijective mapping in the sphere of directions, so that the inverse warping function is analytically defined. Moreover, as noted in [Fleming et al. 2009], the compression of reflected light patterns reflects the anisotropy of curvature, defined as the ratio of principal curvatures. As this is likely to be a salient shape cue for the HVS, we design the warping function so that it deforms incoming illumination in different ways along principal curvature directions u and v. Since our descriptor provides curvature information in the form of a symmetric tensor in image space, we also require our warping function to be symmetric with respect to u and v, and to leave z invariant. Every light direction is thus transformed into the {u, v, z} reference frame prior to warping, and transformed back afterwards. Curvature and light directions are not expressed in the same coordinate system though. In order to establish correspondences between cartesian and angular spaces, we use a stereographic projection on the image plane. The process is illustrated in Figure 7 : (1) the light direction l is stereographically projected on the plane z = 1 to give l  ? = S( l); (2) l  ? is warped according to curvature information, yielding l  ? ? = W S ( l);  ? (3) l  ? ? is mapped back to the sphere of directions to give the warped light direction l ? = S ?1 ( l  ? ? ). Given a light direction l = (l u , l v , l z ), the stereographic projection S is defined by: 2l u 2l v S( l) = (a, b, c) = , , 1 . l z + 1 l z + 1 The (0, 0, 1) direction is projected on the origin of the stereographic plane. Intermediate light directions are projected further from the origin as they get closer to the (0, 0, ?1) direction, which is projected to infinity. The warping function is simply defined as a curvature-dependent non-linear scaling on the stereographic plane (see Figure 7 -right): W S ( l)  ? = (a ? , b ? , c ? ) = ( ? u a, ? v b, 1). The scaling factors ? u|v are computed by mapping the local curvatures ? u|v into an angular deviation on the sphere. In our implementation, we use ? u|v = tan(arctan( ?? u|v )/6 + ? /4). It guarantees that at most one half of the lighting energy found on one side of the hemisphere of directions is warped to the other one. In this formulation, ? is a user-defined parameter that controls the amount of warping performed according to the curvature, while the anisotropy of curvature is naturally taken into account. The inverse stereographic projection is given by: S ?1 ( l)  ? = (l ? u , l ? v , l z ? ) = (a ? t, b ? t, 2t ? 1) where t = 4/(4 + a ?2 + b ?2 ) describes the parametric location of the intersection between the sphere and the projection direction. We concatenate these operations into a single warping function W = S ?1 ? W S ? S , yielding: 2t ? u l u 2t ? v l v (1 + l z ) 2 W( l) = , , 2t ? 1 , t = . 1 + l z 1 + l z (1 + l z ) 2 + ? u 2 l 2 u + ? v 2 l 2 v Note that the inverse warping function W ?1 = S ?1 ? W S ?1 ? S is simply obtained by using the inverse of ? u and ? v , (i.e., by replacing ? by ? ? ). The final stage in our system is to render 3D objects with arbitrary materials and illumination, while taking into account the way the environment lighting must be warped at each surface point. We illustrate this approach with photorealistic as well as nonphotorealistic scenarios, with both real-time and off-line renderers. We first reformulate the reflected radiance equation to take the light warping into account: where p is the surface point, e is the viewpoint direction, l is the incoming lighting direction, ? is the sphere of directions, ? is the BRDF and W is the warping function as defined in Section 5. We clamp light directions (both original and warped) to the hemisphere of directions around n, in a way similar to the clamping done when using bump or normal maps. The discretization of Equation 1 may raise performance and quality issues though. Indeed, it is common to sample light sources in pre-process to reduce noise in the results (e.g., Kriv?nek and Colbert [2008]). However, since the light warping is different at every point, such approaches become intractable with Equation 1. To enable pre-sampling of light sources, we re-write L ? by substituting l ? = W( l) to l: where J is the jacobian of W ?1 (see supplemental materials): J = 4 ? u 3 ? v 3 (1 + l z ? ) 2 ( ? u 2 ? v 2 (1 + l ? z ) 2 + ? v 2 l u ?2 + ? u 2 l ?2 v ) 2 We implemented the light warping approach in different renderers. In both cases, we used Ashikmin?s BRDF model [Ashikhmin et al. 2000]. Our real-time rendering system evaluates Equation 2 using pre-sampled environment lights; however, it avoids computing visibility information and ignores indirect illumination. Figure 1 shows how the shape of an input 3D object is enhanced in two different illumination settings with this system. Note how the enhancement remains coherent while the patterns of reflected lighting are completely different in each image; indeed, the only cue we provide here is the deformation of patterns. Additional real-time captures using this rendering system are shown in the supplemental video. In our off-line rendering system, we compute full global illumination results, with Equation 1 for indirect lighting and Equation 2 for direct lighting, using our path tracer [Dutr? et al. 2006], applying light warping only to the first ray bounce. We also implemented a warped ambient occlusion used with diffuse materials. After being warped, light rays have a different visibility; this change of visibility enhances shape as is best seen in supplemental results. Renderings are shown in Figure 8 , where the shape of the same input 3D object is enhanced in each configuration. Again, surface features are enhanced no matter the material characteristics. We also experimented with purely reflective and refractive materials as shown in supplemental materials. The complexity of our light warping approach is linear in the number of sampled light directions. In practice, applying the warping function is negligible with global illumination, but decreases frame rate by 50% with direct illumination. However, we still get real-time frame rates in practice: for instance, the results in Figure 1 are obtained at 37 fps in 800 ? 600 using 54 lights. Finally, we experimented with non-photorealistic rendering techniques using our light warping approach. In order to exaggerate the enhancement obtained by light warping, we incorporate a curvature-dependent contrast enhancement. The exaggerated reflected radiance is then given by where ? ? [?1, 1] is a contrast parameter. When both ? u = 0 and ? v = 0, L ? ? = L ? ; in other cases, contrast is increased depending on  curvature and warping magnitudes. When applied to an object with diffuse material and minimal illumination, this method comes close to the mean curvature shading technique [Kindlmann et al. 2003]. Figure 9 shows the effect of using Equation 3 in our real-time renderer with both natural and minimal illumination. We also applied a stylized quantization algorithm [Winnem?ller et al. 2006] to both renderings that shows how the very same warped lighting is able to enhance stylized shading. The end result is a compelling cartoon style that works with arbitrary materials and illumination. Our local shape descriptor bears some similarities with the work of Judd et al. [2007] and Vergne et al. [2008] who have investigated view-dependent approaches to shape depiction in the past. However, they provide only partial curvature information: either the maximum principal curvature in [Judd et al. 2007], or a blending between objectand view-centered curvatures in [Vergne et al. 2008]. In the former case, it limits the method to line-based renderings, while in the latter case, it results in objectionable artifacts around silhouettes. Our method is considerably simpler: it requires no pre-process in object space and basically consists in applying filtering operations to normal and depth buffers in the picture plane. When getting close to a surface mesh, our descriptor starts enhancing geometry tesselation, as shown in the inset image (zoomed from Figure 5). This may be seen as a limitation, but it is no surprise as normals are only C 0 continuous across triangle edges due to Phong interpolation. The simplest way to address this issue is to use dynamically subdivided meshes or implicit surfaces for instance. We also believe that adapting the descriptor?s scale based on surface depth could smooth areas of coarse tesselation. The light warping approach we introduced enhances surface features with arbitrary materials, illuminations and styles. It may produce results similar to exaggerated shading or 3D unsharp masking as shown in Figure 10 . Compared to [Rusinkiewicz et al. 2006], it enhances surface shape with a much wider range of materials. Furthermore, exaggerated shading suffers from light direction sensitivity, and tends to flatten the overall shape perception, as shown in the supplemental video. Besides, it requires a time-consuming pre-process and yet does not incorporate automatic simplification behaviors as opposed to our approach. Compared to [Ritschel et al. 2008], our system offers a greater control as it enhances the surface features uniformly, while 3D unsharp masking increases indiscriminately radiance contrast, resulting in irregular enhancement and alteration of material properties. Our approach is also simpler to control compared to previous work, as it offers 3 intuitive parameters: warping magnitude ? , lighting contrast ? , and feature scale s. The light warping technique shows some limitations though. First, it depends on the existence of lighting variations in the scene; this appears to be related to the statistics of natural environments [Fleming et al. 2009]. In practice, it is always possible to enhance surface shape using Equation 3 in cases where the environment lighting has few variations. Second, it reaches its limits with pure reflections and refractions on objects exhibiting many surface details, because it tends to make the picture less legible as a whole. The overall shape of cast shadows may also be distorted to favor the depiction of sur- face features. A better balance between surface and shadow shape depiction might thus be needed. Moreover, our warping function tends to sharpen shading transitions when ? is pushed to high values, hence affecting material perception. Finally, warping increases noise in off-line renderings and adds a relatively small overhead in real-time renderings. Extending our approach to pre-computed warped radiance transfer would be an interesting solution to increase performance. We have presented a new approach to surface shape enhancement called light warping that preserves material and illumination characteristics as well as stylistic choices. It also has a number of practical advantages over previous methods such as flexibility with respect to input data representations, automatic as well as controllable levels-of-detail, and real-time rendering on the GPU. Moreover, we presented one way of performing light warping in stereographic space, but we would like to investigate other potential functions. In particular, we could imagine making use of additional information such as an explicit description of the environment illumination. We thank the members of the IPARLA team and Roland Fleming for their useful feedback, and Ma?tena Vives for mentioning the work of Norman Rockwell. This work has been sponsored by the ANR-08-JCJC-0078-01 project.",
  "resources" : [ ]
}