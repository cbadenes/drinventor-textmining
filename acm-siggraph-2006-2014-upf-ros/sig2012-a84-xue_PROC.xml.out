{
  "uri" : "sig2012-a84-xue_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a84-xue_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Understanding and Improving the Realism of Image Composites",
    "published" : null,
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ ]
  },
  "bagOfWords" : [ "we", "show", "number", "composit", "result", "evaluate", "performance", "both", "we", "algorithm", "previous", "work", "human", "subject", "study", "since", "task", "beyond", "ability", "today?s", "algorithm", "most", "technique", "simply", "shift", "appearance", "foreground", "better", "resemble", "background", "first", "isolate", "highlight", "match", "color", "brightness", "balance", "mid-tone", "gamma", "correction", "finally", "match", "shadow", "region", "we", "algorithm", "also", "adjust", "base", "zone", "though", "we", "automatically", "choose", "different", "zone", "match", "each", "composite", "give", "statistical", "evidence", "benefit", "use", "zone", "completely", "different", "approach", "make", "image", "region", "compatible", "adjust", "color", "so", "match", "predefined", "set", "template", "think", "encode", "color", "harmony", "-lsb-", "cohen-or", "et", "al.", "2006", "-rsb-", "practice", "most", "whole-object", "composite", "still", "make", "alpha", "matte", "one", "numerical", "outcome", "experiment", "set", "scale", "value", "-lrb-", "Table", "-rrb-", "linearly", "relate", "change", "each", "statistical", "measure", "change", "human", "perception", "realism", "main", "component", "algorithm", "classifier", "predict", "zone", "histogram", "each", "statistical", "measure", "when", "match", "between", "foreground", "background", "produce", "most", "realistic", "composite", "section", "we", "evaluate", "performance", "we", "algorithm", "previous", "work", "scope", "problem", "large", "composite", "realism", "influence", "semantics", "-lrb-", "e.g.", "polar", "bear", "rainforest", "realistic", "-rrb-", "factor", "require", "3d", "reasoning", "analyze", "-lrb-", "e.g.", "inter-reflection", "-rrb-", "Third", "we", "use", "insight", "glean", "from", "above", "experiment", "design", "algorithm", "ad", "just", "foreground", "match", "background", "finally", "we", "validate", "show", "we", "technique", "create", "more", "realistic", "composite", "than", "previous", "automated", "technique", "practice", "background", "composite", "often", "fix", "so", "we", "focus", "adjust", "foreground", "appearance", "so", "its", "composite", "realistic", "possible", "we", "first", "simplify", "assumption", "we", "can", "achieve", "realistic", "composite", "only", "adjust", "standard", "2d", "statistical", "measure", "e.g.", "luminance", "contrast", "color", "histogram", "we", "ignore", "issue", "like", "semantics", "3d", "property", "we", "could", "model", "probability", "collection", "composite", "both", "realistic", "unrealistic", "along", "human", "rating", "realism", "we", "can", "form", "collection", "real-world", "image", "roughly", "segmented", "foreground", "background", "maximize", "likelihood", "relative", "datum", "i.e.", "maximize", "-lrb-", "real", "-rrb-", "likelihood", "hard", "fully", "model", "more", "specifically", "we", "select", "statistics", "satisfy", "follow", "criterion", "measure", "should", "easy", "adjust", "photograph", "measure", "should", "independent", "each", "other", "possible", "we", "therefore", "group", "set", "statistical", "measure", "five", "individual", "category", "commonly", "adjust", "composite", "luminance", "color", "temperature", "-lrb-", "cct", "-rrb-", "saturation", "local", "contrast", "hue", "we", "denote", "individual", "measure", "within", "overall", "set", "therefore", "we", "look", "both", "standard", "deviation", "well", "correlation", "coefficient", "between", "we", "collect", "4126", "image", "from", "labelme", "data-set", "-lsb-", "Russell", "et", "al.", "2008", "-rsb-", "clearly", "label", "meaningful", "un-occluded", "foreground", "object", "front", "background", "scene", "larger", "value", "-lrb-", "correlation", "coefficient", "between", "foreground", "background", "-rrb-", "indicate", "more", "useful", "measure", "every", "row", "three", "best", "quantity", "highlight", "bold", "we", "also", "compute", "correlation", "coefficient", "corr", "-lrb-", "-rrb-", "instead", "only", "compute", "measure", "over", "entire", "histogram", "category", "luminance", "we", "find", "stronger", "correlation", "we", "separate", "histogram", "luminance", "high", "-lrb-", "-rrb-", "middle", "-lrb-", "whole-histogram", "-rrb-", "-lrb-", "-rrb-", "low", "-lrb-", "-rrb-", "zone", "average", "each", "zone", "use", "individual", "statistical", "measure", "-lrb-", "appendix", "-rrb-", "example", "real", "image", "mean", "luminance", "highlight", "region", "tend", "match", "better", "between", "foreground", "background", "than", "mean", "whole", "histogram", "which", "can", "see", "first", "row", "figure", "we", "show", "likelihood", "offset", "two", "other", "measure", "other", "row", "we", "also", "show", "number", "across", "zone", "Table", "-lrb-", "exception", "hue", "which", "circular", "value", "-lrb-", "appendix", "-rrb-", "so", "undefined", "-rrb-", "we", "can", "draw", "several", "conclusion", "from", "result", "local", "contrast", "saturation", "show", "very", "strong", "correlation", "last", "all", "likelihood", "offset", "have", "single-peak", "distribution", "center", "however", "correlation", "high", "-lrb-", "-rrb-", "low", "-lrb-", "-rrb-", "zone", "much", "stronger", "than", "whole", "histogram", "-lrb-", "-rrb-", "typically", "use", "color", "transfer", "which", "suggest", "match", "zone", "might", "lead", "more", "realistic", "composite", "finally", "average", "means", "histogram", "zone", "show", "stronger", "correlation", "than", "other", "statistical", "measure", "though", "standard", "deviation", "sometimes", "perform", "well", "zone", "means", "enough", "cover", "best", "nearly-best", "value", "Table", "next", "section", "we", "test", "impact", "mismatch", "statistical", "measure", "human", "judgement", "realism", "we", "have", "identify", "several", "key", "statistical", "measure", "strongly", "match", "between", "foreground", "background", "real", "image", "we", "therefore", "perform", "perception", "experiment", "human", "subject", "Amazon", "mechanical", "Turk", "-lrb-", "MTurk", "-rrb-", "numerically", "model", "relationship", "design", "we", "experiment", "important", "have", "natural", "control", "image", "only", "vary", "one", "variable", "otherwise", "issue", "other", "than", "one", "we", "study", "could", "influence", "perception", "realism", "we", "perform", "experiment", "three", "key", "statistical", "measure", "identify", "previous", "section", "luminance", "color", "temperature", "saturation", "we", "measure", "perceive", "decrease", "realism", "we", "acquire", "realism", "score", "-lrb-", "0.0", "1.0", "-rrb-", "each", "composite", "-lrb-", "appendix", "-rrb-", "Third", "except", "extreme", "corner", "matrix", "decrease", "can", "model", "reasonably", "well", "function", "offset", "-lrb-", "where", "mean", "histogram", "i?th", "stimulus", "image", "-rrb-", "we", "therefore", "fit", "rating", "function", "-lrb-", "-rrb-", "gaussian", "function", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "example", "fit", "show", "right", "Figure", "standard", "deviation", "fitted", "gaussian", "function", "all", "20", "image", "correspond", "luminance", "CCT", "saturation", "show", "Figure", "we", "can", "see", "variation", "first", "we", "can", "get", "sense", "how", "human", "realism", "rating", "respond", "variation", "measure", "visualize", "impact", "we", "choose", "example", "close-to-median", "standard", "deviation", "show", "original", "image", "well", "adjust", "version", "whose", "realism", "rating", "reduce", "40", "-lrb-", "figure", "-rrb-", "second", "we", "can", "compute", "robust", "average", "standard", "deviation", "each", "statistical", "measure", "approximately", "place", "each", "measure", "equal", "linear", "scale", "after", "apply", "we", "compute", "scale", "factor", "we", "can", "expect", "equal", "adjustment", "different", "statistical", "measure", "produce", "equal", "decrease", "realism", "rating", "-lrb-", "modulo", "error", "introduce", "use", "average", "-rrb-", "we", "create", "scale", "factor", "robust", "average", "standard", "deviation", "Figure", "after", "remove", "highest", "lowest", "value", "value", "give", "Table", "use", "next", "section", "part", "we", "algorithm", "automatically", "improve", "realism", "composite", "we", "technique", "use", "machine", "learn", "automatically", "choose", "zone", "histogram", "match", "between", "foreground", "background", "luminance", "CCT", "saturation", "give", "evidence", "section", "most", "straightforward", "approach", "align", "statistics", "luminance", "CCT", "saturation", "select", "zone", "each", "measure?s", "histogram", "lowest", "standard", "deviation", "-lrb-", "Table", "-rrb-", "we", "denote", "set", "zone", "region", "-lrb-", "where", "-lcb-", "-rcb-", "-rrb-", "give", "statistical", "measure", "we", "denote", "-lrb-", "-rrb-", "mean", "value", "measure", "k?th", "image", "we", "collection", "zone", "we", "use", "mean", "absolute", "error", "-lrb-", "ma", "-rrb-", "statistical", "measure", "incur", "select", "specific", "zone", "match", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "number", "image", "we", "collection", "error", "scheme", "show", "first", "three", "column", "Table", "low", "error", "indicate", "match", "means", "between", "foreground", "background", "use", "statistical", "measure", "better", "able", "reproduce", "original", "image", "we", "collection", "however", "error", "never", "exactly", "zero", "since", "offset", "natural", "image", "exactly", "zero", "-lrb-", "i.e.", "always", "tail", "distribution", "figure", "-rrb-", "error", "lowest", "we", "use", "portion", "luminance", "histogram", "portion", "cct", "histogram", "portion", "saturation", "histogram", "note", "use", "entire", "histogram", "-lrb-", "-rrb-", "-lrb-", "i.e.", "use", "zone", "-rrb-", "optimal", "any", "measure", "which", "support", "we", "claim", "use", "zone", "more", "effective", "algorithm", "ignore", "particular", "characteristic", "composite", "select", "zone", "could", "we", "do", "better", "select", "zone", "dynamically", "per", "composite", "idea", "motivate", "plot", "Figure", "left", "plot", "show", "offset", "between", "background", "foreground", "zone", "luminance", "histogram", "particular", "image", "may", "very", "close", "zero", "however", "right", "plot", "show", "typically", "least", "one", "zone", "do", "have", "nearzero", "offset", "fourth", "column", "Table", "show", "error", "hypothetical", "oracle", "algorithm", "always", "predict", "best", "zone", "produce", "smallest", "offset", "note", "error", "still", "zero", "since", "smallest", "offset", "still", "exactly", "zero", "each", "test", "image", "nonetheless", "performance", "much", "better", "than", "always", "use", "same", "zone", "however", "may", "more", "than", "one", "zone", "near-zero", "offset", "so", "treat", "zone", "unsuitable", "match", "simply", "because", "another", "zone", "perform", "slightly", "better", "might", "confuse", "classifier", "instead", "we", "take", "multi-label", "classification", "approach", "-lsb-", "Tsoumakas", "Katakis", "2007", "-rsb-", "where", "more", "than", "one", "label", "can", "apply", "instance", "specifically", "we", "train", "three", "separate", "binary", "classifier", "per", "statistical", "measure", "-lrb-", "one", "per", "zone", "-rrb-", "where", "each", "classifier", "predict", "whether", "zone", "-lcb-", "-rcb-", "individually", "suitable", "composite", "matching", "form", "binary", "training", "datum", "from", "we", "collection", "4126", "image", "from", "section", "we", "compare", "offset", "between", "foreground", "background", "realism", "threshold", "each", "zone", "luminance", "CCT", "saturation", "offset", "training", "image", "below", "threshold", "we", "assign", "label", "since", "match", "use", "measure", "would", "produce", "realistic", "image", "similar", "original", "otherwise", "compute", "threshold", "we", "use", "scale", "version", "robust", "standard", "deviation", "compute", "Table", "namely", "0.1", "choose", "several", "iteration", "cross-validation", "use", "small-scale", "trial", "version", "we", "mturk", "evaluation", "study", "describe", "section", "note", "we", "use", "same", "value", "each", "zone", "what", "feature", "should", "we", "use", "we", "classifier", "we", "expectation", "shape", "histogram", "feature", "correlate", "which", "zone", "might", "match", "well", "we", "therefore", "use", "feature", "statistical", "property", "histogram", "measure", "both", "foreground", "background", "image", "20", "where", "20", "portion", "jth", "bin", "luminance", "histogram", "we", "separately", "compute", "feature", "foreground", "background", "histogram", "we", "use", "random", "forest", "classifier", "-lsb-", "Liaw", "Wiener", "2002", "-rsb-", "default", "setting", "we", "also", "try", "svm", "-lsb-", "Chang", "Lin", "2011", "-rsb-", "rbf", "kernel", "find", "work", "slightly", "less", "well", "misclassification", "rate", "-lrb-", "percentage", "compute", "use", "10-fold", "crossvalidation", "training", "datum", "-rrb-", "we", "classifier", "show", "Table", "Luminance", "CCT", "Saturation", "give", "output", "three", "zone", "classifier", "specific", "statistical", "measure", "we", "finally", "combine", "they", "single", "choice", "match", "produce", "composite", "combination", "easy", "only", "one", "three", "classifier", "return", "what", "multiple", "zone", "suitable", "match", "more", "than", "one", "candidate", "zone", "we", "consider", "each", "candidate", "zone", "turn", "sum", "absolute", "mean", "shift", "histogram", "all", "candidate", "zone", "induce", "match", "note", "any", "clipping", "may", "occur", "right", "left", "histogram", "take", "account", "during", "computation", "finally", "we", "select", "minimum", "sum", "zone", "candidate", "we", "simply", "select", "result", "above", "technique", "choice", "single", "zone", "per", "composite", "we", "show", "error", "rate", "we", "technique", "fifth", "column", "Table", "compute", "10-fold", "cross-validation", "lower", "than", "always", "choose", "same", "zone", "low", "perfect", "oracle", "selector", "use", "select", "zone", "from", "previous", "section", "we", "adjust", "each", "key", "statistical", "measure", "turn", "instead", "inspire", "Paris", "et", "al.", "-lsb-", "2011", "-rsb-", "we", "use", "s-shaped", "curve", "adjust", "local", "contrast", "we", "select", "zone", "contrast", "histogram", "choose", "shape", "s-curve", "best", "match", "local", "contrast", "describe", "Appendix", "C.", "note", "contrast", "luminance", "independent", "adjustment", "however", "we", "contrast", "adjustment", "technique", "design", "move", "mean", "luminance", "histogram", "after", "contrast", "we", "adjust", "luminance", "CCT", "saturation", "turn", "we", "experiment", "we", "find", "other", "order", "generally", "work", "well", "long", "contrast", "adjustment", "perform", "first", "we", "adjustment", "algorithm", "greedy", "alternatively", "could", "iterate", "several", "time", "over", "sequence", "step", "however", "we", "find", "result", "do", "change", "significantly", "after", "first", "iteration", "Figure", "show", "intermediate", "result", "step", "we", "pipeline", "input", "adjust", "local", "contrast", "use", "s-shape", "correction", "adjust", "luminance", "Select", "zone", "best", "matching", "shift", "so", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "adjust", "CCT", "Select", "zone", "best", "matching", "shift", "so", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "adjust", "saturation", "Select", "zone", "best", "matching", "shift", "so", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "output", "adjust", "we", "show", "number", "composit", "result", "adjust", "use", "we", "automatic", "method", "compare", "they", "other", "technique", "Table", "show", "one", "way", "evaluate", "success", "we", "method", "image", "we", "already", "know", "real", "however", "more", "useful", "evaluation", "involve", "adjustment", "composite", "region", "take", "from", "separate", "image", "source", "we", "therefore", "create", "test", "set", "48", "composite", "make", "effort", "ensure", "each", "composite", "semantically", "reasonable", "we", "create", "adjust", "result", "use", "five", "technique", "simple", "cut-and-paste", "manually", "adjust", "composite", "Photoshop", "Match", "Color", "method", "Lalonde", "Efros", "-lsb-", "2007", "-rsb-", "-lrb-", "which", "we", "label", "ColorComp", "-rrb-", "we", "method", "six", "example", "show", "Figure", "11", "manually-adjusted", "composite", "create", "Photoshop", "one", "author", "who", "have", "extensive", "Photoshop", "experience", "typically", "take", "3-4", "minute", "create", "we", "compute", "result", "ColorComp", "use", "code", "datum", "provide", "its", "author", "which", "typically", "take", "3-5", "minute", "execute", "per", "example", "Photoshop", "Match", "Color", "use", "standard", "color", "transfer", "technique", "similar", "Reinhard", "et", "al.", "-lsb-", "2004", "-rsb-", "color", "transfer", "useful", "number", "creative", "task", "compositing", "necessarily", "its", "main", "application", "we", "method", "take", "5-15", "seconds", "execute", "use", "unoptimized", "Matlab", "code", "bottleneck", "contrast", "adjustment", "step", "result", "all", "technique", "all", "48", "composite", "give", "supplemental", "material", "evaluate", "relative", "realism", "five", "technique", "we", "perform", "experiment", "use", "mechanical", "Turk", "simplify", "task", "we", "use", "forced", "choice", "test", "between", "two", "alternate", "version", "same", "composite", "where", "subject", "ask", "choose", "most", "realistic", "alternative", "however", "those", "rating", "would", "more", "sensitive", "factor", "influence", "realism", "we", "study", "semantic", "likelihood", "depict", "scene", "-rrb-", "methodology", "we", "experiment", "describe", "appendix", "we", "collect", "average", "12.3", "human", "choice", "each", "480", "possible", "comparison", "first", "we", "use", "one-tailed", "t-test", "compare", "each", "method", "against", "each", "other", "method", "we", "show", "which", "method", "better", "than", "other", "significance", "level", "0.05", "Figure", "10", "we", "method", "outperform", "all", "other", "automatic", "method", "its", "performance", "significantly", "different", "than", "manual", "adjustment", "second", "we", "convert", "series", "paired", "comparison", "scale", "result", "place", "performance", "each", "method", "single", "scale", "scaling", "can", "perform", "individually", "each", "composite", "-lrb-", "figure", "11", "inset", "each", "row", "well", "supplemental", "material", "-rrb-", "average", "over", "all", "composite", "-lrb-", "figure", "12", "-rrb-", "we", "use", "Thurstone?s", "Law", "comparative", "judgment", "Case", "-lsb-", "David", "1988", "-rsb-", "which", "assume", "each", "method", "have", "single", "quality", "score", "observer", "estimate", "score", "normally", "distribute", "resultant", "scale", "value", "linear", "multiple", "score", "so", "difference", "between", "scale", "value", "unit", "standard", "deviation", "preference", "higher", "value", "better", "Figure", "12", "show", "we", "method", "perform", "slightly", "worse", "than", "manual", "adjustment", "much", "better", "than", "all", "other", "automatic", "method", "ColorComp", "significantly", "outperform", "color", "transfer", "however", "surprisingly", "do", "outperform", "cut-and-paste", "may", "because", "many", "we", "scene", "have", "natural", "lighting", "thus", "do", "require", "large", "adjustment", "appear", "natural", "scale", "individual", "composite", "-lrb-", "inset", "Figure", "11", "supplemental", "material", "-rrb-", "can", "diverge", "quite", "significantly", "from", "average", "example", "fifth", "row", "Figure", "11", "ColorComp", "perform", "best", "however", "fourth", "row", "show", "typical", "failure", "color", "transfer", "-lrb-", "which", "also", "component", "ColorComp", "-rrb-", "where", "green", "color", "forest", "unnaturally", "transfer", "person", "last", "row", "we", "method", "significantly", "outperform", "other", "automatic", "technique", "however", "also", "show", "limitation", "we", "method", "we", "can", "correct", "harsh", "lighting", "foreground", "still", "make", "composite", "unrealistic", "along", "example", "Figure", "11", "supplemental", "material", "show", "additional", "failure", "case", "several", "source", "error", "first", "we", "classifier", "sometimes", "choose", "wrong", "offset", "match", "second", "even", "minimal", "offset", "between", "foreground", "background", "real", "image", "exactly", "zero", "so", "perfect", "classification", "still", "yield", "perfect", "adjustment", "even", "most", "peaked", "distribution", "figure", "have", "tail", "so", "any", "algorithm", "use", "meanmatching", "have", "error", "Third", "we", "do", "use", "spatial", "proximity", "cue", "even", "though", "area", "background", "close", "foreground", "probably", "more", "relevant", "than", "area", "farther", "away", "example", "lighting", "can", "change", "respect", "depth", "scene", "proximity", "light", "source", "fourth", "we", "use", "hard", "threshold", "decide", "zone", "appropriate", "match", "during", "training", "phase", "near-threshold", "value", "can", "cause", "incorrect", "decision", "finally", "natural", "question", "whether", "we", "method", "could", "use", "main", "problem", "address", "Lalonde", "Efros", "-lsb-", "2007", "-rsb-", "predict", "whether", "choose", "foreground", "background", "appear", "realistic", "together", "unfortunately", "realism", "rating", "we", "collect", "section", "measure", "human", "response", "variation", "along", "single", "axis", "-lrb-", "e.g.", "luminance", "-rrb-", "unclear", "how", "combine", "simultaneous", "variation", "along", "multiple", "axis", "single", "realism", "prediction", "paper", "we", "study", "problem", "adjust", "composite", "appear", "realistic", "we", "automatic", "technique", "significantly", "outperform", "previous", "method", "biggest", "limitation", "we", "method", "we", "limit", "we", "scope", "standard", "2d", "image", "processing", "adjustment", "some", "composite", "need", "more", "specific", "complicated", "adjustment", "relighting", "truly", "appear", "realistic", "finally", "while", "we", "have", "identify", "image", "statistics", "correlate", "composite", "realism", "still", "much", "do", "truly", "understand", "factor", "influence", "human", "perception", "realism", "why", "statistics", "more", "correlate", "realism", "than", "other", "relationship", "causation", "correlation", "also", "we", "zone", "selection", "classifier", "black", "box", "how", "do", "determine", "which", "zone", "best", "match", "finally", "while", "we", "have", "evaluate", "we", "result", "human", "subject", "would", "expert", "compositor", "have", "different", "ranking", "method", "we", "thank", "Aaron", "Hertzmann", "Peter", "O?Donovan", "Dan", "Goldman", "Sylvain", "Paris", "helpful", "discussion", "work", "support", "part", "Adobe", "image", "result", "courtesy", "Flickr", "user", "joeldinda", "limbic", "essjaynz", "greekadman", "Dell?s", "Pics", "et", "al.", "under", "Creative", "Commons", "license", "ychkovsky", "V.", "ari", "S.", "HAN", "E.", "URAND", "F.", "2011", "Learning", "photographic", "global", "tonal", "adjustment", "database", "input/output", "image", "pair", "Proceedings", "CVPR", "97", "104", "hang", "c.-c.", "c.-j", "2011", "LIBSVM", "library", "support", "vector", "machine", "ACM", "Trans", "Intelligent", "Systems", "Technology", "27:1", "27:27", "Software", "available", "http", "www.csie.ntu.edu.tw", "cjlin/libsvm", "T.", "-lrb-", "July", "-rrb-", "ijsenij", "a.", "ever", "t.", "VAN", "de", "eijer", "J.", "2011", "computational", "color", "constancy", "survey", "experiment", "IEEE", "Trans", "image", "processing", "20", "-lrb-", "sep", "-rrb-", "2475", "2489", "ia", "J.", "UN", "J.", "ang", "c.-k.", "and-drop", "pasting", "use", "color", "compatus", "P.", "iaw", "a.", "randomforest", "opez", "oreno", "J.", "utierrez", "D.", "2010", "consistency", "otto", "R.", "perception", "gden", "J.", "M.", "DELSON", "E.", "H.", "J.", "1985", "pyramid-based", "computer", "graphic", "hta", "N.", "obertson", "a.", "R.", "2005", "Wiley", "Chichester", "strovsky", "Y.", "AVANAGH", "P.", "INHA", "P.", "2005", "perceive", "illumination", "inconsistency", "scene", "perception", "34", "11", "1301", "1314", "ari", "S.", "ASINOFF", "S.", "W.", "AUTZ", "J.", "2011", "local", "laplacian", "filter", "edge-aware", "image", "processing", "laplacian", "pyramid", "ACM", "Trans", "Graphics", "30", "-lrb-", "aug", "-rrb-", "68:1", "68:12", "elus", "E.", "1990", "contrast", "complex", "image", "Journal", "Optical", "Society", "America", "10", "-lrb-", "oct", "-rrb-", "2032", "2040", "REZ", "P.", "ANGNET", "M.", "lake", "a.", "2003", "Poisson", "image", "editing", "ACM", "Trans", "Graphics", "22", "-lrb-", "Jul", "-rrb-", "313", "318", "oulus", "T.", "einhard", "E.", "2010", "progressive", "histogram", "reshape", "creative", "color", "transfer", "tone", "reproduction", "Proceedings", "NPAR", "81", "90", "einhard", "E.", "SHIKHMIN", "M.", "OOCH", "B.", "HIRLEY", "P.", "S.", "2001", "Color", "transfer", "between", "image", "IEEE", "Computer", "Graphics", "application", "21", "-lrb-", "Sept.", "Oct.", "-rrb-", "34", "41", "einhard", "E.", "UYZ", "a.", "OLBERT", "M.", "UGHES", "C.", "E.", "OC", "ONNOR", "M.", "2004", "real-time", "color", "blending", "render", "capture", "video", "interservice/industry", "training", "simulation", "education", "conference", "hemann", "C.", "OTHER", "C.", "ang", "J.", "ELAUTZ", "M.", "OHLI", "P.", "OTT", "P.", "2009", "perceptually", "motivated", "online", "benchmark", "image", "matting", "Proceedings", "CVPR", "1826", "1833", "ussell", "B.", "orralba", "a.", "urphy", "K.", "reeman", "W.", "2008", "LabelMe", "Database", "Web-Based", "Tool", "image", "annotation", "International", "Journal", "Computer", "Vision", "77", "-lrb-", "May", "-rrb-", "157", "173", "mith", "a.", "R.", "LINN", "J.", "F.", "1996", "Blue", "screen", "matting", "Proceedings", "SIGGRAPH", "96", "259", "268", "toke", "M.", "NDERSON", "M.", "HANDRASEKAR", "S.", "OTTA", "R.", "1996", "standard", "default", "color", "space", "internetsrgb", "Microsoft", "Hewlett-Packard", "Joint", "Report", "UNKAVALLI", "K.", "OHNSON", "M.", "K.", "atusik", "W.", "FIS", "TER", "H.", "2010", "multi-scale", "image", "harmonization", "ACM", "Trans", "Graphics", "29", "-lrb-", "July", "-rrb-", "125:1", "125:10", "ao", "M.", "W.", "OHNSON", "M.", "K.", "ari", "S.", "2010", "errortolerant", "image", "compositing", "european", "conference", "computer", "Vision", "31", "44", "soumaka", "G.", "ATAKIS", "I.", "2007", "multi-label", "classification", "overview", "International", "Journal", "Data", "Warehousing", "Mining", "-lrb-", "july/sept", "-rrb-", "13", "AGBERG", "J.", "2007", "Optpop", "color", "property", "toolbox", "Mar.", "Software", "available", "http://www.mathworks.com/", "ang", "J.", "survey", "image", "statistics", "compute", "image", "statistics", "input", "image", "composite", "srgb", "color", "space", "pixel", "first", "inversely", "Gamma", "correct", "-lsb-", "Stokes", "et", "al.", "1996", "-rsb-", "we", "transform", "image", "statistics", "so", "approximately", "linear", "human", "visual", "perception", "Luminance", "saturation", "convert", "log", "domain", "base", "weber?s", "law", "CCT", "define", "mire", "-lsb-", "Ohta", "Robertson", "2005", "-rsb-", "Luminance", "we", "use", "log", "where", "-lrb-", "normalize", "-lsb-", "1.0", "-rsb-", "-rrb-", "luminance", "channel", "xyy", "space", "where", "3.03", "10", "-lrb-", "correspond", "intensity", "0-255", "greyscale", "image", "before", "inverse", "gamma", "correction", "-rrb-", "use", "avoid", "undefined", "log", "value", "unit", "difference", "log", "domain", "stop", "correlate", "Color", "Temperature", "-lrb-", "CCT", "-rrb-", "we", "use", "mire", "unit", "CCT", "mire", "10", "where", "plankian", "color", "temperature", "Kelvin", "clip", "-lsb-", "1500", "20000", "-rsb-", "normal", "range", "natural", "lighting", "CCT", "compute", "use", "package", "OptProp", "-lsb-", "Wagberg", "2007", "-rsb-", "saturation", "we", "use", "log", "where", "-lsb-", "1.0", "-rsb-", "saturation", "channel", "hsv", "space", "Hue", "we", "use", "where", "circular", "value", "-lsb-", "0.0", "1.0", "-rsb-", "-lrb-", "-lsb-", "360", "-rsb-", "-rrb-", "hue", "channel", "hsv", "space", "local", "contrast", "locally", "define", "Weber", "contrast", "-lsb-", "peli", "1990", "-rsb-", "use", "every", "pixel", "where", "pixel", "luminance", "local", "average", "luminance", "which", "output", "gaussian", "filter", "1.5", "filter", "radius", "Histogram", "Statistics", "each", "statistical", "measure", "statistics", "compute", "across", "all", "pixel", "corresponding", "region", "i.e.", "foreground", "background", "image", "property", "sensitive", "color", "bias", "e.g.", "CCT", "hue", "saturation", "only", "compute", "across", "pixel", "neither", "overnor", "under-exposed", "practice", "we", "use", "pixel", "0.013", "0.88", "where", "two", "threshold", "correspond", "intensity", "60", "240", "0-255", "greyscale", "image", "before", "inverse", "gamma", "correction", "statistics", "define", "99.9", "where", "represent", "0.1", "99.9", "mean", "pixel", "luminance", "which", "greater", "than", "99.9", "quantile", "kurtosis", "we", "use", "definition", "which", "normal", "distribution", "have", "kurtosis", "Entropy", "compute", "first", "scaling", "-lsb-", "255", "-rsb-", "use", "computation", "routine", "standard", "greyscale", "image", "entropy", "-lrb-", "e.g.", "function", "entropy", "Matlab", "-rrb-", "other", "statistics", "standard", "definition", "use", "notably", "circular", "statistics", "-lsb-", "beren", "2009", "-rsb-", "use", "hue", "example", "circular", "mean", "hue", "use", "place", "ordinary", "mean", "Alpha", "Mattes", "we", "assume", "every", "composite", "have", "alpha", "matte", "foreground", "object", "when", "compute", "we", "morphologically", "erode", "alpha", "matte", "avoid", "inaccuracy", "matte", "boundary", "compute", "region", "where", "matte", "value", "greater", "than", "0.5", "when", "compute", "original", "alpha", "matte", "morphologically", "dilate", "compute", "region", "where", "matte", "value", "lower", "than", "0.5", "avoid", "distant", "background", "only", "compute", "within", "area", "equal", "bound", "box", "foreground", "scale", "element", "erosion", "dilation", "operation", "disk", "whose", "radius", "erosion", "0.03", "min", "-lrb-", "-rrb-", "radius", "dilation", "0.15", "min", "-lrb-", "-rrb-", "where", "width", "height", "bound", "box", "foreground", "object", "impact", "human", "realism", "rating", "twenty", "natural", "image", "use", "input", "manually", "create", "foreground", "object", "alpha", "matte", "luminance", "foreground", "background", "every", "image", "respectively", "manipulate", "level", "via", "shift", "-lrb-", "step", "0.5", "stop", "-rrb-", "which", "composit", "form", "49", "composite", "total", "we", "generate", "20", "980", "composite", "mturk", "worker", "evaluate", "same", "number", "stimulus", "generate", "CCT", "saturation", "similar", "procedure", "step", "manipulate", "CCT", "40", "mire", "step", "saturation", "0.25", "stop", "log", "domain", "image", "property", "some", "pixel", "exceed", "define", "range", "Appendix", "after", "manipulation", "clipping", "perform", "every", "mturk", "worker", "present", "series", "-lrb-", "23", "-rrb-", "composite", "evaluate", "two-alternative", "forced", "choice", "manipulate", "real", "instruction", "example", "give", "time", "each", "evaluation", "limit", "12", "seconds", "we", "present", "out", "23", "evaluation", "test", "case", "very", "obvious", "status", "real", "manipulate", "answer", "fail", "pass", "least", "two", "three", "test", "question", "classify", "random", "answer", "discard", "analysis", "end", "1360", "valid", "response", "luminance", "969", "CCT", "1048", "saturation", "every", "composite", "evaluate", "15", "time", "rating", "composite", "proportion", "answer", "real", "we", "control", "each", "specific", "mturk", "task", "so", "23", "composite", "-lrb-", "20", "actual", "study", "test", "-rrb-", "present", "worker", "all", "from", "different", "natural", "image", "any", "one", "worker", "prohibit", "from", "participate", "more", "than", "one", "experiment", "24", "hour", "avoid", "see", "same", "foreground", "background", "twice", "succession", "evaluate", "composite", "we", "use", "all", "48", "composite", "stimulus", "which", "we", "compare", "five", "alpha-matte-based", "adjust", "method", "all", "result", "show", "supplemental", "material", "user", "study", "we", "use", "scheme", "forced", "two-alternative", "choice", "-lrb-", "fac", "-rrb-", "where", "every", "participant", "request", "compare", "series", "pair", "result", "pick", "more", "realistic", "one", "-lrb-", "12", "pair", "actual", "comparison", "pair", "test", "-rrb-", "order", "comparison", "randomize", "test", "example", "pair", "one", "obviously", "real", "image", "obviously", "fake", "composite", "answer", "fail", "correctly", "classify", "least", "three", "out", "four", "test", "comparison", "regard", "random", "answer", "653", "worker", "respond", "we", "test", "151", "random", "answer", "discard", "every", "pair", "comparison", "evaluate", "10", "-lrb-", "average", "12.3", "-rrb-", "worker", "statistical", "robustness", "we", "increase", "local", "11", "-lrb-", "1,1", "-rrb-", "11", "-lrb-", "1,1", "-rrb-", "contrast", "global", "out", "out", "pixel-wise", "luminance", "transformation", "use", "shape", "curve", "inP", "02", "12", "02", "12", "verse", "curve", "decrease", "local", "contrast", "show", "inset", "input", "-lrb-", "0,0", "-rrb-", "01", "-lrb-", "0,0", "-rrb-", "01", "S?curve", "0.5", "inverse", "s?curve", "0.5", "luminance", "value", "re-mapped", "new", "output", "value", "out", "along", "curve", "same", "curve", "use", "across", "image", "transformation", "luminance", "define", "-lrb-", "-rrb-", "xyy", "space", "-lrb-", "log", "domain", "-rrb-", "turn", "point", "curve", "average", "luminance", "which", "curve", "divide", "upper", "lower", "sub-curve", "each", "sub-curve", "Bezier", "curve", "three", "anchor", "upper", "curve", "control", "lower", "curve", "degree", "transformation", "control", "where", "11", "-lrb-", "12", "11", "-rrb-", "01", "-lrb-", "02", "01", "-rrb-", "0.5", "curve", "increase", "local", "contrast", "0.5", "inverse", "curve", "decrease", "local", "contrast", "0.5", "degrade", "straight", "line", "practice", "we", "search", "-lsb-", "0.4", "0.6", "-rsb-", "find", "best", "curve", "match", "local", "contrast", "between", "foreground", "background" ],
  "content" : "We show a number of compositing results, and evaluate the performance of both our algorithm and previous work with a human subjects study. Since this task is beyond the ability of today?s algorithms, most techniques simply shift the appearance of the foreground to better resemble the background. First, they isolate the highlights and match their color and brightness, then balance the mid-tones with gamma correction, and finally match the shadow regions. Our algorithm also adjusts based on zones, though we automatically choose different zones to match for each composite, and give statistical evidence for the benefits of using zones. A completely different approach to making image regions compatible is to adjust their colors so that they match a predefined set of templates that are thought to encode color harmony [Cohen-Or et al. 2006]. In practice, most whole-object composites are still made with alpha mattes. One numerical outcome of this experiment is a set of scaling values ( Table 2 ) that linearly relates a change in each statistical measure into a change in the human perception of realism. The main component of the algorithm is a classifier that predicts the zone of the histogram of each statistical measure that, when matched between foreground and background, will produce the most realistic composite. In Section 5, we evaluate the performance of our algorithm and previous work. The scope of this problem is large: composite realism is influenced by semantics (e.g., is a polar bear in a rainforest realistic), and factors that require 3D reasoning to analyze (e.g., inter-reflections). Third, we use the insights gleaned from the above experiments to design an algorithm to ad- just a foreground to match a background. Finally, we validate and show that our technique creates more realistic composites than previous automated techniques. In practice the background of a composite is often fixed, so we focus on adjusting the foreground appearance f into f ? so that its composite with b is as realistic as possible. Our first simplifying assumption is that we can achieve a realistic composite by only adjusting standard 2D statistical measures, e.g., luminance, contrast, and color histograms; we ignore issues like semantics and 3D properties. We could model this probability with a collection of composites, both realistic and unrealistic, along with human ratings of realism. We can form a collection of real-world images with roughly segmented foreground and backgrounds, and maximize the likelihood of f ? relative to this data, i.e., maximize P (M f |M b , Real). This likelihood is hard to fully model. More specifically, we select statistics that satisfy the following criteria: 1. The measure should be easy to adjust in a photograph. The measures should be as independent of each other as possible. We therefore group the set of statistical measures M into five individual categories that are commonly adjusted in composites: luminance, color temperature (CCT), saturation, local contrast, and hue. We denote an individual measure within this overall set as M i . Therefore we look both at the standard deviation of ? i as well as the correlation coefficient between M f i and M b i . We collect 4126 images from the LabelMe data-set [Russell et al. 2008] with clearly labeled, meaningful, and un-occluded foreground objects in front of background scenes. and larger values of r (correlation coefficient between foreground and background) indicate more useful measures. For every row, the three best quantities are highlighted in bold. We also compute the correlation coefficient, r = corr(M f i , M b i ). Instead of only computing these measures over the entire histogram of a category such as luminance, we found stronger correlations if we separated the histograms of luminance into high (H), middle (whole-histogram) (M), and low (L) zones. The average of each zone is used as an individual statistical measure (Appendix A). For example, in real images the mean luminance of highlight regions tend to match better between foreground and background than the mean of the whole histogram, which can be seen in the first row of Figure 2 ; we show the likelihoods of the offsets for two other measures in the other rows. We also show numbers for ? ? and r across zones in Table 1 . (An exception is hue, which is a circular value (Appendix A) so H and L are undefined. ) We can draw several conclusions from these results. Local contrast and saturation show very strong correlation. Last, all the likelihoods of offsets have single-peak distributions centered at 0. However, the correlations of the high (H) and low (L) zones are much stronger than the for the whole histogram (M) typically used by color transfer, which suggests that matching by zones might lead to more realistic composites. Finally, on average the means of the histogram zones H,M,and L show stronger correlation than the other statistical measures. Though the standard deviation sometimes performs well, the zone means are enough to cover the best or nearly-best values in Table 1 . In the next section, we test the impact of mismatches in these statistical measures on human judgements of realism. We have identified several key statistical measures that are strongly matched between foreground and background in real images. We therefore perform a perception experiment with human subjects on Amazon Mechanical Turk (MTurk) to numerically model this relationship. To design our experiment it is important to have a natural control image, and only vary one variable; otherwise, issues other than the one we are studying could influence the perception of realism. We perform this experiment for three key statistical measures identified in the previous section: luminance, color temperature, and saturation. We then measure the perceived decrease in realism. We then acquire a realism score (0.0 ? 1.0) for each composite (Appendix B). Third, except for the extreme corners of the matrix, this decrease can be modeled reasonably well as a function of the offset M f i ? M i b (where M i is the mean of the histogram for the i?th stimuli image). We therefore fit a rating function R(M i f , M b i ) as a Gaussian function G: R(M f i , M i b ) ? G(M f i ? M i b ) = G(? i ) (1) An example of this fit is shown on the right in Figure 4 . The standard deviations ? of fitted Gaussian functions for all 20 images corresponding to luminance, CCT, saturation are shown in Figure 5 . We can see the variation in ?. First, we can get a sense of how human realism ratings respond to variations in these measures. To visualize this impact, we choose an example with close-to-median standard deviation, and show the original image as well an adjusted version whose realism rating is reduced by 40% ( Figure 6 ). Second, we can compute robust average standard deviations for each statistical measure that approximately places each measure on an equal, linear scale. That is, after applying our computed scale factors we can expect equal adjustments of different statistical measures to produce equal decreases in realism rating (modulo the error introduced by using an average). We create these scale factors ? g as robust averages of the standard deviations in Figure 5 , after removing the highest and lowest values. These values are given in Table 2 , and are used in the next section as part of our algorithm to automatically improve the realism of a composite. Our technique uses machine learning to automatically choose a zone of the histogram to match between foreground and background for luminance, CCT, and saturation. Given the evidence in Section 2, the most straightforward approach to aligning the statistics of luminance, CCT, and saturation is to select the zone of each measure?s histogram with the lowest standard deviation (? ? in Table 1 ). We denote the set of zone regions as z (where z ? {H, M, L}), and given a statistical measure M , we denote M (k, z) as the mean value of that measure on the k?th image in our collection in the zone z. We use the mean absolute error (MAE) in this statistical measure that is incurred by selecting a specific zone to match, or E(z) = n 1 n k=1 |M f (k, z)?M b (k, z)|, where n is the number of images in our collection. The errors of these schemes are shown in the first three columns of Table 3 . A low error indicates that matching the means between foreground and background using that statistical measure is better able to reproduce the original images of our collection. However, the error will never be exactly zero, since the offsets of natural images are not exactly zero (i.e., there are always tails in the distributions in Figure 2 ). The error is lowest if we use the H portion of the luminance histogram, the L portion of the CCT histogram, and the H portion of the saturation histogram. Note that using the entire histogram (M) (i.e., not using zones) is not optimal for any measure, which supports our claim that using zones is more effective. This algorithm ignores the particular characteristics of a composite in selecting zones; could we do better by selecting a zone dynamically per composite? This idea is motivated by the plots in Figure 8. The left plot shows that the offset between background and foreground for a zone of the luminance histogram for a particular image may not be very close to zero. However, the right plot shows that typically at least one of the H, M, L zones does have a nearzero offset. The fourth column of Table 3 shows the error of a hypothetical ?oracle? algorithm always predicting the best zone that produces the smallest offset. Note that this error is still not zero since the smallest offset is still not exactly zero for each test image. Nonetheless, the performance is much better than always using the same zone. However, there may be more than one zone with a near-zero offset, so treating such a zone as unsuitable for matching simply because another zone performs slightly better might confuse a classifier. Instead, we take a multi-label classification approach [Tsoumakas and Katakis 2007], where more than one label can be applied to an instance. Specifically, we train three separate binary classifiers per statistical measure (one per zone), where each classifier predicts whether the zones {H, M, L} are individually suitable for composite matching. To form binary training data from our collection of 4126 images from Section 2, we compare the offset between foreground and background to a realism threshold T for each zone z of luminance, CCT, and saturation. If the offset of the training image is below this threshold we assign label 1, since matching using this measure would produce a realistic image similar to the original, and 0 otherwise. To compute threshold T we use a scaled version of the robust standard deviations computed in Table 2 , namely T = s ? ? g , with s = 0.1 chosen by several iterations of cross-validation using a small-scale trial version of our MTurk evaluation study described in Section 5. Note that we use the same T value for each zone. What features should we use for our classifier? Our expectation is that the shape of the histogram for a feature is correlated with which zone might match well. We therefore use as features statistical properties of the histogram of that measure for both the foreground and background of the image. , p 20 , where p j , j = 1, 2, .. , 20, is the portion of the jth bin in the luminance histogram. We separately compute these features on the foreground and background histograms. We use a random forest classifier [Liaw and Wiener 2002] with default settings. We also tried SVM [Chang and Lin 2011] with an RBF kernel, but found it worked slightly less well. The misclassification rates (as percentages, computed using 10-fold crossvalidation on the training data) of our classifiers are shown in Table 4. Luminance CCT Saturation Given the output of three zone classifiers for a specific statistical measure, we finally combine them into a single choice to match to produce a composite. This combination is easy if only one of the three classifiers returns 1, but what if multiple zones are suitable for matching? If there is more than one candidate zone, we consider each candidate zone z i in turn, and sum the absolute mean shift of the histogram for all candidate zones that is induced by matching z i . Note that any clipping that may occur at the right and left of the histogram is taken into account during this computation. Finally we select the z i with the minimum sum. If no zones are candidates, we simply select M.  The result of the above technique is the choice of a single zone per composite. We show the error rate of our technique in the fifth column of Table 3 , computed with 10-fold cross-validation; it is lower than always choosing the same zone, but not as low as the perfect ?oracle? selector. Using the selected zone from the previous section, we adjust each key statistical measure in turn. Instead, inspired by Paris et al. [2011] we use an S-shaped curve to adjust local contrast. We select the H zone of the contrast histogram, and choose the shape of the S-curve to best match local contrast as described in Appendix C. Note that contrast and luminance are not independent adjustments; however, our contrast adjustment technique is designed to not move the mean of the luminance histogram. After contrast, we adjust luminance, CCT, and saturation, in turn. In our experiments we found other orders generally work as well, as long as contrast adjustment is performed first. Our adjustment algorithm is greedy; alternatively, it could iterate several times over this sequence of steps. However, we found the results did not change significantly after the first iteration. Figure 9 shows the intermediate results of the steps of our pipeline. Input: f and b 1. Adjust local contrast M f 0 using S-shape correction. Adjust luminance M f 1 . Select zone z that is best for matching. Shift M f 1 , so that M f 1 (z) ? M b 1 (z) = 0. Adjust CCT M f 2 . Select zone z that is best for matching. Shift M f 2 , so that M f 2 (z) ? M b 2 (z) = 0. Adjust saturation M f 3 . Select zone z that is best for matching. Shift M f 3 , so that M f 3 (z) ? M b 3 (z) = 0. Output: adjusted f ? and b. We show a number of compositing results adjusted using our automatic method, and compare them to other techniques. Table 3 shows one way to evaluate the success of our method on images that we already know are real; however, a more useful evaluation involves the adjustment of composites with regions taken from separate image sources. We therefore created a test set of 48 composites, and made an effort to ensure that each composite is semantically reasonable. We then created adjusted results using five techniques: simple cut-and-paste, a manually adjusted composite, Photoshop Match Color, the method of Lalonde and Efros [2007] (which we label ColorComp), and our method. Six examples are shown in Figure 11 . The manually-adjusted composite was created in Photoshop by one of the authors who has extensive Photoshop experience, and typically took 3-4 minutes to create. We computed the results of ColorComp using code and data provided by its authors, which typically took 3-5 minutes to execute per example. Photoshop Match Color uses a standard color transfer technique similar to Reinhard et al. [2004]; color transfer is useful for a number of creative tasks, and compositing is not necessarily its main application. Our method takes 5-15 seconds to execute using unoptimized Matlab code; the bottleneck is the contrast adjustment step. The results of all these techniques on all 48 composites are given in supplemental materials. To evaluate the relative realism of these five techniques, we performed an experiment using Mechanical Turk. To simplify the task, we used a forced choice test between two alternate versions of the same composite, where the subject was asked to choose the most realistic alternative. However, those ratings would be more sensitive to factors that influence realism that we are not studying, such as the semantic likelihood of the depicted scene.) The methodology of our experiment is described in Appendix B; we collected, on average, 12.3 human choices for each of the 480 possible comparisons. First, we use one-tailed t-tests to compare each method against each other method. We show which methods are better than others with significance level p < 0.05 in Figure 10 . Our method outperforms all other automatic methods, and its performance is not significantly different than manual adjustment. Second, we convert the series of paired comparisons into scaling results that place the performance of each method on a single scale; this scaling can be performed individually for each composite ( Figure 11 , inset in each row, as well as supplemental materials), and averaged over all composites ( Figure 12 ). We use Thurstone?s Law of Comparative Judgments, Case V [David 1988], which assumes that each method has a single quality score, and observer estimates of this score are normally distributed. The resultant scale values are linear multiples of this score, so that differences between scale values are in the units of standard deviation of preference. Higher values are better. Figure 12 shows that our method performs slightly worse than manual adjustment, but much better than all other automatic methods. ColorComp significantly outperforms color transfer; however, surprisingly it does not outperform cut-and-paste. This may be because many of our scenes have natural lighting, and thus do not require large adjustments to appear natural. The scales for individual composites (the insets in Figure 11 , and supplemental materials) can diverge quite significantly from the average. For example, in the fifth row of Figure 11 , ColorComp performs best. However, the fourth row shows a typical failure of color transfer (which is also a component of ColorComp), where the green color of the forest is unnaturally transferred to the person. In the last row, our method significantly outperforms the other automatic techniques. However, it also shows a limitation of our method: we cannot correct the harsh lighting of the foreground that still makes this composite unrealistic. Along with the examples in Figure 11 , the supplemental materials show additional failure cases. There are several sources of error. First, our classifier sometimes chooses the wrong offset to match. Second, even the minimal offsets between foreground and background for real images are not exactly zero, so perfect classification will still not yield perfect adjustments. Even the most peaked distributions in Figure 2 have tails, so any algorithm that uses meanmatching will have errors. Third, we do not use spatial or proximity cues, even though areas of the background close the foreground are probably more relevant than areas farther away. For example, lighting can change with respect to depth in the scene or proximity to a light source. Fourth, we use hard thresholds to decide if a zone is appropriate for matching during the training phase; near-threshold values can cause incorrect decisions. Finally, a natural question is whether our method could be used for the main problem addressed by Lalonde and Efros [2007]: predicting whether a chosen foreground and background will appear realistic together. Unfortunately, the realism ratings we collect in Section 3 measure human response to variations along a single axis (e.g., luminance); it is unclear how to combine simultaneous variations along multiple axes into a single realism prediction. In this paper we studied the problem of adjusting a composite to appear realistic; our automatic technique significantly outperforms previous methods. The biggest limitation of our method is that we limit our scope to standard 2D image processing adjustments; some composites will need more specific or complicated adjustments such as relighting to truly appear realistic. Finally, while we have identified image statistics that are correlated with composite realism, there is still much to be done to truly understand the factors that influence human perception of realism. Why are these statistics more correlated with realism than others, and is the relationship causation or correlation? Also, our zone selection classifier is a black box; how does it determine which zone is best  for matching? Finally, while we have evaluated our results with human subjects, would expert compositors have different rankings of methods? We thank Aaron Hertzmann, Peter O?Donovan, Dan Goldman, and Sylvain Paris for helpful discussions. This work was supported in part by Adobe. The images in the results are courtesy of Flickr users joeldinda, Limbic, EssjayNZ, greekadman, Dell?s Pics et al. under Creative Commons license. B YCHKOVSKY , V., P ARIS , S., C HAN , E., AND D URAND , F. 2011. Learning photographic global tonal adjustment with a database of input/output image pairs. In Proceedings of CVPR, 97?104. C HANG , C.-C., AND L IN , C.-J. 2011. LIBSVM: A library for support vector machines. ACM Trans. on Intelligent Systems and Technology 2, 27:1?27:27. Software available at http: //www.csie.ntu.edu.tw/ ?cjlin/libsvm. , T., X U , (July), G IJSENIJ , A., G EVERS , T., AND VAN DE W EIJER , J. 2011. Computational color constancy: Survey and experiments. IEEE Trans. on Image Processing 20, 9 (Sep), 2475 ?2489. J IA , J., S UN , J., T ANG , C.-K., S and-drop pasting. Using color compati- B , P. 5,\n        L IAW , A., AND W by randomforest. L OPEZ -M ORENO , J., S G UTIERREZ , D. 2010. consistencies. In L OTTO , R., P perception. O GDEN , J. M., A DELSON , E. H., B , J., 1985. Pyramid-based computer graphics. O HTA , N., AND R OBERTSON , A. R. 2005. Wiley, Chichester. O STROVSKY , Y., C AVANAGH , P., AND S INHA , P. 2005. Perceiving illumination inconsistencies in scenes. Perception 34, 11, 1301?1314. P ARIS , S., H ASINOFF , S. W., AND K AUTZ , J. 2011. Local Laplacian filters: edge-aware image processing with a Laplacian pyramid. ACM Trans. on Graphics 30 (Aug), 68:1?68:12. P ELI , E. 1990. Contrast in complex images. Journal of Optical Society of America 7, 10 (Oct), 2032?2040. P ? REZ , P., G ANGNET , M., AND B LAKE , A. 2003. Poisson image editing. ACM Trans. on Graphics 22 (Jul), 313?318. P OULI , T., AND R EINHARD , E. 2010. Progressive histogram reshaping for creative color transfer and tone reproduction. In Proceedings of NPAR, 81?90. R EINHARD , E., A SHIKHMIN , M., G OOCH , B., AND S HIRLEY , P. S. 2001. Color transfer between images. IEEE Computer Graphics & Applications 21, 5 (Sept. /Oct.), 34?41. R EINHARD , E., A K UYZ  ? , A., C OLBERT , M., H UGHES , C. E., AND OC ONNOR , M. 2004. Real-time color blending of rendered and captured video. In Interservice/Industry Training, Simulation and Education Conference, 1?9. R HEMANN , C., R OTHER , C., W ANG , J., G ELAUTZ , M., K OHLI , P., AND R OTT , P. 2009. A perceptually motivated online benchmark for image matting. In Proceedings of CVPR, 1826?1833. R USSELL , B., T ORRALBA , A., M URPHY , K., AND F REEMAN , W. 2008. LabelMe: A Database and Web-Based Tool for Image Annotation. International Journal of Computer Vision 77, 1 (May), 157?173. S MITH , A. R., AND B LINN , J. F. 1996. Blue screen matting. In Proceedings of SIGGRAPH 96, 259?268. S TOKES , M., A NDERSON , M., C HANDRASEKAR , S., AND M OTTA , R. 1996. A standard default color space for the internetsrgb. Microsoft and Hewlett-Packard Joint Report. S UNKAVALLI , K., J OHNSON , M. K., M ATUSIK , W., AND P FIS TER , H. 2010. Multi-scale image harmonization. ACM Trans. on Graphics 29, 4 (July), 125:1?125:10. T AO , M. W., J OHNSON , M. K., AND P ARIS , S. 2010. Errortolerant image compositing. In European Conference on Computer Vision, 31?44. T SOUMAKAS , G., AND K ATAKIS , I. 2007. Multi-label classification: An overview. International Journal of Data Warehousing and Mining 3 (July/Sept.), 1?13. W AGBERG , J., 2007. Optpop a color properties toolbox, Mar. Software available at http://www.mathworks.com/\n        C\n        W ANG , J., survey. Image Statistics To compute image statistics on an input image or composite in sRGB color space, the pixels are first inversely Gamma corrected [Stokes et al. 1996]. We then transform image statistics so that they are approximately linear to human visual perception. Luminance and saturation are converted into log domain based on Weber?s law, and CCT is defined by mired [Ohta and Robertson 2005]. Luminance: we use log 2 Y , where Y (normalized to [ , 1.0]) is the luminance channel of xyY space, where = 3.03 ? 10 ?4 (corresponding to intensity 1 in a 0-255 greyscale image before inverse Gamma correction) is used to avoid undefined log values. The unit of difference in log 2 domain is a stop. Correlated Color Temperature (CCT): we use ?mired? as the unit of CCT. 1 mired = 10 6 /K, where K is the Plankian color temperature in Kelvin, clipped in [1500, 20000] that is the normal range of natural lighting. CCT is computed using the package OptProp [Wagberg 2007]. Saturation: we use log 2 S, where S ? [ , 1.0] is the saturation channel of HSV space. Hue: we use H, where H is a circular value in [0.0, 1.0] (or [0 ? , 360 ? ]), the hue channel of HSV space. Local contrast: Locally defined Weber contrast [Peli 1990], c x , is used. For every pixel x, c x = L x /L x , where L x is the pixel luminance, L x is the local average luminance, which is the output of a Gaussian filter with ? = 1.5, filter radius r = 3. Histogram Statistics For each statistical measure M i , the statistics are computed across all pixels in corresponding regions, i.e., foreground or background. For image properties sensitive to color bias, e.g., CCT, hue, and saturation, they are only computed across pixels that are neither overnor under-exposed. In practice, we use the pixels with 0.013 ? Y ? 0.88, where two thresholds correspond to intensity 60 and 240 in a 0-255 greyscale image before inverse Gamma correction. The statistics H, M, L are defined as H = M i > M i , M = 99.9% M i , L = M i < M i , where M i > M i represents the 0.1% 99.9% mean of pixel luminance which are greater than the 99.9% quantile. For kurtosis, we use the definition by which normal distributions have kurtosis of 0. Entropy is computed by first scaling M i into [0, 255], and using the computation routine of standard greyscale image entropies (e.g., function entropy in Matlab). For other statistics, standard definitions are used. Notably, circular statistics [Berens 2009] are used for hue. For example, circular mean of hue is used in place of ordinary mean. Alpha Mattes We assume that every composite has an alpha matte for the foreground object. When computing M f , we morphologically erode the alpha matte to avoid inaccuracy of matte boundary, and then compute M f in the region where matte values are greater than 0.5. When computing M b , the original alpha matte is morphologically dilated, and then M b is computed in the region where matte values are lower than 0.5. To avoid distant background, M b is only computed within an area equal to the bounding box of the foreground scaled by 3. The elements for erosion and dilation operations are disks, whose radius for erosion is r e = 0.03 ? min(w, h) and radius for dilation is r d = 0.15 ? min(w, h), where w, h are the width and height of the bounding box of the foreground object. Impact on Human Realism Ratings Twenty natural images are used as input, with manually created foreground object alpha mattes. For luminance, the foreground and background of every image are respectively manipulated to 7 levels via shifting (?3 steps by 0.5 stop), which are then composited to  form 7 2 = 49 composites. In total, we generated 20 ? 7 2 = 980 composites for MTurk workers to evaluate. The same number of stimuli are generated for CCT and saturation by similar procedures. The step in manipulating CCT is 40 mired, and the step of saturation is 0.25 stop in log 2 domain. If the image properties of some pixels exceed the defined range in Appendix A after manipulation, clipping is performed. Every MTurk worker is presented with a series (23) of composites to evaluate, with a two-alternative forced choice, ?manipulated? or ?real?. Instructions and examples are given. The time for each evaluation is limited to 12 seconds. We present 3 out of 23 evaluations as test cases with very obvious status of ?real? or ?manipulated?. Answers that fail to pass at least two of the three test questions are classified as random answers, and discarded in analysis. In the end, there are 1360 valid responses for luminance, 969 for CCT, and 1048 for saturation. Every composite is evaluated 15+ times. The rating for a composite is the proportion of answers of ?real?. We control each specific MTurk task so that the 23 composites (20 for actual study, 3 for test) presented to a worker are all from different natural images. Any one worker is prohibited from participating in more than one experiments in 24 hours, to avoid seeing the same foreground or background twice in succession. Evaluating Composites We use all 48 composites as the stimuli, for which we compare five alpha-matte-based adjusting methods. All results are shown in the supplemental materials. In the user study, we use the scheme of forced two-alternative choice (FAC), where every participant is requested to compare a series of pairs of results and pick a more realistic one (12 pairs for actual comparison and 4 pairs for test). The order of comparisons is randomized. The 4 test examples pair one obviously real image with an obviously fake composite. The answers that fail to correctly classify at least three out of four test comparisons are regarded as random answers. 653 workers responded to our test and 151 random answers are discarded. Every pair of comparison is evaluated by 10+ (by average 12.3) workers for statistical robustness. We increase local P 11 P 1 =(1,1) P 11 P 1 =(1,1) contrast by a global L out P U L out pixel-wise luminance P? 1 P? 1 transformation using an P U S shape curve; the inP 02 P m P 12 P 02 P L P m P 12 verse S curve decreases P? 0 P? 0 local contrast, as shown P L in the inset. The input P 0  =(0,0) P 01 L in P 0  =(0,0) P 01 L in S?curve, ?<0.5 Inverse S?curve, ?>0.5 luminance value L in is re-mapped to a new output value L out along the curve; the same curve is used across the image. In this transformation, luminance is defined by Y (0 ? 1) of xyY space (not in log domain). The turning point p m of S curve is the average luminance, by which the curve is divided into an upper and a lower sub-curve. Each sub-curve is a Bezier curve with three anchors. The upper curve is controlled by p m , p U , p 1 , and the lower curve by p m , p L , p 0 . The degree of transformation is controlled by p U and p L , where p U = p 11 + ?(p 12 ? p 11 ) and p L = p 01 + ?(p 02 ? p 01 ). If ? < 0.5, it is an S curve that increases local contrast; if ? > 0.5, it is an inverse S curve that decreases local contrast. If ? = 0.5, it degrades to a straight line. In practice, we search ? ? [0.4, 0.6] to find the best curve that matches H of local contrasts between foreground and background.",
  "resources" : [ ]
}