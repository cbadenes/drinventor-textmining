{
  "uri" : "sig2013-a58-vangorp_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a58-vangorp_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Perception of Perspective Distortions in Image-Based Rendering",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Peter-Vangorp",
      "name" : "Peter",
      "surname" : "Vangorp"
    }, {
      "uri" : "http://drinventor/Christian-Richardt",
      "name" : "Christian",
      "surname" : "Richardt"
    }, {
      "uri" : "http://drinventor/Emily A.-Cooper",
      "name" : "Emily A.",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Gaurav-Chaurasia",
      "name" : "Gaurav",
      "surname" : "Chaurasia"
    }, {
      "uri" : "http://drinventor/Martin S.-Banks",
      "name" : "Martin S.",
      "surname" : "Banks"
    }, {
      "uri" : "http://drinventor/George-Drettakis",
      "name" : "George",
      "surname" : "Drettakis"
    } ]
  },
  "bagOfWords" : [ "we", "modify", "latter", "hypothesis", "so", "extend", "street-level", "ibr", "however", "result", "image", "only", "correct", "when", "view", "from", "where", "original", "photograph", "be", "take", "when", "move", "away", "from", "point", "distortion", "can", "become", "quite", "large", "show", "fig.", "-lrb-", "-rrb-", "two", "term", "important", "understand", "image", "create", "street-level", "ibr", "user", "perception", "those", "image", "we", "make", "three", "primary", "contribution", "we", "modify", "retinal", "hypothesis", "extend", "street-level", "ibr", "quantitative", "model", "emerge", "from", "modification", "use", "perspective", "information", "generate", "predict", "3d", "percept", "two", "perceptual", "experiment", "we", "determine", "how", "well", "extend", "retinal", "scene", "hypothesis", "predict", "perceptual", "outcome", "we", "find", "outcome", "fall", "in-between", "two", "prediction", "those", "outcome", "very", "consistent", "viewer", "rating", "how", "acceptable", "different", "view", "we", "develop", "predictive", "model", "perceptual", "distortion", "streetlevel", "ibr", "use", "present", "guideline", "acceptability", "novel", "view", "capture", "camera", "density", "we", "also", "perform", "validation", "study", "we", "prediction", "illustrate", "use", "guide", "user", "street-level", "ibr", "so", "image", "novel", "view", "acceptable", "during", "navigation", "when", "perspectively", "correct", "picture", "view", "from", "cop", "people", "quite", "accurate", "recover", "3d", "geometry", "original", "scene", "include", "slant", "surface", "scene", "-lsb-", "Smith", "Smith", "1961", "Cooper", "et", "al.", "2012", "-rsb-", "some", "situation", "experimental", "evidence", "favor", "scene", "hypothesis", "other", "situation", "experimental", "evidence", "favor", "retinal", "hypothesis", "even", "small", "off-axis", "displacement", "-lsb-", "bank", "et", "al.", "2009", "-rsb-", "thus", "scene", "retinal", "hypothesis", "account", "best", "perceptual", "outcome", "different", "situation", "we", "analysis", "also", "apply", "other", "type", "streetlevel", "ibr", "Microsoft", "PhotosynthTM", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "Street", "Slide", "-lsb-", "Kopf", "et", "al.", "2010", "-rsb-", "even", "multi-perspective", "image", "-lsb-", "Yu", "et", "al.", "2010", "-rsb-", "long", "corner", "fa", "ade", "only", "deform", "perspective", "projection", "image", "deformation", "apply", "align", "stitch", "image", "panorama", "section", "we", "describe", "extended", "retinal", "hypothesis", "streetlevel", "ibr", "describe", "hypothesis", "we", "consider", "viewing", "corner", "building", "fa", "ade", "-lrb-", "e.g.", "corner", "balcony", "-rrb-", "result", "prediction", "perceive", "angle", "corner", "four", "step", "involve", "process", "capture", "street-level", "panorama", "-lrb-", "photograph", "-rrb-", "capture", "camera", "position", "orientation", "register", "projection", "capture", "imagery", "map", "onto", "simplify", "geometry", "often", "only", "ground", "plane", "few", "fa", "ade", "plane", "simulation", "scene", "visualize", "from", "different", "viewpoint", "virtual", "simulation", "camera", "display", "result", "image", "present", "display", "device", "view", "user", "explain", "Sec", "develop", "we", "extended", "retinal", "hypothesis", "we", "derive", "coordinate", "vanish", "point", "arbitrary", "corner", "see", "simulation", "camera", "image", "-lrb-", "e.g.", "balcony", "corner", "-rrb-", "allow", "we", "predict", "angle", "perceive", "accord", "project", "retinal", "image", "we", "restrict", "we", "derivation", "experiment", "planar", "proxy", "box-like", "balcony", "we", "show", "validation", "study", "how", "we", "result", "can", "apply", "other", "situation", "sketch", "fig.", "capture", "camera", "coordinate", "system", "have", "its", "origin", "camera?s", "nodal", "point", "-lrb-", "optical", "center", "-rrb-", "x-axis", "point", "right", "y-axis", "point", "up", "z-axis", "point", "forward", "camera", "capture", "fa", "ade", "corner", "-lrb-", "black", "-rrb-", "we", "use", "follow", "two", "angle", "extensively", "eccentricity", "angle", "denote", "define", "direction", "normal", "fa", "ade", "-lrb-", "grey", "left", "-rrb-", "line", "between", "corner", "origin", "simulation", "angle", "denote", "which", "angle", "between", "normal", "direction", "line", "between", "simulation", "camera", "nodal", "point", "corner", "-lrb-", "right", "-rrb-", "left", "inset", "show", "image", "capture", "camera", "while", "righthand", "two", "inset", "show", "simulation", "-lrb-", "virtual", "-rrb-", "image", "lation", "angle", "affect", "final", "image", "quality", "point", "parameter", "run", "infinity", "fa", "ade", "side", "simulation", "camera", "turn", "front", "face", "side", "face", "inset", "fig.", "-lrb-", "left", "-rrb-", "illustrate", "step", "projection", "step", "ibr", "pipeline", "consist", "reproject", "panorama", "onto", "planar", "proxy", "assume", "planar", "proxy", "distance", "project", "image", "onto", "we", "replace", "capture", "camera", "projector", "same", "position", "orientation", "focal", "length", "effect", "vanish", "point", "coordinate", "simply", "set", "coordinate", "we", "assume", "simulation", "camera", "point", "straight", "corner", "eccentricity", "angle", "from", "distance", "simulation", "angle", "-lrb-", "fig.", "center", "-rrb-", "we", "transform", "vanish", "point", "simulation", "camera", "coordinate", "system", "perform", "perspective", "projection", "simulation", "camera", "-lrb-", "i.e.", "ibr", "itself", "-rrb-", "focal", "length", "coordinate", "-lrb-", "-rrb-", "otherwise", "real", "camera", "image", "size", "sensor", "size", "we", "model", "magnify", "image", "from", "sensor", "size", "display", "size", "factor", "viewer", "sit", "straight", "front", "display", "view", "distance", "see", "Fig.", "-lrb-", "right", "-rrb-", "veridical", "vanish", "point", "specifically", "-lrb-", "13", "-rrb-", "otherwise", "have", "opposite", "sign", "perceive", "angle", "discontinuity", "arise", "eccentricity", "angle", "because", "side", "face", "corner", "flip", "between", "face", "leave", "face", "right", "we", "conduct", "two", "psychophysical", "experiment", "determine", "how", "image", "distortion", "typical", "street-level", "ibr", "application", "perceive", "angle-matching", "experiment", "test", "prediction", "scene", "extend", "retinal", "hypothesis", "result", "allow", "we", "predict", "magnitude", "perceive", "distortion", "novel", "scene", "rating", "experiment", "determine", "subjective", "acceptability", "angle", "distortion", "result", "allow", "we", "determine", "which", "perceive", "angle", "distortion", "acceptable", "which", "we", "use", "subsequently", "we", "guideline", "-lrb-", "sec", "certain", "application", "-lrb-", "e.g.", "navigation", "sightseeing", "-rrb-", "one", "may", "interested", "overall", "perception", "shape", "since", "shape", "involve", "many", "angle", "different", "contextual", "prior", "we", "believe", "likely", "lead", "more", "acceptable", "perceive", "distortion", "we", "opt", "study", "angle", "which", "stricter", "criterion", "clear", "quantitative", "task", "use", "before", "Watt", "et", "al.", "-lsb-", "2005", "-rsb-", "we", "simulated", "typical", "unstructured", "lumigraph", "workflow", "fully", "synthetic", "offline", "render", "pipeline", "maximize", "control", "image", "quality", "synthetic", "stimulus", "allow", "we", "create", "depth", "variation", "same", "fa", "ade", "specify", "exact", "consistent", "value", "eccentricity", "capture", "camera", "position", "across", "different", "fa", "ade", "well", "create", "image", "without", "clutter", "-lrb-", "e.g.", "car", "lamp", "post", "tree", "-rrb-", "avoid", "presence", "other", "cue", "stimulus", "three", "fa", "ade", "-lrb-", "fig.", "-rrb-", "be", "create", "90", "convex", "corner", "eccentricity", "angle", "7.1", "32.0", "fa", "ade", "be", "light", "sun", "sky", "illumination", "model", "create", "realistic", "image", "wideangle", "frontoparallel", "image", "three", "realistic", "fa", "ade", "render", "from", "distance", "40", "result", "image", "be", "project", "onto", "single", "plane", "same", "stimulus", "be", "use", "both", "experiment", "fig.", "provide", "overview", "stimulus", "creation", "parameter", "extended", "retinal", "hypothesis", "-lrb-", "eq", "16", "-rrb-", "can", "use", "predict", "perceive", "angle", "corner", "we", "stimulus", "fig.", "-lrb-", "-rrb-", "show", "extended", "retinal", "hypothesis", "prediction", "viewer", "center", "projection", "angle", "perceive", "when", "view", "90", "corner", "plot", "function", "simulation", "angle", "different", "color", "line", "represent", "prediction", "different", "eccentricity", "angle", "each", "corner", "have", "constant", "eccentricity", "angle", "give", "capture", "image", "e.g.", "corner", "leave", "Fig.", "have", "negative", "eccentricity", "angle", "32", "relative", "blue", "capture", "camera", "bottom", "so", "represent", "dark", "blue", "line", "fig.", "-lrb-", "-rrb-", "simulation", "camera", "show", "green", "rotate", "around", "corner", "from", "leave", "right", "simulation", "angle", "increase", "predict", "perceive", "angle", "gradually", "decrease", "towards", "correct", "value", "when", "eccentricity", "angle", "positive", "predict", "perceive", "corner", "angle", "increase", "increase", "simulation", "angle", "we", "use", "four", "type", "display", "differ", "substantially", "size", "screen", "diagonal", "range", "from", "3.5", "55", "device", "be", "hold", "upright", "-lrb-", "pc", "tv", "-rrb-", "angle", "about", "45", "-lrb-", "phone", "tablet", "-rrb-", "fix", "stand", "viewer", "position", "specify", "distance", "more", "detail", "experiment", "implementation", "supplemental", "material", "video", "common", "practice", "vision", "science", "use", "relatively", "small", "number", "participant", "who", "test", "quite", "extensively", "-lrb-", "e.g.", "Ernst", "Banks", "-lsb-", "2002", "-rsb-", "-rrb-", "we", "follow", "practice", "test", "six", "participant", "extensively", "-lrb-", "average", "7.5", "total", "3072", "measurement", "each", "-rrb-", "so", "do", "we", "be", "able", "test", "many", "experimental", "condition", "thereby", "put", "hypothesis", "rigorous", "test", "six", "participant", "be", "20", "32", "year", "old", "three", "be", "female", "all", "have", "normal", "corrected-to-normal", "vision", "be", "pay", "participation", "all", "participate", "both", "experiment", "be", "na?ve", "about", "experimental", "hypothesis", "task", "experiment", "quite", "involve", "so", "we", "need", "ensure", "participant", "understand", "instruction", "could", "give", "reasonably", "consistent", "response", "do", "candidate", "pass", "pre-test", "only", "those", "satisfy", "comprehension", "consistency", "requirement", "be", "invite", "perform", "entire", "test", "detail", "pre-test", "give", "supplementary", "material", "both", "experiment", "all", "stimulus", "five", "simulation", "angle", "four", "eccentricity", "angle", "three", "fa", "ade", "each", "three", "balcony", "depths", "be", "present", "twice", "addition", "three", "particular", "stimulus", "be", "present", "eight", "time", "allow", "we", "assess", "consistency", "response", "thus", "be", "384", "stimulus", "presentation", "each", "four", "display", "device", "type", "each", "experiment", "order", "stimulus", "present", "randomize", "addition", "we", "use", "within-subject", "counterbalancing", "be", "two", "experiment", "-lrb-", "hinge", "setting", "rating", "-rrb-", "four", "device", "give", "eight", "condition", "we", "further", "split", "each", "three", "part", "give", "24", "part", "24", "part", "be", "randomize", "split", "separate", "session", "usually", "four", "2-hour", "session", "participant", "take", "break", "whenever", "need", "each", "part", "participant", "be", "give", "extensive", "instruction", "first", "experiment", "determine", "how", "90", "corner", "perceive", "after", "undergo", "distortion", "due", "position", "simulation", "camera", "participant", "indicate", "perceive", "angle", "corner", "set", "real", "convex", "hinge", "look", "same", "-lrb-", "see", "Figure", "-rrb-", "we", "use", "real", "hinge", "similar", "Watt", "et", "al.", "-lsb-", "2005", "-rsb-", "because", "image", "hinge", "may", "themselves", "affect", "perceptual", "distortion", "participant", "be", "show", "image", "from", "we", "pool", "stimulus", "-lrb-", "sec", "4.1", "-rrb-", "random", "order", "participant", "rotate", "hinge", "device", "about", "vertical", "axis", "until", "slant", "one", "side", "hinge", "appear", "match", "side", "corner", "image", "open", "close", "hinge", "until", "perceive", "angle", "hinge", "corner", "be", "equal", "procedure", "illustrate", "accompany", "video", "angle-matching", "experiment", "take", "4.3", "hour", "average", "complete", "about", "10", "seconds", "per", "trial", "we", "use", "eight", "repeat", "presentation", "three", "stimulus", "assess", "response", "consistency", "those", "datum", "show", "participant", "be", "self-consistent", "-lrb-", "average", "standard", "deviation", "within-subject", "5.3", "-rrb-", "datum", "from", "different", "participant", "be", "also", "quite", "similar", "-lrb-", "average", "standard", "deviation", "between-subject", "8.7", "-rrb-", "so", "we", "average", "across", "participant", "create", "datum", "figure", "show", "here", "similarly", "we", "do", "observe", "systematic", "difference", "across", "display", "device", "-lrb-", "average", "standard", "deviation", "between", "device", "6.3", "-rrb-", "so", "we", "average", "over", "device", "see", "supplementary", "material", "datum", "from", "individual", "participant", "device", "figure", "show", "predict", "-lrb-", "-rrb-", "observe", "-lrb-", "b?e", "-rrb-", "hinge", "angle", "setting", "each", "panel", "plot", "angle", "setting", "function", "simulation", "angle", "different", "colored", "line", "represent", "different", "eccentricity", "angle", "color", "line", "panel", "-lrb-", "-rrb-", "represent", "prediction", "ex", "tend", "retinal", "hypothesis", "horizontal", "dot", "line", "90", "prediction", "scene", "hypothesis", "result", "qualitatively", "quite", "similar", "prediction", "extend", "retinal", "hypothesis", "positive", "eccentricity", "angle", "extended", "retinal", "hypothesis", "predict", "increase", "simulation", "angle", "yield", "increase", "perceive", "angle", "negative", "eccentricity", "prediction", "opposite", "all", "datum", "consistent", "those", "prediction", "however", "range", "perceive", "angle", "more", "compress", "than", "predict", "retinal", "hypothesis", "which", "suggest", "influence", "scene", "hypothesis", "partial", "influence", "scene", "hypothesis", "greater", "small", "fa", "ade", "depths", "-lrb-", "figure", "b?d", "-rrb-", "determine", "which", "effect", "statistically", "significant", "we", "perform", "repeated-measures", "analysis", "variance", "-lrb-", "anova", "-rrb-", "datum", "simulation", "angle", "eccentricity", "angle", "display", "device", "fa", "ade", "depth", "factor", "result", "show", "Table", "be", "statistically", "significant", "main", "effect", "all", "factor", "only", "eccentricity", "angle", "fa", "ade", "depth", "have", "large", "enough", "effect", "size", "-lrb-", "0.26", "-rrb-", "result", "noticeable", "difference", "perceive", "angle", "-lsb-", "Bakeman", "2005", "-rsb-", "also", "significant", "large", "two-way", "interaction", "between", "simulation", "angle", "eccentricity", "angle", "predict", "retinal", "hypothesis", "main", "effect", "fa", "ade", "depth", "reflect", "fact", "shallower", "fa", "ade", "tend", "perceive", "have", "smaller", "angle", "deviation", "than", "deeper", "fa", "ade", "can", "clearly", "see", "fig.", "-lrb-", "b?d", "-rrb-", "because", "deeper", "fa", "ade", "reduce", "uncertainty", "retinal", "angle", "cue", "allow", "retinal", "hypothesis", "dominate", "more", "strongly", "over", "scene", "hypothesis", "previous", "work", "have", "demonstrate", "perceive", "distortion", "do", "become", "more", "objectionable", "even", "greater", "depths", "-lsb-", "Vangorp", "et", "al.", "2011", "-rsb-", "retinal", "hypothesis", "predict", "perceive", "angle", "deviate", "more", "from", "90", "simulation", "camera", "move", "laterally", "from", "capture", "position", "-lrb-", "see", "fig.", "top", "center", "-rrb-", "i.e.", "predict", "greater", "perceive", "angle", "when", "simulation", "eccentricity", "angle", "large", "have", "same", "sign", "follow-up", "experiment", "we", "present", "stimulus", "create", "from", "real", "photograph", "same", "experimental", "setting", "five", "out", "original", "participant", "be", "available", "follow-up", "experiment", "we", "use", "pc", "display", "only", "follow-up", "datum", "real", "image", "-lrb-", "available", "supplemental", "material", "-rrb-", "be", "very", "similar", "datum", "synthetic", "image", "validate", "we", "use", "synthetic", "stimulus", "main", "experiment", "retinal", "hypothesis", "-lrb-", "eq", "12", "16", "-rrb-", "predict", "different", "result", "different", "device", "because", "view", "distance", "relative", "cop", "differ", "across", "device", "we", "do", "observe", "systematic", "difference", "across", "device", "-lrb-", "see", "datum", "supplemental", "material", "-rrb-", "we", "explanation", "lack", "device", "effect", "effect", "distance", "from", "cop", "overshadow", "compression", "towards", "90", "due", "familiarity", "cube-like", "shape", "-lsb-", "Yang", "Kubovy", "1999", "Perkins", "1972", "-rsb-", "reason", "fig.", "-lrb-", "-rrb-", "plot", "prediction", "retinal", "hypothesis", "viewer", "cop", "second", "experiment", "design", "determine", "acceptability", "various", "angle", "distortion", "we", "ask", "participant", "indicate", "how", "acceptable", "give", "corner", "simulation", "90", "corner", "participant", "be", "show", "same", "image", "experiment", "again", "random", "order", "rate", "how", "close", "indicate", "corner", "each", "image", "look", "right", "angle", "5-point", "scale", "where", "correspond", "perfect", "close", "enough", "kind", "really", "way", "participant", "enter", "each", "rating", "use", "numerical", "keypad", "confirm", "entry", "press", "enter", "-lrb-", "see", "Fig.", "-rrb-", "next", "stimulus", "appear", "process", "repeat", "bonferroni-corrected", "significance", "level", "0.0125", "kendall?s", "coefficient", "concordance", "indicate", "effect", "size", "brief", "training", "session", "we", "encourage", "participant", "use", "full", "range", "scale", "distribute", "response", "uniformly", "instruction", "we", "show", "example", "almost", "severe", "angle", "distortion", "help", "they", "understand", "scale", "avoid", "bias", "we", "do", "tell", "they", "which", "rating", "assign", "those", "stimulus", "median", "rating", "5-point", "scale", "very", "informative", "so", "we", "use", "interpolated", "median", "-lsb-", "Revelle", "2008", "-rsb-", "summary", "statistic", "only", "assume", "equal", "distance", "between", "level", "uniformly", "distribute", "observation", "within", "level", "-lsb-", "Zar", "2010", "-rsb-", "which", "weaker", "assumption", "than", "one", "require", "compute", "means", "rating", "experiment", "take", "average", "3.1", "hour", "complete", "all", "four", "display", "condition", "seconds", "per", "trial", "within-subject", "reliability", "rating", "again", "fairly", "good", "over", "time", "quartile", "variation", "coefficient", "less", "than", "0.65", "all", "participant", "device", "common", "rating", "scale", "some", "participant", "give", "higher", "lower", "rating", "overall", "than", "other", "determine", "which", "effect", "statistically", "significant", "we", "perform", "repeated-measure", "Friedman", "test", "rating", "separately", "factor", "simulation", "angle", "eccentricity", "angle", "display", "device", "fa", "ade", "depth", "result", "show", "Table", "only", "statistically", "significant", "main", "effect", "fa", "ade", "depth", "large", "effect", "size", "-lrb-", "0.5", "-rrb-", "Fa", "ade", "depth", "average", "over", "Depths", "-rrb-", "-rrb-", "15", "30", "15", "30", "15", "30", "30", "15", "30", "15", "7.1", "32", "simulation", "angle", "-lsb-", "-rsb-", "7.1", "average", "over", "Depths", "Fa", "ade", "Depth", "0.33", "fa", "ade", "Depth", "0.67", "fa", "ade", "depth", "figure", "show", "result", "across", "participant", "each", "panel", "abscissa", "ordinate", "respectively", "simulation", "angle", "eccentricity", "angle", "color", "represent", "different", "interpolated", "median", "rating", "upper", "left", "panel", "show", "datum", "across", "fa", "ade", "depths", "most", "unacceptable", "stimulus", "be", "lower", "left", "upper", "right", "corner", "plot", "those", "stimulus", "have", "large", "simulation", "eccentricity", "angle", "same", "sign", "most", "acceptable", "stimulus", "be", "middle", "plot", "where", "simulation", "eccentricity", "angle", "small", "magnitude", "upper", "left", "lower", "right", "where", "large", "simulation", "eccentricity", "angle", "opposite", "sign", "comparison", "figure", "show", "most", "unacceptable", "stimulus", "be", "generally", "those", "have", "perceive", "angle", "most", "different", "from", "90", "other", "three", "panel", "show", "datum", "different", "fa", "ade", "depths", "largest", "most", "systematic", "effect", "be", "observe", "largest", "depth", "supplemental", "material", "plot", "different", "device", "which", "show", "rating", "quite", "consistent", "across", "display", "device", "we", "also", "perform", "follow-up", "experiment", "real", "stimulus", "which", "be", "consistent", "synthetic", "datum", "result", "supplemental", "material", "we", "primary", "goal", "develop", "predictive", "model", "perspective", "distortion", "base", "exist", "hypothesis", "we", "experimental", "result", "result", "angle-matching", "experiment", "fall", "in-between", "prediction", "retinal", "scene", "hypothesis", "i.e.", "predict", "retinal", "hypothesis", "corner", "be", "often", "perceive", "specify", "angle", "different", "than", "90", "those", "perceive", "angle", "be", "closer", "90", "than", "predict", "hypothesis", "however", "depth", "fa", "ade", "affect", "result", "greater", "depths", "yield", "result", "closer", "retinal", "hypothesis", "useful", "predictive", "model", "must", "incorporate", "effect", "create", "model", "we", "find", "best-fitting", "weight", "retinal", "scene", "prediction", "each", "depth", "value", "linear", "contribution", "depth", "value", "give", "where", "total", "define", "eq", "Figure", "10", "-lrb-", "-rrb-", "plot", "prediction", "we", "model", "-lrb-", "-rrb-", "show", "experimental", "result", "comparison", "fit", "angle", "measurement", "datum", "can", "improve", "take", "account", "flatten", "effect", "due", "other", "cue", "-lrb-", "see", "supplemental", "material", "-rrb-", "we", "do", "adopt", "because", "adversely", "affect", "rating", "prediction", "which", "more", "important", "application", "-lrb-", "section", "-rrb-", "we", "experiment", "provide", "wealth", "datum", "can", "use", "predict", "distortion", "provide", "guideline", "street-level", "ibr", "we", "now", "investigate", "correlation", "between", "perceive", "angle", "rating", "show", "allow", "we", "develop", "predictor", "distortion", "acceptability", "we", "discuss", "how", "we", "experimental", "result", "provide", "two", "guideline", "street-level", "ibr", "how", "use", "predictor", "ibr", "how", "specify", "capture", "position", "minimize", "distortion", "both", "guideline", "position", "orientation", "novel", "simulation", "camera", "must", "first", "convert", "parameter", "use", "we", "predictive", "model", "-lrb-", "see", "supplemental", "material", "-rrb-", "datum", "angle-matching", "rating", "experiment", "generally", "consistent", "one", "another", "evident", "from", "Figure", "12", "-lrb-", "-rrb-", "show", "deviation", "perceive", "angle", "from", "90", "-lrb-", "-rrb-", "show", "rating", "same", "stimulus", "plot", "qualitatively", "quite", "similar", "Figure", "11", "show", "consistency", "another", "way", "plot", "observe", "rating", "function", "predict", "perceive", "angle", "expect", "lowest", "-lrb-", "most", "acceptable", "-rrb-", "rating", "close", "90", "rating", "increase", "-lrb-", "become", "less", "acceptable", "-rrb-", "greater", "smaller", "angle", "we", "fit", "piecewise-linear", "function", "plot", "-lrb-", "represent", "line", "figure", "-rrb-", "use", "function", "predict", "rating", "from", "predict", "perceive", "angle", "2.05", "0.12", "-lrb-", "93.0", "-rrb-", "93.0", "2.05", "0.08", "-lrb-", "93.0", "-rrb-", "93.0", "-lrb-", "20", "-rrb-", "clamp", "range", "-lsb-", "-rsb-", "use", "experiment", "8.2", "predict", "rating", "novel", "condition", "naturally", "we", "want", "predict", "rating", "eccentricity", "simulation", "angle", "be", "sample", "we", "experiment", "give", "symmetry", "result", "both", "experiment", "we", "combine", "datum", "from", "appropriate", "condition", "-lrb-", "simulation", "eccentricity", "angle", "both", "inverted", "-rrb-", "obtain", "symmetric", "prediction", "even", "lower", "variance", "experimental", "rating", "datum", "could", "interpolate", "directly", "-lrb-", "fig.", "12", "-rrb-", "perceive", "angle", "datum", "could", "interpolate", "map", "rating", "-lrb-", "fig.", "12", "-rrb-", "however", "both", "approach", "limit", "range", "simulation", "eccentricity", "angle", "sample", "we", "experiment", "larger", "angle", "can", "occur", "street-level", "ibr", "therefore", "we", "use", "equation", "20", "expand", "perceive", "angle", "prediction", "60", "-lrb-", "fig.", "12", "-rrb-", "discontinuity", "preserve", "procedure", "novel", "camera", "position", "orientation", "might", "occur", "street-level", "ibr", "we", "first", "convert", "look", "up", "rating", "from", "Fig.", "12", "-lrb-", "-rrb-", "we", "describe", "look-up", "procedure", "supplemental", "material", "we", "show", "how", "prediction", "novel", "position", "orientation", "use", "street-level", "ibr", "application", "we", "want", "determine", "how", "positioning", "simulation", "camera", "affect", "acceptability", "result", "image", "left", "side", "Fig.", "13", "plot", "predict", "acceptability", "various", "camera", "position", "orientation", "relative", "fa", "ade", "large", "depth", "range", "each", "panel", "capture", "camera", "-lrb-", "represent", "black", "icon", "-rrb-", "face", "fa", "ade", "directly", "distance", "10", "abscissa", "each", "panel", "horizontal", "position", "simulation", "camera", "relative", "capture", "camera", "ordinate", "distance", "simulation", "camera", "fa", "ade", "different", "camera", "orientation", "show", "different", "panel", "15", "30", "from", "top", "bottom", "high", "low", "acceptability", "represent", "blue", "red", "respectively", "plot", "be", "derive", "part", "from", "Fig.", "12", "column", "appropriate", "simulation", "angle", "Fig.", "12", "-lrb-", "-rrb-", "map", "row", "Fig.", "13", "simulation", "distance", "equal", "capture", "distance", "plot", "show", "how", "one", "can", "position", "simulation", "camera", "maintain", "acceptable", "imagery", "acceptable", "region", "do", "vary", "much", "simulation", "angle", "narrow", "simulation", "camera", "get", "closer", "fa", "ade", "eccentricity", "simulation", "angle", "roughly", "opposite", "one", "another", "acceptable", "zone", "right", "side", "Fig.", "13", "provide", "insight", "how", "density", "capture", "camera", "affect", "image", "acceptability", "various", "simulation", "camera", "position", "abscissa", "ordinate", "panel", "same", "those", "left", "side", "from", "upper", "lower", "panel", "capture", "camera", "-lrb-", "black", "icon", "-rrb-", "have", "be", "position", "laterally", "interval", "c/2", "c/4", "one", "can", "see", "density", "capture", "camera", "have", "predictable", "effect", "acceptability", "result", "image", "wide", "range", "simulation", "camera", "position", "we", "analysis", "therefore", "provide", "principled", "method", "determine", "how", "many", "capture", "require", "give", "street", "scene", "set", "navigation", "path", "capture", "camera", "space", "out", "distance", "get", "from", "region", "cover", "one", "camera", "region", "cover", "next", "involve", "move", "through", "region", "large", "distortion", "-lrb-", "yellow", "red", "-rrb-", "inter-camera", "distance", "c/4", "-lrb-", "third", "row", "-rrb-", "result", "large", "region", "low", "predict", "distortion", "which", "probably", "sufficient", "street-level", "ibr", "we", "develop", "prototype", "interface", "street-level", "ibr", "use", "prediction", "describe", "previous", "section", "we", "first", "use", "implementation", "visualize", "quality", "give", "path", "perform", "study", "validate", "we", "prediction", "we", "develop", "application", "guide", "user", "stay", "zone", "acceptable", "quality", "predict", "Angles", "rating", "-lrb-", "extended", "domain", "-rrb-", "how", "positioning", "capture", "camera", "affect", "image", "acceptability", "from", "top", "bottom", "capture", "camera", "position", "interval", "c/2", "c/4", "visualization", "we", "implementation", "read", "set", "camera", "register", "use", "structure-from-motion", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "we", "assume", "camera", "directly", "face", "fa", "ade", "-lrb-", "i.e.", "side", "camera", "commercial", "capture", "car", "application", "-rrb-", "we", "fit", "plane", "reconstruction", "which", "serve", "proxy", "fa", "ade", "we", "design", "navigation", "path", "we", "interface", "visualize", "predict", "quality", "during", "process", "illustrate", "Fig.", "14", "interface", "show", "current", "simulated", "view", "inset", "show", "top", "view", "navigation", "path", "navigation", "path", "color", "code", "use", "heat", "map", "Fig.", "12", "-lrb-", "-rrb-", "show", "predict", "rating", "accompany", "video", "provide", "animated", "example", "navigation", "along", "path", "while", "use", "we", "interface", "we", "notice", "one", "significant", "temporal", "effect", "motion", "along", "path", "which", "perceive", "angle", "change", "quickly", "quite", "disconcerting", "we", "can", "use", "we", "heat", "map", "predict", "angle", "predict", "path", "order", "avoid", "they", "during", "navigation", "Validation", "user", "study", "evaluate", "we", "prediction", "we", "employ", "interface", "user", "study", "goal", "study", "determine", "how", "well", "we", "prediction", "agree", "user", "observation", "situation", "quite", "similar", "street-level", "ibr", "navigation", "path", "real", "stimulus", "we", "use", "three", "dataset", "correspond", "different", "scene", "figure", "14", "each", "scene", "we", "provide", "three", "different", "path", "one", "path", "predict", "low", "quality", "-lrb-", "rating", "3.5", "-rrb-", "one", "medium", "quality", "-lrb-", "2.5", "-rrb-", "one", "high", "quality", "-lrb-", "2.5", "-rrb-", "we", "create", "web", "page", "pre-recorded", "video", "each", "path", "we", "instruct", "participant", "look", "specific", "corner", "when", "appear", "middle", "screen", "-lrb-", "indicate", "red", "dot", "-rrb-", "participant", "rate", "perceive", "distortion", "same", "manner", "experiment", "i.e.", "how", "close", "90", "do", "angle", "look", "choose", "value", "same", "five-point", "scale", "-lrb-", "sec", "-rrb-", "total", "nine", "path", "-lrb-", "three", "path", "three", "scene", "-rrb-", "three", "video", "scene", "be", "present", "single", "page", "participant", "be", "ask", "adjust", "relative", "rating", "between", "three", "video", "procedure", "illustrate", "accompany", "video", "91", "people", "perform", "study", "own", "computer", "screen", "result", "summarize", "Fig.", "15", "which", "plot", "observe", "rating", "function", "predict", "rating", "separately", "three", "scene", "correlation", "between", "predict", "observe", "rating", "moderate", "-lrb-", "0.5", "-rrb-", "first", "two", "scene", "strong", "-lrb-", "0.8", "-rrb-", "third", "thus", "prediction", "be", "reasonably", "good", "despite", "many", "difference", "between", "experiment", "use", "generate", "prediction", "-lrb-", "static", "scene", "well", "controlled", "condition", "-rrb-", "user", "study", "-lrb-", "unstructured", "dynamic", "scene", "-rrb-", "application", "we", "also", "develop", "interactive", "application", "base", "implementation", "illustrate", "Fig.", "16", "interface", "show", "simulated", "view", "top", "view", "scenario", "inset", "user", "view", "particular", "position", "translate", "and/or", "rotate", "user", "translate", "-lrb-", "fig.", "16", "-rrb-", "inset", "show", "predict", "rating", "all", "camera", "position", "while", "keep", "same", "camera", "orientation", "Fig.", "13", "-lrb-", "left", "-rrb-", "show", "similar", "visualization", "three", "particular", "simulation", "angle", "when", "user", "turn", "-lrb-", "fig.", "16", "-rrb-", "visualization", "show", "rating", "all", "camera", "orientation", "keep", "camera", "position", "fix", "user", "can", "translate", "turn", "wish", "long", "stay", "within", "zone", "acceptable", "quality", "application", "prevent", "user", "from", "reach", "camera", "position", "orientation", "correspond", "predict", "rating", "higher", "than", "threshold", "instead", "show", "blinking", "placeholder", "current", "camera", "position", "illustrate", "accompany", "video", "we", "use", "rating", "value", "threshold", "work", "we", "extend", "retinal", "hypothesis", "from", "vision", "science", "literature", "so", "could", "apply", "view", "fa", "ade", "street-level", "ibr", "we", "perform", "angle-matching", "experiment", "measure", "perceptual", "distortion", "occur", "system", "which", "show", "perceive", "distortion", "between", "compete", "retinal", "scene", "hypothesis", "depend", "fa", "ade", "depth", "we", "also", "perform", "rating", "experiment", "determine", "acceptability", "distortion", "measure", "first", "experiment", "we", "fit", "analytic", "function", "perceived-angle", "datum", "order", "create", "predictive", "model", "perceptual", "distortion", "street-level", "ibr", "correlate", "model", "rating", "datum", "we", "can", "predict", "acceptability", "different", "distortion", "from", "we", "develop", "guideline", "acceptable", "navigation", "region", "capture", "position", "streetlevel", "ibr", "finally", "we", "develop", "application", "predict", "quality", "associate", "different", "navigation", "path", "perform", "confirmatory", "study", "show", "we", "result", "generalize", "realistic", "navigation", "system", "we", "study", "currently", "limit", "axis-aligned", "geometry", "typical", "fa", "ade", "keep", "complexity", "under", "control", "however", "all", "geometry", "view", "restriction", "can", "address", "derive", "appropriate", "extended", "retinal", "hypothesis", "predictive", "model", "from", "first", "principle", "demonstrate", "Sec", "very", "fruitful", "aspect", "project", "methodology", "we", "develop", "we", "start", "common", "observation", "we", "tolerance", "change", "view", "angle", "when", "look", "texture-mapped", "fa", "ade", "Vision", "science", "provide", "well-founded", "explanation", "process", "underlie", "phenomenon", "do", "provide", "directly", "applicable", "model", "however", "methodology", "geometric", "tool", "from", "vision", "science", "allow", "we", "develop", "we", "extended", "hypothesis", "design", "run", "we", "experiment", "addition", "we", "be", "able", "develop", "useful", "application", "theory", "experimental", "result", "we", "firmly", "believe", "geometric", "experimental", "framework", "provide", "solid", "basis", "study", "perception", "ibr", "more", "general", "setting", "stereo", "viewing", "even", "transition", "artifact", "have", "recently", "draw", "attention", "graphic", "community", "-lsb-", "Morvan", "O?Sullivan", "2009", "Stich", "et", "al.", "2011", "-rsb-", "we", "thank", "reviewer", "comment", "all", "participant", "time", "we", "also", "thank", "Jean-Pierre", "Merlet", "Emmanuelle", "Chapoulie", "val", "morash", "help", "Charles", "Verron", "Michael", "Wand", "Karol", "Myszkowski", "Holly", "Rushmeier", "comment", "Blend", "Swap", "user", "Sebastian", "Erler", "abab", "fa", "ade", "model", "work", "support", "INRIA", "CRISP", "associate", "team", "EU", "IP", "project", "verve", "-lrb-", "www.verveconsortium.eu", "-rrb-", "well", "research", "donation", "Adobe", "Autodesk", "Emily", "A.", "Cooper", "acknowledge", "support", "from", "NDSEG", "Fellowship", "Program", "National", "Science", "Foundation", "under", "Grant", "dge", "1106400" ],
  "content" : "We modified the latter hypothesis so that it extends to street-level IBR. However, the resulting images are only correct when viewed from where the original photographs were taken; when moving away from this point, distortions can become quite large, as shown in Fig. 1(b) . Two terms are important for understanding the images created in street-level IBR and users? perceptions of those images. We make three primary contributions: ? We modify the retinal hypothesis to extend it to street-level IBR. The quantitative model that emerges from this modification uses perspective information to generate predicted 3D percepts. ? In two perceptual experiments, we determine how well the extended retinal and scene hypotheses predict perceptual outcomes. We find that outcomes fall in-between the two predictions and that those outcomes are very consistent with viewers? ratings of how acceptable different views are. ? We develop a predictive model for perceptual distortions in streetlevel IBR, and use it to present guidelines for acceptability of novel views and capture camera density. We also perform a validation study of our predictions, and illustrate their use to guide users of street-level IBR so that images of novel views are acceptable during navigation. When a perspectively correct picture is viewed from the COP, people are quite accurate at recovering the 3D geometry of the original scene, including the slants of surfaces in that scene [Smith and Smith 1961, Cooper et al. 2012]. In some situations, the experimental evidence favors the scene hypothesis. In other situations, the experimental evidence favors the retinal hypothesis, even for small off-axis displacements [Banks et al. 2009]. Thus the scene and retinal hypotheses account best for perceptual outcomes in different situations. Our analysis also applies to other types of streetlevel IBR such as Microsoft PhotosynthTM [Snavely et al. 2006], Street Slide [Kopf et al. 2010], and even multi-perspective images [Yu et al. 2010] as long as corners in fa  ?ades are only deformed by perspective projections and not by the image deformations applied to align and stitch images into panoramas. In this section, we describe an extended retinal hypothesis for streetlevel IBR. In describing the hypothesis, we consider the viewing of corners on building fa  ?ades (e.g., corners of balconies). The result is a prediction for the perceived angle of such corners. The four steps involved in the process are: 1. Capture: Street-level panoramas (or photographs) are captured, and camera positions and orientations are registered. Projection: The captured imagery is mapped onto simplified geometry, often only a ground plane and a few fa  ?ade planes. Simulation: This scene is visualized from the different viewpoint of a virtual or simulation camera. Display: The resulting image is presented on a display device and viewed by the user. As explained in Sec. To develop our extended retinal hypothesis, we derive the coordinates of vanishing points for an arbitrary corner seen in the simulation camera image (e.g., a balcony corner). This will allow us to predict the angle perceived according to the projected retinal image. We restrict our derivation and experiments to planar proxies and box-like balconies. We show in the validation study how our results can be applied in other situations. As sketched in Fig. 2 , the capture camera coordinate system has its origin at the camera?s nodal point (optical center), the x-axis points to the right, the y-axis points up and the z-axis points forward. The camera captures a fa  ?ade with corners (in black). We use the following two angles extensively: the eccentricity angle, denoted ? e , defined by the direction normal to the fa  ?ade (in grey on the left) and the line between the corner and the origin, and the simulation angle, denoted ? s , which is the angle between the normal direction and the line between the simulation camera nodal point and the corner (right). The left inset shows the image of a capture camera, while the righthand two insets show simulation (virtual) images. lation angle affects final image quality. point with parameter t that runs to infinity on the fa  ?ade, to the side that the simulation camera will be turned: For the front face: For the side face: The insets of Fig. 3 (left) illustrate this step. The projection step in the IBR pipeline consists of reprojecting the panorama onto a planar proxy. Assume that the planar proxy is at distance c; to project the image onto it, we replace the capture camera with a projector at the same position, orientation and focal length f c . The effect on vanishing point coordinates is simply to set the coordinate z c to c. We assume that the simulation camera points straight at the corner with eccentricity angle ? e , from a distance d and simulation angle ? s ( Fig. 3 , center). We transform the vanishing points to the simulation camera coordinate system and then perform perspective projection of the simulation camera (i.e., the IBR itself), with focal length f s . These coordinates are: (8)\n          if otherwise In a real camera, the image size is the sensor size. We model this by magnifying the image from sensor size to display size by the factor M . The viewer sits straight in front of the display at a viewing distance v; see Fig. 3 (right). veridical vanishing points. Specifically: s if\n          (13)\n          otherwise If e and s have opposite signs, the perceived angle is: A discontinuity arises at eccentricity angle ? e = 0 because the side face of the corner flips between facing left and facing right. We conducted two psychophysical experiments to determine how the image distortions in typical street-level IBR applications are perceived: 1. An angle-matching experiment that tested the predictions of the scene and extended retinal hypothesis. The results allow us to predict the magnitude of perceived distortions in novel scenes. A rating experiment that determined the subjective acceptability of angle distortions. The results allow us to determine which perceived angle distortions are acceptable and which are not; we will use this subsequently for our guidelines (Sec. For certain applications (e.g., navigation or sightseeing), one may be interested in the overall perception of shape. Since shapes involve many angles and different contextual priors, we believe it is likely to lead to more acceptable perceived distortions. We opted to study angles, which is a stricter criterion with a clear quantitative task, used before by Watt et al. [2005]. We simulated the typical unstructured lumigraph workflow in a fully synthetic, offline rendering pipeline to maximize control and  image quality. Synthetic stimuli allow us to create depth variations of the same fa  ?ade, and to specify exact and consistent values for eccentricities and capture camera positions across different fa  ?ades, as well as to create images without clutter (e.g., cars, lamp posts, trees), and avoiding the presence of other cues in the stimuli. Three fa  ?ades (Figs. 4 and 5) were created with 90? convex corners at eccentricity angles of ? e = ?7.1? and ?32.0?. The fa  ?ades were lit by a sun and sky illumination model to create realistic images. A wideangle frontoparallel image of three realistic fa  ?ades was rendered from a distance of 40 m. The resulting images were then projected onto a single plane. The same stimuli were used for both experiments. Fig. 5 provides an overview of the stimulus creation parameters. The extended retinal hypothesis (Eq. 16) can be used to predict the perceived angle of corners in our stimuli. Fig. 7(a) shows the extended retinal hypothesis prediction for the viewer at the center of projection. The angle perceived when viewing a 90? corner is plotted as a function of simulation angle. Different colored lines represent the predictions for different eccentricity angles. Each corner has a constant eccentricity angle in a given captured image: e.g., the corner ? on the left in Fig. 5 has a negative eccentricity angle ? e = ?32 relative to the blue capture camera at the bottom, so it is represented by the dark blue line in Fig. 7(a) . As the simulation camera shown in green rotates around that corner from left to right, the simulation angle ? s increases and the predicted perceived angle gradually decreases towards the correct value. When the eccentricity angle is positive, the predicted perceived corner angle increases with increasing simulation angle. We used four types of displays that differed substantially in size: screen diagonals ranged from 3.5? to 55?. The devices were held upright (PC, TV) or at an angle of about 45? (phone, tablet) by fixed stands, and the viewer was positioned at the specified distance. More details of the experiment implementation are in the supplemental material and the video. It is common practice in vision science to use a relatively small number of participants who are tested quite extensively (e.g., Ernst and Banks [2002]). We followed this practice by testing six participants extensively (on average 7.5 h for a total of 3072 measurements each). In so doing, we were able to test many experimental conditions and thereby put the hypotheses to rigorous test. The six participants were 20?32 years old; three were female. They all had normal or corrected-to-normal vision, and were paid for their participation. They all participated in both experiments and were na?ve about the experimental hypotheses. The tasks in these experiments are quite involved, so we needed to ensure that participants understood the instructions, and that they could give reasonably consistent responses. To do this, candidates passed a pre-test, and only those satisfying comprehension and consistency requirements were invited to perform the entire test. Details of this pre-test are given in the supplementary material. In both experiments, all stimuli ? five simulation angles, four eccentricity angles and three fa  ?ades, each with three balcony depths ? were presented twice. In addition, three particular stimuli were presented eight times to allow us to assess the consistency of responses. Thus, there were 384 stimulus presentations for each of the four display device types in each experiment. The order of stimuli presented was randomized. In addition, we used within-subject counterbalancing. There were two experiments (hinge setting and rating) and four devices, giving eight conditions. We further split these each into three parts, giving 24 parts. These 24 parts were randomized and split into separate sessions ? usually four 2-hour sessions. Participants took breaks whenever they needed. At the start of each part, participants were given extensive instructions. The first experiment determined how a 90? corner is perceived after it undergoes distortion due to the position of the simulation camera. Participants indicated the perceived angle of a corner by setting a real convex hinge to look the same (see Figure 6 ). We used a real hinge, similar to Watt et al. [2005], because images of a hinge may themselves be affected by perceptual distortions. Participants were shown images from our pool of stimuli (Sec. 4.1) in random order. Participants then rotated the hinge device about the vertical axis until the slant of one side of the hinge appeared to match the side of the corner in the image. They then opened or closed the hinge until the perceived angles of the hinge and corner were equal. This procedure is illustrated in the accompanying video. The angle-matching experiment took 4.3 hours on average to complete, or about 10 seconds per trial. We used the eight repeated presentations of three stimuli to assess response consistency. Those data showed that participants were self-consistent (average standard deviation within-subjects was 5.3?). The data from different participants were also quite similar (average standard deviation between-subjects was 8.7?), so we averaged across participants to create the data figures shown here. Similarly, we did not observe systematic differences across display devices (average standard deviation between devices was 6.3?), so we averaged over devices. See supplementary material for the data from individual participants and devices. Figure 7 shows the predicted (a), and observed (b?e) hinge angle settings. Each panel plots angle setting as a function of simulation angle; different colored lines represent different eccentricity angles. The colored lines in panel (a) represent the predictions for the ex- tended retinal hypothesis and the horizontal dotted lines at 90? the predictions for the scene hypothesis. The results are qualitatively quite similar to the predictions of the extended retinal hypothesis. For positive eccentricity angles, the extended retinal hypothesis predicts that increases in simulation angle will yield increases in perceived angle. For negative eccentricity, the prediction is the opposite. All of the data are consistent with those predictions. However, the range of perceived angles is more compressed than predicted by the retinal hypothesis, which suggests an influence of the scene hypothesis. This partial influence of the scene hypothesis is greater for small fa  ?ade depths ( Figure 7 , b?d). To determine which effects are statistically significant, we performed a repeated-measures analysis of variance (ANOVA) on the data with simulation angle, eccentricity angle, display device and fa  ?ade depth as factors. The results are shown in Table 1 . There were statistically significant main effects of all factors, but only eccentricity angle 2 and fa  ?ade depth have a large enough effect size (? G > 0.26) to result in noticeable differences in perceived angle [Bakeman 2005]. There was also a significant and large two-way interaction between simulation angle and eccentricity angle, as predicted by the retinal hypothesis. The main effect of fa  ?ade depth reflects the fact that shallower fa  ?ades tend to be perceived as having smaller angle deviations than deeper fa  ?ades as can clearly be seen in Fig. 7 (b?d). This is because deeper fa  ?ades reduce the uncertainty in the retinal angle cues, allowing the retinal hypothesis to dominate more strongly over the scene hypothesis. Previous work has demonstrated that the perceived distortions do not become more objectionable with even greater depths [Vangorp et al. 2011]. The retinal hypothesis predicts that perceived angles deviate more from 90? as the simulation camera moves laterally from the capture position (see Fig. 2 , top center); i.e., it predicts greater perceived angles when the simulation and eccentricity angles are large and have the same sign. In a follow-up experiment, we presented stimuli created from real photographs in the same experimental setting. Five out of the 6 original participants were available for this follow-up experiment. We used the PC display only. The follow-up data with real images (available in the supplemental material) were very similar to the data with synthetic images. This validates our use of synthetic stimuli in the main experiment. The retinal hypothesis (Eqs. 12?16) predicts different results for the different devices because the viewing distances relative to the COP differed across devices. But we did not observe systematic differences across devices (see data in supplemental material). Our explanation for the lack of a device effect is that the effect of distance from the COP is overshadowed by the compression towards 90? due to the familiarity of cube-like shapes [Yang and Kubovy 1999, Perkins 1972]. For this reason, Fig. 7 (a) plots the predictions of the retinal hypothesis with the viewer at the COP. The second experiment was designed to determine the acceptability of various angle distortions. We asked participants to indicate how acceptable a given corner was as a simulation of a 90? corner. Participants were shown the same images as in Experiment 1, again in random order. They rated how close the indicated corner in each image looked to a right angle on a 5-point scale where 1 to 5 corresponded to ?perfect?, ?close enough?, ?kind of?, ?not really?, and ?no way! Participants entered each rating using a numerical keypad and confirmed the entry by pressing ?Enter? (see Fig. 8 ). The next stimulus then appeared and the process repeated. for the Bonferroni-corrected significance level ? = 0.0125, and Kendall?s coefficient of concordance W indicating effect size. In a brief training session, we encouraged participants to use the full range of the scale and to distribute their responses uniformly. In the instructions, we showed examples with ?almost no? and ?severe? angle distortions to help them understand the scale, but to avoid bias we did not tell them which ratings to assign for those stimuli. Medians of ratings on a 5-point scale are not very informative, so we use interpolated medians [Revelle 2008]. This summary statistic only assumes equal distances between the levels and uniformly distributed observations within the levels [Zar 2010], which are weaker assumptions than the ones required for computing means. The rating experiment took on average 3.1 hours to complete on all four display conditions, or 8 seconds per trial. The within-subject reliability of ratings is again fairly good over time, with a quartile variation coefficient of less than 0.65 for all participants and devices. As is common with rating scales, some participants gave higher or lower ratings overall than others. To determine which effects are statistically significant, we performed repeated-measures Friedman tests on the ratings separately for the factors simulation angle, eccentricity angle, display device, and fa  ?ade depth. The results are shown in Table 2 . There was only a statistically significant main effect of fa  ?ade depth with a large effect size (W > 0.5). Fa  ?ade Depth 1 m Average over Depths d) e) 15 30 0 15 30 0 15 30 ?30 ?15 ?30 ?15 7.1? 32? Simulation Angle ? s [?] ?7.1?\n          Average over Depths Fa  ?ade Depth 0.33 m Fa  ?ade Depth 0.67 m Fa  ?ade Depth 1 m Figure 9 shows the results across participants. In each panel the abscissa and ordinate are respectively simulation angle and eccentricity angle, and colors represent different interpolated medians of ratings. The upper left panel shows the data across fa  ?ade depths. The most unacceptable stimuli were in the lower left and upper right corners of these plots. Those stimuli have large simulation and eccentricity angles of the same sign. The most acceptable stimuli were in the middle of the plot ? where the simulation and eccentricity angles are small in magnitude ? and the upper left and lower right where the large simulation and eccentricity angles are opposite in sign. A comparison with Figure 7 shows that the most unacceptable stimuli were generally those that had perceived angles most different from 90?. The other three panels show the data for different fa  ?ade depths. The largest and most systematic effects were observed for the largest depth. In supplemental material are the plots for different devices, which show that the ratings are quite consistent across display devices. We also performed a follow-up experiment with real stimuli, which were consistent with the synthetic data; results are in the supplemental material. Our primary goal is to develop a predictive model of perspective distortion based on existing hypotheses and our experimental results. The results of the angle-matching experiment fell in-between the predictions of the retinal and scene hypotheses: i.e., as predicted by the retinal hypothesis, corners were often perceived as specifying angles different than 90?, but those perceived angles were closer to 90? than predicted by that hypothesis. However, the depth of the fa  ?ade affected the results, with greater depths yielding results closer to the retinal hypothesis. A useful predictive model must incorporate these effects. To create such a model, we found best-fitting weights for the retinal and scene predictions for each depth value. The linear contribution of depth value is given by: where ? total is defined in Eq. In Figure 10 , (a) plots the predictions of our model and (b) shows the experimental results for comparison. The fit to the angle measurement data can be improved by taking into account a ?flattening? effect due to other cues (see supplemental material), but we did not adopt it because it adversely affects rating predictions, which are more important for applications (Section 8). Our experiments provide a wealth of data that can be used to predict distortions and provide guidelines for street-level IBR. We now investigate the correlation between perceived angles and ratings, and show that it allows us to develop a predictor of distortion acceptability. We then discuss how our experimental results provide two guidelines for street-level IBR: how to use the predictor for IBR and how to specify capture positions to minimize distortion. For both guidelines, the position and orientation of a novel simulation camera must first be converted to the parameters ? s , ? e and d used in our predictive model (see supplemental material). The data of the angle-matching and rating experiments are generally consistent with one another. This is evident from Figure 12: (a) shows the deviation of perceived angle from 90? and (b) shows the ratings for the same stimuli. The plots are qualitatively quite similar. Figure 11 shows the consistency in another way by plotting observed ratings as a function of the predicted perceived angle. As expected, the lowest (most acceptable) ratings are close to 90? and the ratings increase (become less acceptable) for greater and smaller angles. We fit a piecewise-linear function to this plot (represented by lines in the figure) and use this function to predict ratings from predicted perceived angles: 2.05 + 0.12 ? (? ? 93.0?)/? if ? ? 93.0? = 2.05 ? 0.08 ? (? ? 93.0?)/? if ? ? 93.0?, (20) clamped to the range [1, 5] used in the experiment. 8.2 Predicting Ratings in Novel Conditions\n        Naturally, we wanted to predict ratings at eccentricity and simulation angles that were not sampled in our experiments. Given the symmetry in the results of both experiments, we combined the data from the appropriate conditions (simulation and eccentricity angle both inverted) to obtain symmetric predictions with even lower variance. The experimental rating data could be interpolated directly ( Fig. 12 , b) or the perceived angle data could be interpolated and then mapped to ratings ( Fig. 12 , a). However, both approaches are limited to the range of simulation and eccentricity angles sampled in our experiments, and larger angles can occur in street-level IBR. Therefore, we used Equation 20 to expand perceived angle predictions to ?60? ( Fig. 12 , d). The discontinuity at ? e = 0? is preserved in this procedure. For novel camera positions and orientations that might occur in street-level IBR, we first convert to ? e and ? s , and then look up the rating from Fig. 12 (d). We describe the look-up procedure in supplemental material. 9 we show how predictions for novel positions and orientations are used in a street-level IBR application. We want to determine how positioning of the simulation camera affects the acceptability of the resulting images. The left side of Fig. 13 plots predicted acceptability for various camera positions and orientations relative to a fa  ?ade with large depth range. In each panel, the capture camera (represented by a black icon) is facing the fa  ?ade directly at a distance c of 10 m. The abscissa of each panel is the horizontal position of the simulation camera relative to the capture camera and the ordinate is the distance of the simulation camera to the fa  ?ade. Different camera orientations are shown in different panels: ? s is 0?, 15? and 30? from top to bottom. High and low acceptability is represented by blue and red, respectively. These plots were derived in part from Fig. 12 : a column at the appropriate simulation angle in Fig. 12 (d) maps to a row in Fig. 13 for a simulation distance equal to the capture distance. These plots show how one can position the simulation camera to maintain acceptable imagery. The acceptable region does not vary much with simulation angle and it narrows as the simulation camera gets closer to the fa  ?ade. The eccentricity and simulation angles are roughly opposite to one another in the acceptable zone. The right side of Fig. 13 provides insight into how the density of capture cameras affects image acceptability for various simulation camera positions. The abscissas and ordinates of these panels are the same as those on the left side. From the upper to lower panel, the capture cameras (black icons) have been positioned laterally at intervals of c, c/2, and c/4. As one can see, the density of capture cameras has predictable effects on the acceptability of the resulting images for a wide range of simulation camera positions. Our analysis, therefore, provides a principled method for determining how many captures are required for a given street scene and set of navigation paths. If the capture cameras are spaced out by distance c, getting from the region covered by one camera to the region covered by the next involves moving through regions of large distortion (yellow or red). An inter-camera distance of c/4 (third row) results in large regions of low predicted distortion, which is probably sufficient for street-level IBR. We developed a prototype interface for street-level IBR that uses the predictions described in the previous section. We first use this implementation to visualize the quality of a given path, and perform a study to validate our predictions. We then develop an application that guides users to stay in zones of acceptable quality. Predicted Angles to Ratings (Extended Domain) How positioning of capture cameras affects image acceptability. From top to bottom, capture cameras are positioned at intervals of c, c/2, and c/4. Visualization Our implementation reads a set of cameras registered using structure-from-motion [Snavely et al. 2006]. We assume that the cameras are directly facing the fa  ?ade (i.e., a side camera in a commercial ?capture car? for such applications). We fit a plane to the reconstruction which serves as the proxy for the fa  ?ade. We designed navigation paths in our interface, visualizing the predicted quality during the process. This is illustrated in Fig. 14 . The interface shows the current simulated view and the inset shows a top view of the navigation path. The navigation path is color coded using the heat map of Fig. 12 (d) to show the predicted ratings. The accompanying video provides an animated example of navigation along the path. While using our interface we noticed one significant temporal effect: motions along a path on which the perceived angle changes quickly are quite disconcerting. We can use our heat map of predicted angles to predict such paths in order to avoid them during navigation. Validation User Study To evaluate our predictions, we employed this interface in a user study. The goal of the study was to determine how well our predictions agree with user observations in a situation quite similar to street-level IBR: a navigation path with real stimuli. We used three datasets corresponding to the different scenes in Figures 1, 2 and 14. For each scene we provide three different paths: one path was predicted to be low quality (rating of 3?3.5), one medium quality (2.5?3), and one high quality (2?2.5). We created a web page with pre-recorded videos of each path. We instructed participants to look at a specific corner when it appeared in the middle of the screen (indicated by a red dot). Participants then rated the perceived distortion in the same manner as for Experiment 2: i.e., ?how close to 90? does the angle look? They chose a value on the same five-point scale (Sec. 6) for a total of nine paths (three paths for three scenes). The three videos of a scene were presented on a single page, and participants were asked to adjust their relative ratings between the three videos. This procedure is illustrated in the accompanying video. 91 people performed the study on their own computer screens. The results are summarized in Fig. 15 , which plots observed ratings as a function of predicted ratings separately for the three scenes. The correlation between predicted and observed ratings was moderate (r > 0.5) for the first two scenes and strong (r > 0.8) for the third. Thus, the predictions were reasonably good despite the many differences between the experiments used to generate the predictions (static scenes with well controlled conditions) and this user study (unstructured dynamic scenes). Application We also developed an interactive application based on this implementation, illustrated in Fig. 16 . The interface shows the simulated view and a top view of the scenario in the inset. The user starts viewing at a particular position, and then translates and/or rotates. If the user is translating ( Fig. 16 , a), the inset shows predicted ratings for all camera positions while keeping the same camera orientation. Fig. 13 (left) shows a similar visualization for three particular simulation angles. When the user turns ( Fig. 16 , b), the visualization shows ratings for all camera orientations keeping the camera position fixed. The user can translate and turn as they wish as long as they stay within the zone of ?acceptable? quality. The application prevents the user from reaching a camera position or orientation that corresponds to a predicted rating higher than a threshold, and instead shows a blinking placeholder at the current camera position. This is illustrated in the accompanying video. We used a rating value of 3 as the threshold. In this work, we extended the retinal hypothesis from the vision science literature so that it could be applied to viewing fa  ?ades in street-level IBR. We then performed an angle-matching experiment to measure the perceptual distortions that occur in such systems, which showed that perceived distortions are in between the competing retinal and scene hypotheses ? depending on the fa  ?ade depth. We also performed a rating experiment to determine the acceptability of the distortions measured in the first experiment. We fit analytic functions to the perceived-angle data in order to create a predictive model of perceptual distortions in street-level IBR. By correlating the model to the rating data, we can predict the acceptability of different distortions. From this, we developed guidelines for acceptable navigation regions and capture positions for streetlevel IBR. Finally, we developed an application that predicts the quality associated with different navigation paths, and performed a confirmatory study that showed that our results generalize to realistic navigation systems. Our study is currently limited to the axis-aligned geometry of typical fa  ?ades to keep the complexity under control. However, all geometry and viewing restrictions can be addressed by deriving the appropriate extended retinal hypothesis and predictive model from first principles as demonstrated in Sec. A very fruitful aspect of this project is the methodology we developed. We started with a common observation: our tolerance to changes in viewing angle when looking at texture-mapped fa  ?ades. Vision science provided well-founded explanations of the processes underlying this phenomenon, but did not provide a directly applicable model. However, methodologies and geometric tools from  vision science allowed us to develop our extended hypothesis, and to design and run our experiments. In addition, we were able to develop useful applications of the theory and experimental results. We firmly believe that this geometric and experimental framework provides a solid basis for studying perception of IBR in more general settings, such as stereo viewing, and even transition artifacts that have recently drawn attention in the graphics community [Morvan and O?Sullivan 2009, Stich et al. 2011]. We thank the reviewers for their comments and all participants for their time. We also thank Jean-Pierre Merlet, Emmanuelle Chapoulie and Val Morash for their help, Charles Verron, Michael Wand, Karol Myszkowski and Holly Rushmeier for their comments, and Blend Swap users Sebastian Erler and abab for the fa  ?ade models. This work was supported by the INRIA CRISP associate team, the EU IP project VERVE ( www.verveconsortium.eu ) as well as research donations by Adobe and Autodesk. Emily A. Cooper acknowledges support from the NDSEG Fellowship Program and National Science Foundation under Grant No. DGE 1106400.",
  "resources" : [ ]
}