{
  "uri" : "sig2011a-a130-dale_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011a/a130-dale_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Video Face Replacement",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Mahmoud-Afifi",
      "name" : "Mahmoud",
      "surname" : "Afifi"
    }, {
      "uri" : "http://drinventor/Khaled F.-Hussain",
      "name" : "Khaled F.",
      "surname" : "Hussain"
    }, {
      "uri" : "http://drinventor/Hosny M.-Ibrahim",
      "name" : "Hosny M.",
      "surname" : "Ibrahim"
    }, {
      "uri" : "http://drinventor/Nagwa M.-Omar",
      "name" : "Nagwa M.",
      "surname" : "Omar"
    } ]
  },
  "bagOfWords" : [ "we", "showcase", "use", "we", "method", "variety", "example", "present", "result", "user", "study", "suggest", "we", "result", "difficult", "distinguish", "from", "real", "video", "footage", "we", "track", "face", "both", "source", "target", "video", "use", "3d", "multilinear", "model", "we", "warp", "source", "video", "both", "space", "time", "align", "target", "instead", "choose", "single", "best", "take", "final", "cut", "we", "system", "can", "combine", "e.g.", "mouth", "performance", "from", "one", "take", "eye", "brow", "expression", "from", "another", "produce", "video", "montage", "we", "video-based", "method", "particularly", "suitable", "case", "because", "we", "have", "control", "over", "capture", "exist", "footage", "contrast", "retargeting", "where", "source", "footage", "shoot", "use", "target", "audiovisual", "guide", "roughly", "match", "timing", "performance", "source", "target", "can", "very", "different", "similar", "dub", "different", "subject", "we", "system", "do", "rely", "few", "assumption", "about", "input", "video", "second", "we", "assume", "pose", "face", "source", "target", "video", "45", "from", "frontal", "otherwise", "automatic", "tracking", "alignment", "face", "fail", "-lrb-", "Sec", "we", "demonstrate", "applicability", "we", "approach", "number", "example", "four", "scenario", "video", "montage", "-lrb-", "fig.", "-rrb-", "dubbing", "-lrb-", "fig.", "-rrb-", "retargeting", "-lrb-", "fig.", "10", "-rrb-", "replacement", "-lrb-", "fig.", "-rrb-", "we", "briefly", "describe", "previous", "work", "face", "replacement", "compare", "approach", "we", "system", "example", "method", "Blanz", "et", "al.", "-lsb-", "2004", "-rsb-", "fit", "morphable", "model", "face", "both", "source", "target", "image", "render", "source", "face", "parameter", "estimate", "from", "target", "image", "flow", "derive", "from", "3d", "morphable", "model", "fit", "source", "target", "photo", "clear", "whether", "any", "method", "could", "achieve", "temporally", "coherent", "result", "when", "apply", "video", "sequence", "notable", "example", "include", "recreation", "actor", "Matrix", "Reloaded", "-lsb-", "Borshukov", "et", "al.", "2003", "-rsb-", "curious", "case", "Benjamin", "Button", "-lsb-", "Robertson", "2009", "-rsb-", "Digital", "Emily", "project", "-lsb-", "Alexander", "et", "al.", "2009", "-rsb-", "Bregler", "et", "al.", "-lsb-", "1997", "-rsb-", "Ezzat", "et", "al.", "-lsb-", "2002", "-rsb-", "replace", "mouth", "region", "video", "match", "phoneme", "novel", "audio", "input", "use", "database", "training", "image", "same", "actor", "Kemelmacher-Shlizerman", "et", "al.", "-lsb-", "2010", "-rsb-", "make", "use", "image", "collection", "video", "celebrity", "available", "online", "replace", "face", "photo", "real-time", "base", "expression", "pose", "similarity", "we", "use", "multilinear", "model", "Vlasic", "al.", "-lsb-", "2005", "-rsb-", "capture", "identity", "expression", "viseme", "source", "target", "video", "contrast", "we", "approach", "blend", "source", "actor?s", "complete", "face", "performance", "all", "its", "nuance", "intact", "target", "figure", "show", "overview", "we", "method", "order", "replace", "source", "face", "target", "face", "we", "first", "model", "track", "facial", "performance", "both", "source", "target", "multilinear", "method", "datum", "Vlasic", "et", "al.", "-lsb-", "2005", "-rsb-", "track", "parameter", "attribute", "3d", "pose", "face", "-lrb-", "give", "rotation", "translation", "scale", "-rrb-", "over", "video", "sequence", "each", "frame", "pose", "multilinear", "model", "its", "parameter", "can", "use", "generate", "3d", "mesh", "match", "geometry", "subject?s", "face", "we", "reprocess", "original", "training", "datum", "from", "Vlasic", "et", "al.", "cover", "16", "identity", "expression", "visemes?a", "total", "400", "face", "scans?placing", "they", "correspondence", "face", "mesh", "extend", "beyond", "jaw", "chin", "region", "-lrb-", "sec", "some", "scenario", "important", "timing", "facial", "performance", "match", "precisely", "source", "target", "footage", "however", "might", "very", "tedious", "match", "timing", "exactly", "demonstrate", "numerous", "take", "typically", "necessary", "obtain", "compelling", "voiceover", "-lrb-", "e.g.", "when", "re-record", "dialog", "film", "-rrb-", "instead", "we", "only", "require", "coarse", "synchronization", "between", "source", "target", "video", "automatically", "retime", "footage", "generate", "precise", "match", "replacement", "after", "tracking", "retiming", "we", "blend", "source", "performance", "target", "video", "produce", "final", "result", "blending", "make", "use", "gradient-domain", "compositing", "merge", "source", "actor?s", "face", "target", "video", "minimize", "artifact", "we", "automatically", "compute", "optimal", "spatiotemporal", "seam", "through", "source", "target", "minimize", "difference", "across", "seam", "face", "mesh", "ensure", "region", "be", "combine", "compatible", "second", "stage", "we", "use", "seam", "merge", "gradient", "recover", "final", "composite", "video", "input", "footage", "all", "example", "except", "those", "reuse", "exist", "footage", "capture", "Canon", "T2i", "camera", "85", "mm", "50", "mm", "lens", "30", "frame", "per", "second", "in-lab", "sequence", "be", "light", "300", "studio", "light", "place", "left", "right", "front", "subject", "soften", "umbrella", "reflector", "when", "appropriate", "we", "use", "target", "video", "audio-visual", "guide", "during", "capture", "source", "-lrb-", "vice", "versa", "-rrb-", "approximately", "match", "timing", "pose", "actor", "be", "simply", "instruct", "face", "camera", "natural", "head", "motion", "account", "tracking", "track", "track", "face", "across", "sequence", "frame", "method", "Vlasic", "et", "al.", "-lsb-", "2005", "-rsb-", "compute", "pose", "attribute", "parameter", "multilinear", "face", "model", "best", "explain", "optical", "flow", "between", "adjacent", "frame", "sequence", "multilinear", "model", "hand", "original", "face", "datum", "can", "interpolate", "extrapolate", "generate", "new", "face", "where", "mode", "correspond", "vertex", "position", "4-mode", "model", "column", "vector", "parameter", "attribute", "correspond", "th", "mode", "-lrb-", "i.e.", "one", "expression", "viseme", "identity", "-rrb-", "3k-element", "column", "vector", "new", "vertex", "position", "operator", "mode-n", "product", "define", "between", "tensor", "matrix", "we", "refer", "reader", "Vlasic", "et", "al.", "-lsb-", "2005", "-rsb-", "more", "detail", "Initialization", "since", "tracking", "base", "optical", "flow", "initialization", "critical", "error", "initialization", "propagate", "throughout", "sequence", "moreover", "tracking", "can", "go", "astray", "troublesome", "frame", "e.g.", "due", "motion", "blur", "extreme", "pose", "change", "high", "frequency", "lighting", "occlusion", "therefore", "we", "also", "provide", "simple", "user", "interface", "can", "ensure", "good", "initialization", "can", "correct", "tracking", "troublesome", "frame", "interface", "allow", "user", "adjust", "position", "marker", "eye", "eyebrow", "nose", "mouth", "jawline", "from", "which", "best-fit", "pose", "model", "parameter", "compute", "user", "can", "alternate", "between", "adjust", "pose", "each", "attribute", "individually", "typically", "iteration", "each", "sufficient", "good", "initialization", "-lrb-", "fig.", "-rrb-", "we", "automatically", "detect", "face", "-lsb-", "Viola", "Jones", "2001", "-rsb-", "next", "we", "localize", "facial", "feature", "-lsb-", "Everingham", "et", "al.", "2006", "-rsb-", "-lrb-", "e.g.", "corner", "mouth", "eye", "nose", "-rrb-", "first", "frame", "sequence", "initial", "face", "mesh", "generate", "from", "multilinear", "model", "use", "user-specified", "set", "initial", "attribute", "correspond", "most", "appropriate", "expression", "viseme", "identity", "hold", "all", "one", "attribute?s", "parameter", "fix", "we", "can", "project", "multilinear", "model", "onto", "subspace", "correspond", "remain", "attribute", "e.g.", "third", "attribute", "3k", "matrix", "give", "fit", "parameter", "th", "attribute", "image", "space", "marker", "we", "take", "subset", "multilinear", "model", "correspond", "-lrb-", "-rrb-", "coordinate", "mesh", "vertex", "should", "align", "marker", "apply", "eq", "populate", "marker", "position", "transform", "coordinate", "frame", "model", "via", "inverse", "pose", "transformation", "even", "after", "multiple", "iteration", "tracking", "each", "which", "update", "identity", "parameter", "those", "parameter", "change", "very", "little", "from", "initial", "value", "therefore", "important", "have", "accurate", "initialization", "identity", "we", "employ", "FaceGen", "Modeller", "-lsb-", "Singular", "Inversions", "Inc.", "FaceGen", "generate", "3d", "mesh", "base", "frontal", "face", "image", "optionally", "profile", "image", "input", "image", "can", "extract", "from", "original", "video", "sequence", "download", "from", "internet", "when", "reuse", "exist", "footage", "input", "image", "need", "depict", "subject", "closed-mouth", "neutral", "expression", "FaceGen", "require", "minimal", "user", "input", "specify", "about", "10", "marker", "per", "image", "all", "mesh", "create", "FaceGen", "themselves", "correspondence", "therefore", "we", "can", "register", "FaceGen", "mesh", "multilinear", "model", "use", "same", "template-fitting", "procedure", "-lsb-", "Vlasic", "et", "al.", "2005", "-rsb-", "we", "use", "register", "original", "scan", "datum", "we", "fit", "multilinear", "model", "register", "FaceGen", "mesh", "use", "procruste", "alignment", "we", "current", "best-fit", "mesh", "use", "eq", "optimization", "we", "only", "use", "about", "percent", "original", "mesh", "vertex", "process", "typically", "converge", "10", "iteration", "key", "frame", "we", "can", "use", "same", "interface", "-lrb-", "fig.", "-rrb-", "adjust", "pose", "attribute", "parameter", "specific", "key", "frame", "where", "automatic", "tracking", "fail", "first", "we", "track", "subsequence", "between", "each", "pair", "user-adjusted", "key", "frame", "both", "forward", "reverse", "direction", "linearly", "interpolate", "two", "result", "we", "perform", "additional", "tracking", "iteration", "full", "sequence", "refine", "pose", "parameter", "estimate", "across", "key", "frame", "boundary", "note", "none", "result", "show", "paper", "require", "key", "framing", "spatial", "alignment", "from", "image", "sequence", "where", "-lrb-", "-rrb-", "denote", "value", "pixel", "position", "frame", "track", "produce", "sequence", "attribute", "parameter", "pose", "transformation", "each", "frame", "-lrb-", "-rrb-", "column", "vector", "vertex", "position", "compute", "from", "attribute", "parameter", "time", "use", "eq", "-lrb-", "-rrb-", "th", "vertex", "time", "t.", "subscript", "denote", "source", "target", "respectively", "align", "source", "face", "target", "frame", "we", "use", "face", "geometry", "from", "source", "sequence", "pose", "from", "target", "sequence", "frame", "align", "position", "th", "source", "vertex", "position", "give", "we", "also", "take", "texture", "from", "source", "image", "texture", "coordinate", "compute", "similarly", "eq", "use", "instead", "both", "source", "geometry", "source", "pose", "while", "we", "track", "full", "face", "mesh", "both", "source", "target", "sequence", "user", "may", "choose", "replace", "only", "part", "target", "face", "example", "multi-take", "video", "montage", "result", "fig.", "case", "user", "either", "select", "from", "predefined", "set", "mask", "eye", "eye", "nose", "mouth", "paint", "arbitrary", "mask", "face", "case", "represent", "only", "those", "vertex", "within", "user-specified", "mask", "retime", "we", "retime", "footage", "use", "Dynamic", "Time", "Warping", "-lrb-", "DTW", "-rrb-", "-lsb-", "Rabiner", "Juang", "1993", "-rsb-", "dtw", "dynamic", "programming", "algorithm", "seek", "monotonic", "mapping", "between", "two", "sequence", "minimize", "total", "cost", "pairwise", "mapping", "output", "DTW", "provide", "reordering", "one", "sequence", "best", "match", "other", "here", "we", "define", "pairwise", "cost", "between", "source", "target", "frame", "accord", "motion", "mouth", "each", "frame", "we", "find", "compute", "cost", "base", "motion", "instead", "absolute", "position", "more", "robust", "across", "difference", "mouth", "shape", "articulation", "different", "subject", "specifically", "loop", "vertex", "along", "interior", "upper", "lower", "lip", "we", "compare", "average", "minimum", "euclidean", "distance", "between", "first", "partial", "derivative", "respect", "time", "compare", "velocity", "mouth", "vertex", "step", "oppose", "position", "ensure", "robustness", "difference", "mouth", "shape", "between", "source", "target", "we", "compute", "partial", "derivative", "use", "first", "order", "difference", "original", "vertex", "position", "without", "transform", "image", "space", "let", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "partial", "derivative", "th", "vertex", "source", "mouth", "time", "th", "vertex", "target", "mouth", "time", "respectively", "cost", "map", "source", "frame", "target", "frame", "DTW", "min", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "min", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "dtw", "do", "consider", "temporal", "continuity", "result", "mapping", "may", "include", "stairstepp", "where", "give", "frame", "repeat", "multiple", "time", "follow", "non-consecutive", "frame", "which", "appear", "unnatural", "retimed", "video", "we", "smooth", "mapping", "low-pass", "filter", "round", "result", "nearest", "integer", "frame", "maintain", "sufficient", "synchronization", "while", "remove", "discontinuity", "while", "more", "sophisticated", "method", "can", "directly", "enforce", "continuity", "e.g.", "Hidden", "Markov", "model", "-lrb-", "hmm", "-rrb-", "well", "those", "temporal", "resampling", "we", "find", "approach", "fast", "well-suited", "we", "input", "datum", "where", "timing", "already", "fairly", "close", "since", "timing", "original", "source", "target", "video", "already", "close", "mapping", "can", "apply", "from", "source", "target", "vice", "versa", "-lrb-", "example", "maintain", "important", "motion", "background", "target", "capture", "subtle", "timing", "source", "actor?s", "performance", "-rrb-", "simplicity", "follow", "section", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "well", "corresponding", "texture", "coordinate", "texture", "datum", "refer", "retimed", "sequence", "when", "retiming", "employ", "original", "sequence", "when", "fig.", "highlight", "result", "retime", "input", "dialog", "dtw", "optimal", "Seam", "Finding", "have", "align", "source", "face", "texture", "target", "face", "we", "would", "like", "create", "truly", "photo-realistic", "composite", "blend", "two", "together", "while", "edge", "face", "mesh", "could", "use", "seam", "many", "case", "cut", "across", "feature", "video", "lead", "artifact", "bleeding", "-lrb-", "see", "Fig.", "-rrb-", "we", "solve", "problem", "automatically", "estimate", "seam", "spacetime", "minimize", "difference", "between", "align", "target", "video", "thereby", "avoid", "bleeding", "artifact", "second", "important", "seam", "temporally", "coherent", "ensure", "composit", "region", "do", "change", "substantially", "from", "frame", "frame", "lead", "flicker", "artifact", "-lrb-", "see", "Fig.", "-rrb-", "we", "algorithm", "incorporate", "requirement", "novel", "graphcut", "framework", "estimate", "optimal", "seam", "face", "mesh", "every", "frame", "video", "we", "compute", "closed", "polygon", "face", "mesh", "separate", "source", "region", "from", "target", "region", "project", "polygon", "onto", "frame", "give", "we", "ths", "correspond", "image-space", "seam", "estimate", "seam", "mesh-space", "help", "we", "handle", "we", "two", "requirement", "first", "when", "face", "deform", "source", "target", "video", "face", "mesh", "deform", "track", "without", "any", "change", "its", "topology", "mesh", "already", "account", "deformation", "make", "seam", "computation", "invariant", "change", "thus", "polygon", "correspond", "vertex", "define", "timevary", "seam", "stay", "true", "motion", "mouth", "second", "estimate", "seam", "mesh", "allow", "we", "enforce", "temporal", "constraint", "encourage", "seam", "pass", "through", "same", "vertex", "over", "time", "since", "face", "vertice", "track", "same", "face", "feature", "over", "time", "mean", "same", "part", "face", "preserve", "from", "source", "video", "every", "frame", "we", "do", "construct", "graph", "basis", "face", "mesh", "compute", "min-cut", "graph", "node", "graph", "correspond", "vertex", "face", "align", "mesh", "over", "time", "-lrb-", "i.e.", "-lrb-", "-rrb-", "-rrb-", "similar", "previous", "work", "graphcut", "texture", "-lsb-", "Kwatra", "et", "al.", "2003", "-rsb-", "photomontage", "-lsb-", "Agarwala", "et", "al.", "2004", "-rsb-", "we", "want", "seam", "cut", "through", "edge", "where", "difference", "between", "source", "target", "video", "frame", "minimal", "-lrb-", "-rrb-", "when", "both", "source", "target", "video", "have", "very", "similar", "pixel", "value", "vertex", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "corresponding", "weight", "term", "take", "very", "small", "value", "make", "favorable", "min-cut", "cut", "across", "edge", "we", "would", "also", "like", "seam", "stay", "temporally", "coherent", "ensure", "final", "composite", "do", "flicker", "we", "ensure", "set", "weight", "temporal", "edge", "graph", "follow", "make", "seam", "temporally", "coherent", "while", "retain", "ability", "shift", "avoid", "feature", "cause", "large", "difference", "intensity", "value", "practice", "we", "set", "ratio", "sum", "spatial", "temporal", "weight", "i.e.", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "ensure", "spatial", "temporal", "term", "weighted", "approximately", "equally", "vertex", "boundary", "face", "mesh", "every", "frame", "label", "target", "vertex", "definitely", "come", "from", "target", "video", "similarly", "small", "set", "vertex", "interior", "mesh", "label", "source", "vertex", "have", "construct", "graph", "we", "use", "alpha-expansion", "algorithm", "-lsb-", "Boykov", "et", "al.", "2001", "-rsb-", "label", "mesh", "vertex", "belong", "either", "source", "target", "video", "construction", "graph", "ensure", "every", "frame", "graph-cut", "seam", "form", "closed", "polygon", "separate", "target", "vertex", "from", "source", "vertex", "from", "label", "we", "can", "explicitly", "compute", "closed", "polygon", "-lrb-", "-rrb-", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rcb-", "every", "frame", "addition", "we", "also", "project", "label", "onto", "frame", "compute", "corresponding", "image-space", "mask", "compositing", "fig.", "show", "result", "estimate", "seam", "use", "we", "technique", "example", "video", "sequence", "can", "see", "example", "use", "edge", "face", "mesh", "seam", "lead", "strong", "bleeding", "artifact", "Computing", "optimal", "seam", "ensure", "artifact", "don?t", "occur", "however", "without", "temporal", "coherence", "optimal", "seam", "jump", "from", "frame", "frame", "lead", "flicker", "video", "compute", "seam", "mesh", "use", "we", "combination", "spatial", "temporal", "weight", "we", "able", "produce", "realistic", "composite", "stay", "coherent", "over", "time", "please", "see", "accompany", "video", "observe", "effect", "composit", "have", "estimate", "optimal", "seam", "compositing", "we", "blend", "source", "target", "video", "use", "gradient-domain", "fusion", "we", "do", "use", "recently", "propose", "technique", "use", "mean", "value", "coordinate", "-lsb-", "Farbman", "et", "al.", "2009", "-rsb-", "interpolate", "difference", "between", "source", "target", "frame", "along", "boundary", "particular", "every", "frame", "video", "we", "compute", "difference", "between", "source", "target", "frame", "along", "seam", "-lrb-", "-rrb-", "interpolate", "they", "remain", "source", "vertex", "use", "mean", "value", "coordinate", "difference", "project", "onto", "image", "add", "source", "video", "compute", "final", "blended", "composite", "video", "result", "we", "show", "result", "number", "different", "subject", "capture", "condition", "replacement", "scenario", "fig.", "show", "multi-take", "video", "montage", "example", "both", "shot", "outdoors", "handheld", "camera", "fig.", "show", "dub", "result", "translation", "scenario", "where", "source", "target", "depict", "same", "subject", "speaking", "different", "language", "source", "capture", "studio", "setting", "target", "capture", "outdoors", "Figs.", "show", "replacement", "result", "different", "source", "target", "subject", "notably", "different", "performance", "fig.", "10", "show", "retargeting", "result", "different", "subject", "where", "target", "use", "audiovisual", "guide", "source", "retime", "match", "target", "user", "interaction", "although", "majority", "we", "system", "automatic", "some", "user", "interaction", "require", "include", "place", "marker", "FaceGen", "adjust", "marker", "track", "initialization", "specify", "initial", "blending", "mask", "interaction", "FaceGen", "require", "2-3", "minute", "per", "subject", "track", "initialization", "perform", "less", "than", "minute", "all", "video", "use", "we", "result", "amount", "interaction", "here", "depend", "accuracy", "automatic", "face", "detection", "degree", "which", "subject?s", "expression", "viseme", "differ", "from", "closed-mouth", "neutral", "finally", "specify", "mask", "blend", "first", "frame", "every", "example", "take", "between", "30", "seconds", "minute", "comparison", "Vlasic", "et", "al.", "-lsb-", "2005", "-rsb-", "we", "reprocess", "original", "scan", "datum", "-lsb-", "Vlasic", "et", "al.", "2005", "-rsb-", "place", "correspondence", "face", "mesh", "cover", "full", "face", "include", "jaw", "do", "two", "reason", "first", "original", "model", "only", "cover", "interior", "face", "restricted", "we", "scenario", "where", "timing", "source", "target?s", "mouth", "motion", "must", "match", "exactly", "while", "case", "multi-take", "montage", "some", "dub", "scenario", "when", "speech", "same", "both", "source", "target", "video", "present", "problem", "other", "situation", "when", "motion", "target", "jaw", "source", "mouth", "do", "match", "situation", "change", "language", "during", "dub", "arbitrary", "face", "replacement", "full", "face", "model", "necessary", "so", "source?s", "jaw", "can", "also", "transfer", "-lrb-", "fig.", "-rrb-", "second", "we", "experience", "use", "original", "interior-only", "face", "model", "confirm", "earlier", "psychological", "study", "have", "conclude", "face", "shape", "one", "stronger", "cue", "identity", "when", "source", "target", "subject", "differ", "replace", "interior", "face", "always", "sufficient", "convey", "identity", "source", "subject", "particularly", "when", "source", "target", "face", "shape", "differ", "significantly", "Vlasic", "et", "al.", "face", "texture", "can", "come", "from", "either", "source", "target", "morphable", "model", "parameter", "can", "mixture", "source", "target", "when", "target", "texture", "use", "puppetry", "application", "blend", "warped", "texture", "relatively", "easy", "other", "hand", "take", "face", "texture", "from", "source", "make", "task", "blend", "far", "more", "difficult", "can", "see", "Fig.", "na?ve", "blending", "source", "face", "texture", "target", "use", "Vlasic", "et", "al.", "produce", "bleeding", "flicker", "artifact", "mitigate", "we", "seam", "finding", "blending", "method", "user", "study", "quantitatively", "objectively", "evaluate", "we", "system", "we", "run", "user", "study", "use", "Amazon?s", "mechanical", "Turk", "we", "test", "set", "consist", "24", "video", "10", "unmodified", "video", "10", "video", "replace", "face", "four", "additional", "video", "design", "verify", "subject", "be", "watch", "video", "simply", "click", "random", "response", "all", "video", "be", "present", "640", "360", "pixel", "five", "seconds", "disappear", "from", "page", "prevent", "subject", "from", "analyze", "final", "frame", "subject", "be", "inform", "video", "view", "either", "capture", "directly", "video", "camera", "manipulate", "computer", "program", "be", "ask", "respond", "statement", "video", "capture", "directly", "video", "camera", "choose", "response", "from", "five-point", "Likert", "scale", "strongly", "agree", "-lrb-", "-rrb-", "agree", "-lrb-", "-rrb-", "neither", "agree", "nor", "disagree", "-lrb-", "-rrb-", "disagree", "-lrb-", "-rrb-", "strongly", "disagree", "-lrb-", "-rrb-", "we", "collect", "40", "distinct", "opinion", "per", "video", "pay", "subject", "0.04", "per", "opinion", "per", "video", "additional", "four", "video", "begin", "similar", "footage", "rest", "instruct", "subject", "click", "specific", "response", "e.g.", "agree", "verify", "be", "pay", "attention", "subject", "who", "do", "respond", "instruct", "video", "be", "discard", "from", "study", "approximately", "20", "opinion", "per", "video", "remain", "after", "remove", "user", "average", "response", "face-replaced", "video", "4.1", "indicate", "subject", "believe", "video", "be", "capture", "directly", "camera", "be", "manipulate", "computer", "program", "average", "response", "authentic", "video", "4.3", "indicate", "slightly", "stronger", "belief", "video", "be", "capture", "camera", "none", "face-replaced", "video", "have", "median", "score", "below", "three", "video", "have", "median", "score", "result", "indicate", "we", "method", "can", "produce", "convincing", "video", "look", "similar", "those", "come", "directly", "from", "camera", "Limitations", "we", "approach", "without", "limitation", "-lrb-", "fig.", "-rrb-", "track", "base", "optical", "flow", "which", "require", "lighting", "change", "slowly", "over", "face", "high", "frequency", "lighting", "hard", "shadow", "must", "avoid", "ensure", "good", "tracking", "additionally", "method", "assume", "orthographic", "camera", "while", "estimation", "parameter", "more", "sophisticated", "camera", "model", "possible", "we", "use", "simple", "model", "shoot", "we", "input", "video", "longer", "focal", "length", "better", "approximate", "orthographic", "projection", "finally", "track", "often", "degrade", "beyond", "range", "pose", "outside", "45", "from", "frontal", "even", "successful", "tracking", "geometric", "fit", "can", "cause", "artifact", "final", "result", "example", "fit", "sometimes", "insufficient", "large", "pose", "difference", "between", "source", "target", "particularly", "noticeable", "nose", "area", "when", "example", "head", "significantly", "tilted", "downward", "cause", "nose", "distort", "slightly", "pose", "also", "constrain", "sufficiently", "similar", "between", "source", "target", "prevent", "occluded", "region", "source", "face", "from", "appear", "pose-transformed", "target", "frame", "case", "where", "we", "have", "control", "over", "source", "acquisition", "source", "subject", "can", "capture", "frontal", "pose", "we", "do", "here", "pose", "similar", "target", "both", "ensure", "occluded", "region", "however", "when", "exist", "footage", "use", "source", "necessary", "ensure", "compatible", "pose", "between", "source", "target", "issue", "could", "alleviate", "automatic", "user-assisted", "inpainting", "derive", "miss", "texture", "from", "spatially", "temporally", "adjacent", "pixel", "video", "sequence", "all", "example", "show", "here", "source", "target", "pair", "same", "gender", "approximate", "age", "thus", "roughly", "similar", "proportion", "any", "difference", "face", "shape", "can", "account", "single", "global", "scale", "ensure", "source", "face", "cover", "target", "vastly", "different", "face", "shape", "e.g.", "child", "adult", "may", "sufficient", "however", "plausible", "add", "2d", "warping", "step", "similar", "use", "-lsb-", "Jain", "et", "al.", "2010", "-rsb-", "warp", "target", "face", "nearby", "background", "match", "source", "before", "blend", "Lighting", "must", "also", "similar", "between", "source", "target", "multitake", "montage", "scenario", "where", "source", "target", "typically", "cap", "-lrb-", "-rrb-", "when", "tracking", "fail", "source", "content", "replacement", "distort", "see", "here", "after", "alignment", "-lrb-", "-rrb-", "significant", "difference", "lighting", "between", "source", "target", "lead", "unrealistic", "blended", "result", "where", "lighting", "right", "darker", "source", "face", "target", "environment", "ture", "close", "succession", "same", "setting", "condition", "trivially", "meet", "likewise", "when", "either", "source", "target", "capture", "studio", "setting", "full", "control", "over", "lighting", "setup", "condition", "can", "also", "meet", "same", "effort", "require", "plausible", "green", "screening", "finally", "seam", "finding", "blending", "can", "fail", "difficult", "input", "example", "when", "hair", "fall", "along", "forehead", "may", "seam", "generate", "natural", "blend", "between", "source", "target", "fig.", "show", "some", "example", "where", "limitation", "manifest", "final", "result", "we", "have", "present", "system", "produce", "face", "replacement", "video", "require", "only", "single-camera", "video", "minimal", "user", "input", "robust", "under", "significant", "difference", "between", "source", "target", "we", "have", "show", "user", "study", "result", "generate", "method", "perceive", "realistic", "we", "method", "useful", "variety", "situation", "include", "multi-take", "montage", "dub", "retargeting", "face", "replacement", "future", "improvement", "inpaint", "occlusion", "during", "large", "pose", "variation", "2d", "background", "warping", "vastly", "different", "face", "shape", "lighting", "transfer", "between", "source", "target", "make", "approach", "applicable", "even", "broader", "range", "scenario", "author", "thank", "Karen", "Xiao", "help", "reprocess", "face", "scan", "datum", "Karen", "Michelle", "Borkin", "Amanda", "Peters", "participate", "video", "subject", "work", "support", "part", "National", "Science", "Foundation", "under", "Grants", "phy-0835713", "dms-0739255", "orshukov", "G.", "iponus", "D.", "ARSEN", "O.", "EWIS", "J.", "empelaar", "ietz", "C.", "2003", "Universal", "capture", "imagebased", "facial", "animation", "Matrix", "Reloaded", "ACM", "SIG", "oykov", "Y.", "EKSLER", "O.", "abih", "R.", "2001", "fast", "approximate", "energy", "minimization", "via", "graph", "cut", "IEEE", "Trans", "pattern", "analysis", "machine", "Intelligence", "23", "11", "1222", "1239", "radley", "D.", "EIDRICH", "W.", "opa", "T.", "heffer", "a.", "2010", "high", "resolution", "passive", "facial", "performance", "capture", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "41:1", "10", "regler", "C.", "OVELL", "M.", "LANEY", "M.", "1997", "Video", "Rewrite", "drive", "visual", "speech", "audio", "353", "360", "arlo", "D.", "etaxa", "D.", "1996", "integration", "optical", "flow", "deformable", "model", "application", "human", "face", "shape", "motion", "estimation", "IEEE", "Conf", "computer", "Vision", "Pattern", "recognition", "-lrb-", "cvpr", "-rrb-", "231", "238", "ssa", "i.", "ASU", "S.", "ARRELL", "T.", "entland", "a.", "1996", "modeling", "tracking", "interactive", "animation", "face", "head", "use", "input", "from", "video", "computer", "animation", "68", "79", "veringham", "m.", "ivic", "J.", "isserman", "a.", "2006", "my", "name", "...", "Buffy", "automatic", "naming", "character", "tv", "video", "british", "machine", "Vision", "Conference", "-lrb-", "BMVC", "-rrb-", "899", "908", "t.", "2002", "trainable", "vide", "lagg", "M.", "AKAZAWA", "a.", "hang", "Q.", "ang", "S.", "B.", "YU", "Y.", "K.", "SSA", "I.", "ehg", "J.", "M.", "2009", "human", "video", "texture", "199", "206", "H.", "55", "66", "ia", "J.", "UN", "J.", "ang", "c.-k.", "hum", "h.-y", "2006", "dragand-drop", "pasting", "631", "637", "E.", "WATRA", "V.", "ch", "odl", "a.", "ssa", "i.", "urk", "G.", "obick", "a.", "2003", "graphcut", "texture", "image", "video", "synthesis", "use", "graph", "cut", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "22", "277", "286", "H.", "dam", "B.", "UIBAS", "L.", "J.", "auly", "M.", "2009", "robust", "single-view", "geometry", "motion", "reconstruction", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "28", "175:1", "10", "w.-c.", "one", "a.", "HIANG", "J.-Y.", "AWKINS", "T.", "RED", "ERIKSEN", "S.", "eer", "P.", "UKOVIC", "M.", "UHYOUNG", "M.", "EBEVEC", "P.", "2008", "facial", "performance", "synthesis", "use", "deformation-driven", "polynomial", "displacement", "map", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "Asia", "-rrb-", "27", "121:1", "10", "REZ", "P.", "ANGNET", "M.", "lake", "a.", "2003", "Poisson", "image", "editing", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "22", "313", "318", "ighin", "F.", "H.", "zeliskus", "R.", "ALESIN", "D.", "1999", "resynthesize", "facial", "animation", "through", "3d", "model-based", "tracking", "computer", "Vision", "-lrb-", "iccv", "-rrb-", "143", "150", "abiner", "L.", "uang", "b.-h", "1993", "fundamental", "speech", "recognition", "Prentice-Hall", "Inc.", "Upper", "Saddle", "River", "NJ", "USA", "obertson", "B.", "2009", "what?s", "old", "new", "again", "Computer", "Graphics", "World", "32", "ingular", "nversion", "nc", "FaceGen", "Modeller", "manual", "www.facegen.com", "UNKAVALLI", "K.", "OHNSON", "M.", "K.", "atusik", "W.", "FIS", "TER", "H.", "2010", "multi-scale", "image", "harmonization", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "29", "125:1", "10", "iolum", "P.", "A.", "ONES", "M.", "J.", "2001", "robust", "real-time", "face", "detection", "computer", "Vision", "-lrb-", "iccv", "-rrb-", "747", "755", "lasic", "D.", "RAND", "M.", "fister", "H.", "opovus", "J.", "2005", "Face", "transfer", "multilinear", "model", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "24", "426", "433", "EISE", "T.", "H.", "OOL", "L.", "V.", "auly", "M.", "2009", "Face/Off", "Live", "facial", "puppetry", "siggraph/eurographics", "Symp", "computer", "animation", "16", "ILLIAMS", "L.", "1990", "performance-driven", "facial", "animation", "Computer", "Graphics", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "24", "235", "242", "ang", "F.", "ang", "J.", "hechtman", "E.", "OURDEV", "L.", "etaxa", "D.", "2011", "expression", "flow", "3d-aware", "face", "component", "transfer", "ACM", "Trans", "graphic", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "27", "60:1", "10", "hang", "L.", "navely", "N.", "URLESS", "B.", "EITZ", "S.", "M.", "2004", "Spacetime", "face", "high", "resolution", "capture", "modeling", "animation", "ACM", "Trans", "Graphics", "23", "548", "558" ],
  "content" : "We showcase the use of our method on a variety of examples and present the result of a user study that suggests our results are difficult to distinguish from real video footage. We track the face in both the source and target videos using a 3D multilinear model. Then we warp the source video in both space and time to align it to the target. Instead of choosing the single best take for the final cut, our system can combine, e.g., the mouth performance from one take and the eyes, brow, and expressions from another to produce a video montage. Our video-based method is particularly suitable in this case because we have no control over the capture of the existing footage. In contrast to retargeting, where the source footage is shot using the target as an audiovisual guide to roughly match the timings, the performance of the source and target can be very different, similar to dubbing but with different subjects. Our system does rely on a few assumptions about the input videos. Second, we assume that the pose of faces in the source and target videos is ?45 o from frontal, otherwise automatic tracking and alignment of the faces will fail (Sec. We demonstrate the applicability of our approach on a number of examples in four scenarios: video montage ( Fig. 6 ), dubbing ( Fig. 7 ), retargeting (Figs. 1 and 10), and replacement ( Fig. 9 ). We briefly  describe previous work on face replacement and compare these approaches to our system. For example, the method by Blanz et al. [2004] fits a morphable model to faces in both the source and target images and renders the source face with the parameters estimated from the target image. The flow is derived from 3D morphable models that are fit to the source and target photos. It is not clear whether any of these methods could achieve temporally coherent results when applied to a video sequence. Notable examples include the recreation of actors for The Matrix Reloaded [Borshukov et al. 2003], The Curious Case of Benjamin Button [Robertson 2009], and the Digital Emily project [Alexander et al. 2009]. Bregler et al. [1997] and Ezzat et al. [2002] replace the mouth region in video to match phonemes of novel audio input using a database of training images of the same actor. Kemelmacher-Shlizerman et al. [2010] make use of image collections and videos of celebrities available online and replace face photos in real-time based on expression and pose similarity. We use the multilinear model by Vlasic at al. [2005] that captures identity, expression, and visemes in the source and target videos. In contrast, our approach blends the source actor?s complete face and performance, with all of its nuances intact, into the target. Figure 2 shows an overview of our method. In order to replace a source face with a target face, we first model and track facial performances of both source and target with the multilinear method and data of Vlasic et al. [2005]. It tracks parameters for these attributes and the 3D pose of the face (given as a rotation, translation, and scale) over a video sequence. At each frame, the pose, the multilinear model, and its parameters can be used to generate a 3D mesh that matches the geometry of the subject?s face. We reprocessed the original training data from Vlasic et al. covering 16 identities ? 5 expressions ? 5 visemes?a total of 400 face scans?placing them into correspondence with a face mesh that extends beyond the jaw and chin regions (Sec. In some scenarios it is important that the timing of the facial performance matches precisely in the source and the target footage. However, it might be very tedious to match these timings exactly as demonstrated by the numerous takes that are typically necessary to obtain compelling voiceovers (e.g., when re-recording a dialog for a film.) Instead, we only require a coarse synchronization between source and target videos and automatically retime the footage to generate a precise match for the replacement. After tracking and retiming, we blend the source performance into the target video to produce the final result. This blending makes use of gradient-domain compositing to merge the source actor?s face into the target video. To minimize these artifacts we automatically compute an optimal spatiotemporal seam through the source and target that minimizes the difference across the seam on the face mesh and ensure that the regions being combined are compatible. In the second stage we use this seam to merge the gradients and recover the final composite video. Input Footage for all examples, except those that reuse existing footage, was captured with a Canon T2i camera with 85 mm and 50 mm lenses at 30 frames per second. In-lab sequences were lit with 300 W studio lights placed on the left and right and in front of the subject, softened by umbrella reflectors. When appropriate, we used the target video as an audio-visual guide during capture of the source (or vice versa) to approximately match timing. For pose, actors were simply instructed to face the camera; natural head motion is accounted for with tracking. Tracking To track a face across a sequence of frames, the method of Vlasic et al. [2005] computes the pose and attribute parameters of the multilinear face model that best explain the optical flow between adjacent frames in the sequence. With the multilinear model in hand, the original face data can be interpolated or extrapolated to generate a new face as where mode 1 corresponds to vertex positions in the 4-mode model, w i is a D i ? 1 column vector of parameters for the attribute corresponding to the i th mode (i.e., one of expression, viseme, or identity), f is a 3K-element column vector of new vertex positions, and the ? n operator is the mode-n product, defined between a tensor and a matrix. We refer the reader to Vlasic et al. [2005] for more details. Initialization Since tracking is based on optical flow, initialization is critical, as errors in the initialization will be propagated throughout the sequence. Moreover, tracking can go astray on troublesome frames, e.g., due to motion blur, extreme pose change, high frequency lighting, or occlusions. Therefore, we also provide a simple  user interface that can ensure good initialization and can correct tracking for troublesome frames. The interface allows the user to adjust positions of markers on the eyes, eyebrows, nose, mouth, and jawline, from which the best-fit pose and model parameters are computed. The user can alternate between adjusting pose and each attribute individually; typically, 1 iteration of each is sufficient for good initialization ( Fig. 3 ). We start by automatically detecting the face [Viola and Jones 2001]. Next, we localize facial features [Everingham et al. 2006] (e.g., the corners of the mouth, eyes, and nose) in the first frame of a sequence. This initial face mesh is generated from the multilinear model using a user-specified set of initial attributes corresponding to the most appropriate expression, viseme, and identity. Holding all but one attribute?s parameters fixed, we can project the multilinear model M onto the subspace corresponding to the remaining attribute, e.g., for the third attribute: for the 3K ? D 3 matrix A 3 . 3 is given as To fit parameters for the i th attribute to image space markers, we take the subset of the multilinear model corresponding to the (x, y) coordinates of mesh vertices that should align to the markers and apply Eq. 4, populating g with marker positions, transformed to the coordinate frame of the model via an inverse pose transformation. Even after multiple iterations of tracking, each of which updates identity parameters, those parameters changed very little from their initial values. Therefore it is important to have an accurate initialization of identity. We employ the FaceGen Modeller [Singular Inversions Inc. FaceGen generates a 3D mesh based on a frontal face image and, optionally, a profile image. The input images can be extracted from the original video sequences or downloaded from the Internet when reusing existing footage. The input images need to depict the subject with a closed-mouth neutral expression. FaceGen requires minimal user input to specify about 10 markers per image. All meshes created by FaceGen are themselves in correspondence. Therefore, we can register the FaceGen mesh with the multilinear model using the same template-fitting procedure [Vlasic et al. 2005] we used to register the original scan data. We then fit the multilinear model to the registered FaceGen mesh using procrustes alignments to our current best-fit mesh and using Eqs. In this optimization we only use about 1 percent of the original mesh vertices. The process typically converges in 10 iterations. Key framing We can use the same interface ( Fig. 3 ) for adjusting pose and attribute parameters at specific key frames where automatic tracking fails. First, we track the subsequences between each pair of user-adjusted key frames in both the forward and reverse directions and linearly interpolate the two results. We then perform additional tracking iterations on the full sequence to refine pose and parameter estimates across key frame boundaries. Note that none of the results shown in the paper required key framing. Spatial alignment From an image sequence I, where I(x, t) denotes the value at pixel position x in frame t, tracking produces a sequence of attribute parameters and pose transformations. For each frame t, f (t) is the column vector of vertex positions computed from attribute parameters at time t using Eq. 1, and f i (t), the i th vertex at time t. Subscripts S and T denote source and target, respectively. To align the source face in the target frame, we use the face geometry from the source sequence and pose from the target sequence. That is, for frame t, the aligned position of the i th source vertex position is given as We also take texture from the source image I S ; texture coordinates are computed similarly to Eq. 5 using instead both source geometry and source pose. While we track the full face mesh in both source and target sequences, the user may choose to replace only part of the target face, for example, in the multi-take video montage result in Fig. 6 . In this case, the user either selects from a predefined set of masks ? eyes, eyes and nose, or mouth ? or paints an arbitrary mask on the face. In these cases, f S represents only those vertices within the user-specified mask. Retiming We retime the footage using Dynamic Time Warping (DTW) [Rabiner and Juang 1993]. DTW is a dynamic programming algorithm that seeks a monotonic mapping between two sequences that minimizes the total cost of pairwise mappings. The output of DTW provides a reordering of one sequence to best match the other. Here we define pairwise cost between source and target frames according to the motion of the mouth in each frame. We found that computing cost based on motion instead of absolute position was more robust across differences in mouth shape and articulation in different subjects. Specifically, for the loop of vertices along the interior of the upper and lower lip, we compare the average minimum Euclidean distance between the first partial derivatives with respect to time. Comparing velocity of mouth vertices for this step, as opposed to position, ensures robustness to differences in mouth shape between source and target. We compute these partial derivatives using first order differencing on the original vertex positions without transforming to image space. Let m i,S (t 1 ) and m j,T (t 2 ) be the partial derivatives for the i th vertex in the source mouth at time t 1 and the j th vertex in the target mouth at time t 2 , respectively. Then the cost of mapping source frame t 1 to target frame t 2 for DTW is min ||m i,S (t 1 ) ? m j,T (t 2 )|| + min ||m j,S (t 1 ) ? m i,T (t 2 )||. j j i (6) DTW does not consider temporal continuity. The resulting mapping may include ?stairstepping?, where a given frame is repeated multiple times, followed by a non-consecutive frame, which appears unnatural in the retimed video. We smooth the mapping with a low-pass filter and round the result to the nearest integer frame. This maintains sufficient synchronization while removing discontinuities. While there are more sophisticated methods that can directly enforce continuity e.g., Hidden Markov Models (HMMs), as well as those for temporal resampling, we found this approach to be fast and well-suited to our input data, where timing is already fairly close. Since the timing of the original source and target videos is already close, the mapping can be applied from source to target and vice versa (for example, to maintain important motion in the background of the target or to capture the subtle timing of the source actor?s performance.) For simplicity, in the following sections f S (t) and f T (t), as well as their corresponding texture coordinates and texture data, refer to the retimed sequences when retiming is employed and to the original sequences when it is not. Fig. 4 highlights the result of retiming inputs with dialog with DTW. Optimal Seam Finding Having aligned the source face texture to the target face, we would like to create a truly photo-realistic composite by blending the two together. While the edge of face mesh could be used as the seam, in many cases it cuts across features in the video leading to artifacts such as bleeding (see Fig. 5 ). We solve this problem by automatically estimating a seam in spacetime that minimizes the differences between the aligned and target videos, thereby avoiding bleeding artifacts. Second, it is important that the seam be temporally coherent to ensure that the composited region does not change substantially from frame to frame leading to flickering artifacts (see Fig. 5 ). Our algorithm incorporates these requirements in a novel graphcut framework that estimates the optimal seam on the face mesh. For every frame in the video, we compute a closed polygon on the face mesh that separates the source region from the target region; projecting this polygon onto the frame gives us ths corresponding image-space seam. Estimating the seam in mesh-space helps us handle our two requirements. First, when the face deforms in the source and target videos, the face mesh deforms to track it without any changes in its topology. The mesh already accounts for these deformations, making the seam computation invariant to these changes. Thus, a polygon corresponding to these vertices defines a timevarying seam that stays true to the motion of the mouth. Second, estimating the seam on the mesh allows us to enforce temporal constraints that encourage the seam to pass through the same vertices over time. Since the face vertices track the same face features over time this means that same parts of the face are preserved from the source video in every frame. We do this by constructing a graph on the basis of the face mesh and computing the min-cut of this graph. The nodes of this graph correspond to the vertices in the face aligned mesh over time (i.e., f i (t)?i, t). Similar to previous work on graphcut textures [Kwatra et al. 2003] and photomontage [Agarwala et al. 2004], we want the seam to cut through edges where the differences between the source and target video frames are minimal. and f j (t) as: When both the source and the target videos have very similar pixel values at vertices f i (t) and f j (t), the corresponding weight term takes on a very small value. This makes it favorable for a min-cut to cut across this this edge. We would also like the seam to stay temporally coherent to ensure that the final composite does not flicker. We ensure this by setting the weights for the temporal edges of the graph as follows: This makes the seam temporally coherent while retaining the ability to shift to avoid features that cause large differences in intensity values. In practice, we set ? as the ratio of the sum of the spatial and temporal weights, i.e., ? = i,j,t W s (f i (t), f j (t), t)/ i,j,t W t (f i (t), f i (t + 1)). This ensures that the spatial and temporal terms are weighted approximately equally. The vertices on the boundary of the face mesh in every frame are labeled as target vertices as they definitely come from the target videos. Similarly, a small set of vertices in the interior of the mesh are labeled as source vertices. Having constructed this graph, we use the alpha-expansion algorithm [Boykov et al. 2001] to label the mesh vertices as belonging to the either the source or target videos. The construction of the graph ensures that, in every frame, the graph-cut seam forms a closed polygon that separates the target vertices from the source vertices. From these labels we can explicitly compute this closed polygon ?P (t) = {p 0 (t), p 1 (t), ? ? ? , p m t (t)} for every frame. In addition, we also project these labels onto the frames to compute the corresponding image-space mask for compositing. Fig. 5 shows the results of estimating the seam using our technique on an example video sequence. As can be seen in this example, using the edge of the face mesh as the seam leads to strong bleeding artifacts. Computing an optimal seam ensures that these artifacts don?t occur. However, without temporal coherence, the optimal seam ?jumps? from frame to frame, leading to flickering in the video. By computing the seam on the mesh using our combination of spatial and temporal weights we are able to produce a realistic composite that stays coherent over time. Please see the accompanying video to observe these effects. Compositing Having estimated the optimal seam for compositing, we blend the source and target videos using gradient-domain fusion. We do this using a recently proposed technique that uses mean value coordinates [Farbman et al. 2009] to interpolate the differences between the source and target frames along the boundary. In particular, for every frame of the video, we compute the differences between source and target frames along the seam ?P (t), and interpolate them at the remaining source vertices using mean value coordinates. These differences are then projected onto the image and added to the source video to compute the final blended composite video. Results We show results for a number of different subjects, capture conditions, and replacement scenarios. Fig. 6 shows multi-take video montage examples, both shot outdoors with a handheld camera. Fig. 7 shows dubbing results of a translation scenario, where the source and target depict the same subject speaking in different languages, with source captured in a studio setting and target captured outdoors. Figs. 9 shows a replacement result with different source and target subjects and notably different performances. Fig. 10 shows a retargeting result with different subjects, where the target was used as an audiovisual guide and the source retimed to match the target. User interaction Although the majority of our system is automatic, some user interaction is required. This includes placing markers in FaceGen, adjusting markers for tracking initialization, and specifying the initial blending mask. Interaction in FaceGen required 2-3 minutes per subject. Tracking initialization was performed in less than a minute for all videos used in our results; the amount of interaction here depends on the accuracy of the automatic face detection and the degree to which the subject?s expression and viseme differ from closed-mouth neutral. Finally, specifying the mask for blending in the first frame of every example took between 30 seconds and 1 minute. Comparison with Vlasic et al. [2005] We reprocessed the original scan data [Vlasic et al. 2005] to place it into correspondence with a face mesh that covers the full face, including the jaw. This was done for two reasons. First, the original model only covered the interior of the face; this restricted us to scenarios where the timing of the source and target?s mouth motion must match exactly. While this is the case for multi-take montage and some dubbing scenarios when the speech is the same in both source and target videos, it presents a problem for other situations when the motion of the target jaw and source mouth do not match. For these situations ? changing the language during dubbing or in arbitrary face replacements ? a full face model is necessary so that the source?s jaw can also be transferred ( Fig. 8 a). Second, our experience using the original interior-only face model confirmed earlier psychological studies that had concluded that face shape is one of the stronger cues for identity. When source and target subjects differ, replacing the interior of the face was not always sufficient to convey the identity of the source subject, particularly when source and target face shapes differ significantly. In Vlasic et al., face texture can come from either the source or the target, and morphable model parameters can be a mixture of source and target. When the target texture is used, as in their puppetry application, blending the warped texture is relatively easy. On the other hand, taking face texture from the source makes the task of blending far more difficult; as can be seen in Fig. 5 , the na?ve blending of source face texture into the target used in Vlasic et al. produces bleeding and flickering artifacts that are mitigated with our seam finding and blending method. User study To quantitatively and objectively evaluate our system, we ran a user study using Amazon?s Mechanical Turk. Our test set consisted of 24 videos: 10 unmodified videos, 10 videos with replaced faces, and four additional videos designed to verify that the subjects were watching the videos and not simply clicking on random responses. All videos were presented at 640 ? 360 pixels for five seconds and then disappeared from the page to prevent the subject from analyzing the final frame. The subjects were informed that the video they viewed was either ?captured directly by a video camera? or ?manipulated by a computer program. ? They were asked to respond to the statement ?This video was captured directly by a video camera? by choosing a response from a five-point Likert scale: strongly agree (5), agree (4), neither agree nor disagree (3), disagree (2), or strongly disagree (1). We collected 40 distinct opinions per video and paid the subjects $0.04 per opinion per video. The additional four videos began with similar footage as the rest but then instructed the subjects to click a specific response, e.g., ?agree?, to verify that they were paying attention. Subjects who did not respond as instructed to these videos were discarded from the study. Approximately 20 opinions per video remained after removing these users. The average response for the face-replaced videos was 4.1, indicating that the subjects believed the videos were captured directly by a camera and were not manipulated by a computer program. The average response for the authentic videos was 4.3, indicating a slightly stronger belief that the videos were captured by a camera. None of the face-replaced videos had a median score below 4 and three of the videos had a median score of 5. These results indicate that our method can produce convincing videos that look similar to those coming directly from a camera. Limitations Our approach is not without limitations ( Fig. 8 ). Tracking is based on optical flow, which requires that the lighting change slowly over the face. High frequency lighting, such as hard shadows, must be avoided to ensure good tracking. Additionally, the method assumes an orthographic camera; while estimation of parameters of a more sophisticated camera model is possible, we use the simple model and shot our input videos with longer focal lengths that better approximate an orthographic projection. Finally, tracking often degrades beyond the range of poses outside ?45 o from frontal. Even with successful tracking, the geometric fit can cause artifacts in the final result. For example, the fit is sometimes insufficient for the large pose differences between source and target. This is particularly noticeable in the nose area when, for example, the head is significantly tilted downwards, causing the nose to distort slightly. Pose is also constrained to be sufficiently similar between source and target to prevent occluded regions in the source face from appearing in the pose-transformed target frame. For cases where we have control over source acquisition, the source subject can be captured in a frontal pose as we do here, or in a pose similar to the target, both ensuring no occluded regions. However when existing footage is used as the source, it is necessary to ensure compatible pose between source and target. This issue could be alleviated by automatic or user-assisted inpainting that derives the missing texture from spatially and temporally adjacent pixels in the video sequence. In all examples shown here, source / target pairs are of the same gender and approximate age and thus of roughly similar proportions. Any difference in face shape can be accounted for by a single global scale to ensure the source face covers the target. For vastly different face shape, e.g., a child and adult, this may not be sufficient. However it is plausible to add a 2D warping step, similar to that used in [Jain et al. 2010], that warps the target face and nearby background to match the source before blending. Lighting must also be similar between source and target. For multitake montage scenarios, where source and target are typically cap- (b) When the tracking fails, the source content for replacement is distorted, seen here after alignment. (c) Significant differences in lighting between source and target lead to an unrealistic blended result, where the lighting on the right is darker on the source face but not in the target environment. tured in close succession in the same setting, this condition is trivially met. Likewise, when either the source or target is captured in a studio setting, with full control over the lighting setup, this condition can also be met with the same efforts required for plausible green screening. Finally, seam finding and blending can fail for difficult inputs. For example, when hair falls along the forehead, there may be no seam that generates a natural blend between source and target. Fig. 8 shows some examples where these limitations are manifested in the final result. We have presented a system for producing face replacements in video that requires only single-camera video and minimal user input and is robust under significant differences between source and  target. We have shown with a user study that results generated with this method are perceived as realistic. Our method is useful in a variety of situations, including multi-take montage, dubbing, retargeting, and face replacement. Future improvements such as inpainting for occlusions during large pose variations, 2D background warping for vastly different face shapes, and lighting transfer between source and target will make this approach applicable to an even broader range of scenarios. The authors thank Karen Xiao for help reprocessing the face scan data and Karen, Michelle Borkin, and Amanda Peters for participating as video subjects. This work was supported in part by the National Science Foundation under Grants No. PHY-0835713 and DMS-0739255. B ORSHUKOV , G., P IPONI , D., L ARSEN , O., L EWIS , J., AND T EMPELAAR -L IETZ , C. 2003. Universal capture ? Imagebased facial animation for ?The Matrix Reloaded?. In ACM SIG- B OYKOV , Y., V EKSLER , O., AND Z ABIH , R. 2001. Fast approximate energy minimization via graph cuts. IEEE Trans. Pattern Analysis and Machine Intelligence 23, 11, 1222?1239. B RADLEY , D., H EIDRICH , W., P OPA , T., AND S HEFFER , A. 2010. High resolution passive facial performance capture. ACM Trans. Graphics (Proc. SIGGRAPH), 4, 41:1?10. B REGLER , C., C OVELL , M., AND S LANEY , M. 1997. Video Rewrite: Driving visual speech with audio. In 353?360. D E C ARLO , D., AND M ETAXAS , D. 1996. The integration of optical flow and deformable models with applications to human face shape and motion estimation. IEEE Conf. Computer Vision and Pattern Recognition (CVPR), 231?238. E SSA , I., B ASU , S., D ARRELL , T., AND P ENTLAND , A. 1996. Modeling, tracking and interactive animation of faces and heads: Using input from video. Computer Animation, 68?79. E VERINGHAM , M., S IVIC , J., AND Z ISSERMAN , A. 2006. My name is... Buffy? ? automatic naming of characters in TV video. British Machine Vision Conference (BMVC), 899?908. , T. 2002. Trainable vide- F LAGG , M., N AKAZAWA , A., Z HANG , Q., K ANG , S. B., R YU , Y. K., E SSA , I., AND R EHG , J. M. 2009. Human video textures. In 199?206. , H., 55?66. J IA , J., S UN , J., T ANG , C.-K., AND S HUM , H.-Y. 2006. Dragand-drop pasting. 3, 631?637. ,\n        K E.,\n        S K WATRA , V., S CH ODL  ? , A., E SSA , I., T URK , G., AND B OBICK , A. 2003. Graphcut textures: Image and video synthesis using graph cuts. ACM Trans. Graphics (Proc. SIGGRAPH) 22, 3, 277?286. L I , H., A DAMS , B., G UIBAS , L. J., AND P AULY , M. 2009. Robust single-view geometry and motion reconstruction. ACM Trans. Graphics (Proc. SIGGRAPH) 28, 5, 175:1?10. M A , W.-C., J ONES , A., C HIANG , J.-Y., H AWKINS , T., F RED ERIKSEN , S., P EERS , P., V UKOVIC , M., O UHYOUNG , M., AND D EBEVEC , P. 2008. Facial performance synthesis using deformation-driven polynomial displacement maps. ACM Trans. Graphics (Proc. SIGGRAPH Asia) 27, 5, 121:1?10. P ? REZ , P., G ANGNET , M., AND B LAKE , A. 2003. Poisson image editing. ACM Trans. Graphics (Proc. SIGGRAPH) 22, 3, 313? 318. P IGHIN , F. H., S ZELISKI , R., AND S ALESIN , D. 1999. Resynthesizing facial animation through 3d model-based tracking. Computer Vision (ICCV), 143?150. R ABINER , L., AND J UANG , B.-H. 1993. Fundamentals of speech recognition. Prentice-Hall, Inc. , Upper Saddle River, NJ, USA. R OBERTSON , B. 2009. What?s old is new again. Computer Graphics World 32, 1. S INGULAR I NVERSIONS I NC . FaceGen Modeller manual. www.facegen.com. S UNKAVALLI , K., J OHNSON , M. K., M ATUSIK , W., AND P FIS TER , H. 2010. Multi-scale image harmonization. ACM Trans. Graphics (Proc. SIGGRAPH) 29, 4, 125:1?10. V IOLA , P. A., AND J ONES , M. J. 2001. Robust real-time face detection. Computer Vision (ICCV), 747?755. V LASIC , D., B RAND , M., P FISTER , H., AND P OPOVI ? , J. 2005. Face transfer with multilinear models. ACM Trans. Graphics (Proc. SIGGRAPH) 24, 3, 426?433. W EISE , T., L I , H., G OOL , L. V., AND P AULY , M. 2009. Face/Off: Live facial puppetry. SIGGRAPH/Eurographics Symp. Computer Animation, 7?16. W ILLIAMS , L. 1990. Performance-driven facial animation. Computer Graphics (Proc. SIGGRAPH) 24, 4, 235?242. Y ANG , F., W ANG , J., S HECHTMAN , E., B OURDEV , L., AND M ETAXAS , D. 2011. Expression flow for 3D-aware face component transfer. ACM Trans. Graphics (Proc. SIGGRAPH) 27, 3, 60:1?10. Z HANG , L., S NAVELY , N., C URLESS , B., AND S EITZ , S. M. 2004. Spacetime faces: High resolution capture for modeling and animation. ACM Trans. Graphics 23, 3, 548?558.",
  "resources" : [ ]
}