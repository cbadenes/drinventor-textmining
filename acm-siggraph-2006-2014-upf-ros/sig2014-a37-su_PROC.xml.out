{
  "uri" : "sig2014-a37-su_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a37-su_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Estimating Image Depth Using Shape Collections",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Hao-Su",
      "name" : "Hao",
      "surname" : "Su"
    }, {
      "uri" : "http://drinventor/Qixing-Huang",
      "name" : "Qixing",
      "surname" : "Huang"
    }, {
      "uri" : "http://drinventor/Niloy J.-Mitra",
      "name" : "Niloy J.",
      "surname" : "Mitra"
    }, {
      "uri" : "http://drinventor/Yangyan-Li",
      "name" : "Yangyan",
      "surname" : "Li"
    }, {
      "uri" : "http://drinventor/Leonidas J.-Guibas",
      "name" : "Leonidas J.",
      "surname" : "Guibas"
    } ]
  },
  "bagOfWords" : [ "we", "key", "insight", "even", "when", "image", "object", "contain", "shape", "collection", "network", "shape", "implicitly", "characterize", "shape-specific", "deformation", "subspace", "regularize", "problem", "enable", "robust", "diffusion", "depth", "information", "from", "shape", "collection", "input", "image", "key", "we", "approach", "estimation", "correspondence", "between", "image", "multiple", "model", "help", "correspondence", "estimate", "between", "model", "themselves", "experimental", "result", "show", "propose", "approach", "recover", "depth", "information", "close", "kinect", "scan", "significantly", "more", "accurate", "than", "state-of-the-art", "image-based", "modeling", "technique", "we", "demonstrate", "depth-enhanced", "image", "editing", "illustrate", "possibility", "offer", "we", "approach", "emergence", "large", "shape", "collection", "provide", "we", "platform", "aggregate", "information", "from", "multiple", "shape", "improve", "analysis", "processing", "individual", "shape", "major", "limitation", "approach", "project", "view", "3d", "shape", "only", "contain", "partial", "information", "from", "original", "shape", "particular", "we", "show", "match", "image", "collection", "3d", "shape", "boost", "matching", "quality", "enforce", "consistency", "between", "image-shape", "map", "shape-shape", "map", "however", "approach", "design", "object", "simple", "texture", "shape", "and/or", "under", "specific", "lighting", "condition", "since", "obtain", "3d", "shape", "similar", "global", "structure", "image", "object", "easy", "we", "estimate", "depth", "information", "unsupervised", "manner", "i.e.", "directly", "match", "image", "3d", "shape", "thus", "avoid", "tedious", "task", "perform", "instance", "specific", "learning", "propose", "image-based", "shape-driven", "modeling", "approach", "take", "single", "image", "object", "segmented", "from", "background", "collection", "shape", "-lcb-", "-rcb-", "same", "class", "input", "simultaneously", "estimate", "object", "pose", "show", "reconstruct", "3d", "point", "cloud", "from", "i.", "simplicity", "we", "assume", "all", "input", "shape", "support", "same", "ground", "plane", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "so", "common", "vertical", "direction", "available", "variable", "optimize", "point", "cloud", "parameterize", "camera", "pose", "z-coordinate", "-lrb-", "pixel", "depths", "-rrb-", "image", "object", "deformation", "set", "similar", "shape", "first", "depth", "coordinate", "point", "cloud", "unconstrained", "yet", "we", "can", "allow", "shape", "deform", "arbitrarily", "since", "otherwise", "both", "point", "cloud", "shape", "may", "stretch", "undesirably", "when", "be", "align", "second", "success", "non-rigid", "alignment", "depend", "good", "initialization", "both", "camera", "pose", "point", "cloud", "Third", "even", "good", "initialization", "challenge", "solve", "induce", "optimization", "problem", "involve", "depth", "each", "pixel", "effectively", "illustrate", "figure", "pipeline", "consist", "preprocessing", "stage", "reconstruction", "stage", "motivation", "come", "from", "fact", "plausible", "deformation", "each", "shape", "typically", "lie", "lowdimensional", "space", "when", "compare", "number", "parameter", "general", "deformation", "model", "-lsb-", "Averkiou", "et", "al.", "2014", "-rsb-", "essentially", "we", "learn", "local", "structure", "shape", "space", "reconstruction", "stage", "proceeds", "three", "step", "where", "first", "two", "step", "provide", "initial", "solution", "-lrb-", "set", "similar", "shape", "initial", "point", "cloud", "-rrb-", "third", "step", "optimize", "point", "cloud", "minimize", "its", "distance", "deform", "similar", "shape", "consider", "pose", "estimation", "problem", "although", "pose", "estimation", "use", "single", "shape", "hard", "we", "find", "when", "collection", "orient", "shape", "available", "simple", "cumulative", "score", "which", "sum", "weighted", "similarity", "score", "render", "image", "input", "image", "work", "remarkably", "well", "can", "understand", "fact", "input", "shape", "align", "best", "camera", "pose", "vote", "all", "relevant", "input", "shape", "together", "pose", "tend", "much", "more", "stable", "than", "those", "generate", "from", "individual", "shape", "same", "spirit", "when", "generate", "similar", "shape", "we", "combine", "both", "image-shape", "distance", "shape-shape", "distance", "generate", "more", "robust", "set", "similar", "shape", "later", "step", "pipeline", "give", "render", "image", "similar", "align", "shape", "second", "step", "proceed", "initialize", "depth", "information", "-lrb-", "z-coordinate", "-rrb-", "build", "dense", "correspondence", "between", "image", "object", "similar", "shape", "transfer", "depth", "information", "due", "difference", "between", "input", "image", "3d", "shape", "we", "observe", "extremely", "hard", "obtain", "reliable", "correspondence", "via", "pair-wise", "image-shape", "matching", "however", "input", "shape", "align", "we", "exploit", "consistency", "correspondence", "across", "set", "similar", "shape", "so", "we", "can", "obtain", "much", "more", "reliable", "depth", "information", "conceptually", "similar", "state-of-the-art", "technique", "data-driven", "shape", "match", "technique", "-lsb-", "Kim", "et", "al.", "2012a", "Huang", "Guibas", "2013", "-rsb-", "enforce", "consistency", "correspondence", "along", "cycle", "improve", "quality", "isolate", "correspondence", "finally", "third", "step", "we", "refine", "camera", "pose", "depth", "information", "use", "non-rigid", "registration", "formulate", "solve", "continuous", "optimization", "problem", "we", "achieve", "goal", "align", "all", "input", "shape", "learn", "deformation", "prior", "each", "shape", "deformation", "model", "joint", "shape", "alignment", "we", "use", "embedded", "deformation", "model", "-lsb-", "Sumner", "et", "al.", "2007", "-rsb-", "parameterize", "deformation", "each", "shape", "align", "input", "shape", "we", "employ", "method", "describe", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "which", "jointly", "optimize", "deformation", "all", "input", "shape", "minimize", "sum", "distance", "between", "corresponding", "point", "compute", "use", "pair-wise", "alignment", "we", "denote", "optimize", "embedded", "deformation", "shape", "deformation-prior", "learning", "we", "assume", "plausible", "deformation", "each", "shape", "-lrb-", "parameterize", "vector", "collect", "all", "control", "point", "-rrb-", "lie", "low", "dimensional", "space", "define", "shape?s", "neighborhood", "-lrb-", "c.f.", "-lsb-", "Ovsjanikov", "et", "al.", "2011", "-rsb-", "-rrb-", "we", "learn", "space", "from", "optimal", "deformation", "each", "shape", "other", "shape", "which", "provide", "sample", "plausible", "deformation", "-lrb-", "see", "Figure", "-rrb-", "each", "shape", "each", "neighbor", "shape", "we", "transform", "original", "control", "point", "-lrb-", "i.e.", "rest", "state", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "each", "neighbor", "shape", "give", "rise", "deformation", "sample", "prior", "model", "deformation", "each", "shape", "give", "intuitively", "deformation", "lead", "small", "objective", "value", "follow", "majority", "deformation", "sample", "note", "shift", "introduce", "make", "prior", "term", "well-defined", "we", "only", "have", "limit", "deformation", "sample", "many", "principal", "value", "zero", "reconstruction", "stage", "solve", "joint", "optimization", "recover", "geometry", "image", "object", "align", "deform", "version", "set", "similar", "shape", "we", "begin", "introduce", "camera", "model", "we", "show", "how", "initialize", "approximate", "solution", "section", "5.2", "section", "5.3", "refine", "obtain", "final", "solution", "section", "5.4", "we", "use", "simplify", "nine", "parameter", "camera", "configuration", "-lrb-", "-rrb-", "here", "-lrb-", "-rrb-", "specify", "rigid", "motion", "from", "common", "coordinate", "system", "associate", "input", "shape", "camera", "coordinate", "system", "specify", "focal", "length", "specify", "effective", "size", "pixel", "horizontal", "vertical", "direction", "convenience", "we", "denote", "map", "from", "-lrb-", "-rrb-", "candidate", "generation", "we", "candidate", "camera", "pose", "sampling", "strategy", "similar", "most", "pose", "estimation", "algorithm", "-lsb-", "Zia", "et", "al.", "2013", "-rsb-", "which", "sample", "view", "direction", "fix", "rest", "parameter", "default", "value", "specifically", "we", "let", "camera", "position", "move", "view", "sphere", "center", "origin", "radius", "5d", "where", "average", "shape", "diameter", "rest", "parameter", "fix", "follow", "focal", "point", "place", "origin", "fix", "we", "let", "up-right", "direction", "camera", "system", "lie", "plane", "view", "direction", "axis", "finally", "we", "set", "3d", "set", "so", "average", "each", "object", "occupy", "half", "render", "image", "we", "generate", "candidate", "camera", "configuration", "each", "shape", "uniformly", "sampling", "500", "view", "direction", "view", "sphere", "let", "cand", "collect", "all", "candidate", "camera", "configuration", "each", "cand", "we", "denote", "render", "image", "shape", "crop", "use", "tight", "bound", "box", "surround", "object", "optimal", "candidate", "when", "pick", "optimal", "candidate", "we", "follow", "common", "strategy", "evaluate", "render", "image", "compare", "they", "input", "image", "however", "we", "find", "when", "collection", "align", "shape", "present", "simple", "cumulative", "similarity", "score", "between", "input", "image", "render", "image", "sufficient", "-lrb-", "see", "Figure", "-rrb-", "where", "-lrb-", "-rrb-", "give", "feature", "descriptor", "min", "-lcb-", "???", "-rcb-", "c?c", "cand", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "exponential", "operator", "introduce", "down-weight", "contribution", "image", "less", "similar", "input", "image", "we", "have", "various", "image", "descriptor", "include", "gist", "-lsb-", "oliva", "Torralba", "2001", "-rsb-", "hog", "-lsb-", "Dalal", "Triggs", "2005", "-rsb-", "light-field", "descriptor", "-lsb-", "Chen", "et", "al.", "2003", "-rsb-", "experimentally", "we", "find", "feature", "descriptor", "combine", "all", "three", "feature", "together", "yield", "best", "result", "give", "initial", "camera", "configuration", "we", "generate", "initial", "point", "cloud", "from", "do", "select", "set", "similar", "shape", "input", "image", "establish", "dense", "correspondence", "between", "similar", "shape", "transfer", "depth", "information", "performance", "step", "crucial", "since", "govern", "global", "behavior", "final", "reconstruction", "although", "both", "image-based", "retrieval", "image-image", "matching", "have", "be", "study", "considerably", "past", "we", "find", "even", "state-of-the-art", "algorithm", "insufficient", "purpose", "transfer", "depth", "instead", "key", "idea", "propose", "approach", "utilize", "regularization", "provide", "collection", "align", "shape", "boost", "performance", "each", "step", "-rrb-", "similar", "shape", "extract", "from", "match", "render", "image", "have", "similar", "each", "other", "3d", "space", "ii", "-rrb-", "pixel", "render", "image", "different", "shape", "correspond", "same", "object", "image", "pixel", "should", "come", "from", "point", "close", "each", "other", "3d", "space", "where", "align", "model", "live", "experimental", "result", "show", "even", "standard", "pair-wise", "technique", "overall", "performance", "joint", "matching", "approach", "sufficient", "purpose", "depth", "initialization", "-lrb-", "see", "Figure", "-rrb-", "similar", "shape", "extraction", "naive", "approach", "extract", "similar", "shape", "compare", "input", "image", "render", "image", "-lrb-", "accord", "select", "view", "-rrb-", "one-by-one", "however", "even", "learn", "feature", "similarity", "metric", "approach", "insufficient", "due", "diversity", "lighting", "texture", "input", "image", "since", "we", "input", "shape", "align", "we", "use", "distance", "between", "shape", "guide", "selection", "similar", "shape", "specifically", "we", "first", "use", "pairwise", "similarity", "score", "define", "-lrb-", "-rrb-", "extract", "32", "similar", "shape", "-lrb-", "i.e.", "initial", "similar", "shape", "set", "-rrb-", "we", "build", "small", "weighted", "clique", "graph", "which", "consist", "input", "image", "initial", "set", "similar", "shape", "use", "diffusion", "distance", "-lsb-", "Coifman", "et", "al.", "2005", "-rsb-", "sort", "initial", "similar", "shape", "give", "sort", "shape", "we", "select", "top", "shape", "final", "similar", "shape", "set", "simplify", "notation", "let", "similar", "shape", "denote", "correspondence", "initialization", "we", "initialize", "image-shape", "correspondence", "match", "input", "image", "object", "-lrb-", "background", "remove", "-rrb-", "render", "image", "object", "each", "shape", "give", "two", "image", "object", "we", "first", "apply", "-lsb-", "Munich", "Perona", "1999", "-rsb-", "build", "dense", "correspondence", "between", "silhouette", "curve", "treat", "correspondence", "landmark", "correspondence", "we", "employ", "laplacian", "deformation", "-lsb-", "sorkine", "et", "al.", "2004", "-rsb-", "align", "after", "alignment", "we", "derive", "initial", "pixel-shape", "correspondence", "from", "overlaid", "image", "object", "we", "denote", "initial", "correspondence", "from", "note", "some", "pixel", "may", "have", "correspondence", "due", "hole", "shape", "correspondence", "pruning", "so", "far", "we", "only", "compute", "imageshape", "correspondence", "between", "input", "image", "each", "shape", "isolation", "more", "precisely", "give", "two", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "distance", "between", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "large", "least", "one", "two", "correspondence", "incorrect", "addition", "enforce", "consistency", "property", "we", "also", "prioritize", "smoothness", "correspondence", "i.e.", "give", "two", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "neighbor", "so", "should", "we", "favor", "either", "both", "they", "select", "both", "they", "prune", "both", "consistency", "property", "smoothness", "prior", "only", "involve", "pair", "correspondence", "we", "formulate", "correspondence", "pruning", "step", "solve", "binary", "second-order", "mrf", "problem", "we", "introduce", "binary", "random", "variable", "-lcb-", "-rcb-", "each", "initial", "correspondence", "where", "select", "otherwise", "we", "define", "two", "type", "pair-wise", "potential", "function", "each", "correspondence", "pair", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "define", "consistency", "potential", "otherwise", "where", "set", "0.05", "time", "average", "shape", "diameter", "each", "pair", "correspondence", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "two", "pair", "neighbor", "pixel", "we", "define", "smoothness", "potential", "function", "otherwise", "total", "potential", "function", "simply", "sum", "all", "pair-wise", "potential", "function", "where", "collect", "all", "pair", "correspondence", "consideration", "optimization", "we", "apply", "tree-reweighted", "belief", "propagation", "-lrb-", "trbp", "-rrb-", "-lsb-", "Szeliski", "et", "al.", "2008", "-rsb-", "which", "very", "effectively", "binary", "mrf", "problem", "convenience", "we", "still", "use", "denote", "remain", "correspondence", "between", "after", "stage", "geometry", "initialization", "where", "give", "note", "each", "pixel", "do", "belong", "any", "correspondence", "we", "copy", "value", "from", "closest", "pixel", "have", "correspondence", "we", "generate", "initial", "point", "cloud", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "accord", "-lrb-", "-rrb-", "we", "refine", "initial", "image-shape", "correspondence", "initial", "point", "cloud", "solve", "joint", "alignment", "problem", "whose", "objective", "function", "minimize", "distance", "-lrb-", "define", "via", "correspondence", "-rrb-", "between", "point", "cloud", "deform", "similar", "shape", "we", "employ", "icp-like", "procedure", "alternate", "between", "continuous", "optimization", "step", "which", "optimize", "continuous", "variable", "include", "camera", "configuration", "z-coordinate", "each", "pixel", "-lcb-", "-rcb-", "deformation", "each", "shape", "discrete", "optimization", "step", "which", "update", "image-shape", "correspondence", "continuous", "optimization", "step", "we", "consider", "multiple", "objective", "align", "induce", "point-cloud", "similar", "shape", "we", "set", "interior", "point", "20", "point", "close", "silhouette", "note", "another", "option", "measure", "distance", "image", "domain", "however", "due", "distance", "distortion", "projection", "two", "point", "close", "each", "other", "image", "domain", "may", "far", "from", "each", "other", "original", "shape", "turn", "out", "measure", "distance", "image", "domain", "lead", "far", "less", "accurate", "result", "-lrb-", "see", "Figure", "-rrb-", "datum", "consider", "each", "pixel", "independently", "we", "next", "introduce", "second", "term", "regularize", "z-coordinate", "neighbor", "pixel", "finally", "third", "term", "apply", "key", "deformation", "prior", "learn", "preprocessing", "stage", "prior", "-lrb-", "-rrb-", "combine", "datum", "regu", "prior", "energy", "minimization", "problem", "continuous", "optimization", "step", "take", "form", "we", "experiment", "we", "use", "throughout", "same", "set", "parameter", "0.01", "we", "again", "apply", "alternate", "optimization", "effectively", "optimize", "-lrb-", "-rrb-", "each", "step", "we", "first", "fix", "z-coordinate", "optimize", "camera", "configuration", "shape", "deformation", "we", "fix", "optimize", "z-coordinate", "key", "advantage", "alternate", "optimization", "strategy", "-lrb-", "-rrb-", "-lrb-", "10", "-rrb-", "either", "sparse", "-lrb-", "constrain", "neighbor", "pixel", "-rrb-", "small-scale", "-lrb-", "camera", "configuration", "deformation", "parameter", "-rrb-", "enable", "we", "apply", "second-order", "Newton", "method", "optimize", "they", "effectively", "i.e.", "we", "solve", "sparse", "small-scale", "linear", "system", "each", "Newton", "iteration", "objective", "term", "consist", "non-linear", "least", "square", "we", "apply", "gauss-newton", "method", "optimize", "equation", "-lrb-", "-rrb-", "-lrb-", "10", "-rrb-", "derivation", "quite", "standard", "we", "omit", "detail", "discrete", "optimization", "step", "give", "optimize", "point-cloud", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "deform", "shape", "we", "proceed", "optimize", "image-shape", "correspondence", "we", "first", "convert", "each", "deform", "shape", "-lrb-", "-rrb-", "point-cloud", "simulate", "scan", "from", "current", "camera", "configuration", "we", "initialize", "collect", "closest", "point-pair", "init", "-lcb-", "-lrb-", "-rrb-", "argmin", "p?q", "argmin", "-rcb-", "may", "only", "exist", "partial", "similarity", "between", "image", "object", "each", "shape", "we", "adopt", "median", "thresholding", "scheme", "-lsb-", "rusinkiewicz", "Levoy", "2001", "-rsb-", "remove", "correspondence", "far", "from", "each", "other", "leave", "init", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rcb-", "init", "where", "median", "among", "each", "figure", "show", "example", "non-rigid", "alignment", "process", "practice", "only", "4-6", "alternate", "update", "sufficient", "good", "result", "we", "evaluate", "propose", "shape-driven", "image-based", "modeling", "various", "kinect", "scan", "associate", "color", "information", "we", "consider", "five", "category", "object", "chair", "table", "cup", "lamp", "car", "each", "category", "consist", "4-6", "kinect", "scan", "object", "different", "shape", "Figure", "show", "representative", "result", "each", "category", "we", "assume", "all", "object", "capture", "we", "standard", "setting", "where", "background", "easy", "remove", "kinect", "scan", "evaluation", "purpose", "only", "3d", "shape", "from", "Trimble", "warehouse", "each", "category", "contain", "2k-7k", "shape", "-lrb-", "see", "Table", "-rrb-", "where", "Chair", "datum", "set", "from", "-lsb-", "Kim", "et", "al.", "2013", "-rsb-", "car", "datum", "set", "from", "-lsb-", "Huang", "et", "al.", "2013", "-rsb-", "three", "remain", "dataset", "be", "collect", "use", "similar", "strategy", "describe", "-lsb-", "Kim", "et", "al.", "2013", "-rsb-", "note", "even", "thousand", "shape", "shape", "space", "densely", "cover", "can", "see", "from", "extract", "similar", "shape", "-lrb-", "see", "Figure", "-rrb-", "evaluation", "protocol", "we", "evaluate", "reconstruct", "point-cloud", "each", "object", "image", "against", "kinect", "depth", "scan", "factor", "out", "free", "scaling", "degree", "freedom", "we", "first", "compute", "similarity", "transform", "align", "reconstructed", "point", "cloud", "Kinect", "scan", "give", "calibrate", "reconstruction", "Kinect", "scan", "kinect", "we", "propose", "two", "metric", "evaluate", "quality", "first", "metric", "evaluate", "Hausdorff", "distance", "between", "kinect", "-lrb-", "kinect", "-rrb-", "min", "kinect", "P.", "q?P", "second", "metric", "evaluate", "deviation", "between", "pair", "corresponding", "point", "-lrb-", "-rrb-", "kinect", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "P.", "best", "match", "single", "shape", "those", "superscript", "number", "we", "algorithm", "#shapes", "haus", "b", "haus", "b", "haus", "haus", "deviation", "deviation", "chair", "7.3", "0.17", "0.15", "0.05", "0.03", "0.11", "0.10", "Table", "4.2", "0.14", "0.12", "0.06", "0.06", "0.12", "0.13", "Cup", "1.1", "0.15", "0.11", "0.05", "0.04", "0.09", "0.09", "lamp", "2.0", "0.13", "0.15", "0.06", "0.03", "0.10", "0.11", "car", "1.7", "0.12", "0.11", "0.05", "0.03", "0.09", "0.08", "clear", "Hausdorff", "distance", "invariant", "interior", "drift", "surface", "while", "correspondence", "deviation", "more", "strict", "each", "distance", "metric", "we", "collect", "statistics", "mean", "variance", "-lrb-", "-rrb-", "over", "all", "point", "-lrb-", "see", "Table", "-rrb-", "baseline", "we", "calculate", "Hausdorff", "metric", "obtain", "most", "similar", "shape", "Table", "Figure", "show", "representative", "result", "propose", "approach", "overall", "result", "reasonably", "good", "despite", "obvious", "difficulty", "problem", "68.2", "correspondence", "fall", "below", "0.02", "time", "average", "shape", "diameter", "all", "dataset", "Hausdorff", "distance", "error", "considerably", "lower", "than", "deviation", "error", "shape", "point", "cloud", "drive", "shape", "collection", "show", "use", "shape", "collection", "good", "prior", "distribution", "point", "restricted", "drift", "along", "common", "shape", "space", "deviation", "error", "large", "because", "correspondence", "may", "glide", "along", "shape", "which", "exactly", "same", "we", "next", "discuss", "result", "each", "category", "chair", "table", "we", "evaluate", "chair", "table", "category", "because", "fine", "geometric", "detail", "present", "shape", "like", "other", "man-made", "object", "chair", "table", "usually", "have", "strong", "symmetry", "imply", "lower-dimensional", "deformation", "space", "other", "hand", "four", "leg", "may", "introduce", "match", "ambiguity", "category", "we", "find", "we", "algorithm", "limit", "when", "selfocclusion", "present", "lower", "board", "occlude", "front", "leg", "row", "figure", "consequently", "part", "attach", "leg", "cup", "relatively", "small", "household", "item", "usually", "have", "circular", "symmetrical", "body", "interestingly", "we", "method", "produce", "visually", "more", "please", "result", "compare", "Kinect", "because", "object", "size", "reach", "resolution", "limit", "sensor", "surface", "specular", "which", "challenge", "structural", "light", "mechanism", "Kinect", "we", "choose", "category", "because", "have", "large", "variation", "possible", "shape", "particularly", "curvature", "pole", "can", "see", "we", "algorithm", "succeed", "both", "lamp", "example", "figure", "success", "can", "attribute", "two", "reason", "first", "we", "use", "data-driven", "approach", "implicitly", "combine", "part", "from", "different", "shape", "second", "we", "use", "non-rigid", "deformation", "field", "which", "allow", "bend", "pole", "we", "choose", "category", "common", "outdoor", "object", "have", "fine", "geometric", "detail", "-lrb-", "e.g.", "wheel", "side", "mirror", "-rrb-", "we", "algorithm", "could", "accurately", "estimate", "depth", "car", "other", "hand", "Kinect", "have", "problem", "detect", "window", "wheel", "because", "too", "reflective", "too", "dark", "respectively", "comparison", "Automatic", "Pop-Up", "software", "assume", "simple", "geometric", "prior", "tend", "work", "poorly", "complicated", "indoor", "object", "thin", "fine", "feature", "we", "show", "effect", "Automatic", "Pop-Up", "chair", "model", "Figure", "use", "pre-trained", "classifier", "we", "algorithm", "visu", "ally", "significantly", "better", "than", "output", "from", "software", "-lrb-", "compare", "last", "column", "figure", "-rrb-", "image-shape", "matching", "versus", "image-image", "matching", "imageimage", "matching", "far", "less", "accurate", "than", "image-shape", "matching", "-lrb-", "figure", "-rrb-", "two", "reason", "account", "difference", "first", "projection", "from", "2d", "3d", "perspective", "two", "point", "close", "2d", "may", "far", "away", "3d", "second", "3d", "point", "cloud", "shape", "obtainable", "we", "setting", "project", "2d", "lose", "important", "information", "fact", "strong", "perspective", "projection", "might", "still", "hurt", "performance", "we", "algorithm", "example", "seat", "second", "row", "Figure", "estimate", "very", "thick", "which", "can", "attribute", "very", "strong", "perspective", "effect", "close", "chair", "leg", "deformation", "prior", "important", "we", "find", "we", "deformation", "shape", "prior", "key", "success", "illustrate", "Figure", "deformation", "space", "low-dimensional", "point", "generally", "restricted", "move", "parallel", "meaningful", "axis", "scale", "coordinate", "manner", "thus", "constraint", "make", "sure", "local", "deformation", "maintain", "global", "symmetry", "evidently", "Figure", "10", "because", "problem", "single", "image", "depth", "reconstruction", "intrinsically", "underdetermined", "poor", "result", "obtain", "without", "prior", "-lrb-", "top", "-rrb-", "contrary", "isotropic", "symmetry", "learn", "prior", "ensure", "deformation", "subspace", "have", "only", "1d", "which", "coordinate", "scale", "around", "y-axis", "all", "experiment", "be", "conduct", "standard", "desktop", "platform", "2.4", "GHz", "Intel", "Core", "duo-core", "12gb", "RAM", "each", "category", "pre-processing", "stage", "share", "all", "image", "which", "take", "3507", "chair", "2184", "table", "516", "cup", "1223", "lamp", "1109", "car", "camera", "initialization", "stage", "take", "average", "0.3", "each", "input", "image", "most", "time", "spend", "extract", "image", "feature", "each", "image", "object", "point", "cloud", "initialization", "stage", "take", "7", "average", "1", "correspondence", "initialization", "6", "correspondence", "pruning", "point", "cloud", "optimization", "stage", "take", "25", "average", "total", "run", "time", "process", "image", "object", "33", "section", "we", "use", "series", "application", "show", "usefulness", "reconstructed", "point", "cloud", "include", "relight", "image", "object", "synthesize", "unseen", "novel", "view", "depth-aware", "image", "composition", "end", "we", "show", "some", "situation", "reasonable", "full", "mesh", "image", "object", "can", "recover", "use", "we", "point", "cloud", "input", "exploit", "shape", "symmetry", "relighting", "give", "image", "we", "estimate", "depth", "each", "pixel", "use", "local", "pca", "analysis", "estimate", "normal", "simulate", "lighting", "effect", "under", "different", "illumination", "condition", "Figure", "11", "we", "assume", "ambient", "light", "diffusion", "reflection", "surface", "Notice", "synthesize", "image", "almost", "photo-realistic", "except", "some", "artifact", "top-right", "corner", "due", "inaccurate", "depth", "estimation", "novel", "View", "synthesis", "since", "full", "3d", "information", "each", "pixel", "available", "we", "can", "simulate", "movement", "camera", "3d", "synthesize", "novel", "view", "Figure", "12", "synthesize", "view", "from", "we", "depth", "estimation", "use", "inverse", "warping", "method", "-lsb-", "Marcato", "Jr", "1998", "-rsb-", "almost", "photo-realistic", "particular", "we", "can", "even", "accurately", "recover", "depth", "information", "back", "mirror", "car", "appearance", "around", "back", "mirror", "quite", "natural", "when", "car", "rotate", "counter-clockwise", "direction", "-lrb-", "-30", "-15", "deg", "-rrb-", "note", "miss", "part", "invisible", "image", "can", "possibly", "recover", "exploit", "model", "symmetry", "Depth-Aware", "image", "composition", "Figure", "13", "we", "demonstrate", "experiment", "which", "we", "compose", "3d", "model", "woman", "sofa", "image", "so", "woman", "sit", "sofa", "correct", "composition", "should", "make", "sure", "woman?s", "body", "leg", "cover", "back", "arm", "she", "hip", "cover", "front", "chair", "arm", "since", "depth", "sofa", "can", "recover", "we", "can", "compute", "correct", "occlusion", "each", "pixel", "-lrb-", "right", "-rrb-", "oppose", "unnatural", "occlusion", "pattern", "depth", "information", "available", "-lrb-", "left", "-rrb-", "symmetry-based", "surface", "reconstruction", "infer", "point", "cloud", "only", "have", "point", "visible", "from", "camera", "view", "however", "we", "show", "experiment", "key", "intermediate", "step", "towards", "full", "3d", "model", "reconstruction", "we", "hallucinate", "miss", "part", "exploit", "model", "symmetry", "we", "use", "-lsb-", "Mitra", "et", "al.", "2006", "-rsb-", "extract", "symmetry", "pattern", "from", "similar", "shape", "transport", "they", "point", "cloud", "Figure", "14", "we", "discover", "circular", "symme", "try", "cup", "body", "plane", "symmetry", "handle", "which", "also", "induce", "segmentation", "cup", "thus", "we", "can", "transport", "symmetry", "parameter", "achieve", "full", "shape", "recovery", "use", "-lsb-", "Mitra", "et", "al.", "2007", "-rsb-", "finally", "we", "apply", "Poisson", "reconstruction", "-lsb-", "Kazhdan", "et", "al.", "2006", "-rsb-", "extract", "point", "cloud", "surface", "reconstruction", "smoothing", "final", "result", "bottom", "row", "Figure", "14", "we", "see", "reconstructed", "mesh", "generally", "look", "natural", "from", "all", "view", "however", "because", "we", "only", "apply", "plane", "symmetry", "handle", "part", "gap", "bottom", "handle", "reconstructed", "surface", "connect", "discover", "better", "structural", "predictor", "close", "small", "gap", "interesting", "open", "problem", "further", "exploration", "paper", "we", "have", "present", "data-driven", "algorithm", "add", "depth", "information", "image", "object", "algorithm", "take", "input", "image", "segmented", "object", "collection", "3d", "shape", "same", "object", "class", "compute", "various", "geometric", "prior", "from", "shape", "collection", "optimize", "depth", "estimation", "image", "object", "procedure", "fully", "automatic", "we", "have", "evaluate", "performance", "present", "approach", "benchmark", "consist", "Kinect", "scan", "variety", "common", "object", "take", "under", "different", "lighting", "condition", "experimental", "result", "show", "we", "approach", "produce", "depth", "close", "ground-truth", "superior", "state-of-the-art", "depth", "estimator", "we", "have", "also", "show", "usefulness", "we", "approach", "various", "application", "besides", "application", "demonstrate", "paper", "present", "depth", "estimator", "enable", "variety", "other", "application", "both", "computer", "graphic", "computer", "vision", "example", "shape", "collection", "can", "serve", "hub", "link", "many", "image", "object", "particularly", "useful", "retrieve", "similar", "image", "object", "be", "take", "from", "drastically", "different", "view", "point", "can", "match", "well", "pure", "image", "method", "another", "example", "help", "image-shape", "network", "we", "can", "propagate", "rich", "image", "label", "purpose", "categorize", "shape", "challenging", "problem", "shape", "analysis", "due", "lack", "label", "shape", "datum", "combine", "3d", "shape", "label", "Limitations", "course", "state", "we", "approach", "require", "segmented", "image", "object", "knowledge", "object", "class", "present", "method", "work", "best", "man-made", "object", "whose", "3d", "model", "can", "well", "align", "where", "variation", "shape", "pose", "modest", "do", "apply", "well", "object", "high", "variability", "tree", "building", "high", "articulation", "animal", "object", "important", "utilize", "more", "specialized", "domain", "knowledge", "-lrb-", "i.e.", "skeleton", "regular", "structure", "-rrb-", "establish", "correspondence", "estimate", "depth", "finally", "we", "experience", "minimum", "couple", "hundred", "shape", "necessary", "algorithm", "succeed", "intuition", "each", "part", "object", "image", "need", "have", "multiple", "correspondence", "good", "regularization", "ample", "opportunity", "future", "research", "while", "so", "far", "we", "have", "focus", "estimate", "depth", "single", "segmented", "object", "would", "very", "interesting", "generalize", "approach", "estimate", "depth", "entire", "scene", "we", "thank", "reviewer", "comment", "suggestion", "paper", "work", "support", "part", "NSF", "grant", "IIS", "1016324", "DMS", "1228304", "AFOSR", "grant", "fa9550-12-1-0372", "NSFC", "grant", "61202221", "Max", "Plack", "Center", "Visual", "Computing", "Communications", "Google", "Motorola", "research", "award", "gift", "from", "HTC", "corporation", "Marie", "Curie", "Career", "integration", "Grant", "303541", "ERC", "start", "Grant", "SmartGeometry", "-lrb-", "stg-2013335373", "-rrb-", "gift", "from", "Adobe" ],
  "content" : "Our key insight is that, even when the imaged object is not contained in the shape collection, the network of shapes implicitly characterizes a shape-specific deformation subspace that regularizes the problem and enables robust diffusion of depth information from the shape collection to the input image. Key to our approach is the estimation of correspondences between the image and multiple models, with the help of correspondences estimated between the models themselves. Experimental results show that the proposed approach recovers depth information that is close to Kinect scans, and is significantly more accurate than state-of-the-art image-based modeling techniques. We demonstrate depth-enhanced image editing to illustrate the possibilities offered by our approach. The emergence of large shape collections provides us with a platform to aggregate information from multiple shapes to improve the analysis and processing of individual shapes. The major limitation of these approaches is that a projected view of a 3D shape only contains partial information from the original shape. In particular, we show that matching an image with a collection of 3D shapes boosts the matching quality by enforcing consistency between image-shape maps and shape-shape maps. However, these approaches are designed for objects with simple textures and shapes and/or under specific lighting conditions. Since obtaining 3D shapes that are similar in global structure to image objects is easy, we estimate depth information in an unsupervised manner, i.e., by directly matching images with 3D shapes, and thus avoiding the tedious task of performing instance specific learning. The proposed image-based but shape-driven modeling approach takes a single image object I segmented from background and a collection of shapes S = {S 1 , S 2 , ? ? ? , S N } of the same class as  input, and simultaneously estimates the object pose shown in I and reconstructs a 3D point cloud P from I. For simplicity, we assume that all input shapes are supported by the same ground plane [Huang et al. 2013], so a common vertical direction is available. The variables to be optimized are the point cloud, parameterized by the camera pose and the z-coordinates (pixel depths) of the image object, and deformations of a set of similar shapes. First, the depth coordinates of the point cloud are unconstrained yet we cannot allow the shapes to be deformed arbitrarily, since otherwise both the point cloud and the shapes may be stretched undesirably when being aligned. Second, the success of the non-rigid alignment depends on a good initialization for both the camera pose and the point cloud. Third, even with good initialization, it is challenging to solve the induced optimization problem involving the depth of each pixel effectively. As illustrated in Figure 2 , the pipeline consists of a preprocessing stage and a reconstruction stage. The motivation comes from the fact that plausible deformations of each shape typically lie in a lowdimensional space, when compared with the number of parameters in a general deformation model [Averkiou et al. 2014]. Essentially, we learn the local structure of the shape space. The reconstruction stage proceeds in three steps, where the first two steps provide an initial solution (a set of similar shapes and an initial point cloud) and the third step optimizes this point cloud to minimize its distance to the deformed similar shapes. This is considered as a pose estimation problem. Although pose estimation using a single shape is hard, we found that when a collection of oriented shapes are available, a simple cumulative score, which sums the weighted similarity scores of the rendered images to the input image, works remarkably well. This can be understood by the fact that the input shapes are aligned, and the best camera pose is voted on by all relevant input shapes together, then the pose tends to be much more stable than those generated from individual shapes. In the same spirit, when generating similar shapes, we combine both the image-shape distances and shape-shape distances to generate a more robust set of similar shapes for later steps of the pipeline. Given the rendered images of similar aligned shapes, the second step proceeds to initialize the depth information (z-coordinates) by building dense correspondences between the image object and similar shapes and transferring the depth information. Due to the differences between the input image and 3D shapes, we observed that it is extremely hard to obtain reliable correspondences via pair-wise image-shape matching. However, as the input shapes are aligned, we exploit the consistency of correspondences across the set of similar shapes, so that we can obtain much more reliable depth information. This is conceptually similar to state-of-the-art techniques in data-driven shape matching techniques [Kim et al. 2012a; Huang and Guibas 2013] to enforce consistency of correspondences along cycles to improve quality of isolated correspondences. Finally, in the third step we refine the camera pose and depth information using non-rigid registration formulated as solving a continuous optimization problem. We achieve this goal by aligning all input shapes and then learning a deformation prior for each shape. Deformation model and joint shape alignment. We use the embedded deformation model [Sumner et al. 2007] to parameterize the deformation of each shape. To align the input shapes, we employ the method described in [Huang et al. 2013], which jointly optimizes the deformations of all input shapes to minimize the sum of distances between corresponding points computed using pair-wise alignment. We denote the optimized embedded deformation of shape S i by D i ? . Deformation-prior learning. We assume that plausible deformations of each shape (parameterized by a vector that collects all control points) lie in a low dimensional space defined by the shape?s neighborhood (c.f. , [Ovsjanikov et al. 2011]). We learn this space from the optimal deformations of each shape S i to other shapes, which provide samples of plausible deformations (see Figure 3 ). For each shape S i and each neighboring shape S j , we transform the original control point p ? (i.e., in the rest state) of D i ? to (D j ? ?1 ? D i ? )(p ? ). Then each neighboring shape S j gives rise to a deformation sample c i,j ? c 0 i . The prior model on the deformation of each shape is given by Intuitively, a deformation leads to a small objective value if it follows the majority of the deformation samples. Note that the shift ? is introduced to make the prior term well-defined as we only have limited deformation samples and many principal values are zero. The reconstruction stage solves a joint optimization to recover the geometry of an image object that aligns with the deformed versions of a set of similar shapes. We begin by introducing the camera model. Then we show how to initialize an approximate solution in Section 5.2 and Section 5.3, and refine it to obtain the final solution in Section 5.4. We use a simplified nine parameter camera configuration C = (R, t, z f , s x , s y ). Here (R, t) specifies the rigid motion from the common coordinate system ? associated with the input shapes to the camera coordinate system ? C ; z f specifies the focal length; s x and s y specify the effective size of the pixels in the horizontal and vertical directions. For convenience, we denote the map from p, z p to q as C(p, z p ). Candidate generation. Our candidate camera pose sampling strategy is similar to most pose estimation algorithms [Zia et al. 2013], which sample the viewing direction and fix the rest of the parameters to default values. Specifically, we let the camera position move on a viewing sphere centered at the origin with radius 5d, where d is the averaged shape diameter. The rest of the parameters are fixed as follows. The focal point t is placed at the origin. To fix R, we let the up-right direction of the camera system lie in the plane of the viewing direction and the z axis. Finally, we set z p = 3d, and set s x , s y so that, on the average, each object occupies half of the rendered image. We generate candidate camera configurations for each shape by uniformly sampling 500 viewing directions on the viewing sphere. Let C cand collect all candidate camera configurations. For each C ? C cand , we denote I i C as the rendered image of shape S i cropped using a tight bounding box surrounding object. Optimal candidate. When picking the optimal candidate, we follow the common strategy of evaluating rendered images by comparing them with the input image. However, we found that when a collection of aligned shapes are present, a simple cumulative similarity score between the input image and the rendered images is sufficient (see Figure 4 ): N where f (?) is a given feature descriptor, ? = min i?{1,??? ,n},C?C cand f (I) ? f (I i C ) , and the exponential operator is introduced to down-weight the contribution of images that are less similar to the input image. We have various image descriptors including GIST [Oliva and Torralba 2001], HOG [Dalal and Triggs 2005], and the light-field descriptor [Chen et al. 2003]. Experimentally, we found that the feature descriptor that combines all the three features together yields the best result. Given the initial camera configuration we generate an initial point cloud P from I. This is done by selecting a set of similar shapes to the input image, and then establishing dense correspondences between I and the similar shapes for transferring depth information. The performance of this step is crucial since it governs the global behavior of the final reconstruction. Although both image-based retrieval and image-image matching have been studied considerably in the past, we found that even state-of-the-art algorithms are insufficient for the purpose of transferring depth. Instead, the key idea of the proposed approach is to utilize the regularization provided by a collection of aligned shapes to boost the performance in each step: i) the similar shapes extracted from matching rendered images have to be similar with each other in the 3D space, and ii) pixels in the rendered images of different shapes corresponding to the same object image pixel should come from points close to each other in the 3D space where the aligned models live. Experimental results show that even with standard pair-wise techniques, the overall performance of joint matching approaches is sufficient for the purpose of depth initialization (see Figure 5 ). Similar shape extraction. A naive approach to extract the similar shapes is to compare the input image with rendered images (according to the selected view) one-by-one. However, even with learned feature similarity metrics, such an approach is insufficient due to the diversity in lighting and texture of the input image. Since our input shapes are aligned, we use the distances between shapes to guide the selection of similar shapes. Specifically, we first use the pairwise similarity score defined in (5) to extract K 0 = 32 similar shapes (i.e., an initial similar shape set). We then build a small weighted clique graph, which consists of the input image and the initial set of similar shapes, and use the diffusion distance [Coifman et al. 2005] to sort the initial similar shapes. Given the sorted shapes, we select the top K = 6 shapes as the final similar shape set. To simplify the notation let the similar shapes be denoted as S 1 , ? ? ? , S K . Correspondence initialization. We initialize the image-shape correspondences by matching the input image object (background is removed) and the rendered image object I i C of each shape S i . Given two image objects, we first apply [Munich and Perona 1999] to build dense correspondences between silhouette curves. Treating these correspondences as landmark correspondences, we then employ Laplacian deformation [Sorkine et al. 2004] to align I and I i C . After alignment, we derive the initial pixel-shape correspondences from the overlaid image objects. With M i ? I ? S i we denote the initial correspondences from I ? S i . Note that some pixels may not have correspondences due to ?holes? in the shapes. Correspondence pruning. So far we only compute the imageshape correspondences between the input image and each shape in isolation. More precisely, given two correspondences (p, q i ) ? M i and (p, q j ) ? M j , if the distance between D i ? (q i ) and D j ? (q j ) is large, then at least one of these two correspondences is incorrect. In addition to enforcing this consistency property, we also prioritize the smoothness of correspondences, i.e., given two correspondences (p, q i ), (p ? , q ? i ) ? M i where p and p ? are neighbors and so should be q i and q ? i , we favor that either both of them are selected or both of them are pruned. As both the consistency property and the smoothness prior only involve pairs of correspondences, we formulate the correspondence pruning step as solving a binary second-order MRF problem. We introduce a binary random variable x c ? {0, 1} for each initial correspondence c ? ? K i=1 M i , where x c = 1 if c is selected and x c = 0 otherwise. We then define the two types of pair-wise potential functions. For each correspondence pair c i = (p, q i ) ? M i and c j = (p, q j ) ? M j , we define a consistency potential: otherwise, where ? is set as the 0.05 times the averaged shape diameter. For each pair of correspondences c = (p, q i ) ? M i and c ? = (p ? , q ? i ) ? M i where p, p ? and q i , q ? i are two pairs of neighboring pixels, we define a smoothness potential function as otherwise. Then the total potential function simply sums all pair-wise potential functions where P collects all pairs of correspondences of consideration. For optimization, we apply tree-reweighted belief propagation (TRBP) [Szeliski et al. 2008], which is very effectively on binary MRF problems. For convenience, we still use M i to denote the remaining correspondences between I and S i after this stage. Geometry initialization. where R, t are given by C ? . Note that for each pixel p that does not belong to any correspondence, we copy the value of z p from the closest pixel that has correspondences. We then generate the initial point cloud P = {C(p, z p )|p ? I} according to (4). We refine the initial image-shape correspondences and the initial point cloud by solving a joint alignment problem, whose objective function minimizes the distance (defined via correspondences) between the point cloud and deformed similar shapes. We employ an ICP-like procedure, alternating between a continuous optimization step, which optimizes the continuous variables including camera configuration C, the z-coordinates of each pixel {z p |p ? I} and the deformation of each shape D i , 1 ? i ? K; and a discrete optimization step, which updates image-shape correspondences. Continuous optimization step. We consider multiple objectives for aligning the induced point-cloud and the similar shapes. We set w p = 1 for interior points and w p = 20 for points close to silhouettes. Note that another option is to measure the distance in the image domain. However, due to distance distortions in projection, two points that are close to each other in the image domain may be far from each other on the original shape. It turns out measuring the distance in the image domain leads to far less accurate results (see Figure 6 ). As f data considers each pixel independently, we next introduce a second term to regularize the z-coordinate of neighboring pixels: Finally, the third term applies the key deformation priors learned in the preprocessing stage: prior(D i ). Combining data regu , and prior , the energy minimization problem in the continuous optimization step takes the form: In our experiments, we use throughout the same set of parameters: ? r = 0.01 and ? p = 1. We again apply alternating optimization to effectively optimize (8). In each step, we first fix the z-coordinates z p to optimize the camera configuration C and the shape deformations D i : We then fix C and D i to optimize the z-coordinates z p : The key advantage of this alternating optimization strategy is that (9) and (10) are either sparse (constraining neighboring pixels) or of small-scale (camera configuration and deformation parameters). This enables us to apply second-order Newton methods to optimize them effectively, i.e., we solve a sparse or a small-scale linear system at each Newton iteration. As the objective terms consists of non-linear least squares, we apply a Gauss-Newton method for optimizing Equations (9) and (10). The derivation is quite standard and we omit the details. Discrete optimization step. Given the optimized point-cloud P = {C(p, z p )|p ? I} and deformed shapes, we proceed to optimize the image-shape correspondences. We first convert each deformed shape D i (S i ) into a point-cloud S i ? by simulating a scan from the current camera configuration. We then initialize M i to collect closest point-pairs M i init = {(p, q)|q = argmin p?q ? or p = argmin p ? ?q }. q ? ?S ? p ? ?P i  As there may only exist partial similarity between the image object I and each shape S i , we adopt the median thresholding scheme [Rusinkiewicz and Levoy 2001] to remove correspondences that are far from each other, leaving init M i = {(p, q)| p ? q ? 2? i , (p, q) ? M i }, init where ? i is the median of p ? q among each M i . Figure 8 shows an example of the non-rigid alignment process. In practice, only 4-6 alternating updates are sufficient for good results. We evaluated the proposed shape-driven image-based modeling on various Kinect scans with associated color information. We consider five categories of objects: chairs, tables, cups, lamps and cars. Each category consists of 4-6 Kinect scans of objects with different shapes. Figure 7 shows representative results in each category. We assume all the objects are captured in our standard setting, where background is easy to remove. The Kinect scans are for evaluation purposes only. The 3D shapes are from Trimble warehouse. Each category contains 2K-7K shapes (see Table 1 ), where the Chair data set is from [Kim et al. 2013], the Car data set is from [Huang et al. 2013], and the three remaining datasets were collected using a similar strategy to that described in [Kim et al. 2013]. Note that even with thousands of shapes, the shape space is not densely covered as can be seen from the extracted similar shapes (see Figure 7 ). Evaluation protocol. We evaluate the reconstructed point-cloud of each object image against the Kinect depth scans. To factor out the free scaling degree of freedom we first compute a similarity transform that aligns the reconstructed point cloud with the Kinect scan. Given the calibrated reconstruction P and the Kinect scan P Kinect , we propose two metrics to evaluate the quality of P. The first metric evaluates the Hausdorff distance between P and P Kinect : d(p, P Kinect ) = min Kinect p ? q , ?p ? P. q?P The second metric evaluates the deviation between the pair of corresponding points p and f (p) in P and P Kinect : d(p, f (p))) = p ? f (p) , ?p ? P. the best matched single shape and those with no superscript are numbers by our algorithm. #shapes ? haus bs /? haus bs ? haus /? haus ? deviation /? deviation Chair 7.3K 0.17 / 0.15 0.05 / 0.03 0.11 / 0.10 Table 4.2K 0.14 / 0.12 0.06 / 0.06 0.12 / 0.13 Cup 1.1K 0.15 / 0.11 0.05 / 0.04 0.09 / 0.09 Lamp 2.0K 0.13 / 0.15 0.06 / 0.03 0.10 / 0.11 Car 1.7K 0.12 / 0.11 0.05 / 0.03 0.09 / 0.08\n        It is clear that the Hausdorff distance is invariant to interior drifting on the surface, while the correspondence deviation is more strict. For each distance metric we collect statistics on the mean and variance of d(p, P) over all points (see Table 1 ). As a baseline, we calculated the Hausdorff metric obtained by the most similar shape. Table 1 and Figure 7 shows representative results for the proposed approach. Overall the results are reasonably good despite the obvious difficulty of the problem, with 68.2% correspondences falling below 0.02 times the averaged shape diameter. For all datasets, the Hausdorff distance error is considerably lower than that of the deviation error. As the shape of the point cloud is driven by the shape collection, this shows that using the shape collection as a good prior, the distribution of points is restricted to drift along the common shape space. The deviation error is large because the correspondences may glide along the shapes, which are not exactly the same. We next discuss the results for each category. Chair and tables. We evaluate on the chair and table categories because fine geometric details are present in these shapes. Like other man-made objects, chair and tables usually have strong symmetries, implying a lower-dimensional deformation space. On the other hand, the four legs may introduce matching ambiguities. On these categories, we find that our algorithm is limited when selfocclusion presents: the lower board is occluded by the front leg in Row 6 of Figure 7 and consequently part of it is attached to the leg. Cups are relatively small household items and usually have a circular symmetrical body. Interestingly, our method produces visually more pleasing results compared with the Kinect, because the object size is reaching the resolution limit of the sensor and the surface is specular, which is challenging for the structural light mechanism of the Kinect. We choose this category because it has large variations in the possible shapes, particularly in the curvature of the pole. It can be seen that our algorithm succeeds in both lamp examples in Figure 7 . The success can be attributed to two reasons. First, we use a data-driven approach to implicitly combine parts from different shapes. Second, we use a non-rigid deformation field, which allows the bending of the pole. We choose this category as a common outdoor object having fine geometric details (e.g., wheels, side mirrors). Our algorithm could accurately estimates the depth of cars. On the other hand, the Kinect has problems in detecting windows and wheels, because they are too reflective or too dark respectively. Comparison to Automatic Pop-Up. The software assumes simple geometric priors and tend to work poorly for complicated indoor objects with thin and fine features. We show the effect of Automatic Pop-Up on a chair model in Figure 9 using the pre-trained classifiers. Our algorithm is visu- ally significantly better than the output from this software (compare the last column of Figures 2 and 9). Image-Shape matching versus Image-Image matching. Imageimage matching is far less accurate than image-shape matching ( Figure 6 ). Two reasons accounts for the difference. First, the projection from 2D to 3D is perspective and two points close in 2D may be far away in 3D. Second, a 3D point cloud for a shape is obtainable in our setting and projecting it to 2D loses important information. In fact, strong perspective projection might still hurt the performance of our algorithm. For example, the seat of the second row in Figure 7 is estimated to be very thick, which can be attributed to very strong perspective effect close to the chair leg. Deformation prior is important. We find that our deformation shape prior is key to success. As illustrated in Figure 3 , the deformation space is low-dimensional and points are generally restricted to move parallel to meaningful axes or scale in a coordinated manner. Thus, such constraints makes sure that the local deformation maintain global symmetries. Evidently, in Figure 10 , because the problem of single image depth reconstruction is intrinsically underdetermined, poor results are obtained without a prior (top). On the contrary, the isotropic symmetry learned by the prior ensures that the deformation subspace has only 1D, which is a coordinated scaling around the y-axis. All experiments were conducted on a standard desktop platform with a 2.4GHz Intel Core 2 Duo-core and 12GB of RAM. For each category, the pre-processing stage is shared by all images, which takes 3507s for chairs, 2184s for tables, 516s for cups, 1223s for lamps and 1109s for cars. The camera initialization stage takes on average 0.3s for each input image, and most of the time was spent on extracting image features. For each image object, the point cloud initialization stage took ?7s in average, with ?1s on correspondence initialization and ? 6s on correspondence pruning. The point cloud optimization stage took ? 25s in average. The total running time for processing an image object was ? 33s. In this section, we use a series of applications to show the usefulness of the reconstructed point cloud, including relighting image object, synthesizing unseen novel views, and depth-aware image composition. In the end, we show that, in some situations, a reasonable full mesh of an image object can be recovered using our point cloud as input, by exploiting shape symmetries. Relighting. Given an image, we estimate the depth of each pixel and use local PCA analysis to estimate normals and simulate the lighting effects under different illumination conditions. In Figure 11, we assume an ambient light and a diffusion reflection on the surface. Notice that the synthesized image is almost photo-realistic,  except some artifact at the top-right corner due to inaccurate depth estimation. Novel View Synthesis. Since the full 3D information for each pixel is available, we can simulate the movement of the camera in 3D and synthesize novel views. In Figure 12 , the synthesized view from our depth estimation using the inverse warping method [Marcato Jr 1998] is almost photo-realistic. In particular, as we can even accurately recover the depth information of the back mirror of the car, the appearance around the back mirror is quite natural when the car is rotating in the counter-clockwise direction (-30 and -15 deg). Note that the missing parts that are invisible in the image can possibly be recovered by exploiting model symmetry. Depth-Aware Image Composition. In Figure 13 we demonstrate an experiment in which we compose a 3D model of a woman with a sofa image, so that the woman is ?sitting? on the sofa. A correct composition should make sure that the woman?s body and legs cover the back arm, and her hip is covered by the front chair arm. Since the depth of the sofa can be recovered, we can compute this correct occlusion for each pixel (right), as opposed to unnatural occlusion patterns if no depth information is available (left). Symmetry-based Surface Reconstruction. The inferred point cloud only has points visible from the camera view. However, we show in this experiment that it is a key intermediate step towards full 3D model reconstruction. We hallucinate the missing parts by exploiting the model symmetry. We use [Mitra et al. 2006] to extract symmetry patterns from similar shapes and transport them to the point cloud. In Figure 14 , we discover a circular symme- try for the cup body and a plane symmetry for the handle, which also induces a segmentation of the cup. Thus, we can transport the symmetry parameters and achieve full shape recovery using [Mitra et al. 2007]. Finally, we apply Poisson reconstruction [Kazhdan et al. 2006] on the extracted point cloud for surface reconstruction with smoothing. The final result is in the bottom row of Figure 14 . We see that the reconstructed mesh generally looks natural from all views. However, because we only apply plane symmetry at the handle part, there is gap at the bottom of the handle and the reconstructed surface is not connected. Discovering better structural predictors to close the small gap is an interesting open problem for further exploration. In this paper, we have presented a data-driven algorithm for adding depth information to an image object. The algorithm takes as input an image of a segmented object and a collection of 3D shapes of the same object class, and computes various geometric priors from the shape collection to optimize the depth estimation of the image object. This procedure is fully automatic. We have evaluated the performance of the presented approach on a benchmark that consists of Kinect scans of a variety of common objects taken under different lighting conditions. Experimental results show that our approach produces depth that is close to the ground-truth, and is superior to state-of-the-art depth estimators. We have also shown the usefulness of the our approach for various applications. Besides the applications demonstrated in this paper, the presented depth estimator enables a variety of other applications in both computer graphics and computer vision. As an example, the shape collection can serve as the hub that links many image objects. This is particularly useful for retrieving similar image objects that were taken from drastically different view points that cannot be matched well by pure image methods. As another example, with the help of the image-shape network, we can propagate rich image labels for the purpose of categorizing shapes ? a challenging problem in shape analysis due to the lack of labeled shapes or of data combining 3D shapes and labels. Limitations. Of course, as stated, our approach requires a segmented image of an object and a knowledge of the object class. The presented method works best with man-made objects whose 3D models can be well aligned and where the variation in shape poses is modest. It does not apply well to objects of high variability, such as trees, or buildings, or of high articulation, such as animals. For these objects, it is important to utilize more specialized domain knowledge (i.e., skeletons and regular structures) to establish correspondences and estimate depth. Finally, in our experience, a minimum of a couple of hundreds of shapes is necessary for the algorithm to succeed. The intuition is that each part of the object in the image needs to have multiple correspondences for good regularization. There are ample opportunities for future research. While so far we have focused on estimating the depth of a single segmented object, it would be very interesting to generalize this approach to estimate the depth of an entire scene. We thank the reviewers for their comments and suggestions on the paper. This work was supported in part by NSF grants IIS 1016324 and DMS 1228304, AFOSR grant FA9550-12-1-0372, NSFC grant 61202221, the Max Plack Center for Visual Computing and Communications, Google and Motorola research awards, a gift from HTC corporation, the Marie Curie Career Integration Grant 303541, the ERC Starting Grant SmartGeometry (StG-2013335373), and gifts from Adobe.",
  "resources" : [ ]
}