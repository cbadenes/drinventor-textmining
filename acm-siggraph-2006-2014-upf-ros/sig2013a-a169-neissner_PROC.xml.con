{
  "uri" : "sig2013a-a169-neissner_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013a/a169-neissner_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Real-time 3D Reconstruction at Scale using Voxel Hashing",
    "published" : null,
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ ]
  },
  "bagOfWords" : [ "we", "have", "implement", "we", "datum", "structure", "use", "DirectX", "11", "Compute", "Shaders", "we", "use", "Asus", "Xtion", "scene", "Fig.", "10", "kinect", "Windows", "camera", "all", "other", "scene", "both", "provide", "rgb-d", "datum", "30Hz", "result", "live", "scene", "capture", "we", "test", "scene", "show", "figure", "11", "well", "supplementary", "material", "we", "capture", "variety", "indoor", "outdoor", "scene", "under", "variety", "lighting", "condition", "while", "quality", "active", "infrared", "sensor", "affect", "significantly", "outdoor", "scene", "we", "system", "still", "manage", "reconstruct", "large-scale", "outdoor", "scene", "fine", "quality", "statue", "fig.", "show", "result", "after", "online", "scan", "20m", "long", "corridor", "museum", "about", "4m", "high", "statue", "which", "capture", "reconstruct", "live", "under", "minute", "passageway", "-lrb-", "fig.", "11", "top", "-rrb-", "show", "pathway", "shop", "30m", "long", "reconstruct", "live", "QUEENS", "-lrb-", "Fig.", "11", "middle", "-rrb-", "show", "large", "courtyard", "-lrb-", "stretch", "16m", "12m", "2m", "-rrb-", "reconstruct", "approximately", "minute", "finally", "bookshop", "-lrb-", "fig.", "11", "bottom", "-rrb-", "show", "three", "level", "bookstore", "reconstruct", "under", "minute", "reconstruction", "demonstrate", "both", "scale", "quality", "be", "all", "reconstruct", "well", "above", "30hz", "frame", "rate", "kinect", "show", "figure", "allow", "potential", "increase", "voxel", "resolution", "additional", "icp", "step", "more", "robust", "camera", "tracking", "we", "use", "voxel", "size", "4mm", "fig.", "10", "10mm", "fig.", "11", "we", "also", "test", "we", "system", "2mm", "voxel", "without", "visible", "improvement", "overall", "reconstruction", "quality", "while", "highlight", "limit", "current", "depth", "sense", "technology", "we", "believe", "open", "up", "new", "possibility", "future", "depth", "acquisition", "hardware", "we", "measure", "performance", "we", "entire", "pipeline", "include", "run-time", "overhead", "-lrb-", "display", "rendering", "-rrb-", "Intel", "Core", "i7", "3.4", "GHz", "CPU", "16gb", "RAM", "single", "NVIDIA", "GeForce", "GTX", "Titan", "average", "timing", "among", "all", "test", "scene", "21.8", "m", "-lrb-", "46f", "p", "-rrb-", "8.0", "m", "-lrb-", "37", "overall", "pipeline", "-rrb-", "ICP", "pose", "estimation", "-lrb-", "15", "iteration", "-rrb-", "4.6", "m", "-lrb-", "21", "-rrb-", "surface", "integration", "4.8", "m", "-lrb-", "22", "-rrb-", "surface", "extraction", "shading", "-lrb-", "include", "color", "phong", "shade", "-rrb-", "4.4", "m", "-lrb-", "20", "-rrb-", "streaming", "input", "datum", "processing", "separate", "timing", "each", "test", "scene", "provide", "fig.", "we", "datum", "structure", "use", "total", "34mb", "hash", "table", "all", "auxiliary", "buffer", "allow", "hash", "table", "21", "entry", "each", "contain", "12", "byte", "we", "experiment", "show", "bucket", "size", "two", "provide", "best", "performance", "leave", "we", "about", "million", "bucket", "we", "pre-allocate", "1gb", "heap", "memory", "provide", "space", "voxel", "datum", "GPU", "voxel", "per", "block", "-lrb-", "byte", "per", "voxel", "-rrb-", "correspond", "18", "voxel", "block", "note", "21", "hash", "entry", "only", "index", "18", "voxel", "block", "result", "low", "hash", "occupancy", "thus", "minimize", "hash", "collision", "average", "we", "find", "about", "140k", "voxel", "block", "allocate", "when", "capture", "we", "test", "scene", "voxel", "size", "8mm", "-lrb-", "vary", "scene", "complexity", "-rrb-", "correspond", "equal", "amount", "occupied", "hash", "entry", "result", "hash", "table", "occupancy", "120k", "bucket", "single", "entry", "10k", "bucket", "two", "entry", "bucket", "size", "two", "hash", "table", "size", "21", "all", "test", "scene", "run", "only", "0.1", "bucket", "overflow", "handle", "link", "list", "across", "all", "scene", "largest", "list", "length", "three", "total", "700", "link", "list", "entry", "allocate", "across", "all", "scene", "which", "negligible", "compare", "hash", "table", "size", "average", "less", "than", "300mb", "memory", "allocate", "surface", "datum", "-lrb-", "less", "than", "600mb", "color", "-rrb-", "compare", "favorably", "regular", "grid", "would", "require", "well", "over", "5gb", "-lrb-", "include", "color", "-rrb-", "same", "voxel", "resolution", "-lrb-", "8mm", "-rrb-", "spatial", "extent", "-lrb-", "8m", "depth", "-rrb-", "also", "leave", "enough", "space", "encode", "RGB", "datum", "directly", "store", "voxel", "-lrb-", "see", "Fig.", "11", "-rrb-", "practice", "simple", "hash", "scheme", "small", "bucket", "size", "large", "hash", "table", "size", "work", "well", "we", "scenario", "we", "can", "tolerate", "larger", "sparser", "-lrb-", "21", "-rrb-", "hash", "table", "size", "because", "memory", "footprint", "hash", "table", "insignificant", "-lrb-", "34mb", "-rrb-", "compare", "voxel", "block", "buffer", "-lrb-", "which", "pre-allocated", "1gb", "-rrb-", "smaller", "hash", "table", "size", "cause", "higher", "occupancy", "decrease", "performance", "example", "statue", "scene", "we", "standard", "setting", "-lrb-", "21", "element", "-rrb-", "occupy", "6.4", "hash", "table", "run", "21m", "200k", "element", "occupancy", "rise", "65", "performance", "reduce", "24.8", "m", "160k", "element", "occupancy", "rise", "81", "performance", "further", "fall", "25.6", "ms.", "we", "live", "system", "we", "choose", "larger", "table", "size", "we", "favor", "performance", "over", "small", "memory", "gain", "we", "pipeline", "currently", "use", "atomic", "operation", "per", "hash", "bucket", "allocation", "streaming", "show", "we", "timing", "across", "all", "scene", "sequential", "operation", "cause", "negligible", "performance", "overhead", "due", "hash", "collision", "be", "minimal", "more", "sophisticated", "hash", "approach", "-lsb-", "Lefebvre", "Hoppe", "2006", "Bastos", "Celes", "2008", "Alcantara", "et", "al.", "2009", "Pan", "Manocha", "2011", "Garc?a", "et", "al.", "2011", "-rsb-", "could", "further", "reduce", "collision", "allow", "smaller", "hash", "table", "however", "how", "method", "deal", "high", "throughput", "datum", "fusion", "streaming", "unclear", "also", "important", "stress", "we", "simple", "hash", "method", "work", "well", "practice", "handle", "scalability", "quality", "framerate", "40fp", "across", "all", "scene", "assageway", "reconstruction", "Fig.", "we", "show", "quality", "performance", "we", "method", "compare", "previous", "work", "all", "code", "test", "same", "hardware", "-lrb-", "see", "above", "-rrb-", "fixed", "number", "ICP", "iteration", "-lrb-", "15", "-rrb-", "we", "algorithm", "support", "real-time", "streaming", "we", "conduct", "comparison", "similar", "move", "volume", "approach", "first", "we", "compare", "against", "Extended", "fusion", "-lsb-", "Roth", "Vona", "2012", "Whelan", "et", "al.", "2012", "-rsb-", "use", "regular", "uniform", "grid", "include", "streaming", "scale-up", "volumetric", "fusion", "second", "we", "compare", "against", "hierarchical", "fusion", "-lsb-", "Chen", "et", "al.", "2013", "-rsb-", "support", "larger", "move", "volume", "than", "other", "approach", "correspond", "timing", "show", "fig.", "most", "significant", "limitation", "hierarchy", "datum", "structure", "overhead", "cause", "performance", "drop", "particularly", "complex", "scene", "we", "test", "scene", "entire", "hierarchy", "pipeline", "-lrb-", "include", "pose", "estimation", "fusion", "streaming", "-rrb-", "run", "15hz", "which", "lower", "than", "input", "frame", "rate", "note", "measurement", "base", "reference", "implementation", "Chen", "et", "al.", "-lsb-", "2013", "-rsb-", "we", "system", "also", "perform", "favorably", "compare", "streaming", "regular", "grid", "term", "frame-rate", "-lrb-", "label", "Extended", "Fig.", "-rrb-", "we", "attribute", "processing", "empty", "voxel", "regular", "grid", "-lrb-", "particularly", "during", "random", "GPU", "memory", "access", "e.g.", "raycast", "-rrb-", "streaming", "overhead", "further", "show", "fig.", "we", "reconstruction", "quality", "higher", "than", "approach", "quality", "Extended", "fusion", "limit", "small", "spatial", "extent", "move", "volume", "which", "mean", "much", "Kinect", "datum", "out", "range", "integrate", "hierarchical", "fusion", "suffer", "from", "poor", "frame", "rate", "cause", "input", "datum", "skip", "severely", "affect", "pose", "estimation", "quality", "result", "inaccurate", "surface", "integration", "drift", "large-scale", "scene", "type", "drift", "might", "cause", "unnaturally", "twisted", "model", "show", "Fig.", "give", "we", "more", "efficient", "datum", "structure", "which", "run", "faster", "than", "Kinect", "camera", "frame", "rate", "additional", "time", "can", "spend", "improve", "accuracy", "pose", "estimation", "increase", "number", "ICP", "iteration", "we", "find", "we", "result", "encourage", "particularly", "give", "drift", "correction", "explicitly", "handle", "fig.", "10", "scene", "capture", "process", "offline", "use", "method", "-lsb-", "Zhou", "Koltun", "2013", "-rsb-", "which", "use", "multi-pass", "global", "optimization", "mitigate", "drift", "compare", "we", "online", "method", "while", "we", "method", "do", "suffer", "from", "small", "drift", "we", "system", "produce", "comparable", "result", "can", "use", "real-time", "application", "we", "online", "method", "can", "also", "use", "live", "preview", "combine", "approach", "higher-quality", "offline", "reconstruction", "Fig.", "we", "show", "quality", "performance", "we", "method", "compare", "previous", "work", "all", "code", "test", "same", "hardware", "-lrb-", "see", "above", "-rrb-", "fixed", "number", "ICP", "iteration", "-lrb-", "15", "-rrb-", "we", "algorithm", "support", "real-time", "streaming", "we", "conduct", "comparison", "similar", "move", "volume", "approach", "first", "we", "compare", "against", "Extended", "fusion", "-lsb-", "Roth", "Vona", "2012", "Whelan", "et", "al.", "2012", "-rsb-", "use", "regular", "uniform", "grid", "include", "streaming", "scale-up", "volumetric", "fusion", "second", "we", "compare", "against", "hierarchical", "fusion", "-lsb-", "Chen", "et", "al.", "2013", "-rsb-", "support", "larger", "move", "volume", "than", "other", "approach", "correspond", "timing", "show", "fig.", "most", "significant", "limitation", "hierarchy", "datum", "structure", "overhead", "cause", "performance", "drop", "particularly", "complex", "scene", "we", "test", "scene", "entire", "hierarchy", "pipeline", "-lrb-", "include", "pose", "estimation", "fusion", "streaming", "-rrb-", "run", "15hz", "which", "lower", "than", "input", "frame", "rate", "note", "measurement", "base", "reference", "implementation", "Chen", "et", "al.", "-lsb-", "2013", "-rsb-", "we", "system", "also", "perform", "favorably", "compare", "streaming", "regular", "grid", "term", "frame-rate", "-lrb-", "label", "Extended", "Fig.", "-rrb-", "we", "attribute", "processing", "empty", "voxel", "regular", "grid", "-lrb-", "particularly", "during", "random", "GPU", "memory", "access", "e.g.", "raycast", "-rrb-", "streaming", "overhead", "further", "show", "fig.", "we", "reconstruction", "quality", "higher", "than", "approach", "quality", "Extended", "fusion", "limit", "small", "spatial", "extent", "move", "volume", "which", "mean", "much", "Kinect", "datum", "out", "range", "integrate", "hierarchical", "fusion", "suffer", "from", "poor", "frame", "rate", "cause", "input", "datum", "skip", "severely", "affect", "pose", "estimation", "quality", "result", "inaccurate", "surface", "integration", "drift", "large-scale", "scene", "type", "drift", "might", "cause", "unnaturally", "twisted", "model", "show", "Fig.", "give", "we", "more", "efficient", "datum", "structure", "which", "run", "faster", "than", "Kinect", "camera", "frame", "rate", "additional", "time", "can", "spend", "improve", "accuracy", "pose", "estimation", "increase", "number", "ICP", "iteration", "we", "find", "we", "result", "encourage", "particularly", "give", "drift", "correction", "explicitly", "handle", "fig.", "10", "scene", "capture", "process", "offline", "use", "method", "-lsb-", "Zhou", "Koltun", "2013", "-rsb-", "which", "use", "multi-pass", "global", "optimization", "mitigate", "drift", "compare", "we", "online", "method", "while", "we", "method", "do", "suffer", "from", "small", "drift", "we", "system", "produce", "comparable", "result", "can", "use", "real-time", "application", "we", "online", "method", "can", "also", "use", "live", "preview", "combine", "approach", "higher-quality", "offline", "reconstruction", "we", "have", "present", "new", "datum", "structure", "design", "specifically", "online", "reconstruction", "use", "widely-available", "consumer", "depth", "camera", "we", "approach", "leverage", "power", "implicit", "surface", "volumetric", "fusion", "reconstruction", "do", "so", "use", "compact", "spatial", "hash", "scheme", "which", "remove", "both", "overhead", "regular", "grid", "hierarchical", "datum", "structure", "we", "hash", "scheme", "support", "real-time", "performance", "without", "forgo", "scale", "finer", "quality", "reconstruction", "all", "operation", "design", "efficient", "parallel", "graphic", "hardware", "inherent", "unstructured", "nature", "we", "method", "remove", "overhead", "hierarchical", "spatial", "datum", "structure", "capture", "key", "quality", "volumetric", "fusion", "further", "extend", "bound", "reconstruction", "we", "method", "support", "lightweight", "streaming", "without", "major", "datum", "structure", "reorganization", "we", "have", "demonstrate", "performance", "increase", "over", "state-of-theart", "even", "regular", "grid", "implementation", "datum", "structure", "memory", "efficient", "can", "allow", "color", "datum", "directly", "incorporate", "reconstruction", "which", "can", "also", "use", "improve", "robustness", "registration", "due", "high", "performance", "we", "datum", "structure", "available", "time", "budget", "can", "utilize", "further", "improve", "camera", "pose", "estimation", "which", "directly", "improve", "reconstruction", "quality", "over", "exist", "online", "approach", "we", "believe", "advantage", "we", "method", "even", "more", "evident", "when", "future", "depth", "camera", "higher", "resolution", "sense", "emerge", "we", "datum", "structure", "already", "capable", "reconstruct", "surface", "beyond", "resolution", "exist", "depth", "sensor", "kinect" ],
  "content" : "We have implemented our data structure using DirectX 11 Compute Shaders. We use an Asus Xtion for scenes in Fig. 10 and a Kinect for Windows camera for all other scenes, both providing RGB-D data at 30Hz. Results of live scene captures for our test scenes are shown in Figures 1 and 11 as well as supplementary material. We captured a variety of indoor and outdoor scenes under a variety of lighting conditions. While the quality of active infrared sensors is affected significantly in outdoor scenes, our system still manages to reconstruct large-scale outdoor scenes with fine quality. STATUES in Fig. 1 shows the result after an online scan of a ? 20m long corridor in a museum with about 4m high statues, which was captured and reconstructed live in under 5 minutes. PASSAGEWAY ( Fig. 11 top) shows a pathway of shops ? 30m long reconstructed live. QUEENS ( Fig. 11 middle) shows a large courtyard (stretching ? 16m ? 12m ? 2m) reconstructed in approximately 4 minutes. Finally, BOOKSHOP ( Fig. 11 bottom) shows three levels of a bookstore reconstructed in under 6 minutes. These reconstructions demonstrate both scale and quality, and were all reconstructed well above the 30Hz frame rate of the Kinect as shown in Figure 7 . This allows for potential increase of voxel resolution and additional ICP steps for more robust camera tracking. We use a voxel size of 4mm for Fig. 8 , 10 and 10mm for Fig. 1 , 9, 11. We also tested our system with < 2mm voxels without visible improvements in overall reconstruction quality. While this highlights limits of current depth sensing technology, we believe that this opens up new possibilities for future depth acquisition hardware. We measured performance of our entire pipeline including run-time overhead (such as display rendering) on an Intel Core i7 3.4GHz CPU, 16GB of RAM, and a single NVIDIA GeForce GTX Titan. Average timings among all test scenes is 21.8ms (?46f ps) with 8.0ms (37% of the overall pipeline) for ICP pose estimation (15 iterations), 4.6ms (21%) for surface integration, 4.8ms (22%) for surface extraction and shading (including colored phong shading), and 4.4ms (20%) for streaming and input data processing. Separate timings for each test scene are provided in Fig. 7 . Our data structure uses a total of 34MB for the hash table and all auxiliary buffers. This allows a hash table with 2 21 entries, each containing 12 bytes. Our experiments show that a bucket size of two provides best performance leaving us with about 1 million buckets. We pre-allocate 1GB of heap memory to provide space for voxel data on the GPU. With 8 3 voxels per block (8 byte per voxel) this corresponds to 2 18 voxel blocks. Note that 2 21 hash entries only index to 2 18 voxel blocks resulting in a low hash occupancy, thus minimizing hash collisions. On average we found that about 140K voxel blocks are allocated when capturing our test scenes at a voxel size of 8mm (varying with scene complexity). This corresponds to an equal amount of occupied hash entries, resulting in a hash table occupancy with 120K buckets with a single entry, and 10K buckets with two entries. With a bucket size of two and hash table size of 2 21 , all test scenes run with only 0.1% bucket overflow. These are handled by linked lists and across all scenes the largest list length is three. In total ?700 linked list entries are allocated across all scenes, which is negligible compared to the hash table size. On average less than 300MB memory is allocated for surface data (less than 600MB with color). This compares favorably to a regular grid that would require well over 5GB (including color) at the same voxel resolution (8mm) and spatial extent (8m in depth). This also leaves enough space to encode RGB data directly into the stored voxels (see Fig. 11 ). In practice this simple hashing scheme with small bucket size and large hash table size works well. In our scenario we can tolerate larger and sparser (2 21 ) hash table sizes, because the memory footprint of the hash table is insignificant (?34MB) compared to the voxel block buffer (which is pre-allocated to 1GB). Smaller hash table sizes cause higher occupancy and decrease performance. For example, in the STATUES scene our standard settings (2 21 elements) occupies ?6.4% of the hash table and runs at ?21ms, with 200K elements occupancy rises to ?65% and performance is reduced to ?24.8ms, and with 160K elements occupancy rises to ?81% with performance further falling to 25.6ms. In our live system, we chose larger table sizes as we favored performance over the small memory gains. Our pipeline currently uses atomic operations per hash bucket for allocation and streaming. As shown by our timings across all scenes, these sequential operations cause negligible performance overheads, due to hash collisions being minimal. More sophisticated hashing approaches [Lefebvre and Hoppe 2006; Bastos and Celes 2008; Alcantara et al. 2009; Pan and Manocha 2011; Garc?a et al. 2011] could further reduce collisions and allow smaller hash tables. However, how these methods deal with the high throughput of data, fusion and streaming is unclear. It is also important to stress that our simple hashing method works well in practice, handling scalability and quality at framerates >40fps across all scenes. P ASSAGEWAY reconstruction. In Fig. 9 we show the quality and performance of our method compared to previous work. All code was tested on the same hardware (see above) with a fixed number of ICP iterations (15). As our algorithm supports real-time streaming, we conducted comparisons with similar moving volume approaches. First, we compare against Extended Fusion [Roth and Vona 2012; Whelan et al. 2012] that use a regular uniform grid including streaming to scale-up volumetric fusion. Second, we compare against Hierarchical Fusion [Chen et al. 2013] that supports larger moving volumes than other approaches. Corresponding timings are shown in Fig. 7 . The most significant limitation of the hierarchy is the data structure overhead causing a performance drop, particularly in complex scenes. In our test scenes the entire hierarchy pipeline (including pose estimation, fusion, and streaming) runs at ? 15Hz, which is lower than the input frame rate. Note that these measurements are based on the reference implementation by Chen et al. [2013]. Our system also performs favorably compared to streaming regular grids in terms of frame-rate (labeled Extended in Fig. 7 ). We attribute this to processing of empty voxels in the regular grid (particularly during random GPU memory access; e.g., raycasting) and streaming overhead. Further, as shown in Fig. 8 , our reconstruction quality is higher than these approaches. The quality of Extended Fusion is limited by the small spatial extent of the moving volume, which means much of the Kinect data is out of range and not integrated. Hierarchical Fusion suffers from the poor frame rate causing input data to be skipped. This severely affects pose estimation quality resulting in inaccurate surface integration and drift. In large-scale scenes this type of drift might cause unnaturally twisted models as shown in Fig. 9 . Given our more efficient data structure, which runs faster than the Kinect camera frame rate, additional time can be spent improving the accuracy of the pose estimation by increasing the number of ICP iterations. We find our results encouraging, particularly given no drift correction is explicitly handled. In Fig. 10 scenes captured and processed offline using the method of [Zhou and Koltun 2013], which uses a multi-pass global optimization to mitigate drift, are compared to our online method. While our method does suffer from small drifts, our system produces comparable results, and can be used for real-time applications. Our online method can also be used as a live preview, and combined with such approaches for higher-quality offline reconstruction. In Fig. 9 we show the quality and performance of our method compared to previous work. All code was tested on the same hardware (see above) with a fixed number of ICP iterations (15). As our algorithm supports real-time streaming, we conducted comparisons with similar moving volume approaches. First, we compare against Extended Fusion [Roth and Vona 2012; Whelan et al. 2012] that use a regular uniform grid including streaming to scale-up volumetric fusion. Second, we compare against Hierarchical Fusion [Chen et al. 2013] that supports larger moving volumes than other approaches. Corresponding timings are shown in Fig. 7 . The most significant limitation of the hierarchy is the data structure overhead causing a performance drop, particularly in complex scenes. In our test scenes the entire hierarchy pipeline (including pose estimation, fusion, and streaming) runs at ? 15Hz, which is lower than the input frame rate. Note that these measurements are based on the reference implementation by Chen et al. [2013]. Our system also performs favorably compared to streaming regular grids in terms of frame-rate (labeled Extended in Fig. 7 ). We attribute this to processing of empty voxels in the regular grid (particularly during random GPU memory access; e.g., raycasting) and streaming overhead. Further, as shown in Fig. 8 , our reconstruction quality is higher than these approaches. The quality of Extended Fusion is limited by the small spatial extent of the moving volume, which means much of the Kinect data is out of range and not integrated. Hierarchical Fusion suffers from the poor frame rate causing input data to be skipped. This severely affects pose estimation quality resulting in inaccurate surface integration and drift. In large-scale scenes this type of drift might cause unnaturally twisted models as shown in Fig. 9 . Given our more efficient data structure, which runs faster than the Kinect camera frame rate, additional time can be spent improving the accuracy of the pose estimation by increasing the number of ICP iterations. We find our results encouraging, particularly given no drift correction is explicitly handled. In Fig. 10 scenes captured and processed offline using the method of [Zhou and Koltun 2013], which uses a multi-pass global optimization to mitigate drift, are compared to our online method. While our method does suffer from small drifts, our system produces comparable results, and can be used for real-time applications. Our online method can also be used as a live preview, and combined with such approaches for higher-quality offline reconstruction. We have presented a new data structure designed specifically for online reconstruction using widely-available consumer depth cameras. Our approach leverages the power of implicit surfaces and volumetric fusion for reconstruction, but does so using a compact spatial hashing scheme, which removes both the overhead of regular grids and hierarchical data structures. Our hashing scheme supports real-time performance without forgoing scale or finer quality reconstruction. All operations are designed to be efficient for parallel graphics hardware. The inherent unstructured nature of our method removes the overhead of hierarchical spatial data structures, but captures the key qualities of volumetric fusion. To further extend the bounds of reconstruction, our method supports lightweight streaming without major data structure reorganization. We have demonstrated performance increases over the state-of-theart, even regular grid implementations. The data structure is memory  efficient and can allow color data to be directly incorporated in the reconstruction, which can also be used to improve the robustness of registration. Due to the high performance of our data structure, the available time budget can be utilized for further improving camera pose estimation, which directly improves reconstruction quality over existing online approaches. We believe the advantages of our method will be even more evident when future depth cameras with higher resolution sensing emerge, as our data structure is already capable of reconstructing surfaces beyond the resolution of existing depth sensors such as Kinect.",
  "resources" : [ ]
}