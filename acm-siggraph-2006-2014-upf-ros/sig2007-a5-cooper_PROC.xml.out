{
  "uri" : "sig2007-a5-cooper_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2007/a5-cooper_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Active Learning for Real-Time Motion Controllers",
    "published" : "2007",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Seth-Cooper",
      "name" : "Seth",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Aaron-Hertzmann",
      "name" : "Aaron",
      "surname" : "Hertzmann"
    }, {
      "uri" : "http://drinventor/Zoran-Popovic",
      "name" : "Zoran",
      "surname" : "Popovic"
    } ]
  },
  "bagOfWords" : [ "example", "interpolate", "concatenate", "motion", "realistic", "new", "animation", "can", "generate", "real-time", "response", "user", "control", "other", "input", "even", "moderate", "set", "task", "lead", "huge", "set", "initial", "configuration", "control", "vector", "however", "even", "small", "controller", "space", "nonlinearity", "space", "mean", "careful", "sampling", "require", "after", "each", "motion", "capture", "system", "automatically", "identify", "specific", "task", "controller", "perform", "poorly", "base", "suite", "error", "metric", "user", "choose", "one", "task", "perform", "controller", "update", "new", "motion", "clip", "way", "system", "continue", "refine", "controller", "until", "capable", "perform", "any", "task", "from", "any", "state", "any", "control", "vector", "time", "available", "capture", "have", "elapse", "else", "number", "datum", "sample", "have", "be", "reach", "predefined", "maximum", "we", "demonstrate", "feasibility", "approach", "use", "we", "system", "construct", "three", "example", "controller", "we", "validate", "active", "learning", "approach", "compare", "one", "controller", "manually-constructed", "controller", "make", "decision", "normally", "entail", "selecting", "from", "among", "few", "alternative", "present", "system", "contribution", "worth", "note", "many", "method", "typically", "require", "large", "motion", "database", "need", "database", "could", "mitigate", "we", "active", "learning", "approach", "few", "author", "have", "describe", "method", "generate", "new", "pose", "response", "real-time", "input", "we", "motion", "controller", "model", "most", "similar", "method", "transition", "between", "interpolated", "sequence", "Shin", "oh", "-lsb-", "2006", "-rsb-", "perform", "interpolation", "graph", "edge", "simple", "model", "locomotion", "other", "repetitive", "motion", "paper", "we", "show", "how", "use", "adaptive", "selection", "motion", "sequence", "allow", "creation", "controller", "greater", "complexity", "while", "allow", "fine-scale", "parameterize", "control", "capture", "relatively", "few", "motion", "overall", "consequently", "automatic", "selection", "test", "case", "have", "be", "extensively", "study", "simplest", "method", "determine", "all", "test", "point", "advance", "e.g.", "via", "space-filling", "function", "optimize", "objective", "function", "we", "approach", "distinct", "from", "exist", "active", "learning", "method", "two", "way", "first", "instead", "choose", "next", "sample", "capture", "we", "system", "identify", "set", "candidate", "from", "which", "user", "choose", "sample", "improve", "second", "we", "assume", "metric", "correctness", "provide", "which", "candidate", "may", "choose", "before", "describe", "we", "active", "learning", "framework", "which", "main", "contribution", "paper", "we", "briefly", "discuss", "we", "model", "motion", "controller", "we", "framework", "kinematic", "controller", "generate", "motion", "clip", "character", "state", "perform", "task", "parameterize", "control", "vector", "U.", "define", "set", "all", "permissible", "character", "state", "discrete", "set", "controller", "task", "define", "operational", "range", "control", "input", "task", "define", "space", "output", "motion", "we", "represent", "motion", "clip", "sequence", "pose", "we", "represent", "state", "character", "vector", "contain", "pose", "next", "ten", "frame", "currentlyplay", "clip", "use", "multiple", "frame", "state", "facilitate", "find", "smooth", "motion", "blend", "we", "consider", "data-driven", "controller", "which", "create", "new", "clip", "interpolate", "example", "motion", "capture", "clip", "-lcb-", "-rcb-", "associate", "task", "parameter", "-lcb-", "-rcb-", "however", "all", "sample", "clip", "can", "blended", "together", "meaningful", "way", "consider", "tennis", "controller", "would", "make", "sense", "blend", "forehand", "backhand", "stroke", "motion", "sample", "reason", "each", "controller", "consist", "group", "blendable", "clip", "which", "we", "refer", "cluster", "each", "cluster", "contain", "set", "blendable", "motion", "sample", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "share", "common", "state", "continuous", "blend", "map", "-lrb-", "-rrb-", "which", "produce", "motion", "blend", "weight", "w.", "First", "we", "find", "t-specific", "cluster", "closest", "more", "than", "one", "we", "use", "one", "which", "have", "closest", "u.", "second", "output", "motion", "generate", "compute", "blend", "weight", "-lrb-", "-rrb-", "interpolate", "weight", "new", "clip", "interpolation", "perform", "time-aligned", "translationand", "rotation-invariant", "manner", "similar", "-lsb-", "Kovar", "Gleicher", "2003", "-rsb-", "new", "motion", "blended", "currently-playing", "motion", "however", "current", "character", "state", "very", "dissimilar", "beginning", "new", "controller", "motion", "ignore", "current", "motion", "continue", "until", "subsequent", "state", "produce", "successful", "controller", "motion", "function", "ik", "transform", "motion", "satisfy", "kinematic", "constraint", "define", "joint", "angle", "relevant", "end-effector", "-lrb-", "i.e.", "shoulder", "elbow", "hand", "-rrb-", "first", "optimize", "satisfy", "kinematic", "constraint", "time", "instant", "use", "ik", "enable", "controller", "achieve", "much", "greater", "variety", "motion", "from", "few", "sample", "thus", "enable", "active", "learning", "use", "significantly", "fewer", "sample", "overall", "general", "more", "powerful", "controller", "fewer", "sample", "active", "learning", "require", "full", "coverage", "controller", "new", "clip", "usually", "play", "from", "beginning", "particular", "occur", "when", "new", "motion", "start", "from", "same", "state", "current", "motion", "would", "generate", "same", "cluster", "imply", "new", "motion", "would", "generate", "blend", "same", "example", "current", "motion", "case", "new", "motion", "can", "start", "time", "correspond", "current", "time", "index", "current", "clip", "since", "all", "motion", "belong", "same", "cluster", "can", "blended", "any", "portion", "clip", "cluster", "motion-blend", "model", "use", "we", "experiment", "motivate", "similar", "model", "use", "game", "industry", "real", "game", "controller", "specific", "cluster", "sample", "within", "each", "cluster", "all", "determine", "ad-hoc", "manual", "process", "manual", "process", "invariably", "produce", "suboptimal", "controller", "multiple", "trip", "motion", "capture", "lab", "we", "point", "out", "active", "learning", "framework", "introduce", "follow", "section", "do", "depend", "specific", "detail", "controller", "specification", "addition", "datum", "sample", "need", "capture", "can", "easily", "come", "from", "alternate", "source", "animator-created", "motion", "clip", "key", "assumption", "we", "input", "space", "controller", "define", "union", "taskspecific", "control", "space", "Task", "Task", "-lrb-", "-rrb-", "approach", "motion", "controller", "do", "need", "able", "from", "any", "initial", "state", "only", "from", "state", "might", "generate", "controller", "hence", "we", "only", "consider", "possible", "start", "pose", "might", "arise", "from", "another", "clip", "generate", "controller", "order", "seed", "active", "learning", "user", "provide", "some", "initial", "state", "active", "learning", "process", "proceed", "follow", "outer", "loop", "system", "identify", "set", "controller", "input", "-lrb-", "-rrb-", "motion", "controller", "can", "handle", "well", "user", "select", "one", "improve", "system", "generate", "improve", "pseudoexample", "motion", "task", "pseudoexample", "approve", "user", "motion", "controller", "update", "step", "can", "often", "save", "user", "from", "have", "perform", "motion", "pseudoexample", "reject", "user", "perform", "task", "new", "motion", "clip", "use", "update", "motion", "controller", "we", "have", "set", "up", "we", "system", "motion", "capture", "lab", "system?s", "display", "project", "wall", "lab", "user", "see", "goal", "step", "identify", "control", "problem", "motion", "controller", "can", "handle", "well", "candidate", "problem", "fully", "specify", "state", "task", "control", "vector", "u.", "because", "evaluation", "motion", "can", "difficult", "do", "purely", "numerically", "we", "do", "have", "explicit", "mathematical", "function", "can", "precisely", "identify", "which", "motion", "most", "need", "improvement", "instead", "we", "find", "multiple", "candidate", "accord", "different", "motion", "metric", "each", "which", "provide", "different", "way", "evaluate", "motion", "generate", "controller", "we", "let", "user", "determine", "candidate", "improve", "we", "use", "follow", "metric", "Task", "error", "each", "task", "user", "specify", "one", "more", "metric", "measure", "how", "well", "motion", "perform", "task", "input", "u.", "example", "task", "require", "character?s", "hand", "position", "specific", "time", "metric", "might", "measure", "euclidean", "distance", "between", "character?s", "hand", "time", "address", "we", "compute", "task", "error", "difference", "task-performing", "metric", "when", "compare", "nearest", "exist", "example", "sum", "task", "error", "when", "more", "than", "one", "task", "metric", "provide", "user", "weighted", "sum", "metric", "also", "use", "separate", "metric", "transition", "error", "Distance", "from", "previous", "example", "order", "encourage", "exploration", "control", "space", "we", "use", "metric", "measure", "distance", "control", "vector", "from", "nearest", "exist", "example", "control", "vector", "find", "worst-performing", "region", "controller", "input", "space", "we", "need", "sample", "very", "large", "controller", "input", "space", "we", "reduce", "amount", "sample", "require", "use", "different", "sampling", "process", "different", "motion", "metric", "generate", "candidate", "use", "each", "task", "error", "metric", "we", "search", "motion", "perform", "task", "poorly", "each", "cluster", "state", "random", "control", "vector", "generate", "its", "task", "uniform", "random", "sampling", "we", "keep", "u?s", "which", "give", "worst", "result", "each", "motion", "metric", "each", "point", "refine", "Nelder-Mead", "search", "-lsb-", "1965", "-rsb-", "maximize", "badness", "respective", "metric", "Distance", "metric", "candidate", "generate", "similarly", "replace", "task", "error", "nearest-neighbor", "distance", "we", "use", "optimize", "along", "which", "generate", "they", "candidate", "generate", "candidate", "use", "transition", "error", "we", "search", "state", "blend", "poorly", "exist", "cluster", "average", "motion", "clip", "generate", "each", "cluster", "average", "example", "cluster", "we", "find", "state", "motion", "furthest", "from", "any", "exist", "cluster?s", "state", "we", "use", "state", "along", "set", "center", "domain", "candidate", "once", "all", "candidate", "have", "be", "generate", "user", "show", "candidate", "each", "metric", "sort", "score", "along", "information", "about", "they", "error", "corresponding", "user", "choose", "one", "candidate", "improve", "provide", "user", "several", "option", "have", "several", "important", "purpose", "first", "user", "able", "qualitatively", "judge", "which", "sample", "would", "best", "improve", "second", "user", "able", "have", "some", "control", "over", "order", "sample", "collect", "example", "can", "more", "convenient", "user", "collect", "several", "consecutive", "sample", "from", "same", "state", "more", "sample", "provide", "quality", "candidate", "improve", "once", "all", "candidate", "consider", "acceptable", "user", "have", "some", "confidence", "controller", "finish", "we", "experience", "although", "many", "candidate", "generate", "user", "usually", "find", "one", "worth", "refining", "after", "watch", "only", "few", "example", "before", "proceeding", "we", "discuss", "detail", "we", "implementation", "per-cluster", "blend", "function", "we", "implement", "blend", "function", "use", "scheme", "similar", "radial", "basis", "function", "-lrb-", "rbf", "-rrb-", "base", "method", "propose", "Rose", "et", "al.", "-lsb-", "1998", "2001", "-rsb-", "we", "use", "different", "form", "linear", "basis", "replace", "residual", "basis", "basis", "non-uniform", "along", "each", "dimension", "we", "believe", "other", "non-isotropic", "representation", "could", "perform", "equally", "well", "blend", "function", "we", "initialize", "all", "element", "distance", "nearest", "neighbor", "solve", "Rose", "et", "al.", "we", "update", "each", "perform", "small", "optimization", "across", "each", "dimension", "we", "place", "number", "evaluation", "sample", "along", "dimension", "neighborhood", "regularly", "sample", "value", "range", "-lsb-", "-rsb-", "we", "scale", "solve", "evaluate", "sum", "task", "error", "metric", "evaluation", "sample", "best", "scale", "keep", "we", "update", "scale", "solve", "one", "last", "time", "pseudoexample", "new", "training", "motion", "define", "linear", "combination", "exist", "motion", "capable", "reshape", "without", "require", "additional", "motion", "sample", "get", "initial", "guess", "correct", "cluster", "weight", "pseudoexample", "we", "iterate", "over", "all", "cluster", "give", "all", "motion", "cluster", "well", "currently", "predict", "motion", "u.", "all", "motion", "one", "which", "perform", "best", "task", "error", "metric", "select", "specifically", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3x", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "otherwise", "provide", "cluster", "weight", "pseudoexample", "we", "optimize", "weight", "within", "cluster", "accord", "sum", "task", "error", "metric", "use", "nelder-mead", "-lsb-", "1965", "-rsb-", "result", "weight", "together", "control", "vector", "constitute", "pseudoexample", "-lrb-", "-rrb-", "clip", "generate", "weight", "its", "evaluate", "task", "error", "show", "user", "user", "approve", "pseudoexample", "permanently", "add", "cluster?s", "rbf", "otherwise", "discard", "user", "present", "appropriate", "start", "state", "taskspecific", "visualization", "-lrb-", "e.g.", "show", "trajectory", "initial", "position", "ball", "catch", "-rrb-", "since", "motion", "determine", "relative", "character", "visualization", "translate", "rotate", "user?s", "movement", "prior", "begin", "capture", "user", "move", "eventually", "get", "-lrb-", "near", "-rrb-", "start", "state", "we", "system", "automatically", "determine", "when", "initial", "state", "condition", "meet", "record", "rest", "capture", "motion", "aspect", "interface", "significantly", "simplify", "task", "perform", "required", "motion", "sample", "once", "capture", "user", "can", "review", "motion", "its", "task", "error", "motion", "satisfactory", "user", "must", "repeat", "motion", "we", "experiment", "vast", "majority", "task", "could", "perform", "three", "fewer", "trial", "we", "find", "difficult", "perform", "task", "typically", "require", "only", "few", "more", "trial", "usually", "more", "than", "seven", "total", "although", "few", "rare", "case", "require", "up", "twelve", "total", "determine", "where", "desire", "motion", "sample", "end", "we", "seek", "local", "minimum", "state", "distance", "capture", "clip", "compare", "against", "beginning", "end", "exist", "cluster", "well", "consider", "actual", "end", "recorded", "motion", "we", "sort", "distance", "let", "user", "select", "appropriate", "ending", "typically", "involve", "nothing", "more", "than", "user", "confirm", "first", "choice", "system", "now", "need", "determine", "appropriate", "cluster", "new", "motion", "motion", "timewarp", "all", "cluster", "similar", "end", "state", "alignment", "result", "show", "user", "sort", "increase", "order", "alignment", "error", "user", "can", "select", "which", "cluster", "assign", "clip", "we", "experiment", "first", "choice", "frequently", "correct", "user", "decide", "cluster", "appropriate", "new", "cluster", "create", "approach", "remove", "need", "manually", "determine", "cluster", "any", "point", "during", "controller", "synthesis", "cluster", "create", "only", "when", "usersupplied", "sample", "sufficiently", "different", "from", "all", "exist", "cluster", "cluster", "update", "new", "clip", "new", "-lrb-", "-rrb-", "clip", "add", "cluster?s", "rbf", "active", "learning", "process", "repeat", "select", "new", "candidate", "task", "active", "learning", "process", "can", "have", "many", "termination", "criterion", "include", "time", "spend", "lab", "number", "sample", "allot", "controller", "well", "overall", "controller", "quality", "controllability", "user", "can", "estimate", "measure", "quality", "controllability", "current", "controller", "evaluate", "quality", "candidate", "task", "all", "poor-performing", "candidate", "appear", "high-quality", "controller", "have", "probably", "reach", "point", "where", "additional", "sample", "can", "significantly", "improve", "quality", "controller", "coverage", "we", "have", "construct", "three", "example", "demonstrate", "we", "framework", "controller", "demonstrate", "accompany", "video", "each", "controller", "construct", "interactively", "short", "time", "motion", "capture", "lab", "we", "capture", "synthesize", "motion", "between", "20", "25", "frame", "per", "second", "computation", "time", "require", "negligible", "each", "active", "learning", "step", "take", "between", "30", "seconds", "depend", "size", "controller", "involve", "majority", "time", "spend", "capture", "especially", "since", "normally", "take", "two", "three", "try", "perform", "motion", "correct", "state", "match", "task", "we", "first", "example", "parameterized", "walk", "control", "space", "three-dimensional", "control", "stride", "length", "-lrb-", "distance", "between", "heel", "maximum", "spacing", "range", "from", "0.2", "0.9", "meter", "-rrb-", "speed", "-lrb-", "range", "from", "15", "25", "centimeter", "per", "frame", "-rrb-", "turn", "angle", "-lrb-", "change", "horizontal", "orientation", "per", "stride", "range", "from", "??", "radian", "-rrb-", "one", "task", "error", "metric", "each", "feature", "-lrb-", "e.g.", "stride", "length", "-rrb-", "measure", "square", "distance", "between", "desire", "feature", "maximum", "feature", "motion", "order", "determine", "stride", "metric", "assume", "sample", "contain", "complete", "walk", "cycle", "beginning", "right", "foot", "forward", "task", "allow", "motion", "from", "middle", "controller", "task", "produce", "only", "cluster", "use", "12", "example", "total", "843", "frame", "10", "pseudoexample", "we", "limit", "lab", "time", "roughly", "half", "hour", "include", "all", "active", "learning", "user", "interaction", "controller", "achieve", "89", "coverage", "-lrb-", "discuss", "section", "-rrb-", "we", "second", "example", "combine", "parameterized", "walk", "projectile", "dodging", "example", "have", "two", "task", "when", "nothing", "dodge", "control", "space", "one-dimensional", "control", "turn", "angle", "when", "walk", "dodge", "control", "space", "four-dimensional", "control", "turn", "angle", "well", "incoming", "height", "incoming", "angle", "distance", "front", "character", "incoming", "projectile", "walk", "turn", "parameter", "ratio", "radian", "per", "meter", "travel", "range", "from", "??", "task", "metric", "measure", "absolute", "value", "difference", "between", "ratio", "target", "value", "walk", "task", "allow", "motion", "from", "middle", "parameter", "incoming", "projectile", "specify", "relative", "character?s", "local", "coordinate", "system", "parameter", "specify", "trajectory", "projectile", "error", "metric", "inverse", "distance", "between", "projectile", "any", "point", "character", "dodge", "task", "also", "include", "same", "turn", "control", "parameter", "use", "walk", "same", "error", "metric", "well", "synthesize", "controller", "contain", "10", "cluster", "use", "total", "30", "example", "total", "1121", "frame", "19", "pseudoexample", "we", "limit", "lab", "time", "roughly", "hour", "achieve", "76", "coverage", "dodge", "task", "we", "third", "example", "catch", "ball", "example", "have", "two", "task", "when", "catch", "zero-dimensional", "stand", "task", "when", "catch", "control", "space", "three-dimensional", "control", "incoming", "position", "plane", "front", "character", "well", "speed", "parameter", "incoming", "ball", "specify", "relative", "character?s", "local", "coordinate", "system", "parameter", "specify", "trajectory", "ball", "task", "error", "metric", "square", "distance", "between", "ball", "character?s", "right", "hand", "also", "example", "we", "use", "ik", "controller", "so", "we", "have", "can", "have", "greater", "reachability", "result", "motion", "result", "controller", "use", "12", "cluster", "use", "33", "example", "total", "1826", "frame", "23", "pseudoexample", "datum", "include", "diving", "motion", "capture", "lab", "session", "limit", "roughly", "hour", "we", "achieve", "57", "coverage", "catch", "task", "normalize", "manual", "controller", "discuss", "section", "worth", "note", "we", "also", "try", "create", "controller", "simpler", "cluster", "model", "do", "include", "IK", "find", "performance", "significantly", "poorer", "due", "nonlinearity", "underlie", "control", "space", "we", "have", "perform", "numerical", "evaluation", "active", "learning", "method", "propose", "use", "catch", "controller", "we", "compare", "active", "learning", "approach", "manual", "sample", "selection", "we", "have", "have", "four", "animation", "researcher", "-lrb-", "two", "associate", "project", "two", "-rrb-", "plan", "motion", "capture", "session", "catch", "controller", "like", "one", "we", "result", "all", "manual", "design", "require", "more", "sample", "than", "learn", "controller", "produce", "controller", "visually", "quantitatively", "be", "significantly", "inferior", "we", "learn", "controller", "Table", "we", "show", "number", "sample", "percentage", "input", "space", "coverage", "each", "controller", "comparison", "we", "only", "consider", "catch", "task", "ignore", "stand", "task", "coverage", "define", "percentage", "input", "which", "controller", "successfully", "match", "state", "complete", "task", "-lrb-", "catch", "ball", "mean", "hand", "within", "small", "distance", "ball", "-rrb-", "percentage", "compute", "randomly", "sampling", "state", "task", "similar", "scheme", "section", "4.1", "comparison", "we", "look", "percentage", "sample", "controllable", "give", "controller", "out", "sample", "controllable", "any", "controller", "remove", "sample", "physically", "impossible", "catch", "manual", "controller", "roughly", "80", "more", "sample", "produce", "significantly", "less", "coverage", "while", "controller", "20", "more", "sample", "cover", "roughly", "half", "space", "learn", "controller", "total", "mocap", "studio", "time", "roughly", "same", "course", "coverage", "perfect", "measure", "controller", "since", "do", "measure", "realism", "motion", "difference", "quality", "controller", "also", "apparent", "accompany", "video", "manual", "controller", "ball", "often", "catch", "very", "strange", "unrealistic", "way", "course", "evaluation", "depend", "skill", "level", "people", "who", "choose", "motion", "dive", "catch", "require", "significant", "motion", "cleanup", "also", "painful", "perform", "order", "facilitate", "prototyping", "testing", "we", "capture", "several", "representative", "dive", "advance", "when", "active", "learning", "system", "request", "dive", "one", "pre-captured", "motion", "provide", "-lrb-", "appropriate", "-rrb-", "instead", "perform", "new", "dive", "we", "also", "show", "chart", "demonstrate", "improvement", "controller", "each", "sample", "add", "Figure", "we", "use", "same", "measure", "coverage", "above", "we", "have", "introduce", "active", "learning", "framework", "create", "realtime", "motion", "controller", "adaptively", "determine", "which", "motion", "add", "model", "system", "create", "finely-controllable", "motion", "model", "reduce", "number", "datum", "clip", "little", "time", "spend", "motion", "capture", "studio", "addition", "always", "present", "worst-performing", "state", "sample", "user", "have", "continuous", "gauge", "quality", "result", "controller", "although", "we", "system", "focus", "one", "specific", "model", "motion", "synthesis", "we", "believe", "general", "approach", "adaptive", "modelbuilding", "useful", "many", "type", "animation", "model", "any", "situation", "which", "minimize", "number", "motion", "sample", "important", "can", "potentially", "benefit", "from", "active", "learning", "we", "have", "design", "system", "fast", "flexible", "so", "relatively", "little", "time", "spend", "wait", "next", "candidate", "select", "hence", "we", "employ", "number", "heuristic", "incremental", "learning", "step", "we", "model", "generally", "optimal", "global", "sense", "we", "system", "do", "make", "formal", "guarantee", "be", "able", "perform", "every", "task", "from", "every", "state", "some", "task", "may", "impossible", "-lrb-", "e.g.", "start", "new", "task", "from", "midair", "-rrb-", "other", "may", "have", "be", "capture", "limited", "time", "available", "motion", "capture", "session", "tradeoff", "between", "lab", "time", "number", "sample", "coverage", "user", "must", "evaluate", "we", "example", "we", "show", "possible", "get", "high", "amount", "coverage", "low", "number", "sample", "short", "lab", "time", "some", "difficulty", "arise", "during", "capture", "process", "necessary", "user", "look", "screen", "well", "mentally", "project", "motion", "onto", "on-screen", "visualization", "we", "believe", "difficulty", "can", "overcome", "through", "use", "augmented", "virtual", "reality", "system", "we", "have", "guarantee", "scalability", "we", "believe", "system", "scale", "well", "handle", "many", "different", "task", "perform", "sequentially", "create", "highly", "flexible", "character", "however", "we", "controller", "example", "do", "fully", "test", "effectiveness", "we", "approach", "very", "high", "dimension", "work", "certainly", "do", "solve", "fundamental", "problem", "curse", "dimensionality", "data-driven", "controller", "we", "believe", "active", "learning", "generalize", "controller", "higher", "dimension", "however", "controller", "10", "more", "task", "input", "together", "37", "dof", "character", "state", "space", "even", "though", "active", "learning", "drastically", "reduce", "number", "sample", "number", "require", "sample", "would", "still", "impractical", "very", "high", "dimension", "become", "imperative", "use", "sophisticated", "motion", "model", "accurately", "represent", "nonlinear", "dynamics", "since", "expressive", "power", "model", "greatly", "reduce", "number", "require", "sample", "author", "would", "like", "thank", "Gary", "Yngve", "he", "help", "video", "anonymous", "reviewer", "comment", "work", "support", "UW", "Animation", "Research", "Labs", "NSF", "grant", "ccr-0092970", "eia-0321235", "electronic", "art", "Sony", "Microsoft", "Research", "NSERC", "CFI", "Alfred", "P.", "Sloan", "Foundation", "m.", "S.", "D.", "2004", "selective", "sampling", "nearest", "neighbor", "classifier", "54", "125", "152", "iu", "C.", "K.", "opovus", "Z.", "2002", "synthesis", "complex", "Dynamic", "Character", "Motion", "from", "simple", "animation", "ACM", "Trans", "Graphics", "21", "-lrb-", "July", "-rrb-", "408", "416", "iu", "C.", "K.", "ERTZMANN", "a.", "opovus", "Z.", "2005", "Learning", "Physics-Based", "Motion", "style", "nonlinear", "inverse", "optimization", "ACM", "transaction", "Graphics", "24", "-lrb-", "Aug.", "-rrb-", "1071", "1081", "S.", "2005", "geostatistical", "motion", "-lrb-", "Aug.", "-rrb-", "1062", "1070", "R.", "1965", "simplex", "method", "308", "313", "ark", "S.", "I.", "hin", "H.", "J.", "IM", "T.", "H.", "HIN", "S.", "Y.", "2004", "on-line", "motion", "blend", "real-time", "locomotion", "generation", "virtual", "world", "15", "125", "138", "evaluate", "C.", "M.", "F.", "B.", "1998", "verb", "adverb", "multidimensional", "Motion", "Interpolation", "IEEE", "Computer", "Graphics", "application", "18", "32", "40", "ose", "iii", "C.", "F.", "LOAN", "p.-p", "J.", "OHEN", "M.", "F.", "2001", "artist-directed", "inverse-kinematic", "use", "radial", "basis", "function", "interpolation", "Computer", "Graphics", "Forum", "20", "239", "250", "T.", "J.", "B.", "J.", "W.", "I.", "2003", "Springer", "hin", "H.", "J.", "H.", "S.", "2006", "Fat", "Graphs", "construct", "interactive", "character", "continuous", "control", "III", "C.", "F.", "M.", "F.", "2001", "orresanus", "L.", "ACKNEY", "P.", "REGLER", "C.", "2007", "Learning", "Motion", "Style", "synthesis", "from", "Perceptual", "observation", "ILEY", "D.", "J.", "ahn", "J.", "K.", "1997", "interpolation", "synthesis", "articulate", "figure", "motion", "IEEE", "Computer", "Graphics", "application", "17", "-lrb-", "Nov.", "Dec.", "-rrb-", "39", "45", "amane", "K.", "UFFNER", "J.", "J.", "odgin", "J.", "K.", "2004", "synthesize", "animation", "human", "manipulation", "task", "ACM", "Trans", "Graphics", "23", "-lrb-", "Aug.", "-rrb-", "532", "539", "ORDAN", "V.", "B.", "AJKOWSKA", "A.", "HIU", "B.", "AST", "M.", "2005", "Dynamic", "Response", "Motion", "Capture", "Animation", "ACM", "transaction", "Graphics", "24", "-lrb-", "Aug.", "-rrb-" ],
  "content" : "For example, by interpolating and concatenating motions, realistic new animations can be generated in real-time in response to user control and other inputs. Even this moderate set of tasks leads to a huge set of initial configurations and control vectors. However, even in a small controller space, nonlinearities in the space will mean that careful sampling is required. After each motion is captured, the system automatically identifies specific tasks that the controller performs poorly, based on a suite of error metrics. The user chooses one of the tasks to perform, and the controller is updated with this new motion clip. In this way, the system continues to refine the controller until it is capable of performing any of the tasks from any state with any control vector, the time available for capture has elapsed, or else the number of data samples has been reached a predefined maximum. We demonstrate the feasibility of this approach by using our system to construct three example controllers. We validate the active learning approach by comparing one of these controllers to manually-constructed controllers. Making these decisions normally entails selecting from among a few alternatives  presented by the system. Contributions. It is worth noting that many of these methods typically require large motion databases; the need for such databases could be mitigated by our active learning approach. A few authors have described methods that generate new poses in response to real-time input. Our motion controller model is most similar to methods that transition between interpolated sequences. Shin and Oh [2006] perform interpolation on graph edges for simple models of locomotion and other repetitive motions. In this paper, we show how the use of adaptive selection of motion sequences allows the creation of controllers with greater complexity, while allowing fine-scale parameterized control and capturing relatively few motions overall. Consequently, automatic selection of test cases has been extensively studied. The simplest methods determine all test points in advance, e.g., via space-filling functions, or by optimizing an objective function. Our approach is distinct from existing active learning methods in two ways: first, instead of choosing the next sample to capture, our system identifies a set of candidates, from which a user chooses a sample to improve; second, we assume that a metric of correctness is provided by which candidates may be chosen. Before describing our active learning framework ? which is the main contribution of this paper ? we will briefly discuss our model for motion controllers. In our framework, a kinematic controller C : S ? T ? U ? M generates a motion clip m ? M that starts at character state s ? S and performs task t ? T parameterized by the control vector u ? U. S defines a set of all permissible character states, T is a discrete set of controller tasks, U defines the operational range of the control inputs for task t, and M defines the space of output motions. We represent a motion clip m ? M as a sequence of poses. We represent the state s of the character as a vector containing the poses in the next ten frames of the currentlyplaying clip. The use of multiple frames for state facilitates finding smooth motion blends. We consider data-driven controllers which create a new clip m by interpolating example motion capture clips {m i } associated with task parameters {u i }. However, not all sampled clips can be blended together in a meaningful way. Consider a tennis controller: it would not make sense to blend forehand and backhand stroke motion samples. For this reason, each controller consists of groups of blendable clips which we refer to as ?clusters. ? Each cluster C j contains a set of blendable motion samples {(m k, j , u k, j )} that share a common start state s j , and a continuous blend map w = b j (u) which produces motion blend weights w. First, we find the t-specific clusters with s j closest to s; if there is more than one, we use the one which has a u k, j closest to u. Second, the output motion is generated by computing the blend weights w = b j (u), and then interpolating with these weights: the new clip is ? k w k m k, j . Interpolation is performed in a time-aligned, translationand rotation-invariant manner similar to [Kovar and Gleicher 2003]. The new motion is then blended in with the currently-playing motion. However, if the current character state is very dissimilar to the beginning of m, then the new controller motion is ignored, and the current motion continues until the subsequent states s produce successful controller motion. The function IK transforms the motion to satisfy kinematic constraints defined The joint angles for the relevant end-effector (i.e., shoulder and elbow for the hand) are first optimized to satisfy the kinematic constraint in that time instant. The use of IK enables the controller to achieve a much greater variety of motions from a few samples and thus enables the active learning to use significantly fewer samples overall. In general, the more powerful the controller, the fewer samples active learning requires for full coverage of the controller. A new clip m is usually played from the beginning. In particular, this occurs when the new motion, if started from the same state as the current motion, would be generated by the same cluster. This implies that the new motion would be generated by blending the same examples as the current motion. In this case, the new motion can be started at the time corresponding to the current time index of the current clip, since all motions belonging to the same cluster can be blended at any portion of the clip. The clustered motion-blend model used in our experiments was motivated by similar models used in the game industry. In real game controllers, specific clusters and the samples within each clusters are all determined in an ad-hoc manual process. This manual process invariably produces suboptimal controllers and multiple trips to the motion capture lab. We point out that the active learning framework introduced in the following sections does not depend on the specific details of the controller specification. In addition, the data samples need not be captured; they can easily come from alternate sources, such as animator-created motion clips. A key assumption of our\n        1 The input space U for the controller is defined as a union of the taskspecific control spaces: U = U k . k\n        Task 1 Task 2 s 1 s 1 s 2 s 2 (b) approach is that the motion controller does not need to be able to start from any initial state s, but only from states that might be generated by the controller. Hence, we only consider possible starting poses that might arise from another clip generated by the controller. In order to ?seed? the active learning, the user provides some initial state s. The active learning process then proceeds in the following outer loop: 1. The system identifies a set of controller inputs (s i , t i , u i ) that the motion controller cannot handle well. The user selects one to be improved. The system generates an improved ?pseudoexample? motion for this task. If the pseudoexample is approved by the user, the motion controller is updated. This step can often save the user from having to perform the motion. If the pseudoexample is rejected, the user performs the task, and the new motion clip is used to update the motion controller. We have set up our system in a motion capture lab. The system?s display is projected on the wall of the lab for the user to see. The goal of this step is to identify control problems that the motion controller cannot handle well. A candidate problem is fully specified by a start state s, a task t, and control vector u. Because the evaluation of motions can be difficult to do purely numerically, we do not have an explicit mathematical function that can precisely identify which motions are most in need of improvement. Instead, we find multiple candidates according to different motion metrics, each of which provides a different way of evaluating the motion generated by a controller, and we let the user determine the candidate to improve. We use the following metrics:  ? Task error. For each task, the user specifies one or more metrics to measure how well a motion m performs task t with inputs u. For example, if the task requires the character?s hand to be at position u at a specific time, the metric might measure the Euclidean distance between the character?s hand and u at that time. To address this, we compute the task error as the difference of the task-performing metric when compared to the nearest existing example. ? Sum of task errors. When more than one task metric is provided by the user, a weighted sum of these metrics is also used as a separate metric. ? Transition error. ? Distance from previous examples. In order to encourage exploration of the control space, we use a metric that measures the distance of a control vector from the nearest existing example control vector. To find the worst-performing regions of controller input space, we need to sample the very large controller input space. We reduce the amount of samples required by using different sampling processes for different motion metrics. To generate candidates using each task error metric, we search for motions that perform their tasks poorly. For each cluster start state, random control vectors u are generated for its task by uniform random sampling in U k . We keep the u?s which give the worst result for each motion metric. Each of these points is refined by Nelder-Mead search [1965], maximizing the badness of their respective metrics. Distance metric candidates are generated similarly, replacing task error with nearest-neighbor distance in U k . We use the optimized u, along with the t and s which generated them, as candidates. To generate candidates using transition error, we search for states that will blend poorly with existing clusters. The average motion clip is generated for each cluster by averaging the examples in that cluster. We then find the state s in this motion that is the furthest from any existing cluster?s start state. We use these states 2 , along with t and u set to the center of the domain U k , as candidates. Once all candidates have been generated, the user is then shown the candidates for each metric, sorted by score, along with information about them, such as their error and corresponding u. The user then chooses one of the candidates to be improved. Providing the user with several options has several important purposes. First, the user is able to qualitatively judge which sample would be best to improve. Second, the user is able to have some control over the order samples are collected in. For example, it can be more convenient for the user to collect several consecutive samples from the same start state. As more samples are provided, the quality of the candidates improves. Once all of the candidates are considered acceptable, the user has some confidence that the controller is finished. In our experience, although many candidates are generated, the user usually finds one worth refining after watching only a few examples. Before proceeding, we will discuss the details of our implementation of the per-cluster blend function, b. We implement blend functions using a scheme similar to Radial Basis Functions (RBFs), based on the method proposed by Rose et al. [1998; 2001]. We use a different form of linear basis, and replace their residual bases R j with bases that are non-uniform along each dimension: We believe that other non-isotropic representations could perform equally well. The blend function is then We then initialize all elements of ? j to the distance to the nearest neighbor of u j , and solve for r as in Rose et al. We then update each ? j by performing a small optimization across each dimension . We place a number of evaluation samples along dimension in the neighborhood of u j . Then, for regularly sampled values in the range [1.. 3], we scale ? j, , solve for r, and evaluate the sum of the task error metrics at the evaluation samples; the best scale is kept. We then update ? j by these scales and solve for r one last time. A pseudoexample is a new training motion defined as a linear combination of existing motions. It is capable of reshaping b without requiring an additional motion sample. To get an initial guess for the correct cluster and weights for the pseudoexample, we iterate over all clusters that start at the given s, and all the motions in the cluster as well as the currently predicted motion at u. Of all these motions, the one which performs best on the task error metric is selected as\n          3 Specifically, h(x) = (1 ? x) 3 + 3x(1 ? x) 2 for 0 ? x ? 1 and h(x) = 0 otherwise. providing the cluster and weights for the pseudoexample. We then optimize the weights within the cluster according to the sum of task errors metric using Nelder-Mead [1965]. The resulting weights, together with the control vector, constitute the pseudoexample (u, w). The clip generated by these weights and its evaluated task error is then shown to the user. If the user approves the pseudoexample, it is permanently added to this cluster?s RBF; otherwise, it is discarded. The user is presented with the appropriate starting state and a taskspecific visualization (e.g., showing the trajectory and initial position of a ball to be caught). Since the motion is determined relative to the character, the visualizations translate and rotate with the user?s movement prior to beginning the capture. The user starts moving and eventually gets to (or near) the starting state. Our system automatically determines when the initial state conditions are met, and records the rest of the captured motion. This aspect of the interface significantly simplifies the task of performing the required motion sample. Once captured, the user can review the motion and its task error. If the motion is not satisfactory, the user must repeat the motion. In our experiments, the vast majority of tasks could be performed in three or fewer trials. We found that difficult to perform tasks typically required only a few more trials, usually no more than seven in total, although a few rare cases required up to twelve in total. To determine where the desired motion sample ends, we seek local minima of state distance in the captured clip, comparing against the beginnings and ends of the existing clusters, as well as considering the actual end of the recorded motion. We sort these by their distance, and let the user select the appropriate ending. This typically involves nothing more than the user confirming the first choice. The system now needs to determine the appropriate cluster for the new motion. The motion is timewarped to all clusters with similar start and end states, and the alignment results are shown to the user, sorted in increasing order of alignment error. The user can then select which cluster to assign the clip to. In our experiments, the first choice is frequently correct. If the user decides that no cluster is appropriate, a new cluster is created. This approach removes the need for manually determining the clusters at any point during controller synthesis. Clusters are created only when the usersupplied samples are sufficiently different from all existing clusters. The cluster is then updated with the new clip, and a new (u, w) for that clip is added to the cluster?s RBF. The active learning process is repeated by selecting a new candidate tasks. The active learning process can have many termination criteria, including the time spent in the lab, number of samples allotted to the controller, as well as the overall controller quality and controllability. The user can estimate the measure the quality and controllability of the current controller by evaluating the quality of the candidate tasks: if all ?poor-performing? candidates appear to be of high-quality, the controller has probably reached the point where no additional samples can significantly improve the quality and controller coverage. We have constructed three examples to demonstrate our framework. The controllers are demonstrated in the accompanying video. Each of these controllers was constructed interactively in a short time in the motion capture lab. We capture and synthesize motions at between 20 and 25 frames per second. The computation time required is negligible: each active learning step took between 5 and 30 seconds, depending on the size of the controller involved. The majority of the time was spent in capture, especially since it normally takes two or three tries to perform a motion with the correct start state that matches the task. Our first example is a parameterized walk. The control space U is three-dimensional, controlling stride length (the distance between the heels at maximum spacing, ranging from 0.2 ? 0.9 meters), speed (ranging from 15 ? 25 centimeters per frame), and turning angle (change in horizontal orientation per stride, ranging from ??/4 to ?/4 radians). There is one task error metric for each of these features (e.g., stride length), measured as the the squared distance between the desired feature and the maximum feature in the motion. In order to determine strides, the metric assumes that a sample contains a complete walk cycle beginning with the right foot forward. This task allows motions to start from the middle. The controller for this task produced only 1 cluster, using 12 examples totaling 843 frames, and 10 pseudoexamples. We limited lab time to roughly half an hour, including all active learning and user interaction. This controller achieved 89% coverage (discussed in section 6). Our second example combines a parameterized walk with projectile dodging. This example has two tasks. When there is nothing to dodge, the control space is one-dimensional, controlling turning angle. When walking and dodging, the control space is four-dimensional, controlling turning angle as well as the incoming height, incoming angle, and distance in front of the character of the incoming projectile. The walk turning parameter as a ratio of radians per meter traveled, ranging from ??/3 to ?/3. The task metric measures the absolute value of the difference between this ratio and a the target value. The walk task allows motions to start from the middle. The parameters of the incoming projectile are specified relative to the character?s local coordinate system. These parameters specify a trajectory for the projectile. The error metric is the inverse of the distance between the projectile and any point on the character. The dodge task also includes the same turning control parameter as used for walking, with the same error metric as well. The synthesized controller contains 10 clusters, using total of 30 examples totaling 1121 frames, and 19 pseudoexamples. We limited lab time to roughly an hour, and achieved 76% coverage of the dodge task. Our third example is catching a ball. This example has two tasks. When not catching, there is a zero-dimensional stand task. When catching, the control space is three-dimensional, controlling the incoming position on the plane in front of the character as well as  speed. The parameters of the incoming ball are specified relative to the character?s local coordinate system. These parameters specify a trajectory for the ball. The task error metric is the squared distance between the ball and the character?s right hand. Also, in this example, we use the IK controller so that we have can have greater reachability of the resulting motion. The resulting controller used 12 clusters, using 33 examples totaling 1826 frames, and 23 pseudoexamples. The data, not including the diving motions 4 , was captured in the lab session limited to roughly an hour. We achieved 57% coverage of the catch task, normalized by the manual controllers discussed in section 6. It is worth noting that we also tried creating the controller with a simpler cluster model that did not include IK, and found that the performance was significantly poorer, due to the nonlinearity of the underlying control space. We have performed a numerical evaluation of the active learning method proposed using the catching controller. We compare the active learning approach with manual sample selection. We have had four animation researchers (two associated with this project and two not) plan a motion capture session for a catching controller like the one in our results. All manual designs required more samples than the learned controller, but produced controllers that, visually and quantitatively, were significantly inferior to our learned controller. In Table 1 , we show the number of samples and the percentage of input space coverage of each controller. For this comparison, we only consider the catch task, ignoring the stand task. The coverage is defined as the percentage of inputs for which the controller successfully matches the start state and completes the task (for catching the ball, this means the hand is within a small distance of the ball). The percentage is computed by randomly sampling state and task, similar to the scheme in Section 4.1. For this comparison, we look at the percentage of samples controllable by a given controller out of the samples controllable by any of the controllers, to remove samples physically impossible to catch. The manual controller with roughly 80% more samples produced significantly less coverage, while the controller with 20% more samples covers roughly half the space of learned controller. The total mocap studio time was roughly the same. Of course, coverage is not a perfect measure of a controller, since it does not measure realism of the motion. The difference in the quality of controllers is also apparent in the accompanying video: with the manual controllers, the balls are often caught in very strange and unrealistic ways. Of course, the evaluation depends on the skill level of people who chose the motions,\n        4 The dive catch required significant motion cleanup and was also painful to perform. In order to facilitate prototyping and testing, we captured several representative dives in advance. Then, when the active learning system requested a dive, one of these pre-captured motions was provided (if appropriate) instead of performing a new dive. We also show a chart demonstrating the improvement of the controller as each sample is added in Figure 6 . We use the same measure of coverage as above. We have introduced an active learning framework for creating realtime motion controllers. By adaptively determining which motions to add to the model, the system creates finely-controllable motion models with a reduced number of data clips and little time spent in the motion capture studio. In addition, by always presenting the worst-performing state samples, the user has a continuous gauge of the quality of the resulting controller. Although our system focuses on one specific model for motion synthesis, we believe that the general approach of adaptive modelbuilding will be useful for many types of animation models ? any situation in which minimizing the number of motion samples is important can potentially benefit from active learning. We have designed the system to be fast and flexible, so that relatively little time is spent waiting for the next candidate to be selected. Hence, we employed a number of heuristic, incremental learning steps, and our models are not generally ?optimal? in a global sense. Our system does not make formal guarantees of being able to perform every task from every start state. Some of these tasks may be impossible (e.g., starting a new task from midair); others may not have been captured in the limited time available in a motion capture session. There is a tradeoff between lab time, number of samples, and coverage that the user must evaluate. In our examples we show that it is possible to get a high amount of coverage with a low number of samples and short lab time. Some difficulties arise during the capture process. It is necessary for the user to look at the screen as well as mentally project their  motion onto the on-screen visualization. We believe that these difficulties can be overcome through the use of an augmented or virtual reality system. We have no guarantee of scalability, but we believe that this system will scale well to handle many different tasks performed sequentially, creating highly flexible characters. However, our controller examples do not fully test the effectiveness of our approach in very high dimensions, and this work certainly does not solve the fundamental problem of ?curse of dimensionality? for data-driven controllers. We believe that active learning will generalize to controllers with higher dimensions. However, for controllers with 10 or more task inputs together with 37 DOF characters state space, even though active learning will drastically reduce the number of samples, the number of required samples would still be impractical. In very high dimensions, it becomes an imperative to use sophisticated motion models that accurately represent nonlinear dynamics, since the expressive power of such models greatly reduces the number of required samples. The authors would like to thank Gary Yngve for his help with the video, and the anonymous reviewers for their comments. This work was supported by the UW Animation Research Labs, NSF grants CCR-0092970, EIA-0321235, Electronic Arts, Sony, Microsoft Research, NSERC, CFI, and the Alfred P. Sloan Foundation. L , M., M , S., R , D. 2004. Selective sampling for nearest neighbor classifiers. 54, 2, 125?152. L IU , C. K., AND P OPOVI C  ? , Z. 2002. Synthesis of Complex Dynamic Character Motion from Simple Animations. ACM Trans. on Graphics 21, 3 (July), 408?416. L IU , C. K., H ERTZMANN , A., AND P OPOVI C  ? , Z. 2005. Learning Physics-Based Motion Style with Nonlinear Inverse Optimization. ACM Transactions on Graphics 24, 3 (Aug.), 1071?1081. , S. 2005. Geostatistical motion 3 (Aug.), 1062?1070. , R. 1965. A simplex method for 7, 4, 308?313. P ARK , S. I., S HIN , H. J., K IM , T. H., AND S HIN , S. Y. 2004. On-line motion blending for real-time locomotion generation. Virtual Worlds 15, 125?138. Evaluating R , C., C , M. F., B , B. 1998. Verbs and Adverbs: Multidimensional Motion Interpolation. IEEE Computer Graphics & Applications 18, 5, 32?40. R OSE III, C. F., S LOAN , P.-P. J., AND C OHEN , M. F. 2001. Artist-Directed Inverse-Kinematics Using Radial Basis Function Interpolation. Computer Graphics Forum 20, 3, 239?250. , T. J., W , B. J., N , W. I. 2003. The Springer. S HIN , H. J., AND O H , H. S. 2006. Fat Graphs: Constructing an interactive character with continuous controls. III, C. F., C , M. F. 2001. T ORRESANI , L., H ACKNEY , P., AND B REGLER , C. 2007. Learning Motion Style Synthesis from Perceptual Observations. W ILEY , D. J., AND H AHN , J. K. 1997. Interpolation synthesis of articulated figure motion. IEEE Computer Graphics and Applications 17, 6 (Nov. /Dec.), 39?45. Y AMANE , K., K UFFNER , J. J., AND H ODGINS , J. K. 2004. Synthesizing animations of human manipulation tasks. ACM Trans. on Graphics 23, 3 (Aug.), 532?539. Z ORDAN , V. B., M AJKOWSKA , A., C HIU , B., AND F AST , M. 2005. Dynamic Response for Motion Capture Animation. ACM Transactions on Graphics 24, 3 (Aug.).",
  "resources" : [ ]
}