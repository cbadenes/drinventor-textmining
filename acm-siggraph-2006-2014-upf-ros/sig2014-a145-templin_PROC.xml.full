{
  "uri" : "sig2014-a145-templin_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a145-templin_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Krzysztof-Templin",
      "name" : "Krzysztof",
      "surname" : "Templin"
    }, {
      "uri" : "http://drinventor/Piotr-Didyk",
      "name" : "Piotr",
      "surname" : "Didyk"
    }, {
      "uri" : "http://drinventor/Karol-Myszkowski",
      "name" : "Karol",
      "surname" : "Myszkowski"
    }, {
      "uri" : "http://drinventor/Mohamed-Hefeeda",
      "name" : "Mohamed",
      "surname" : "Hefeeda"
    }, {
      "uri" : "http://drinventor/Hans-Peter-Seidel",
      "name" : "Hans-Peter",
      "surname" : "Seidel"
    }, {
      "uri" : "http://drinventor/Wojciech-Matusik",
      "name" : "Wojciech",
      "surname" : "Matusik"
    } ]
  },
  "bagOfWords" : [ "81986706cb3a9ea04d5b42b1bd6c9a2e1e2623b8623893ea97d963afa8daef7a", "p77", "10.1145", "2601097.2601148", "name", "identification", "possible", "modeling", "Optimizing", "Eye", "Vergence", "response", "Stereoscopic", "Cuts", "Krzysztof", "Templin", "1,2", "Piotr", "Didyk", "Karol", "Myszkowski", "Mohamed", "M.", "Hefeeda", "Hans-Peter", "Seidel", "Wojciech", "Matusik", "MIT", "CSAIL", "MPI", "Informatik", "Figure", "unexpected", "change", "disparity", "stereoscopic", "3d", "imagery", "those", "introduce", "cut", "s3d", "film", "challenge", "audience", "use", "eye-tracker", "we", "record", "eye", "vergence", "response", "16", "subject", "step-like", "change", "disparity", "-lrb-", "left", "-rrb-", "model", "adaptation", "time", "derive", "parameter", "average", "observer", "estimate", "have", "predictor", "enable", "example", "optimization", "film", "editing", "operation", "best", "match", "human", "depth", "adaptation", "ability", "-lrb-", "right", "-rrb-", "color", "line", "connect", "point", "interest", "before", "after", "cut", "visualize", "corresponding", "vergence", "adaptation", "time", "picture", "from", "Dracula", "4d", "courtesy", "Red", "Star", "3D", "www.redstar3d.com", "Sudden", "temporal", "depth", "change", "cut", "introduce", "video", "edit", "can", "significantly", "degrade", "quality", "stereoscopic", "content", "since", "usually", "encounter", "real", "world", "very", "challenging", "audience", "because", "eye", "vergence", "have", "constantly", "adapt", "new", "disparity", "spite", "conflict", "accommodation", "requirement", "rapid", "disparity", "change", "may", "lead", "confusion", "reduce", "understanding", "scene", "overall", "attractiveness", "content", "most", "case", "problem", "can", "solve", "simply", "match", "depth", "around", "transition", "would", "require", "flatten", "scene", "completely", "better", "understand", "limitation", "human", "visual", "system", "we", "conduct", "series", "eye-tracking", "experiment", "datum", "obtain", "allow", "we", "derive", "evaluate", "model", "describe", "adaptation", "vergence", "disparity", "change", "stereoscopic", "display", "besides", "compute", "user-specific", "model", "we", "also", "estimate", "parameter", "average", "observer", "model", "enable", "range", "strategy", "minimize", "adaptation", "time", "audience", "cr", "category", "i.", "3.3", "-lsb-", "Computer", "Graphics", "-rsb-", "picture/image", "generation?display", "algorithm", "view", "algorithm", "keyword", "binocular", "eye-tracking", "s3d", "Links", "dl", "pdf", "EB", "ACM", "Reference", "Format", "Templin", "K.", "Didyk", "P.", "Myszkowski", "K.", "Hefeeda", "M.", "Seidel", "H.", "Matusik", "W.", "2014", "modeling", "Optimizing", "Eye", "Vergence", "response", "Stereoscopic", "cut", "ACM", "Trans", "graph", "33", "Article", "145", "-lrb-", "July", "2014", "-rrb-", "page", "dous", "10.1145", "2601097.2601148", "http://doi.acm.org/10.1145/2601097.2601148", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "author", "-lrb-", "-rrb-", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "2014", "copyright", "hold", "Owner/Author", "publication", "rights", "license", "ACM", "0730-0301/14", "07-art145", "15.00", "DOI", "http://dx.doi.org/10.1145/2601097.2601148", "Qatar", "Computing", "Research", "Institute", "introduction", "over", "past", "few", "year", "stereoscopic", "3d", "-lrb-", "s3d", "-rrb-", "technology", "have", "be", "constantly", "develop", "now", "have", "become", "ubiquitous", "however", "despite", "significant", "improvement", "only", "display", "device", "also", "image", "generation", "capture", "post-processing", "technique", "many", "consumer", "still", "skeptical", "about", "quality", "current", "s3d", "content", "future", "technology", "itself", "concern", "usually", "relate", "naturalness", "effortlessness", "overall", "appearance", "s3d", "effect", "should", "distraction", "difficulty", "s3d", "production", "sufficient", "produce", "two", "good", "image", "place", "one", "arrive", "good", "stereoscopic", "effect", "-lsb-", "Zilly", "et", "al.", "2011", "-rsb-", "s3d", "strong", "illusion", "since", "isolate", "only", "one", "real-world", "phenomenon", "fail", "reproduce", "many", "other", "prominent", "example", "be", "accommodation", "cue", "impose", "numerous", "restriction", "production", "process", "depth", "range", "variation", "must", "too", "large", "view-dependent", "effect", "need", "handle", "correctly", "image", "carefully", "register", "so", "work", "we", "concern", "rapid", "temporal", "change", "disparity", "human", "have", "good", "understanding", "environment", "observe", "move", "through", "so-called", "mental", "image", "which", "enhance", "capability", "focus", "different", "object", "-lsb-", "finke", "1989", "-rsb-", "however", "when", "scene", "merely", "sequence", "shot", "show", "flat", "screen", "easy", "get", "confused", "lose", "track", "point", "interest", "due", "among", "other", "thing", "unexpected", "change", "location", "camera", "angle", "although", "less", "problematic", "2d", "can", "challenge", "stereoscopic", "3d", "context", "unpredictable", "large", "change", "disparity", "mean", "binocular", "fusion", "lose", "confusing", "double", "image", "see", "-lrb-", "diplopium", "-rrb-", "moreover", "vergence", "system", "need", "quickly", "adapt", "new", "condition", "spite", "conflict", "goal", "interconnected", "accommodation", "system", "have", "be", "identify", "one", "source", "discomfort", "stereoscopic", "viewing", "-lsb-", "Hoffman", "et", "al.", "2008", "Lambooij", "et", "al.", "2009", "-rsb-", "Hollywood", "style", "combine", "shot", "develop", "set", "formal", "convention", "obey", "dynamics", "visual", "attention", "control", "continuity", "space", "time", "action", "modern", "movie", "cut", "play", "most", "important", "role", "-lrb-", "99", "all", "edit", "-rrb-", "while", "dissolve", "wipe", "have", "vanish", "almost", "completely", "extensive", "analysis", "cut", "et", "al.", "-lsb-", "2011", "-rsb-", "show", "average", "shot", "duration", "over", "past", "75", "year", "have", "decline", "from", "ca.", "15", "ca.", "3.5", "clearly", "short", "shot", "increase", "viewer", "engagement", "force", "eye", "quickly", "follow", "newly", "appear", "content", "however", "accumulation", "sharp", "cut", "challenge", "visual", "system", "require", "seamless", "adjustment", "vergence", "between", "many", "shot", "over", "possibly", "wide", "range", "depths", "require", "different", "approach", "editing", "e.", "g.", "ultra-short", "mtvstyle", "shot", "need", "replace", "more", "slow-paced", "edit", "nevertheless", "modern", "movie", "often", "simultaneously", "release", "2d", "s3d", "one", "should", "expect", "director", "cinematographer", "editor", "entirely", "give", "up", "artistic", "vision", "style", "merely", "sake", "s3d", "medium", "limitation", "instead", "apply", "different", "s3d", "post-production", "technique", "make", "depth", "transition", "natural", "effortless", "viewer", "manipulation", "range", "from", "simple", "depth", "manipulation", "cross-dissolve", "type", "cut", "more", "sophisticated", "transition", "where", "multiple", "sequence", "gradually", "change", "depth", "combine", "-lsb-", "Owens", "2013", "-rsb-", "all", "manipulation", "time-consuming", "expensive", "perform", "manually", "example", "Owens", "-lsb-", "2013", "-rsb-", "point", "out", "editing", "transition", "one", "most", "challenging", "step", "post-production", "u2", "concert", "record", "stereoscopic", "3d", "abrupt", "depth", "change", "well", "beyond", "real-world", "experience", "should", "also", "expect", "action", "computer", "game", "address", "problem", "rapid", "depth", "change", "we", "propose", "relate", "transition", "quality", "vergence", "adaptation", "time", "instead", "simpler", "disparity", "difference", "we", "present", "series", "experiment", "human", "observer", "which", "vergence", "response", "be", "measure", "use", "consumer", "s3d", "equipment", "high-framerate", "eye-tracker", "lead", "simple", "model", "describe", "vergence", "adaptation", "curve", "give", "initial", "target", "disparity", "model", "allow", "prediction", "adaptation", "time", "after", "cut", "which", "facilitate", "its", "visualization", "minimization", "impact", "optimization", "visual", "quality", "s3d", "content", "demonstrate", "separate", "experiment", "we", "knowledge", "we", "first", "apply", "principled", "approach", "problem", "summary", "we", "make", "follow", "contribution", "measurement", "vergence", "response", "rapid", "disparity", "change", "define", "initial", "target", "disparity", "derivation", "evaluation", "model", "relate", "disparity", "change", "vergence", "adaptation", "curve", "along", "average", "observer", "parameter", "interactive", "tool", "visualization", "minimization", "adaptation", "time", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "145:2", "K.", "Templin", "et", "al.", "related", "work", "here", "we", "overview", "basic", "finding", "eye", "vergence", "mechanism", "main", "focus", "s3d", "display", "condition", "we", "refer", "reader", "survey", "Meesters", "et", "al.", "-lsb-", "2004", "-rsb-", "in-depth", "discussion", "other", "aspect", "s3d", "display", "perception", "vergence", "Dynamic", "process", "eye", "vergence", "drive", "depth", "change", "target", "object", "can", "perform", "high", "accuracy", "-lrb-", "error", "below", "10", "arcmin", "-rrb-", "both", "real", "world", "s3d", "display", "observation", "condition", "-lsb-", "Okuyama", "1998", "-rsb-", "other", "factor", "blur", "proximity", "target", "size", "luminance", "might", "affect", "vergence", "lesser", "extent", "-lsb-", "Campbell", "Westheimer", "1959", "-rsb-", "Vergence", "relatively", "slow", "process", "when", "compare", "other", "eye", "movement", "e.", "g.", "saccade", "-lrb-", "below", "60", "m", "-rrb-", "require", "about", "195", "750", "m", "convergence", "240", "1000", "m", "divergence", "vergence", "latency", "also", "demonstrate", "similar", "asymmetric", "behavior", "180", "250", "m", "convergence", "200", "210", "m", "divergence", "-lsb-", "Krishnan", "et", "al.", "1973", "Krishnan", "et", "al.", "1977", "Semmlow", "Wetzel", "1979", "-rsb-", "Vergence", "two-stage", "process", "where", "first", "fast", "transient", "-lrb-", "a.k.a.", "phasic", "-rrb-", "mechanism", "-lrb-", "react", "even", "brief", "200", "m", "flash", "-rrb-", "bring", "vergence", "proximity", "target", "depth", "slower", "sustained", "-lrb-", "a.k.a.", "tonic", "-rrb-", "mechanism", "responsible", "precise", "verge", "target", "well", "further", "tracking", "slower", "depth", "change", "Semmlow", "et", "al.", "-lsb-", "1986", "-rsb-", "find", "less", "dynamic", "depth", "change", "ramp", "velocity", "below", "deg/s", "only", "sustained", "mechanism", "active", "above", "deg/s", "transient", "mechanism", "dominate", "otherwise", "both", "mechanism", "active", "Vergence", "adaptation", "-lrb-", "similar", "luminance", "adaptation", "-rrb-", "have", "be", "observe", "which", "sustained", "mechanism", "support", "give", "eye", "vergence", "angle", "comfort", "state", "achieve", "during", "binocular", "vision", "-lsb-", "Hung", "1992", "-rsb-", "small", "depth", "change", "within", "panum?s", "fusional", "area", "motoric", "vergence", "activate", "sensoric", "fusion", "image", "retina", "sufficient", "Vergence", "vs.", "Accommodation", "while", "vergence", "drive", "depth", "accommodation", "drive", "mostly", "retinal", "blur", "both", "system", "reflexively", "couple", "interact", "each", "other", "through", "accommodative", "vergence", "vergence", "accommodation", "-lsb-", "Hung", "2001", "-rsb-", "accommodative", "vergence", "quantify", "ac/a", "ratio", "which", "relate", "change", "vergence", "cause", "change", "accommodation", "absence", "disparity", "analogous", "way", "vergence", "accommodation", "quantify", "CA/C", "ratio", "absence", "blur", "since", "range", "accommodation", "while", "view", "s3d", "display", "determine", "distance", "screen", "unnatural", "decoupling", "vergence", "accommodation", "require", "which", "may", "cause", "visual", "discomfort", "increase", "binocular", "fusion", "time", "-lsb-", "Hoffman", "et", "al.", "2008", "Lambooij", "et", "al.", "2009", "-rsb-", "when", "screen", "disparity", "increase", "beyond", "panum?s", "fusional", "area", "vergence", "eye", "movement", "bring", "disparity", "back", "area", "which", "shift", "accommodation", "away", "from", "screen", "when", "shift", "beyond", "depth", "focus", "-lrb-", "dof", "-rrb-", "zone", "accommodative-vergence", "feedback", "activate", "counteract", "loss", "sharp", "vision", "which", "turn", "direct", "vergence", "back", "towards", "display", "-lsb-", "Lambooij", "et", "al.", "2009", "-rsb-", "range", "vergence", "angle", "assure", "clear", "single", "binocular", "vision", "know", "comfort", "zone", "-lsb-", "Shibata", "et", "al.", "2011", "Zilly", "et", "al.", "2011", "-rsb-", "real", "world", "object", "away", "from", "fixation", "point", "perceive", "blur", "which", "reduce", "visibility", "diplopium", "because", "limit", "fusion", "higher", "low", "spatial", "frequency", "thus", "both", "accommodation", "vergence", "response", "can", "improve", "manipulation", "convergence", "local", "image", "defocus", "respectively", "-lsb-", "Ukai", "Kato", "2002", "Zwicker", "et", "al.", "2006", "-rsb-", "many", "practical", "s3d", "application", "comfort", "zone", "determine", "disparity", "range", "70", "arcmin", "-lsb-", "Lambooij", "et", "al.", "2009", "Zilly", "et", "al.", "2011", "-rsb-", "since", "rather", "conservative", "bind", "work", "we", "assume", "wider", "range", "2.5", "deg", "out-of-screen", "effect", "even", "beyond", "range", "use", "cinematography", "object", "interest", "typically", "move", "steadily", "off", "screen", "case", "so", "viewer", "can", "adapt", "its", "extreme", "position", "-lsb-", "Zilly", "et", "al.", "2011", "-rsb-", "achieve", "extreme", "disparity", "would", "possible", "through", "sudden", "jump", "case", "scene", "cut", "Vergence", "Measurements", "large", "body", "research", "measurement", "vergence", "dynamics", "response", "pulse", "step", "ramp", "sinusoidal", "disparity", "change", "we", "step-like", "change", "most", "relevant", "most", "experiment", "use", "physical", "target", "passively-shifted", "screen", "-lsb-", "erkelen", "et", "al.", "1989", "Hung", "et", "al.", "1994", "-rsb-", "simple", "stimulus", "vertical", "line", "be", "use", "eliminate", "other", "cue", "could", "affect", "vergence", "special", "care", "take", "suppress", "accommodation", "use", "pinhole", "aperture", "blur-free", "viewing", "wide", "range", "disparity", "35", "deg", "have", "be", "consider", "-lsb-", "Erkelens", "et", "al.", "1989", "-rsb-", "typical", "range", "below", "10", "deg", "relatively", "large", "step", "amplitude", "typically", "larger", "than", "deg", "-lsb-", "Hung", "2001", "-rsb-", "work", "we", "focus", "disparity", "range", "2.5", "deg", "lower", "disparity", "step", "amplitude", "which", "important", "comfortable", "experience", "while", "view", "s3d", "display", "assume", "disparity", "range", "correspond", "approximately", "comfort", "zone", "desktop", "viewing", "condition", "give", "Shibata", "et", "al.", "-lsb-", "2011", "Fig.", "23", "-rsb-", "use", "off-the-shelf", "s3d", "display", "we", "experiment", "deal", "real-world", "image", "validation", "step", "we", "ensure", "condition", "possibly", "similar", "one", "expect", "application", "where", "accommodation", "pictorial", "cue", "may", "affect", "vergence", "also", "initial", "disparity", "magnitude", "important", "we", "measurement", "both", "convergence", "divergence", "case", "Vergence", "Modeling", "Schor", "-lsb-", "1979", "-rsb-", "Hung", "-lsb-", "1998", "-rsb-", "propose", "sophisticated", "model", "eye", "vergence", "dynamics", "which", "employ", "concept", "control", "engineering", "simulate", "transient", "sustained", "-lrb-", "negative", "feedback", "loop", "-rrb-", "mechanism", "model", "have", "be", "extend", "handle", "accommodation", "well", "ac/a", "ca/c", "cross-link", "gain", "-lsb-", "Schor", "1992", "Schor", "1999", "Hung", "2001", "-rsb-", "extensive", "validation", "model", "against", "measurement", "datum", "have", "be", "perform", "however", "disparity", "step", "interesting", "we", "have", "be", "treat", "marginally", "furthermore", "viewing", "condition", "do", "force", "decoupling", "accommodation", "vergence", "while", "s3d", "display", "have", "be", "consider", "some", "computational", "model", "main", "goal", "artificially", "alter", "link", "between", "accommodation", "vergence", "system", "study", "change", "pre-task", "post-task", "measure", "ac/a", "ca/c", "-lsb-", "Eadie", "et", "al.", "2000", "-rsb-", "investigate", "developmental", "plasticity", "child", "expose", "s3d", "game", "-lsb-", "Rushton", "Riddell", "1999", "-rsb-", "Alvarez", "et", "al.", "-lsb-", "2005", "-rsb-", "experiment", "constant-sized", "deg", "step", "find", "case", "divergent", "step", "vergence", "dynamics", "dependent", "initial", "disparity", "work", "we", "propose", "simple", "data-driven", "model", "eye", "vergence", "tune", "step-like", "disparity", "change", "we", "emphasize", "here", "vergence", "dynamics", "function", "initial", "target", "disparity", "we", "goal", "minimization", "vergence", "adaptation", "time", "scene", "cut", "through", "disparity", "editing", "temporal", "change", "vs.", "Comfort", "Yano", "et", "al.", "-lsb-", "2004", "-rsb-", "report", "visual", "discomfort", "induce", "image", "be", "move", "depth", "accord", "step", "pulse", "function", "even", "image", "be", "display", "within", "depth", "focus", "related", "work", "Tam", "et", "al.", "-lsb-", "2012", "-rsb-", "influence", "disparity", "velocity", "visual", "comfort", "investigate", "significant", "interaction", "between", "velocity", "disparity", "show", "negative", "effect", "object", "velocity", "visual", "comfort", "apparent", "even", "when", "object", "be", "display", "within", "generally", "accept", "visual", "comfort", "zone", "less", "than", "deg", "horizontal", "disparity", "result", "obtain", "Lambooij", "et", "al.", "-lsb-", "2011", "-rsb-", "show", "rapidly", "move", "object", "change", "screen", "disparity", "indeed", "have", "significant", "effect", "visual", "comfort", "however", "dominant", "role", "confirm", "s3d", "content", "processing", "problem", "scene", "transition", "challenge", "context", "stereoscopic", "content", "since", "scene", "transition", "often", "create", "large", "temporal", "disparity", "discontinuity", "lead", "visual", "discomfort", "solve", "problem", "disparity", "adjustment", "technique", "require", "perform", "either", "during", "acquisition", "step", "modify", "camera", "parameter", "post-processing", "step", "use", "example", "horizontal", "image", "translation", "-lsb-", "Mendiburu", "2009", "-rsb-", "however", "only", "few", "technique", "can", "deal", "temporal", "effect", "disparity", "velocity", "consider", "one", "important", "factor", "disparity", "adjustment", "-lsb-", "Lang", "et", "al.", "2010", "-rsb-", "author", "propose", "interpolate", "between", "different", "disparity", "range", "scene", "cut", "reduce", "large", "discontinuity", "disparity", "end", "different", "disparity", "mapping", "operator", "can", "use", "make", "adjustment", "however", "decision", "how", "interpolation", "define", "leave", "user", "simpler", "technique", "have", "be", "propose", "koppal", "-lsb-", "2011", "-rsb-", "he", "suggest", "solve", "problem", "transition", "cross-fade", "horizontal", "image", "translation", "zero", "cut", "Bernhard", "et", "al.", "-lsb-", "2014", "-rsb-", "show", "how", "binocular", "fusion", "time", "can", "reduce", "means", "active", "manipulation", "convergence", "plane", "object", "interest", "bring", "back", "zero-disparity", "plane", "once", "change", "gaze", "have", "be", "detect", "before", "vergence", "adaptation", "complete", "contrast", "Bernhard", "et", "al.", "active", "approach", "we", "propose", "cut", "optimization", "process", "keep", "disparity", "constant", "during", "vergence", "adaptation", "improvement", "we", "case", "come", "from", "more", "informed", "choice", "initial", "target", "disparity", "nevertheless", "both", "approach", "could", "potentially", "combine", "Heinzle", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "computational", "camera", "rig", "which", "enable", "intuitive", "control", "over", "camera", "parameter", "artist?s", "involvement", "still", "need", "though", "design", "transition", "manually", "without", "any", "feedback", "human", "ability", "adapt", "rapid", "disparity", "change", "Automatic", "control", "over", "camera", "parameter", "propose", "context", "real-time", "system", "-lrb-", "e.", "g.", "game", "-rrb-", "-lsb-", "Oskam", "et", "al.", "2011", "-rsb-", "however", "primary", "goal", "maintain", "scene", "disparity", "range", "within", "give", "limit", "equivalent", "minimize", "vergence", "adaptation", "time", "which", "depend", "only", "disparity", "difference", "also", "initial", "disparity", "value", "we", "approach", "we", "take", "those", "two", "factor", "account", "more", "recently", "metric", "visual", "comfort", "have", "be", "propose", "-lsb-", "Du", "et", "al.", "2013", "-rsb-", "which", "directly", "address", "problem", "temporal", "disparity", "change", "author", "also", "suggest", "can", "use", "optimize", "stereoscopic", "parameter", "however", "metric", "deal", "motion", "unclear", "how", "apply", "technique", "context", "rapid", "disparity", "change", "those", "create", "during", "scene", "transition", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "Modeling", "Optimizing", "Eye", "Vergence", "response", "Stereoscopic", "cut", "145:3", "Model", "Derivation", "section", "we", "experimentally", "derive", "evaluate", "model", "eye", "vergence", "response", "step-like", "change", "disparity", "we", "also", "estimate", "model", "parameter", "average", "observer", "collect", "datum", "useful", "number", "application", "show", "Sec", "participant", "sixteen", "subject", "-lrb-", "-rrb-", "take", "part", "we", "experiment", "be", "member", "computer", "graphic", "computer", "vision", "group", "between", "21", "35", "year", "old", "all", "have", "normal", "corrected-to-normal", "vision", "all", "pass", "test", "stereo-blindness", "equipment", "stimulus", "be", "present", "use", "Nvidia", "3D", "Vision", "kit", "Acer", "gd235hz", "23.6-inch", "screen", "native", "resolution", "1920", "1080", "order", "measure", "vergence", "response", "both", "eye", "be", "track", "use", "EyeLink", "1000", "plus", "eye", "tracker", "desktop", "mount", "tracker", "record", "1000", "sample", "per", "second", "-lrb-", "500", "per", "eye", "-rrb-", "allow", "fine-scale", "analysis", "vergence", "response", "spatial", "accuracy", "accord", "eye-tracker", "manufacturer", "up", "0.25", "0.5", "deg", "chin-rest", "use", "stabilize", "subject?s", "head", "view", "distance", "fix", "55", "cm", "stimulus", "stimulus", "we", "experiment", "low-pass", "filter", "white-noise", "patch", "change", "its", "disparity", "discrete", "step", "over", "time", "patch", "present", "centrally", "screen", "neutral", "grey", "background", "subtend", "ca.", "11", "degree", "visual", "angle", "single", "trial", "consist", "sequence", "disparity", "...", "choose", "from", "fix", "set", "order", "disparity", "randomize", "avoid", "learning", "effect", "only", "eulerian", "path", "be", "use", "i.", "e.", "every", "possible", "transition", "appear", "exactly", "once", "since", "prediction", "have", "be", "show", "have", "influence", "vergence", "response", "-lrb-", "periodic", "disparity", "change", "can", "follow", "vergence", "without", "typical", "latency", "-lsb-", "Hung", "1998", "-rsb-", "-rrb-", "time", "between", "onset", "consecutive", "stimulus", "set", "randomly", "between", "1.25", "2.5", "s.", "Task", "each", "session", "experiment", "start", "calibration", "procedure", "describe", "eye", "tracker", "manual", "next", "every", "participant", "have", "perform", "trial", "task", "simply", "observe", "patch", "participant", "be", "encourage", "take", "break", "whenever", "feel", "tired", "after", "each", "break", "eye", "tracker", "re-calibrated", "entire", "session", "take", "approximately", "40", "minute", "datum", "analysis", "after", "each", "session", "binary", "output", "eye", "tracker", "convert", "plain-text", "version", "use", "converter", "tool", "provide", "manufacturer", "next", "datum", "process", "use", "custom", "parser", "extract", "gaze", "coordinate", "time", "disparity", "change", "read", "MATLAB", "r2012a", "time", "stimulus", "onset", "be", "mark", "output", "file", "timestamp", "functionality", "provide", "tracker?s", "API", "which", "enable", "easy", "synchronization", "gaze", "datum", "stimulus", "each", "transition", "we", "extract", "1-second", "segment", "follow", "smooth", "use", "small", "box", "filter", "convert", "vergence", "value", "Vergence", "calculate", "difference", "between", "x-coordinate", "two", "gaze", "position", "express", "pixel", "miss", "unreliable", "sample", "-lrb-", "due", "e.", "g.", "blink", "saccade", "track", "error", "-rrb-", "be", "interpolate", "linearly", "segment", "require", "interpolation", "more", "than", "50", "sample", "be", "exclude", "datum", "transition", "one", "type", "group", "curve", "fit", "average", "next", "each", "type", "transition", "time", "reach", "95", "require", "vergence", "change", "determine", "two", "surface", "be", "fit", "obtain", "datum", "point", "since", "we", "be", "interested", "relative", "gaze", "position", "significance", "drift", "low", "moreover", "adaptation", "time", "be", "determine", "95", "of-change", "position", "which", "very", "sensitive", "shift", "scaling", "etc.", "base", "premise", "we", "believe", "precision", "sufficient", "we", "purpose", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "145:4", "K.", "Templin", "et", "al.", "3.1", "pilot", "experiment", "order", "gain", "insight", "relation", "vergence", "response", "initial", "end", "disparity", "well", "estimate", "number", "trial", "necessary", "response", "curve", "converge", "we", "conduct", "pilot", "study", "one", "subject", "-lrb-", "s7", "-rrb-", "perform", "30", "trial", "30", "60", "90", "px", "cut-off", "frequency", "low-pass", "filter", "20", "cpd", "give", "30", "1260", "transition", "measure", "result", "present", "fig.", "discussion", "signal", "converge", "quickly", "give", "relatively", "smooth", "datum", "after", "ca.", "repetition", "little", "could", "gain", "after", "ca.", "10", "repetition", "vergence", "response", "can", "model", "very", "well", "sigmoid", "function", "form", "ae", "ct", "know", "Gompertz", "curve", "95", "point", "do", "depend", "parameter", "can", "obtain", "use", "follow", "formulum", "95", "ln", "-lrb-", "ln", "-lrb-", "0.95", "-rrb-", "-rrb-", "obtain", "datum", "point", "can", "model", "almost", "perfectly", "use", "two", "plane", "mean", "error", "close", "standard", "deviation", "ca.", "27", "ms.", "light", "finding", "we", "decide", "limit", "disparity", "value", "use", "main", "experiment", "30", "90", "px", "number", "repetition", "10", "3.2", "Main", "Experiment", "aim", "main", "experiment", "twofold", "confirm", "vergence", "time", "can", "well", "model", "use", "two", "plane", "suggest", "pilot", "experiment", "so", "estimate", "parameter", "average-observer", "model", "useful", "practical", "application", "experiment", "16", "subject", "perform", "10", "trial", "-lrb-", "except", "subject", "s6", "s9", "s10", "whom", "-rrb-", "cut-off", "frequency", "10", "range", "disparity", "subject", "s9", "reduce", "2/3", "due", "report", "problem", "fusion", "result", "present", "fig.", "discussion", "average", "standard", "deviation", "error", "after", "fitting", "plane", "obtain", "datum", "equal", "36", "ms.", "indicate", "very", "good", "fit", "justify", "we", "assumption", "vergence", "adaptation", "time", "can", "model", "use", "plane", "particular", "mean", "datum", "from", "subject", "s9", "who", "see", "rescaled", "disparity", "could", "easily", "include", "average", "model", "expect", "we", "measurement", "show", "give", "initial", "disparity", "direction", "step", "larger", "magnitude", "lead", "longer", "vergence", "adaptation", "time", "interesting", "finding", "adaptation", "time", "depend", "also", "step", "direction", "initial", "disparity", "give", "initial", "disparity", "-lrb-", "fig.", "right", "abscissa", "-rrb-", "step", "magnitude", "-lrb-", "one", "yellow", "one", "green", "line", "per", "magnitude", "-rrb-", "step", "towards", "screen", "generally", "faster", "right", "graph", "yellow", "line", "-lrb-", "convergent", "step", "-rrb-", "have", "lower", "time", "than", "corresponding", "green", "line", "-lrb-", "divergent", "step", "-rrb-", "left", "reverse", "note", "corresponding", "yellow", "green", "line", "intersect", "near", "point", "zero", "initial", "disparity", "-lrb-", "screen", "plane", "-rrb-", "we", "hypothesize", "related", "accommodation-vergence", "coupling", "which", "attract", "vergence", "towards", "screen", "plane", "where", "a/v", "conflict", "disappear", "additionally", "give", "step", "magnitude", "direction", "-lrb-", "fig.", "either", "one", "yellow", "one", "green", "line", "-rrb-", "decrease", "initial", "disparity", "convergent", "step", "get", "slower", "whereas", "divergent", "step", "get", "faster", "effect", "could", "convincingly", "explain", "amount", "a/v", "conflict", "which", "increase", "disparity", "magnitude", "negative", "initial", "disparity", "divergent", "step", "work", "towards", "resolve", "conflict", "whereas", "convergent", "step", "work", "towards", "increase", "positive", "initial", "disparity", "role", "reverse", "larger", "magnitude", "initial", "disparity", "more", "stress", "put", "visual", "system", "demand", "resolve", "-lrb-", "increase", "-rrb-", "conflict", "higher", "thus", "larger", "discrepancy", "between", "convergent", "divergent", "step", "effect", "should", "take", "account", "while", "optimize", "stereoscopic", "content", "simple", "minimization", "disparity", "difference", "necessary", "lead", "shorter", "adaptation", "time", "another", "interesting", "finding", "fix", "target", "disparity", "adaptation", "time", "convergent", "step", "hardly", "dependent", "step", "magnitude", "phenomenon", "first", "unintuitive", "could", "explain", "a/v", "coupling", "well", "larger", "step", "magnitude", "which", "should", "intuitively", "contribute", "longer", "adaptation", "time", "may", "offset", "vary", "initial", "stress", "exert", "a/v", "conflict", "visual", "system", "we", "experiment", "we", "consider", "only", "computer", "display", "observe", "relatively", "short", "distance", "one", "hand", "larger", "view", "distance", "depth", "field", "increase", "thereby", "reduce", "importance", "a/v", "coupling", "hypothesize", "cause", "observe", "variation", "vergence", "adaptation", "time", "other", "hand", "discomfort", "induce", "step-like", "motion", "depth", "have", "be", "observe", "even", "disparity", "within", "dof", "-lsb-", "Yano", "et", "al.", "2004", "-rsb-", "answer", "question", "similar", "effect", "initial", "disparity", "adaptation", "time", "can", "observe", "other", "viewing", "condition", "e.", "g.", "cinema", "require", "further", "investigation", "3.3", "evaluation", "obtain", "model", "derive", "use", "simple", "stimulus", "-lrb-", "flat", "whitenoise", "pattern", "-rrb-", "one", "hand", "approach", "have", "several", "advantage", "exact", "disparity", "know", "regardless", "fixation", "point", "measurement", "can", "repeat", "easily", "learning", "effect", "reduce", "since", "subject", "have", "memory", "related", "spatial", "arrangement", "object", "case", "repeat", "image", "other", "hand", "unclear", "how", "well", "model", "predict", "response", "cut", "between", "natural", "image", "presence", "complex", "luminance", "pattern", "high-level", "process", "relate", "scene", "understanding", "may", "very", "well", "influence", "transition", "time", "therefore", "we", "conduct", "validation", "experiment", "test", "model", "can", "generalize", "participant", "stimulus", "four", "participant", "-lrb-", "s3", "s7", "s11", "s16", "-rrb-", "from", "original", "16", "be", "invite", "take", "part", "evaluation", "model", "six", "3d", "photograph", "take", "LG", "Optimus", "3d", "p725", "smartphone", "be", "use", "-lrb-", "see", "Fig.", "-rrb-", "be", "divide", "two", "group", "three", "one", "smaller", "other", "larger", "disparity", "change", "across", "picture", "disparity", "picture", "be", "estimate", "use", "sift", "flow", "algorithm", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "single", "trial", "6.5", "minute", "random", "sequence", "compose", "three", "photograph", "from", "one", "group", "show", "previously", "single", "appearance", "picture", "last", "between", "1.25", "2.5", "-lrb-", "choose", "randomly", "-rrb-", "be", "break", "between", "appearance", "task", "simply", "observe", "picture", "participant", "be", "ask", "perform", "one", "trial", "each", "group", "datum", "analysis", "result", "after", "cleaning", "segment", "tracking", "datum", "semi-automatic", "procedure", "employ", "group", "segment", "same", "type", "enable", "averaging", "measurement", "first", "automatic", "step", "segment", "where", "saccade", "occur", "time", "cut", "within", "first", "100", "m", "after", "cut", "be", "discard", "initial", "disparity", "estimate", "use", "disparity", "map", "fixation", "coordinate", "just", "before", "cut", "-lrb-", "initial", "fixation", "-rrb-", "target", "disparity", "choose", "use", "follow", "heuristic", "whenever", "duration", "first", "fixation", "shorter", "than", "300", "m", "second", "fixation", "use", "otherwise", "initial", "fixation", "assume", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "Modeling", "Optimizing", "Eye", "Vergence", "response", "Stereoscopic", "cut", "145:5", "90", "30", "30", "repetition", "repetition", "60", "-lsb-", "px", "-rsb-", "-60", "-60", "-lsb-", "px", "-rsb-", "30", "1000", "1000", "Vergence", "30", "30", "vergence", "-30", "10", "repetition", "30", "repetition", "ae", "ct", "-60", "-60", "-60", "1000", "1000", "-90", "time", "-lsb-", "m", "-rsb-", "time", "-lsb-", "m", "-rsb-", "figure", "result", "pilot", "experiment", "left", "panel", "we", "present", "average", "vergence", "response", "subject", "s7", "60", "px", "30", "px", "step", "after", "10", "30", "repetition", "curve", "after", "30", "repetition", "show", "together", "fit", "Gompertz", "function", "middle", "panel", "average", "vergence", "response", "all", "42", "possible", "disparity", "step", "subject", "s7", "show", "-lrb-", "dash", "line", "-rrb-", "together", "fitted", "curve", "-lrb-", "solid", "line", "-rrb-", "point", "where", "curve", "reach", "95", "vergence", "change", "-lrb-", "solid", "circle", "-rrb-", "right", "panel", "we", "plot", "transition", "time", "against", "initial", "target", "disparity", "point", "almost", "perfectly", "model", "two", "plane", "standard", "deviation", "error", "approximately", "27", "ms.", "two", "plane", "represent", "divergence", "-lrb-", "green", "-rrb-", "convergence", "-lrb-", "yellow", "-rrb-", "we", "leave", "gap", "between", "plane", "where", "time", "begin", "increase", "due", "panum?s", "fusional", "area", "tolerance", "visual", "system", "vergence", "error", "diagonal", "singularity", "where", "transition", "present", "because", "initial", "target", "disparity", "equal", "1250", "1250", "-lsb-", "m", "-rsb-", "1000", "-lsb-", "m", "-rsb-", "1000", "0.04", "1.96", "2.3", "3.49", "405.02", "308.72", "Time", "500", "750", "Time", "750 500 250 250", "90", "90", "30", "90", "30", "90", "-30", "-30", "30", "-30", "-30", "30", "initial", "disp", "-lsb-", "px", "-rsb-", "-90", "-90", "target", "disp", "-lsb-", "px", "-rsb-", "initial", "disp", "-lsb-", "px", "-rsb-", "-90", "-90", "target", "disp", "-lsb-", "px", "-rsb-", "figure", "result", "main", "experiment", "left", "panel", "we", "present", "fit", "all", "subject", "after", "exclusion", "outlier", "-lrb-", "subject", "s1", "s6", "s8", "s14", "-rrb-", "subject", "be", "exclude", "due", "serious", "difficulty", "correct", "fusion", "stimulus", "completeness", "we", "provide", "datum", "supplemental", "material", "middle", "panel", "show", "average", "all", "fit", "from", "left", "panel", "along", "equation", "plane", "plane", "describe", "transition", "time", "average", "observer", "right", "panel", "we", "show", "number", "diagonal", "section", "average", "plane", "each", "line", "represent", "disparity", "step", "same", "magnitude", "direction", "different", "initial", "disparity", "see", "text", "detailed", "discussion", "90", "60", "30", "600", "-lsb-", "m", "-rsb-", "400", "Time", "-30", "200", "-60", "90", "-90", "30", "90", "1000", "initial", "disparity", "-lsb-", "px", "-rsb-", "-30", "-90", "-90", "-30", "target", "30", "disparity", "-lsb-", "px", "-rsb-", "800", "+180", "px", "700", "+150", "px", "-lsb-", "m", "-rsb-", "600", "180", "px", "+120", "px", "Time", "500", "150", "px", "+90", "px", "120", "px", "400", "90", "px", "+60", "px", "300", "60", "px", "+30", "px", "30", "px", "200", "-90", "-60", "-30", "30", "60", "90", "initial", "disp", "-lsb-", "px", "-rsb-", "also", "target", "fixation", "second", "manual", "step", "all", "segment", "be", "briefly", "review", "correct", "filter", "target", "fixation", "error", "false", "negative", "be", "case", "when", "saccade", "near", "cut", "small", "enough", "change", "significantly", "vergence", "response", "false", "positive", "be", "non-typical", "case", "include", "limit", "eye-tracker", "error", "clearly", "incorrect", "vergence", "response", "indicate", "lack", "fusion", "segment", "unusually", "large", "saccade-to-fixation", "ratio", "erratic", "saccade", "indicate", "partial", "fixation", "etc.", "end", "718", "out", "3028", "segment", "be", "discard", "we", "provide", "all", "annotated", "segment", "along", "custom", "viewer/editor", "additional", "material", "encourage", "reader", "inspect", "datum", "we", "use", "evaluation", "end", "segment", "same", "initial/target", "disparity", "be", "group", "group", "more", "member", "be", "average", "compare", "against", "model", "prediction", "respective", "subject", "result", "experiment", "present", "Fig.", "discussion", "although", "we", "prediction", "slightly", "overestimate", "time", "transition", "photograph", "we", "model", "correlate", "well", "actual", "time", "indicate", "relatively", "low", "standard", "deviation", "error", "study", "prove", "we", "model", "good", "predictor", "-lrb-", "up", "additive", "constant", "-rrb-", "transition", "time", "natural", "image", "we", "hypothesize", "improve", "performance", "due", "presence", "higher-order", "cue", "absent", "white-noise", "stimulus", "where", "sole", "depth", "cue", "binocular", "disparity", "also", "possible", "transition", "facilitate", "some", "extent", "learning", "effect", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "145:6", "K.", "Templin", "et", "al.", "image", "image", "leave", "view", "disparity", "leave", "view", "variation", "disp", "small", "variation", "disp", "large", "figure", "two", "group", "stimulus", "use", "evaluation", "one", "larger", "one", "smaller", "disparity", "variation", "across", "picture", "black", "bar", "side", "float", "stereoscopic", "window", "add", "avoid", "frame", "violation", "large", "disparity", "step", "edge", "-lrb-", "shoe", "example", "-rrb-", "subject", "s16", "mean", "50.27", "std", "86.86", "subject", "s7", "mean", "5.02", "std", "62.48", "600 400 200", "-lsb-", "m", "-rsb-", "90", "30", "-30", "-90", "-90", "-30", "30", "90", "Time", "90", "30", "-30", "-90", "-90", "-30", "30", "90", "initial", "disp", "-lsb-", "px", "-rsb-", "target", "disp", "-lsb-", "px", "-rsb-", "initial", "disp", "-lsb-", "px", "-rsb-", "target", "disp", "-lsb-", "px", "-rsb-", "Figure", "result", "subject", "s16", "-lrb-", "left", "-rrb-", "s7", "-lrb-", "right", "-rrb-", "plot", "subject", "s3", "s11", "provide", "supplemental", "material", "plane", "show", "model", "prediction", "whereas", "solid", "circle", "represent", "observe", "datum", "mean", "standard", "deviation", "error", "subject", "s16", "s7", "s3", "s11", "respectively", "50", "87", "m", "62", "m", "107", "90", "m", "61", "84", "ms.", "application", "section", "we", "propose", "set", "tool", "aid", "production", "stereoscopic", "content", "utilize", "we", "model", "minimize", "vergence", "adaptation", "time", "we", "also", "analyze", "impact", "minimization", "visual", "quality", "one", "propose", "tool", "use", "object-recognition", "experiment", "4.1", "production", "tool", "transition", "Time", "visualization", "straightforward", "application", "model", "visualization", "tool", "provide", "stereographer", "vfx", "artist", "interactive", "analysis", "transition", "time", "order", "evaluate", "stereoscopic", "transition", "estimate", "transition", "time", "we", "first", "need", "determine", "pair", "disparity", "value", "between", "which", "transition", "occur", "na?ve", "approach", "would", "measure", "transition", "time", "between", "corresponding", "pixel", "both", "sequence", "however", "very", "useful", "most", "case", "people", "change", "fixation", "point", "immediately", "after", "transition", "change", "vergence", "happen", "-lrb-", "see", "datum", "browser", "provide", "supplemental", "material", "-rrb-", "therefore", "fixation", "point", "both", "sequence", "need", "precisely", "determine", "datum", "can", "obtain", "from", "various", "source", "e.", "g.", "possible", "use", "eye-tracker", "datum", "do", "require", "many", "subject", "have", "be", "show", "eye", "scan-paths", "form", "highly", "repetitive", "pattern", "between", "different", "spectator", "same", "video", "sequence", "-lsb-", "Wang", "et", "al.", "2012", "-rsb-", "moreover", "skilled", "director", "capable", "precisely", "guide", "predict", "viewer", "attention", "prediction", "further", "facilitate", "tendency", "increase", "object", "motion", "modern", "movie", "-lsb-", "cut", "et", "al.", "2011", "-rsb-", "fact", "typical", "2d-movie", "cut", "trigger", "saccade", "towards", "screen", "center", "-lsb-", "Mital", "et", "al.", "2011", "Wang", "et", "al.", "2012", "-rsb-", "thus", "information", "about", "fixation", "point", "we", "method", "can", "very", "reliably", "provide", "director", "besides", "Carmi", "Itti", "-lsb-", "2006", "-rsb-", "observe", "saccade", "immediately", "after", "cut", "drive", "mostly", "bottom-up", "factor", "can", "predict", "relatively", "well", "exist", "saliency", "model", "once", "fixation", "point", "before", "after", "cut", "know", "corresponding", "disparity", "value", "need", "determine", "can", "obtain", "directly", "from", "render", "pipeline", "animated", "movie", "use", "user", "input", "case", "2d-to-3d", "conversion", "use", "disparity", "estimation", "technique", "natural", "scene", "when", "depth", "map", "available", "once", "fixation", "point", "along", "disparity", "value", "know", "transition", "time", "can", "directly", "calculate", "from", "model", "since", "compute", "model", "prediction", "inexpensive", "can", "use", "provide", "real-time", "preview", "transition", "time", "camera", "parameter", "Optimization", "apart", "from", "predict", "transition", "time", "visualize", "they", "editing", "purpose", "one", "can", "automate", "process", "stereoscopic", "content", "preparation", "optimization", "problem", "cut", "can", "define", "we", "model", "can", "serve", "core", "cost", "function", "discuss", "Sec", "stereoscopic", "content", "can", "optimize", "manipulate", "various", "parameter", "can", "change", "entire", "sequence", "-lrb-", "e.", "g.", "from", "cut", "cut", "-rrb-", "selectively", "around", "cut", "smooth", "blend", "back", "original", "parameter", "-lsb-", "Lang", "et", "al.", "2010", "Koppal", "et", "al.", "2011", "-rsb-", "wide", "range", "manipulation", "can", "use", "adjust", "stereoscopic", "content", "range", "from", "very", "simple", "one", "like", "change", "camera", "separation", "convergence", "-lrb-", "i.", "e.", "plane", "zero", "parallax", "-rrb-", "more", "complicated", "one", "depth", "remapping", "all", "manipulation", "can", "easily", "integrate", "use", "we", "model", "Cut", "Positioning", "two", "sequence", "between", "which", "cut", "occur", "overlap", "time", "also", "possible", "find", "best", "moment", "cut", "end", "we", "optimize", "only", "stereoscopic", "parameter", "also", "position", "cut", "can", "perform", "efficiently", "simply", "iterate", "over", "all", "possible", "cut", "position", "addition", "all", "horizontal", "shift", "left/right", "view", "optimal", "cut", "can", "choose", "automatically", "can", "show", "editor", "suggestion", "design", "tool", "perform", "task", "show", "fig.", "right", "supplemental", "video", "image", "disparity", "leave", "view", "disparity", "-120", "px", "120", "px", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "cut", "145:7", "4.2", "impact", "visual", "quality", "can", "define", "many", "way", "use", "various", "objective", "subjective", "criterion", "follow", "experiment", "we", "focus", "time", "necessary", "recognize", "3d", "arrangement", "object", "after", "cut", "we", "assume", "shorter", "recognition", "time", "indicator", "higher", "quality", "we", "measure", "time", "need", "recognize", "object", "arrangement", "show", "time", "closely", "match", "we", "model", "practice", "mean", "when", "cut", "optimize", "use", "propose", "production", "tool", "time", "necessary", "recognize", "object", "scene", "minimize", "method", "equipment", "viewing", "condition", "be", "same", "other", "experiment", "eye-tracker", "use", "stimulus", "we", "use", "two", "shot", "correspond", "cut", "3d", "version", "big", "Buck", "Bunny", "animation", "we", "modify", "they", "place", "two", "small", "darkgray", "circle", "between", "eye", "character", "approximately", "same", "disparity", "character", "-lrb-", "see", "Fig.", "leave", "inset", "-rrb-", "two", "3d", "arrangement", "circle", "each", "shot", "be", "consider", "one", "upper", "one", "lower", "circle", "closer", "observer", "disparity", "difference", "between", "circle", "px", "convergence", "shot", "modify", "so", "average", "disparity", "circle", "equal", "before", "after", "cut", "seven", "pair", "disparity", "step", "be", "use", "75", "105/90", "60", "90", "30", "30", "90/60", "30/30", "30", "60/90", "60", "30/90", "75", "90/105", "px", "each", "initial", "disparity", "both", "convergent", "divergent", "step", "possible", "which", "prevent", "anticipatory", "eye", "movement", "subject", "order", "determine", "arrangement", "recognition", "time", "all", "14", "step", "we", "perform", "14", "independent", "quest", "threshold", "estimation", "procedure", "-lsb-", "Watson", "Pelli", "1983", "-rsb-", "each", "estimate", "time", "75", "correctness", "single", "trial", "each", "procedure", "have", "follow", "structure", "First", "first", "shot", "show", "s.", "Next", "second", "shot", "show", "between", "0.1", "1.5", "-lrb-", "control", "quest", "-rrb-", "arrangement", "circle", "choose", "randomly", "every", "trial", "after", "screen", "blank", "subject", "ask", "indicate", "arrangement", "same", "both", "shot", "same", "circle", "-lrb-", "i.", "e.", "upper", "lower", "-rrb-", "closer", "observer", "both", "before", "after", "cut", "subject", "have", "press", "key", "key", "otherwise", "task", "definition", "ensure", "subject", "actually", "perform", "vergence", "transition", "all", "14", "procedure", "be", "perform", "parallel", "randomly", "interleave", "session", "experiment", "last", "20", "min", "-lrb-", "average", "standard", "deviation", "quest", "instance", "73", "m", "-rrb-", "subject", "s3", "s11", "s12", "s15", "s16", "take", "part", "experiment", "s11", "participate", "three", "session", "s16", "two", "remain", "three", "one", "session", "result", "datum", "obtain", "use", "above", "procedure", "fit", "two", "plane", "minimize", "rmse", "plane", "obtain", "from", "all", "subject", "be", "average", "-lrb-", "first", "within", "subject", "between", "subject", "-rrb-", "compare", "average", "model", "result", "present", "fig.", "corrective", "constant", "shift", "83", "m", "minimize", "rmse", "yield", "low", "prediction", "error", "42", "ms.", "correlation", "imply", "optimize", "camera", "convergence", "use", "we", "model", "instead", "disparity", "distance", "cost", "function", "produce", "cut", "shorter", "recognition", "time", "similar", "improvement", "can", "expect", "when", "optimize", "other", "camera", "parameter", "cut", "position", "illustrate", "practical", "importance", "we", "model", "s3d", "game", "film", "conclusion", "we", "propose", "new", "model", "which", "predict", "time", "human", "observer", "need", "adapt", "vergence", "rapid", "disparity", "change", "we", "first", "present", "measurement", "transition", "time", "simple", "stimulus", "demonstrate", "time", "valid", "also", "complex", "scene", "experiment", "reveal", "interesting", "fact", "about", "viewer", "behavior", "during", "Figure", "leave", "stimulus", "use", "object", "recognition", "experiment", "right", "result", "experiment", "gray", "plane", "represent", "rmse", "scene", "cut", "which", "provide", "valuable", "knowledge", "stereoscopic", "content", "creator", "additionally", "we", "propose", "set", "tool", "editing", "stereoscopic", "content", "minimize", "vergence", "adaptation", "time", "after", "cut", "important", "property", "propose", "optimization", "technique", "manipulation", "apply", "only", "locally", "around", "cut", "which", "have", "limit", "effect", "depth", "impression", "create", "artist", "we", "knowledge", "first", "work", "propose", "automatically", "edit", "stereoscopic", "cut", "take", "account", "vary", "performance", "human", "visual", "system", "adapt", "rapid", "disparity", "change", "finally", "we", "demonstrate", "impact", "minimize", "adaptation", "time", "visual", "quality", "s3d", "content", "measure", "subject?s", "performance", "3d", "object", "recognition", "task", "interesting", "avenue", "future", "work", "would", "extensive", "user", "study", "quantify", "how", "shorter", "transition", "time", "influence", "visual", "fatigue", "acknowledgment", "we", "would", "like", "thank", "Aude", "Oliva", "Lavanya", "Sharan", "Zoya", "Bylinskii", "Sylvain", "Paris", "YiChang", "Shih", "Tobias", "Ritschel", "Katarina", "Struckmann", "David", "Levin", "anonymous", "subject", "who", "take", "part", "we", "perceptual", "study", "work", "partially", "support", "nsf", "iis1111415", "nsf", "iis-1116296", "reference", "lvarez", "T.", "L.", "emmlow", "J.", "L.", "edrono", "C.", "2005", "divergence", "eye", "movement", "dependent", "initial", "stimulus", "position", "Vis", "Res", "45", "14", "1847", "55", "ernhard", "M.", "ELLMOUR", "C.", "ECHER", "M.", "tavraki", "E.", "IMMER", "M.", "2014", "effect", "fast", "disparity", "adjustment", "gaze-controlled", "stereoscopic", "application", "Proc", "ETRA", "appear", "ampbell", "F.", "W.", "ESTHEIMER", "G.", "1959", "factor", "influence", "accommodation", "response", "human", "eye", "J.", "Opt", "soc", "be", "49", "568", "71", "armus", "R.", "tti", "L.", "2006", "visual", "cause", "versus", "correlate", "attentional", "selection", "dynamic", "scene", "Vis", "Res", "46", "26", "4333", "45", "utting", "J.", "RUNICK", "K.", "ELONG", "J.", "ricinschi", "C.", "andan", "a.", "2011", "quicker", "faster", "darker", "change", "hollywood", "film", "over", "75", "year", "i-perception", "569", "76", "D.", "2013", "ACM", "Trans", "s.-p.", "asia", "B.", "S.-M.", "UTIERREZ", "metric", "visual", "comfort", "stereoscopic", "motion", "222", "graph", "32", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014", "145:8", "K.", "Templin", "et", "al.", "adie", "a.", "S.", "RAY", "L.", "S.", "ARLIN", "P.", "ILLIAMS", "M.", "2000", "model", "adaptation", "effect", "vergence", "accommodation", "after", "exposure", "simulated", "virtual", "reality", "stimulus", "ophthalmic", "physiol", "opt", "20", "242", "51", "rkelen", "C.", "J.", "DER", "teen", "J.", "teinman", "R.", "M.", "OLLEWIJN", "H.", "1989", "ocular", "vergence", "under", "natural", "condition", "II", "gaze-shift", "between", "real", "target", "differ", "distance", "direction", "Proc", "Royal", "Soc.", "441", "inke", "R.", "1989", "principle", "mental", "imagery", "MIT", "Press", "einzle", "S.", "REISEN", "P.", "ALLUP", "D.", "HEN", "C.", "aner", "D.", "molic", "a.", "urg", "a.", "atusik", "W.", "ross", "M.", "H.", "2011", "computational", "stereo", "camera", "system", "programmable", "control", "loop", "ACM", "Trans", "graph", "30", "94", "OFFMAN", "D.", "IRSHICK", "a.", "keley", "K.", "ANKS", "M.", "2008", "vergence-accommodation", "conflict", "hinder", "visual", "performance", "cause", "visual", "fatigue", "J.", "Vision", "30", "ung", "G.", "K.", "IUFFREDA", "K.", "J.", "emmlow", "J.", "L.", "j.-l", "1994", "vergence", "eye", "movement", "under", "natural", "viewing", "condition", "invest", "ophthalmol", "Vis", "Sci", "35", "3486", "92", "ung", "G.", "K.", "1992", "adaptation", "model", "accommodation", "vergence", "ophthalmic", "physiol", "opt", "12", "319", "26", "ung", "G.", "K.", "1998", "Dynamic", "model", "vergence", "eye", "movement", "system", "simulation", "use", "Matlab/Simulink", "computer", "method", "program", "Biomedicine", "55", "59", "68", "ung", "G.", "K.", "2001", "model", "oculomotor", "control", "World", "Scientific", "Publishing", "Singapore", "OPPAL", "S.", "J.", "ITNICK", "C.", "L.", "OHEN", "M.", "ang", "S.", "B.", "ESSLER", "B.", "olburn", "a.", "2011", "viewer-centric", "editor", "3d", "movie", "IEEE", "Comput", "graph", "appl", "mag", "31", "20", "RISHNAN", "V.", "arazian", "F.", "TARK", "L.", "1973", "analysis", "latency", "prediction", "fusional", "vergence", "system", "be", "J.", "Optometry", "Arch", "be", "academy", "Optometry", "50", "933", "RISHNAN", "V.", "arazian", "F.", "TARK", "L.", "1977", "Dynamic", "measure", "vergence", "accommodation", "American", "Journal", "Optometrics", "physiological", "optics", "54", "470", "ambooij", "M.", "IJ", "SSELSTEIJN", "W.", "ortuin", "m.", "eyn", "derickx", "i.", "2009", "visual", "discomfort", "visual", "fatigue", "stereoscopic", "display", "review", "J.", "Imaging", "Sci", "Technol", "53", "ambooij", "M.", "IJ", "SSELSTEIJN", "W.", "eynderickx", "i.", "2011", "visual", "discomfort", "3d", "tv", "Assessment", "method", "modeling", "display", "32", "209", "18", "visual", "image", "safety", "ang", "m.", "ornung", "a.", "ang", "O.", "oulako", "S.", "molic", "a.", "ross", "M.", "2010", "nonlinear", "disparity", "mapping", "stereoscopic", "3d", "ACM", "Trans", "graph", "29", "75", "iu", "C.", "uen", "J.", "orralba", "a.", "2011", "sift", "flow", "dense", "correspondence", "across", "scene", "its", "application", "pattern", "analysis", "machine", "Intelligence", "IEEE", "transaction", "33", "978", "94", "eester", "L.", "IJ", "SSELSTEIJN", "W.", "EUNTIENS", "P.", "2004", "survey", "perceptual", "evaluation", "requirement", "threedimensional", "tv", "circuit", "Systems", "Video", "Technology", "IEEE", "transaction", "14", "381", "91", "endiburu", "B.", "2009", "3d", "movie", "make", "Stereoscopic", "Digital", "Cinema", "from", "Script", "Screen", "Focal", "Press", "ital", "P.", "mith", "T.", "ILL", "R.", "ENDERSON", "J.", "2011", "clustering", "gaze", "during", "dynamic", "scene", "viewing", "predict", "motion", "cognitive", "computation", "24", "kuyama", "F.", "1998", "human", "visual", "accommodation", "vergence", "eye", "movement", "while", "view", "stereoscopic", "display", "actual", "target", "Proc", "IEEE", "Eng", "Med", "Biol", "Society", "vol", "552", "skam", "t.", "ornung", "a.", "owle", "H.", "ITCHELL", "K.", "ross", "M.", "H.", "2011", "oscam-optimized", "stereoscopic", "camera", "control", "interactive", "3d", "ACM", "Trans", "graph", "30", "189", "wen", "C.", "2013", "invite", "talk", "2nd", "Toronto", "International", "Stereoscopic", "3D", "Conference", "ushton", "S.", "K.", "iddell", "P.", "M.", "1999", "develop", "visual", "system", "exposure", "virtual", "reality", "stereo", "display", "some", "concern", "speculation", "about", "demand", "accommodation", "vergence", "Applied", "Ergonomics", "30", "69", "78", "chor", "C.", "M.", "1979", "relationship", "between", "fusional", "vergence", "eye", "movement", "fixation", "disparity", "Vis", "Res", "19", "12", "1359", "67", "chor", "C.", "M.", "1992", "relationship", "between", "fusional", "vergence", "eye", "movement", "fixation", "disparity", "Optometry", "Vision", "Science", "69", "258", "69", "chor", "C.", "1999", "influence", "interaction", "between", "accommodation", "convergence", "lag", "accommodation", "ophthalmic", "physiol", "opt", "19", "134", "50", "emmlow", "J.", "ETZEL", "P.", "1979", "Dynamic", "contribution", "component", "binocular", "vergence", "josa", "69", "639", "45", "emmlow", "J.", "UNG", "G.", "IUFFREDA", "K.", "1986", "quantitative", "assessment", "disparity", "vergence", "component", "invest", "ophthalmol", "Vis", "Sci", "27", "558", "64", "hiba", "T.", "IM", "J.", "OFFMAN", "D.", "M.", "ANKS", "M.", "S.", "2011", "zone", "comfort", "predict", "visual", "discomfort", "stereo", "display", "J.", "Vision", "11", "11", "AM", "W.", "J.", "peranza", "F.", "ZQUEZ", "C.", "ENAUD", "R.", "ur", "N.", "2012", "visual", "comfort", "stereoscopic", "object", "move", "horizontal", "mid-sagittal", "plane", "Proc", "SPIE", "A.", "J.", "Woods", "N.", "S.", "Holliman", "G.", "E.", "Favalora", "Eds.", "8288:13", "KAI", "K.", "ATO", "Y.", "2002", "use", "video", "refraction", "measure", "dynamic", "property", "near", "triad", "observer", "3-d", "display", "ophthalmic", "physiol", "opt", "22", "385", "ang", "H.", "X.", "reeman", "J.", "ERRIAM", "E.", "P.", "ASSON", "U.", "eeger", "D.", "J.", "2012", "temporal", "eye", "movement", "strategy", "during", "naturalistic", "viewing", "J.", "Vision", "12", "16", "ATSON", "A.", "B.", "ELLI", "D.", "G.", "1983", "quest", "bayesian", "adaptive", "psychometric", "method", "perception", "psychophysic", "33", "113", "20", "ANO", "S.", "MOTO", "M.", "ITSUHASHI", "T.", "2004", "two", "factor", "visual", "fatigue", "cause", "stereoscopic", "hdtv", "image", "display", "25", "-lrb-", "Nov.", "-rrb-", "141", "50", "illy", "F.", "LUGER", "J.", "AUFF", "P.", "2011", "production", "rule", "stereo", "acquisition", "Proc", "IEEE", "99", "590", "606", "wicker", "m.", "atusik", "W.", "URAND", "F.", "fister", "H.", "orline", "c.", "2006", "antialiase", "automultiscopic", "3d", "display", "Proc", "egsr", "73", "82", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "145", "publication", "date", "July", "2014" ],
  "content" : "\n  \n    81986706cb3a9ea04d5b42b1bd6c9a2e1e2623b8623893ea97d963afa8daef7a\n    p77\n    10.1145/2601097.2601148\n    Name identification was not possible. \n  \n  \n    \n      \n        Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts\n      \n      Krzysztof Templin 1,2 Piotr Didyk 1 Karol Myszkowski 2 Mohamed M. Hefeeda 3 Hans-Peter Seidel 2 Wojciech Matusik 1 1 MIT CSAIL 2 MPI Informatik\n      \n        \n        Figure 1: Unexpected changes of disparity in stereoscopic 3D imagery, such as those introduced by cuts in S3D films, are challenging for the audience. Using an eye-tracker, we recorded eye vergence responses of 16 subjects to step-like changes in disparity (left). Then, a model of adaptation time was derived, and parameters for the average observer estimated. Having such a predictor enables, for example, optimization of film editing operations to best match the human depth adaptation abilities (right). The colors of the lines connecting points of interest before and after the cut visualize the corresponding vergence adaptation times. Pictures from Dracula 4D courtesy of Red Star 3D, www.redstar3d.com\n      \n      Sudden temporal depth changes, such as cuts that are introduced by video edits, can significantly degrade the quality of stereoscopic content. Since usually not encountered in the real world, they are very challenging for the audience. This is because the eye vergence has to constantly adapt to new disparities in spite of conflicting accommodation requirements. Such rapid disparity changes may lead to confusion, reduced understanding of the scene, and overall attractiveness of the content. In most cases the problem cannot be solved by simply matching the depth around the transition, as this would require flattening the scene completely. To better understand this limitation of the human visual system, we conducted a series of eye-tracking experiments. The data obtained allowed us to derive and evaluate a model describing adaptation of vergence to disparity changes on a stereoscopic display. Besides computing user-specific models, we also estimated parameters of an average observer model. This enables a range of strategies for minimizing the adaptation time in the audience.\n    \n    \n      CR Categories: I.3.3 [Computer Graphics]: Picture/Image generation?display algorithms,viewing algorithms;\n      Keywords: binocular, eye-tracking, S3D\n      Links:\n      \n        \n      \n      DL PDF W\n      \n        \n        \n      \n      EB\n      ACM Reference Format Templin, K., Didyk, P., Myszkowski, K., Hefeeda, M., Seidel, H., Matusik, W. 2014. Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts. ACM Trans. Graph. 33, 4, Article 145 (July 2014), 8 pages. DOI = 10.1145/2601097.2601148 http://doi.acm.org/10.1145/2601097.2601148. Copyright Notice Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . 2014 Copyright held by the Owner/Author. Publication rights licensed to ACM. 0730-0301/14/07-ART145 $15.00. DOI: http://dx.doi.org/10.1145/2601097.2601148\n      3 Qatar Computing Research Institute\n      \n        1 Introduction\n      \n      Over the past few years, stereoscopic 3D (S3D) technology has been constantly developing, and by now it has become ubiquitous. However, despite the significant improvements, not only in display devices, but also in image generation, capture and post-processing techniques, many consumers are still skeptical about the quality of current S3D content and the future of the technology itself. These concerns are usually related to naturalness, effortlessness, and overall appearance: S3D effect should not be a distraction. The difficulty in S3D production is that it is not sufficient to produce two good images in place of one to arrive at a good stereoscopic effect [Zilly et al. 2011]. S3D is a strong illusion, since it isolates only one real-world phenomenon, failing to reproduce many others, a prominent example being the accommodation cue. This imposes numerous restrictions on the production process: the depth range and variation must not be too large, view-dependent effects need to be handled correctly, images carefully registered, and so on. In this work, we are concerned with rapid temporal changes of disparity. Humans have a good understanding of the environment they observe and move through, a so-called ?mental image?, which enhances their capabilities in focusing on different objects [Finke 1989]. However, when the scene is merely a sequence of shots shown on a flat screen, it is easy to get confused or lose track of the point of interest, due to, among other things, unexpected changes of the location or the camera angle. Although less problematic in 2D, this can be challenging in stereoscopic 3D. In this context, an unpredictable and large change in disparity means that binocular fusion is lost, and a confusing double image is seen (diplopia). Moreover, the vergence system needs to quickly adapt to new conditions, in spite of the conflicting goal of the interconnected accommodation system. This has been identified as one of the sources of discomfort in stereoscopic viewing [Hoffman et al. 2008; Lambooij et al. 2009]. The Hollywood style of combining shots developed into a set of formal conventions that obey the dynamics of visual attention and control the continuity of space, time, and action. In modern movies cuts play the most important role (99% of all edits), while dissolves  and wipes have vanished almost completely. An extensive analysis by Cutting et al. [2011] shows that average shot duration over past 75 years has declined from ca. 15 s to ca. 3.5 s! Clearly, short shots increase the viewer engagement by forcing eyes to quickly follow newly appearing content. However, such accumulation of sharp cuts challenges the visual system by requiring seamless adjustment of vergence between many shots over a possibly wide range of depths. This requires a different approach to editing, e. g., ultra-short ?MTVstyle? shots need to be replaced by more slow-paced edits. Nevertheless, modern movies are often simultaneously released in 2D and S3D, and one should not expect that directors, cinematographers, and editors will entirely give up on their artistic visions and style merely for the sake of S3D medium limitations. Instead, they apply different S3D post-production techniques to make depth transitions natural and effortless for viewers. Such manipulations range from simple depth manipulations and cross-dissolve types of cuts to more sophisticated transitions, where multiple sequences with gradually changing depth are combined [Owens 2013]. All these manipulations are time-consuming and expensive, as they are performed manually. For example, Owens [2013] pointed out that the editing of transitions was one of the most challenging steps in the post-production of the U2 concert recorded in stereoscopic 3D. Abrupt depth changes, well beyond the real-world experience, should be also expected in action computer games. To address the problem of rapid depth changes, we propose to relate the transition quality to vergence adaptation time, instead of simpler disparity difference. We present a series of experiments with human observers, in which vergence responses were measured using consumer S3D equipment and a high-framerate eye-tracker. This leads to a simple model describing the vergence adaptation curve, given the initial and target disparities. The model allows for prediction of adaptation time after cuts, which facilitates its visualization and minimization. Impact of the optimization on the visual quality of S3D content is demonstrated in a separate experiment. To our knowledge, we are the first to apply such a principled approach to this problem. In summary, we make the following contributions: ? measurements of vergence response to rapid disparity changes defined by initial and target disparities; ? derivation and evaluation of a model relating disparity change to vergence adaptation curve, along with average observer parameters; ? interactive tool for visualization and minimization of adaptation time.\n      ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n      145:2\n      ?\n      K. Templin et al.\n      \n        2 Related Work\n        Here we overview basic findings on the eye vergence mechanisms, with the main focus on S3D display conditions. We refer the reader to a survey by Meesters et al. [2004] for an in-depth discussion of other aspects of S3D display perception.  Vergence as a Dynamic Process The eye vergence is driven by the depth changes of a target object, and can be performed with high accuracy (error below 10 arcmin) both in the real world and S3D display observation conditions [Okuyama 1998]. Other factors, such as blur, proximity, target size, and luminance might affect vergence, but to a lesser extent [Campbell and Westheimer 1959]. Vergence is a relatively slow process when compared to other eye movements, e. g., saccades (below 60 ms), and requires about 195?750 ms for convergence and 240?1000 ms for divergence. Vergence latency also demonstrates a similar asymmetric behavior with 180?250 ms for convergence and 200?210 ms for divergence [Krishnan et al. 1973; Krishnan et al. 1977; Semmlow and Wetzel 1979]. Vergence is a two-stage process, where at first the fast transient (a.k.a. phasic) mechanism (reacts even for brief 200 ms flashes) brings the vergence in the proximity of the target depth, and then the slower sustained (a.k.a. tonic) mechanism is responsible for the precise verging on the target, as well as further tracking of slower depth changes. Semmlow et al. [1986] found that for less dynamic depth changes, with the ramp velocity below 2 deg/s, only the sustained mechanism is active, above 9 deg/s the transient mechanism dominates, and otherwise both mechanisms are active. Vergence adaptation (similar to luminance adaptation) has been observed in which the sustained mechanism supports a given eye vergence angle, and comfort state is achieved during binocular vision [Hung 1992]. For small depth changes within Panum?s fusional area, the motoric vergence is not activated, and sensoric fusion of images on the retina is sufficient. Vergence vs. Accommodation While vergence is driven by depth, and accommodation is driven mostly by retinal blur, both systems are reflexively coupled, and they interact with each other through accommodative vergence and vergence accommodation [Hung 2001]. The accommodative vergence is quantified by the AC/A ratio, which relates the change of vergence caused by the change of accommodation in the absence of disparity. In an analogous way, the vergence accommodation is quantified by the CA/C ratio in the absence of blur. Since the range of accommodation while viewing the S3D display is determined by the distance to the screen, unnatural decoupling of vergence and accommodation is required, which may cause visual discomfort and increase binocular fusion times [Hoffman et al. 2008; Lambooij et al. 2009]. When the screen disparity increases beyond Panum?s fusional area, vergence eye movements bring the disparity back to this area, which shifts accommodation away from the screen. When such a shift is beyond the depth of focus (DOF) zone, the accommodative-vergence feedback is activated to counteract the loss of sharp vision, which in turn directs vergence back towards the display [Lambooij et al. 2009]. The range of vergence angles that assure clear and single binocular vision is known as the ?comfort zone? [Shibata et al. 2011; Zilly et al. 2011]. In the real world, objects away from the fixation point are perceived as blurred, which reduces the visibility of diplopia, because the limits of fusion are higher for low spatial frequencies. Thus, both accommodation and vergence response can be improved by manipulation of convergence and local image defocus, respectively [Ukai and Kato 2002; Zwicker et al. 2006]. In many practical S3D applications, the comfort zone is determined by the disparity range of 70 arcmin [Lambooij et al. 2009; Zilly et al. 2011]. Since it is a rather conservative bound, in this work we assume a wider range of ?2.5 deg. Out-of-screen effects even beyond this range are used in cinematography, but the object of interest typically moves steadily off the screen in such cases, so that the viewer can adapt to its extreme position [Zilly et al. 2011]. Achieving such extreme disparities would not be possible through sudden jumps as in the case of scene cuts. Vergence Measurements There is a large body of research on measurements of vergence dynamics in response to pulse, step, ramp, and sinusoidal disparity changes. For us, the step-like changes are the most relevant. Most experiments used physical targets or passively-shifted screens [Erkelens et al. 1989; Hung et al. 1994]. Simple stimuli, such as vertical lines, were used to eliminate other cues that could affect vergence. Special care was taken to suppress accommodation by using pinhole apertures for blur-free viewing. A wide range of disparities ?35 deg have been considered [Erkelens et al. 1989], but a typical range was below ?10 deg with relatively large step amplitudes, typically larger than 2 deg [Hung 2001]. In this work we focus on the disparity range ?2.5 deg and lower disparity step amplitudes, which are important for comfortable experience while viewing S3D displays. The assumed disparity range corresponds approximately to the comfort zone in desktop viewing conditions given by Shibata et al. [2011, Fig. 23]. By using an off-the-shelf S3D display in our experiments, and dealing with real-world images in the validation step, we ensure that the conditions are possibly similar to the ones in expected applications, where accommodation and pictorial cues may affect the vergence. Also, the initial disparity magnitude is important in our measurements, both for the convergence and divergence case. Vergence Modeling Schor [1979] and Hung [1998] proposed sophisticated models of the eye vergence dynamics, which employ the concepts of control engineering to simulate the transient and sustained (a negative feedback loop) mechanisms. The models have been extended to handle accommodation as well as the AC/A and CA/C cross-link gains [Schor 1992; Schor 1999; Hung 2001]. An extensive validation of such models against measurement data has been performed; however, disparity steps interesting for us have been treated marginally. Furthermore, the viewing conditions did not force decoupling of accommodation and vergence. While S3D displays have been considered in some computational models, the main goal was to artificially alter the link between the accommodation and vergence systems to study the change in pre-task and post-task measures of AC/A and CA/C [Eadie et al. 2000], or to investigate developmental plasticity in children exposed to S3D games [Rushton and Riddell 1999]. Alvarez et al. [2005] experimented with constant-sized, 4 deg steps, and found that in case of divergent steps, vergence dynamics are dependent on the initial disparity. In this work, we propose a simple data-driven model of eye vergence that is tuned to step-like disparity changes. We emphasize here on vergence dynamics as a function of the initial and target disparities, and our goal is minimization of the vergence adaptation time at scene cuts through disparity editing. Temporal Changes vs. Comfort Yano et al. [2004] report that visual discomfort was induced if images were moved in depth according to a step pulse function, even if the images were displayed within the depth of focus. In a related work by Tam et al. [2012], influence of disparity and velocity on visual comfort was investigated, and a significant interaction between velocity and disparity was shown. The negative effect of object velocity on visual comfort was apparent even when the objects were displayed within the generally accepted visual comfort zone of less than 1 deg of horizontal disparity. Results obtained by Lambooij et al. [2011] show that rapidly moving objects and changing screen disparity indeed have a significant effect on visual comfort; however, their dominant role was not confirmed. S3D Content Processing The problem of scene transitions is challenging in the context of stereoscopic content, since scene transitions often create large temporal disparity discontinuities leading to visual discomfort. To solve this problem, disparity adjustment techniques are required. They are performed either during the acquisition step by modifying camera parameters or in the post-processing step using, for example, horizontal image translation [Mendiburu 2009]. However, only few techniques can deal with temporal effects. Disparity velocity was considered one of the important factors for disparity adjustment [Lang et al. 2010]. The authors proposed to interpolate between different disparity ranges at scene cuts to reduce large discontinuities in disparity. To this end, different disparity mapping operators can be used to make this adjustment; however, the decision of how this interpolation is defined was left to the user. A simpler technique has been proposed by Koppal [2011]. He suggested to solve the problem of transitions by cross-fading the horizontal image translation to zero at the cut. Bernhard et al. [2014] showed how binocular fusion times can be reduced by means of active manipulation of the convergence plane. The object of interest is brought back to the zero-disparity plane once the change in gaze has been detected, but before the vergence adaptation is complete. In contrast to Bernhard et al.?s active approach, we propose a cut optimization process that keeps the disparities constant during the vergence adaptation. The improvement in our case comes from more informed choice of the initial and target disparities. Nevertheless, both approaches could be potentially combined. Heinzle et al. [2011] proposed a computational camera rig, which enables intuitive control over camera parameters. The artist?s involvement is still needed, though, to design the transitions manually, without any feedback on human abilities to adapt to rapid disparity changes. Automatic control over camera parameters was proposed in the context of real-time systems (e. g., games) [Oskam et al. 2011]. However, their primary goal was to maintain the scene disparity range within given limits. This is not equivalent to minimizing the vergence adaptation time, which depends not only on disparity difference but also the initial disparity value. In our approach, we take those two factors into account. More recently, a metric of visual comfort has been proposed [Du et al. 2013], which directly addresses the problem of temporal disparity changes. The authors also suggest that it can be used for optimizing stereoscopic parameters. However, their metric deals with motion, and it is unclear how to apply their technique in the context of rapid disparity changes such as those created during scene transitions.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n        Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts\n        ?\n        145:3\n      \n      \n        3 Model Derivation\n        In this section, we experimentally derive and evaluate a model of eye vergence response to step-like changes in disparity. We also estimate model parameters for an average observer. The collected data is useful in a number of applications, as shown in Sec. 4.  Participants Sixteen subjects (8 F, 8 M) took part in our experiment. They were members of computer graphics and computer vision groups, between 21 and 35 years old. All had normal or corrected-to-normal vision, and all passed a test for stereo-blindness. Equipment Stimuli were presented using an Nvidia 3D Vision 2 kit and an Acer GD235HZ 23.6-inch screen with native resolution of 1920 ? 1080. In order to measure the vergence responses, both eyes were tracked using an EyeLink 1000 Plus eye tracker with a desktop mount. The tracker records 1000 samples per second (500 per eye), allowing for fine-scale analysis of the vergence response. The spatial accuracy according to the eye-tracker manufacturer is up to 0.25?0.5 deg. A chin-rest was used to stabilize the subject?s head, and the viewing distance was fixed to 55 cm. Stimulus The stimulus in our experiment was a low-pass filtered white-noise patch changing its disparity in discrete steps over time. The patch was presented centrally on the screen, on a neutral grey background, and it subtended ca. 11 degrees of visual angle. A single trial consisted of a sequence of disparities d 1 , d 2 , . . . , d n , chosen from a fixed set D. The ordering of the disparities was randomized to avoid learning effect, but only Eulerian paths were used, i. e., d 1 = d n , and every possible transition appeared exactly once. Since prediction has been shown to have influence on vergence response (periodic disparity changes can be followed by vergence without typical latency [Hung 1998]), the time between the onsets of consecutive stimuli was set randomly between 1.25 s and 2.5 s. Task Each session of the experiment started with a calibration procedure, as described in the eye tracker manual. Next, every participant had to perform m trials, and the task was to simply observe the patch. The participants were encouraged to take breaks whenever they felt tired, and after each break the eye tracker was re-calibrated. The entire session took approximately 40 minutes. Data Analysis After each session, binary output of the eye tracker was converted to a plain-text version using the converter tool provided by the manufacturer. Next, the data was processed using a custom parser to extract gaze coordinates and times of disparity changes, and read into MATLAB R2012a. The times of stimulus onsets were marked in the output files with timestamps ? a functionality provided by the tracker?s API, which enabled easy synchronization of the gaze data with stimuli. For each transition, we extracted the 1-second segment following it, smoothed using a small box filter, and converted it to vergence values. Vergence was calculated as the difference between the x-coordinates of the two gaze positions expressed in pixels. Missing or unreliable samples (due to, e. g., blinks, saccades, or tracking errors) were interpolated linearly, and the segments that required interpolation of more than 50% samples were excluded. Data for transitions of one type was grouped, and a curve was fitted to the average. Next, for each type of a transition, the time to reach 95% of the required vergence change was determined, and two surfaces were fitted to the obtained data points. Since we were interested in relative gaze positions, the significance of drift was low. Moreover, adaptation times were determined by the 95%-of-change position, which is not very sensitive to shifts, scaling, etc. Based on these premises, we believe the precision was sufficient for our purposes.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n        145:4\n        ?\n        K. Templin et al.\n        \n          3.1 Pilot Experiment\n          In order to gain insight into the relation of vergence response to the initial and end disparities, as well as to estimate the number of trials m necessary for the response curves to converge, we conducted a pilot study. In it, one subject (S7) performed m = 30 trials, with d i = 0, ?30, ?60, ?90 px, and the cut-off frequency of the low-pass filter f = 20 cpd. This gave 30 ? 7 ? 6 = 1260 transitions measured. The results are presented in Fig. 2 . Discussion The signal converged quickly, giving relatively smooth data after ca. 5 repetitions, and little could be gained after ca. 10 repetitions. The vergence response can be modeled very well by sigmoid functions of the form v = ae be ct + d, known as the Gompertz curves. The 95%-point does not depend on parameters a and d, and can be obtained using the following formula: p 95 = ln (ln (0.95)/b)/c. The obtained data points can be modeled almost perfectly using two planes, with mean error close to 0, and standard deviation of ca. 27 ms. In light of these findings we decided to limit the disparity values used in the main experiment to d i = ?30, ?90 px, and the number of repetitions m to 10.\n        \n        \n          3.2 Main Experiment\n          The aim of the main experiment was twofold: to confirm that vergence times can be well modeled using two planes, as suggested by the pilot experiment, and, if so, to estimate parameters of the average-observer model, useful in practical applications. In this experiment n = 16 subjects performed m = 10 trials (except subjects S6, S9, and S10 for whom m = 5), with the cut-off frequency f = 10. The range of disparities for subject S9 was reduced to 2/3, due to reported problems with fusion. The results are presented in Fig. 3 . Discussion The average standard deviation of error after fitting the planes to the obtained data equals 36 ms. This indicates a very good fit, and justifies our assumption that the vergence adaptation time can be modeled using planes. In particular, this means that the data from subject S9, who saw rescaled disparities, could be easily included in the average model. As expected, our measurements show that given the initial disparity and direction, steps with larger magnitude lead to longer vergence adaptation times. An interesting finding is that the adaptation time depends also on the step direction and initial disparity. Given the initial disparity ( Fig. 3 , right, abscissae) and step magnitude (one yellow and one green line per magnitude), steps towards the screen are generally faster: To the right of the graph, yellow lines (convergent steps) have lower times than the corresponding green lines (divergent steps). To the left, this is reversed. Note, that corresponding yellow and green lines intersect near the point of zero initial disparity (screen plane). We hypothesize that it is related to accommodation-vergence coupling, which attracts vergence towards the screen plane, where the A/V conflict disappears. Additionally, given the step magnitude and direction ( Fig. 3 , either one yellow or one green line), with decreasing initial disparity, convergent steps get slower whereas divergent steps get faster. This effect could be convincingly explained by the amount of A/V conflict which increases with disparity magnitude. At negative initial disparities, divergent steps work towards resolving the conflict, whereas convergent steps work towards increasing it. With positive initial disparities the roles are reversed. The larger the magnitude of the initial disparity, the more stress is put on the visual system, and the demand to resolve (or not to increase) the conflict is higher. Thus, the larger discrepancy between convergent and divergent steps. These effects should be taken into account while optimizing stereoscopic content, as simple minimization of disparity difference will not necessary lead to shorter adaptation times. Another interesting finding is that with fixed target disparity, adaptation times for convergent steps are hardly dependent on the step magnitude. This phenomenon, at first unintuitive, could be explained by the A/V coupling as well: Larger step magnitudes, which should intuitively contribute to longer adaptation times, may be offset by varying initial stress exerted by the A/V conflict on the visual system. In our experiment we considered only a computer display observed at a relatively short distance. On the one hand, at larger viewing distances the depth of field increases, thereby reducing the importance of the A/V coupling, the hypothesized cause of the observed variation in vergence adaptation time. On the other hand, discomfort induced by step-like motion in depth has been observed even for disparities within the DOF [Yano et al. 2004]. Answering the question, if similar effect of initial disparity on the adaptation time can be observed in other viewing conditions, e. g., in cinema, requires further investigation.\n        \n        \n          3.3 Evaluation\n          The obtained model was derived using simple stimuli (flat whitenoise patterns). On the one hand, this approach has several advantages: the exact disparity is known, regardless of fixation points; the measurements can be repeated easily; and the learning effect is reduced, since the subject has no memory related to spatial arrangement of objects in case of repeated images. On the other hand, it is unclear how well the model predicts response to cuts between natural images: the presence of complex luminance patterns or high-level processes related to scene understanding may very well influence the transition times. Therefore, we conducted a validation experiment, to test if the model can be generalized.  Participants and Stimuli Four participants (S3, S7, S11, and S16) from original 16 were invited to take part in the evaluation of the model. Six 3D photographs taken with an LG Optimus 3D P725 smartphone were used (see Fig. 4 ). They were divided into two groups of three, one with smaller and the other with larger disparity changes across pictures. The disparities in the picture were estimated using the SIFT flow algorithm [Liu et al. 2011]. In a single trial a 6.5minute random sequence composed of the three photographs from one of the groups was shown. As previously, a single appearance of a picture lasted between 1.25 s and 2.5 s (chosen randomly), and there were no breaks between appearances. The task was to simply observe the pictures, and the participants were asked to perform one trial for each group. Data Analysis and Results After cleaning and segmenting of the tracking data, a semi-automatic procedure was employed to group segments of the same type, enabling averaging of measurements. In the first, automatic step segments where a saccade occurred at the time of the cut, or within the first 100 ms after the cut, were discarded. Then, initial disparity was estimated using the disparity map and the fixation coordinates just before the cut (initial fixation). The target disparity was chosen using the following heuristic: whenever the duration of the first fixation was shorter than 300 ms, the second fixation was used; otherwise, the initial fixation was assumed to be\n          ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n          Modeling and Optimizing Eye Vergence Response to Stereoscopic Cuts\n          ?\n          145:5\n          90 30 30 1 repetition 5 repetitions 60 [px] -60 -60 [px] 30 0 1000 0 1000 Vergence 30 30 Vergence -30 0 10 repetitions 30 repetitions t v = ae be ct + d -60 -60 -60 0 1000 0 1000 -90 Time [ms] 0 Time [ms]\n          \n            Figure 2: The results of the pilot experiment. In the left panel we presented averaged vergence responses of subject S7 to a ?60 px ? 30 px step, after 1, 5, 10, and 30 repetitions. The curve after 30 repetitions is shown together with a fit of a Gompertz function. In the middle panel average vergence responses to all 42 possible disparity steps for subject S7 are shown (dashed lines), together with fitted curves (solid lines), and points where the curves reach 95% of vergence change (solid circles). In the right panel, we plotted the transition time against the initial and target disparity. These points are almost perfectly modeled by two planes ? the standard deviation of the error is approximately 27 ms. The two planes represent divergence (green) and convergence (yellow). We leave a gap between the planes, where times begin to increase due to Panum?s fusional area and tolerance of the visual system to vergence errors. The diagonal is a singularity, where no transition is present, because the initial and target disparities are equal.\n          \n          1250 1250 [ms] 1000 [ms] 1000 t = 0.04 t = d i ?1.96 ? 2.3 d d i t + + 3.49 405.02 d t + 308.72 Time 500 750 Time 750 500 250 250 90 90 30 90 30 90 -30 -30 30 -30 -30 30 Initial disp. [px] -90 -90 Target disp. [px] Initial disp. [px] -90 -90 Target disp. [px]\n          \n            Figure 3: The results of the main experiment. In the left panel we presented fits for all subjects, after exclusion of 4 outliers (subjects S1, S6, S8, and S14). These subjects were excluded due to serious difficulties with correct fusion of the stimuli. For completeness we provide their data in the supplemental materials. The middle panel shows the average of all fits from the left panel, along with equations of the planes. These planes describe transition times for the average observer. In the right panel we showed a number of diagonal sections of the average planes. Each line represents disparity steps of the same magnitude and direction, but with different initial disparities. See the text for a detailed discussion.\n          \n          90 60 30 600 0 [ms] 400 Time -30 200 -60 90 -90 30 90 1000 Initial disparity [px] -30 -90 -90 -30 Target 30 disparity [px]\n          800 +180 px 700 +150 px [ms] 600 ?180 px +120 px Time 500 ?150 px +90 px ?120 px 400 ?90 px +60 px 300 ?60 px +30 px ?30 px 200 -90 -60 -30 0 30 60 90 Initial disp. [px]\n          also the target fixation.\n          In the second, manual step, all segments were briefly reviewed to correct filtering and target fixation errors. The false negatives were the cases when the saccade near the cut was small enough not to change significantly the vergence response. The false positives were the non-typical cases, including, but not limited to, eye-tracker errors, clearly incorrect vergence response indicating lack of fusion, segments with unusually large saccade-to-fixation ratio, erratic saccades indicating partial fixations, etc. In the end, 718 out of 3028 segments were discarded. We provide all annotated segments along with a custom viewer/editor as additional materials, and encourage the readers to inspect the data we used in this evaluation.  In the end, segments with the same initial/target disparities were grouped; groups with 5 or more members were averaged and compared against the model prediction for the respective subject. The results of the experiment are presented in Fig. 5 . Discussion Although our prediction slightly overestimated the time of transition for photographs, our model correlated well with the actual time, as indicated by relatively low standard deviation of the error. The study proves that our model is a good predictor (up to an additive constant) of transition time for natural images. We hypothesize that improved performance was due to the presence of higher-order cues, absent in white-noise stimuli, where the sole depth cue was binocular disparity. It is also possible that the transition was facilitated to some extent by the learning effect.\n          ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n          145:6\n          ?\n          K. Templin et al.\n          Image 1 Image 2 left view disparity left view\n          variation disp. small variation disp. large\n          \n            \n            Figure 4: The two groups of stimuli used in the evaluation, one with larger, and one with smaller disparity variation across pictures. The black bars on the sides are floating stereoscopic windows added to avoid frame violation or large disparity steps at the edge (the shoe example).\n          \n          Subject S16, mean: 50.27, std: 86.86 Subject S7, mean: 5.02, std: 62.48 600 400 200 [ms] 90 30 -30 -90 -90 -30 30 90 Time 90 30 -30 -90 -90 -30 30 90 Initial disp.[px] Target disp. [px] Initial disp. [px] Target disp. [px]\n          \n            Figure 5: The results for subjects S16 (left) and S7 (right); plots for subjects S3 and S11 are provided in the supplemental materials. The planes show model predictions, whereas the solid circles represent the observed data. The mean and standard deviation of the error for subjects S16, S7, S3, and S11 are respectively 50 ? 87 ms, 5 ? 62 ms, 107 ? 90 ms, and 61 ? 84 ms.\n          \n        \n      \n      \n        4 Applications\n        In this section, we propose a set of tools for aiding in the production of stereoscopic content, that utilizes our model to minimize vergence adaptation times. We also analyze the impact of the minimization on visual quality in one of the proposed tools using an object-recognition experiment.\n        \n          4.1 Production Tools\n          Transition Time Visualization A straightforward application of the model is a visualization tool providing stereographers and VFX artists with an interactive analysis of transition times. In order to evaluate stereoscopic transition and estimate transition time, we first need to determine the pairs of disparity values between which the transitions occur. A na?ve approach would be to measure the transition time between corresponding pixels in both sequences; however, it is not very useful, as in most cases people change the fixation point immediately after the transition, and no change in vergence happens (see the data browser provided in supplemental materials). Therefore, the fixation points in both sequences need to be precisely determined. Such data can be obtained from various sources, e. g., it is possible to use eye-tracker data. This does not require many subjects, as it  has been shown that eye scan-paths form highly repetitive patterns between different spectators for the same video sequences [Wang et al. 2012]. Moreover, skilled directors are capable of precisely guiding and predicting viewers? attention. Such prediction is further facilitated by the tendency of increasing object motion in modern movies [Cutting et al. 2011] and by the fact that typical 2D-movie cuts trigger saccades towards the screen center [Mital et al. 2011; Wang et al. 2012]. Thus, the information about fixation points for our methods can be very reliably provided by the directors. Besides, Carmi and Itti [2006] observed that the saccades immediately after the cut are driven mostly by the bottom-up factors and can be predicted relatively well by existing saliency models. Once the fixation points before and after the cut are known, the corresponding disparity values need to be determined. This can be obtained directly from the rendering pipeline for animated movies, using user input in the case of 2D-to-3D conversion, or using disparity estimation techniques for natural scenes when the depth map is not available. Once the fixation points along with disparity values are known, transition times can be directly calculated from the model. Since computing model predictions is inexpensive, it can be used to provide real-time preview of transition times. Camera Parameters Optimization Apart from predicting transition times and visualizing them for editing purposes, one can automate the process of stereoscopic content preparation. An optimization problem for cuts can be defined, and our model can serve as the core of the cost function. As discussed in Sec. 2, stereoscopic content can be optimized by manipulating various parameters. These can be changed for the entire sequence (e. g., from cut to cut), or selectively around the cuts, with smooth blending back to original parameters [Lang et al. 2010; Koppal et al. 2011]. There is a wide range of manipulations that can be used to adjust stereoscopic content. They range from very simple ones, like changing camera separation and convergence (i. e., the plane of zero parallax), to more complicated ones, such as depth remapping. All such manipulations can be easily integrated and used with our model. Cut Positioning If the two sequences between which the cut occurs overlap in time, it is also possible to find the best moment for the cut. To this end, we optimize not only stereoscopic parameters, but also the position of the cut. This can be performed efficiently by simply iterating over all possible cut positions, in addition to all horizontal shifts of the left/right views. The optimal cut can be chosen automatically or can be shown to the editor as a suggestion. The design of a tool performing these tasks is shown in Fig. 1 , right, and in the supplemental video.\n          Image 3 disparity left view disparity\n          -120 px 120 px\n          ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n          Cuts\n          ?\n          145:7\n        \n        \n          4.2 Impact on\n          Visual quality can be defined in many ways, using various objective and subjective criteria. In the following experiment, we focus on the time necessary to recognize the 3D arrangement of objects after a cut. We assume shorter recognition times to be an indicator of higher quality. We measured the time needed to recognize object arrangement, and showed that this time closely matches our model. In practice, this means that when cuts are optimized using the proposed production tools, the time necessary to recognize objects in the scene is minimized.  Methods The equipment and viewing conditions were the same as in other experiments, but no eye-tracker was used. As stimuli, we used two shots corresponding to a cut in the 3D version of the Big Buck Bunny animation. We modified them by placing two small darkgray circles between the eyes of the character, with approximately the same disparity as the character (see Fig. 6 , left, inset). Two 3D arrangements of circles for each shot were considered: one with the upper, and one with the lower circle closer to the observer. The disparity difference between the circles was 2 px. The convergence in the shots was modified so that the average disparity of the circles was equal to d i before and d t after the cut. Seven pairs of disparity steps were used: ?75 ? ?105/90, ?60 ? ?90/ ? 30, ?30 ? ?90/60, 0 ? ?30/30, 30 ? ?60/90, 60 ? 30/90, and 75 ? ?90/105 px. For each initial disparity, both a convergent and a divergent step was possible, which prevented anticipatory eye movements in subjects. In order to determine the arrangement recognition time for all 14 steps, we performed 14 independent QUEST threshold estimation procedures [Watson and Pelli 1983], each estimating time of 75% correctness. A single trial of each procedure had the following structure: First, the first shot was shown for 2 s. Next, the second shot was shown for a period between 0.1 and 1.5 s (controlled by QUEST). The arrangement of the circles was chosen randomly in every trial. After the screen was blanked, the subject was asked to indicate if the arrangement was the same in both shots: If the same circle (i. e., upper or lower) was closer to the observer both before and after the cut, the subject had to press the Y key, and the N key otherwise. Such a task definition ensured that the subject actually performed the vergence transition d i ? d t . All 14 procedures were performed in parallel, randomly interleaved. A session of the experiment lasted 20 min (average standard deviation in a QUEST instance 73 ms). Subjects S3, S11, S12, S15, and S16 took part in the experiment. S11 participated in three sessions, S16 in two, and the remaining three in one session. Results The data obtained using the above procedure was fitted with two planes minimizing the RMSE. The planes obtained from all subjects were averaged (first within subjects, then between subjects), and compared to their average model. The results are presented in Fig. 6 . A corrective constant shift of 83 ms minimizes the RMSE, and yields a low prediction error of 42 ms. This correlation implies that optimizing camera convergence using our model instead of disparity distance as the cost function will produce cuts with shorter recognition times. Similar improvement can be expected when optimizing other camera parameters or cut positions. This illustrates the practical importance of our model for S3D games and films.\n          \n            \n          \n        \n      \n      \n        5 Conclusions\n        We proposed a new model which predicts the time a human observer needs to adapt vergence to rapid disparity changes. We first presented measurements of transition times for simple stimuli, and demonstrated that these times are valid also for complex scenes. The experiment revealed interesting facts about viewer behavior during\n        \n          Figure 6: Left: Stimuli used in the object recognition experiment. Right: The results of the experiment; the gray planes represent the\n        \n        RMSE\n        scene cuts, which provides valuable knowledge for stereoscopic content creators. Additionally, we proposed a set of tools for the editing of stereoscopic content to minimize the vergence adaptation time after cuts. An important property of the proposed optimization techniques is that the manipulations are applied only locally around cuts, which has limited effect on the depth impression created by the artist. To our knowledge, this is the first work that proposes to automatically edit stereoscopic cuts taking into account varying performance of the human visual system in adapting to rapid disparity changes. Finally, we demonstrated the impact of minimizing adaptation times on the visual quality of S3D content as measured by a subject?s performance in the 3D object recognition task. An interesting avenue for future work would be an extensive user study quantifying how shorter transition times influence visual fatigue.\n      \n      \n        Acknowledgments\n        We would like to thank Aude Oliva, Lavanya Sharan, Zoya Bylinskii, Sylvain Paris, YiChang Shih, Tobias Ritschel, Katarina Struckmann, David Levin, and the Anonymous Subjects who took part in our perceptual studies. This work was partially supported by NSF IIS1111415 and NSF IIS-1116296.\n      \n      \n        References\n        \n          A LVAREZ , T. L., S EMMLOW , J. L., AND P EDRONO , C. 2005. Divergence eye movements are dependent on initial stimulus position. Vis. Res. 45, 14, 1847?55.\n          B ERNHARD , M., D ELLMOUR , C., H ECHER , M., S TAVRAKIS , E., AND W IMMER , M. 2014. The effects of fast disparity adjustments in gaze-controlled stereoscopic applications. In Proc. ETRA. To appear.\n          C AMPBELL , F. W., AND W ESTHEIMER , G. 1959. Factors influencing accommodation responses of the human eye. J. Opt. Soc. Am. 49, 6, 568?71.\n          C ARMI , R., AND I TTI , L. 2006. Visual causes versus correlates of attentional selection in dynamic scenes. Vis. Res. 46, 26, 4333?45.\n          C UTTING , J., B RUNICK , K., D ELONG , J., I RICINSCHI , C., AND C ANDAN , A. 2011. Quicker, faster, darker: Changes in hollywood film over 75 years. i-PERCEPTION 2, 6, 569?76. , D. 2013.\n        \n      \n      \n        ACM Trans.\n        D U , S.-P., M ASIA , B., H U , S.-M., AND G UTIERREZ A metric of visual comfort for stereoscopic motion. 6, 222.\n      \n      \n        Graph. 32,\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n        145:8\n        ?\n        K. Templin et al.\n        E ADIE , A. S., G RAY , L. S., C ARLIN , P., AND M ON -W ILLIAMS , M. 2000. Modelling adaptation effects in vergence and accommodation after exposure to a simulated virtual reality stimulus. Ophthalmic Physiol. Opt. 20, 3, 242?51. E RKELENS , C. J., V AN DER S TEEN , J., S TEINMAN , R. M., AND C OLLEWIJN , H. 1989. Ocular vergence under natural conditions. II. Gaze-shifts between real targets differing in distance and direction. In Proc. of the Royal. Soc., 441?6. F INKE , R. 1989. Principles of Mental Imagery. MIT Press. H EINZLE , S., G REISEN , P., G ALLUP , D., C HEN , C., S ANER , D., S MOLIC , A., B URG , A., M ATUSIK , W., AND G ROSS , M. H. 2011. Computational stereo camera system with programmable control loop. ACM Trans. Graph. 30, 4, 94. H OFFMAN , D., G IRSHICK , A., A KELEY , K., AND B ANKS , M. 2008. Vergence-accommodation conflicts hinder visual performance and cause visual fatigue. J. Vision 8, 3, 1?30. H UNG , G. K., C IUFFREDA , K. J., S EMMLOW , J. L., AND H OR , J.-L. 1994. Vergence eye movements under natural viewing conditions. Invest. Ophthalmol. Vis. Sci. 35, 3486?92. H UNG , G. K. 1992. Adaptation model of accommodation and vergence. Ophthalmic Physiol. Opt. 12, 3, 319?26. H UNG , G. K. 1998. Dynamic model of the vergence eye movement system: Simulations using Matlab/Simulink. Computer Methods and Programs in Biomedicine 55, 1, 59?68. H UNG , G. K. 2001. Models of oculomotor control. World Scientific Publishing, Singapore. K OPPAL , S. J., Z ITNICK , C. L., C OHEN , M., K ANG , S. B., R ESSLER , B., AND C OLBURN , A. 2011. A viewer-centric editor for 3d movies. IEEE Comput. Graph. Appl. Mag. 31, 1, 20. K RISHNAN , V., F ARAZIAN , F., AND S TARK , L. 1973. An analysis of latencies and prediction in the fusional vergence system. Am. J. Optometry and Arch. Am. Academy of Optometry 50, 933?9. K RISHNAN , V., F ARAZIAN , F., AND S TARK , L. 1977. Dynamic measures of vergence accommodation. American Journal of Optometrics and Physiological Optics 54, 470?3. L AMBOOIJ , M., IJ SSELSTEIJN , W., F ORTUIN , M., AND H EYN DERICKX , I. 2009. Visual discomfort and visual fatigue of stereoscopic displays: A review. J. Imaging Sci. Technol. 53, 3, 1. L AMBOOIJ , M., IJ SSELSTEIJN , W., AND H EYNDERICKX , I. 2011. Visual discomfort of 3D TV: Assessment methods and modeling. Displays 32, 4, 209?18. Visual Image Safety. L ANG , M., H ORNUNG , A., W ANG , O., P OULAKOS , S., S MOLIC , A., AND G ROSS , M. 2010. Nonlinear disparity mapping for stereoscopic 3D. ACM Trans. Graph. 29, 4, 75. L IU , C., Y UEN , J., AND T ORRALBA , A. 2011. Sift flow: Dense correspondence across scenes and its applications. Pattern Analysis and Machine Intelligence, IEEE Transactions on 33, 5, 978?94. M EESTERS , L., IJ SSELSTEIJN , W., AND S EUNTIENS , P. 2004. A survey of perceptual evaluations and requirements of threedimensional tv. Circuits and Systems for Video Technology, IEEE Transactions on 14, 3, 381?91. M ENDIBURU , B. 2009. 3D Movie Making: Stereoscopic Digital Cinema from Script to Screen. Focal Press. M ITAL , P., S MITH , T., H ILL , R., AND H ENDERSON , J. 2011. Clustering of gaze during dynamic scene viewing is predicted by motion. Cognitive Computation 3, 1, 5?24.  O KUYAMA , F. 1998. Human visual accommodation and vergence eye movement while viewing stereoscopic display and actual target. In Proc. IEEE Eng. Med. Biol. Society, vol. 2, 552?5. O SKAM , T., H ORNUNG , A., B OWLES , H., M ITCHELL , K., AND G ROSS , M. H. 2011. Oscam-optimized stereoscopic camera control for interactive 3d. ACM Trans. Graph. 30, 6, 189. O WENS , C., 2013. Invited talk. 2nd Toronto International Stereoscopic 3D Conference. R USHTON , S. K., AND R IDDELL , P. M. 1999. Developing visual systems and exposure to virtual reality and stereo displays: some concerns and speculations about the demands on accommodation and vergence. Applied Ergonomics 30, 1, 69?78. S CHOR , C. M. 1979. The relationship between fusional vergence eye movements and fixation disparity. Vis. Res. 19, 12, 1359?67. S CHOR , C. M. 1992. The relationship between fusional vergence eye movements and fixation disparity. Optometry and Vision Science 69, 4, 258?69. S CHOR , C. 1999. The influence of interactions between accommodation and convergence on the lag of accommodation. Ophthalmic Physiol. Opt. 19, 2, 134?50. S EMMLOW , J., AND W ETZEL , P. 1979. Dynamic contributions of the components of binocular vergence. JOSA 69, 639?45. S EMMLOW , J., H UNG , G., AND C IUFFREDA , K. 1986. Quantitative assessment of disparity vergence components. Invest. Ophthalmol. Vis. Sci. 27, 558?64. S HIBATA , T., K IM , J., H OFFMAN , D. M., AND B ANKS , M. S. 2011. The zone of comfort: Predicting visual discomfort with stereo displays. J. Vision 11, 8, 11. T AM , W. J., S PERANZA , F., V ?ZQUEZ , C., R ENAUD , R., AND H UR , N. 2012. Visual comfort: stereoscopic objects moving in the horizontal and mid-sagittal planes. In Proc. SPIE, A. J. Woods, N. S. Holliman, and G. E. Favalora, Eds., 8288:13. U KAI , K., AND K ATO , Y. 2002. The use of video refraction to measure the dynamic properties of the near triad in observers of a 3-d display. Ophthalmic Physiol. Opt. 22, 5, 385?8. W ANG , H. X., F REEMAN , J., M ERRIAM , E. P., H ASSON , U., AND H EEGER , D. J. 2012. Temporal eye movement strategies during naturalistic viewing. J. Vision 12, 1, 16. W ATSON , A. B., AND P ELLI , D. G. 1983. QUEST: a Bayesian adaptive psychometric method. Perception and Psychophysics 33, 2, 113?20. Y ANO , S., E MOTO , M., AND M ITSUHASHI , T. 2004. Two factors in visual fatigue caused by stereoscopic HDTV images. Displays 25, 4 (Nov.), 141?50. Z ILLY , F., K LUGER , J., AND K AUFF , P. 2011. Production rules for stereo acquisition. Proc. IEEE 99, 4, 590?606. Z WICKER , M., M ATUSIK , W., D URAND , F., P FISTER , H., AND F ORLINES , C. 2006. Antialiasing for automultiscopic 3D displays. In Proc. EGSR, 73?82.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 145, Publication Date: July 2014\n      \n    \n  ",
  "resources" : [ ]
}