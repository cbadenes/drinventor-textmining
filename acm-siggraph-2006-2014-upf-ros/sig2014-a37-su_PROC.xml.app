{
  "uri" : "sig2014-a37-su_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a37-su_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Estimating Image Depth Using Shape Collections",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Hao-Su",
      "name" : "Hao",
      "surname" : "Su"
    }, {
      "uri" : "http://drinventor/Qixing-Huang",
      "name" : "Qixing",
      "surname" : "Huang"
    }, {
      "uri" : "http://drinventor/Niloy J.-Mitra",
      "name" : "Niloy J.",
      "surname" : "Mitra"
    }, {
      "uri" : "http://drinventor/Yangyan-Li",
      "name" : "Yangyan",
      "surname" : "Li"
    }, {
      "uri" : "http://drinventor/Leonidas J.-Guibas",
      "name" : "Leonidas J.",
      "surname" : "Guibas"
    } ]
  },
  "bagOfWords" : [ "image", "while", "easy", "acquire", "view", "publish", "share", "lack", "critical", "depth", "information", "pose", "serious", "bottleneck", "many", "image", "manipulation", "editing", "retrieval", "task", "paper", "we", "consider", "problem", "add", "depth", "image", "object", "effectively", "lifting", "back", "3d", "exploit", "collection", "align", "3d", "model", "related", "object", "we", "evaluate", "we", "fully", "automatic", "approach", "diverse", "challenging", "input", "image", "validate", "result", "against", "kinect", "depth", "reading", "demonstrate", "several", "imaging", "application", "include", "depth-enhanced", "image", "editing", "image", "relighting", "keyword", "data-driven", "shape", "analysis", "pose", "estimation", "depth", "estimation", "image", "retrieval", "shape", "collection", "2d", "projection", "we", "3d", "world", "however", "may", "lack", "certain", "semantical", "information", "pose", "serious", "challenge", "application", "involve", "image", "recognition", "manipulation", "editing", "etc.", "could", "greatly", "benefit", "from", "omit", "information", "hence", "strong", "motivation", "lift", "image", "3d", "infer", "attribute", "lose", "projection", "paper", "we", "specifically", "interested", "infer", "depth", "visible", "object", "area", "key", "coordinate", "miss", "projection", "problem", "recover", "depth", "from", "single", "image", "naturally", "ill-posed", "various", "prior", "have", "be", "propose", "regularization", "however", "large-scale", "deployment", "method", "fundamentally", "limit", "because", "only", "limited", "number", "3d", "model", "available", "most", "often", "we", "do", "even", "have", "3d", "model", "same", "sufficiently", "similar", "object", "from", "which", "image", "take", "paper", "we", "consider", "problem", "estimate", "depth", "image", "object", "exploit", "novel", "joint", "fashion", "collection", "3d", "model", "related", "largely", "different", "object", "-lrb-", "see", "Figure", "-rrb-", "we", "address", "depth", "inference", "problem", "its", "purest", "form", "where", "we", "assume", "object", "image", "have", "be", "segmented", "from", "its", "background", "-lrb-", "image", "now", "commonplace", "shopping", "web", "site", "-rrb-", "while", "we", "3d", "model", "typically", "untextured", "come", "from", "shape", "collection", "Trimble", "3d", "warehouse", "we", "image-based", "shape-driven", "modeling", "technique", "fully", "automatic", "reconstruct", "3d", "point", "cloud", "from", "image", "object", "algorithm", "consist", "preprocessing", "stage", "which", "align", "input", "shape", "each", "other", "learn", "deformation", "model", "each", "shape", "reconstruction", "stage", "which", "use", "continuous", "optimization", "recover", "image", "object", "pose", "reconstruct", "point", "cloud", "from", "image", "align", "relevant", "3d", "model", "extract", "from", "collection", "we", "show", "how", "formulate", "appropriate", "objective", "function", "how", "obtain", "initial", "solution", "how", "effectively", "refine", "solution", "use", "alternate", "optimization", "we", "approach", "we", "jointly", "match", "depth-augmented", "image", "i.e.", "popup", "point", "cloud", "image", "group", "related", "shape", "collection", "we", "pose", "task", "joint", "non-rigid", "registration", "problem", "which", "each", "shape", "can", "deform", "formulation", "have", "two", "key", "feature", "first", "contrast", "utilize", "single", "similar", "shape", "incorporate", "collection", "similar", "shape", "offer", "better", "coverage", "relevant", "neighborhood", "shape", "space", "second", "since", "we", "have", "already", "align", "3d", "model", "each", "other", "enable", "we", "apply", "consistency", "constraint", "-lsb-", "Kim", "et", "al.", "2012a", "Huang", "Guibas", "2013", "-rsb-", "regularize", "image", "3d", "model", "matching", "use", "shape-shape", "correspondence", "joint", "non-rigid", "registration", "formulation", "we", "introduce", "key", "concept", "deformation", "prior", "which", "govern", "deformation", "each", "shape", "-lrb-", "c.f.", "-lsb-", "Averkiou", "et", "al.", "2014", "-rsb-", "-rrb-", "intuitively", "we", "aim", "preserve", "key", "structural", "property", "each", "shape", "deformation", "so", "round", "shape", "stay", "round", "left-to-right", "symmetry", "preserve", "etc.", "particular", "instead", "detect", "property", "form", "each", "shape", "alone", "which", "turn", "out", "unreliable", "we", "learn", "they", "from", "optimal", "deformation", "each", "shape", "other", "shape", "test", "performance", "propose", "approach", "we", "have", "create", "benchmark", "dataset", "consist", "Microsoft", "Kinect", "scan", "various", "category", "object", "include", "chair", "table", "lamp", "cup", "addition", "we", "show", "we", "work", "key", "intermediate", "step", "towards", "goal", "obtain", "full", "3d", "model", "use", "popup", "point", "cloud", "input", "we", "can", "reconstruct", "certain", "case", "full", "mesh", "exploit", "shape", "symmetry", "learn", "from", "shape", "network", "contribution", "we", "present", "first", "best", "we", "knowledge", "fully", "automatic", "method", "utilize", "network", "related", "different", "3d", "object", "order", "reconstruct", "depth", "information", "from", "single", "image", "object", "example", "information", "transfer", "can", "include", "texture", "segmentation", "material", "property", "label", "etc.", "recently", "we", "have", "witness", "success", "data-driven", "technique", "shape", "analysis", "-lsb-", "Huang", "et", "al.", "2011", "Kim", "et", "al.", "2012a", "Kim", "et", "al.", "2013", "Huang", "et", "al.", "2013", "Wang", "et", "al.", "2013", "-rsb-", "shape", "model", "ing", "-lsb-", "Chaudhuri", "et", "al.", "2011", "Kalogerakis", "et", "al.", "2012", "Averkiou", "et", "al.", "2014", "-rsb-", "shape", "reconstruction", "-lsb-", "Nan", "et", "al.", "2012", "Kim", "et", "al.", "2012b", "Shen", "et", "al.", "2012", "-rsb-", "although", "exist", "rich", "technique", "align", "match", "3d", "shape", "problem", "match", "image", "object", "3d", "shape", "which", "major", "focus", "paper", "far", "from", "be", "solve", "image-shape", "matching", "contrast", "we", "formulate", "image", "shape", "match", "problem", "solve", "non-rigid", "alignment", "problem", "3d", "i.e.", "simultaneously", "estimate", "optimize", "depth", "image", "object", "deformation", "3d", "shape", "align", "they", "3d", "space", "problem", "commonly", "formulate", "feature", "correspondence", "problem", "task", "pose", "estimation", "closely", "couple", "other", "task", "image-shape", "matching", "problem", "depth", "estimation", "point", "correspondence", "we", "therefore", "model", "part", "global", "optimization", "problem", "iteratively", "refine", "result", "large", "improvement", "problem", "ill-pose", "when", "input", "single", "image", "exist", "approach", "typically", "incorporate", "additional", "information", "user", "interaction", "-lsb-", "Wu", "et", "al.", "2008", "-rsb-", "shade", "-lsb-", "Lensch", "et", "al.", "2003", "Goldman", "et", "al.", "2005", "-rsb-", "use", "abstracted", "proxy", "shape", "-lsb-", "Zheng", "et", "al.", "2012", "-rsb-", "we", "take", "different", "approach", "shape", "collection", "typically", "do", "contain", "shape", "exactly", "same", "object", "reconstruct", "we", "formulate", "task", "joint", "non-rigid", "alignment", "problem", "however", "several", "challenge", "what", "help", "we", "situation", "fundamental", "difference", "between", "propose", "approach", "other", "shape-driven", "image", "base", "modeling", "technique", "we", "utilize", "information", "provide", "collection", "regularize", "problem", "goal", "preprocessing", "stage", "align", "shape", "learn", "smart", "deformation", "prior", "-lrb-", "local", "model", "-rrb-", "each", "shape", "we", "learn", "deformation", "prior", "each", "shape", "perform", "covariance", "analysis", "over", "its", "optimize", "deformation", "neighbor", "shape", "specifically", "first", "step", "initialize", "camera", "configuration", "extract", "set", "similar", "shape", "despite", "non-linearity", "scale", "optimization", "problem", "we", "show", "can", "optimize", "effectively", "use", "alternate", "optimization", "strategy", "goal", "preprocessing", "stage", "understand", "plausible", "deformation", "each", "shape", "context", "provide", "input", "shape", "collection", "paper", "we", "use", "200", "control", "point", "hence", "each", "shape", "control", "600", "parameter", "we", "directly", "obtain", "deformation", "sample", "compose", "absolute", "optimal", "deformation", "inverse", "deformation", "learn", "prior", "model", "from", "similar", "shape", "we", "only", "consider", "deformation", "sample", "from", "128", "most", "similar", "shape", "each", "shape", "term", "d2", "descriptor", "-lsb-", "Osada", "et", "al.", "2002", "-rsb-", "give", "deformation", "sample", "we", "perform", "covariance", "analysis", "extract", "principal", "value", "principal", "direction", "deformation", "space", "give", "point", "-lrb-", "-rrb-", "its", "corresponding", "pixel", "coordinate", "-lrb-", "-rrb-", "give", "due", "difference", "between", "real", "image", "render", "image", "standard", "single", "shape", "base", "approach", "typically", "require", "feature", "learning", "weight", "each", "image-shape", "edge", "give", "-lrb-", "-rrb-", "while", "image", "descriptor", "replace", "d2", "shape", "descriptor", "-lsb-", "Osada", "et", "al.", "2002", "-rsb-", "shape-shape", "edge", "constraint", "we", "can", "use", "improve", "correspondence", "make", "they", "consistent", "optimal", "deformation", "-lcb-", "-rcb-", "align", "input", "shape", "Automatic", "Pop-Up", "-lsb-", "Hoiem", "et", "al.", "2005", "-rsb-", "automatically", "reconstruct", "3d", "information", "use", "single", "image", "initially", "design", "outdoor", "scene", "use", "plane", "classifier" ],
  "content" : "Images, while easy to acquire, view, publish, and share, they lack critical depth information. This poses a serious bottleneck for many image manipulation, editing, and retrieval tasks. In this paper we consider the problem of adding depth to an image of an object, effectively ?lifting? it back to 3D, by exploiting a collection of aligned 3D models of related objects. We evaluate our fully automatic approach on diverse and challenging input images, validate the results against Kinect depth readings, and demonstrate several imaging applications including depth-enhanced image editing and image relighting. Keywords: data-driven shape analysis, pose estimation, depth estimation, image retrieval, shape collections As 2D projections of our 3D world, however, they may lack certain semantical information. This poses serious challenges to applications involving image recognition, manipulation, editing, etc. that could greatly benefit from this omitted information. Hence, there is a strong motivation to lift images to 3D by inferring attributes lost in the projection. In this paper we are specifically interested in inferring depth for the visible object areas ? the key coordinate missing in the projection. As the problem of recovering depth from single image is naturally ill-posed, various priors have been proposed for regularization. However, large-scale deployment of such a method is fundamentally limited because only a limited number of 3D models is available. Most often, we do not even have a 3D model of the same or sufficiently similar object from which the image was taken. In this paper we consider the problem of estimating depth for an image of an object by exploiting, in a novel joint fashion, a collection of 3D models of related but largely different objects (see Figure 1 ). We address the depth inference problem in its purest form, where we assume that the object image has been segmented from its background (such images are now commonplace in shopping web sites), while our 3D models are typically untextured and come from shape collections, such as the Trimble 3D warehouse. Our image-based but shape-driven modeling technique is fully automatic and reconstructs a 3D point cloud from the imaged object. The algorithm consists of a preprocessing stage, which aligns the input shapes to each other and learns a deformation model for each shape; and a reconstruction stage, which uses a continuous optimization to recover the image object pose and reconstruct a point cloud from the image that aligns with relevant 3D models extracted from the collection. We show how to formulate an appropriate objective function, how to obtain an initial solution, and how to effectively refine the solution using an alternating optimization. In our approach, we jointly match the depth-augmented image, i.e., the popup point cloud of the image, with a group of related shapes in the collection. We pose the task as a joint non-rigid registration problem, in which each shape can be deformed. The formulation has two key features. First, in contrast to utilizing a single similar shape, incorporating a collection of similar shapes offers a better coverage of the relevant neighborhood of shape space. Second, since we have already aligned the 3D models to each other, it enables us to apply consistency constraints [Kim et al. 2012a; Huang and Guibas 2013] to regularize the image to 3D model matching by using the shape-shape correspondences. In the joint non-rigid registration formulation, we introduce the key concept of deformation priors, which govern the deformation of each shape (c.f. , [Averkiou et al. 2014]). Intuitively, we aim to preserve the key structural properties of each shape in the deformation, so that round shapes stay round, left-to-right symmetries are preserved, etc. In particular, instead of detecting these properties form each shape alone, which turns out to be unreliable, we learn them from the optimal deformations of each shape to other shapes. To test the performance of the proposed approach, we have created a benchmark dataset consisting of Microsoft Kinect scans of various categories of objects including chairs, tables, lamps, and cups. In addition, we show that our work is a key intermediate step towards the goal of obtaining full 3D models. Using a popup point cloud as input, we can reconstruct in certain cases a full mesh by exploiting shape symmetries learned from the shape network. Contributions. We present the first, to the best of our knowledge, fully automatic method to utilize a network of related but different 3D objects in order to reconstruct depth information from a single imaged object. Example of such information transfer can include textures, segmentations, material properties, labels, etc. Recently, we have witnessed the success of data-driven techniques in shape analysis [Huang et al. 2011; Kim et al. 2012a; Kim et al. 2013; Huang et al. 2013; Wang et al. 2013], shape model ing [Chaudhuri et al. 2011; Kalogerakis et al. 2012; Averkiou et al. 2014] and shape reconstruction [Nan et al. 2012; Kim et al. 2012b; Shen et al. 2012]. Although there exist rich techniques for aligning and matching 3D shapes, the problem of matching image objects and 3D shapes, which is the major focus of this paper, is far from being solved. Image-shape matching. In contrast, we formulate the image shape matching problem as solving a non-rigid alignment problem in 3D, i.e., simultaneously estimating optimizing the depth of the image object and the deformations of 3D shapes to align them in the 3D space. The problem is commonly formulated as a feature correspondence problem. The task of pose estimation is closely coupled with other tasks in the image-shape matching problem, such as depth estimation and point correspondence. We therefore model it as a part of the global optimization problem and iteratively refine it, resulting in large improvement. This problem is ill-posed when the input is a single image, and existing approaches typically incorporate additional information such as user interaction [Wu et al. 2008] and shading [Lensch et al. 2003; Goldman et al. 2005], or using abstracted proxy shapes [Zheng et al. 2012]. We take a different approach. As the shape collection typically does not contain a shape that is exactly same as the object to be reconstructed, we formulate the task as a joint non-rigid alignment problem. However, there are several challenges. What helps in our situation, and the fundamental difference between the proposed approach and other shape-driven image based modeling techniques, is that we utilize the information provided by the collection to regularize the problem. The goal of the preprocessing stage is to align the shapes and to learn a smart deformation prior (local model) for each shape. We learn the deformation prior of each shape by performing covariance analysis over its optimized deformations to neighboring shapes. Specifically, the first step initializes a camera configuration and extracts a set of similar shapes. Despite the non-linearity and scale of this optimization problem, we show that it can be optimized effectively using an alternating optimization strategy. The goal of the preprocessing stage is to understand the plausible deformations of each shape in the context provided by the input shape collection. In this paper, we use 200 control points, and hence each shape is controlled by M = 600 parameters. We directly obtain these deformation samples by composing the absolute optimal deformations D i ? and their inverse deformations D i ? ?1 . To learn the prior model from similar shapes, we only consider the deformation samples from the 128 most similar shapes to each shape, in terms of the D2 descriptor [Osada et al. 2002]. Given the deformation samples, we perform covariance analysis to extract the principal values ? 1 ? ? 2 ? ? ? ? ? M and principal directions u 1 , u 2 , ? ? ? , u M of the deformation space. Given a point q = (q x , q y , q z ) T in ?, its corresponding pixel coordinate p = (p x , p y ) T is given by Due to differences between real images and rendered images, standard single shape based approaches  typically require feature learning. The weight of each image-shape edge is given by (5), while the image descriptor is replaced by the D2 shape descriptor [Osada et al. 2002] for a shape-shape edge. A constraint that we can use to improve these correspondence is to make them consistent with the optimal deformations {D i ? } that align the input shapes. Automatic Pop-Up [Hoiem et al. 2005] automatically reconstructs 3D information using a single image and was initially designed for outdoor scenes using plane classifiers.",
  "resources" : [ ]
}