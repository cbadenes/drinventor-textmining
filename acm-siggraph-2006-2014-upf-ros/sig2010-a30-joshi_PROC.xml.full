{
  "uri" : "sig2010-a30-joshi_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2010/a30-joshi_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Image Deblurring using Inertial Measurement Sensors",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Ondrej-Sindelar",
      "name" : "Ondrej",
      "surname" : "Sindelar"
    }, {
      "uri" : "http://drinventor/Filip-Sroubek",
      "name" : "Filip",
      "surname" : "Sroubek"
    } ]
  },
  "bagOfWords" : [ "4ceb827e6a483db80b16aa1b759c48a08f6b9cdcb12301eb1f9bcb7fc9f55681", "mig", "10.1145", "1778765.1778767", "name", "identification", "possible", "image", "deblurring", "use", "Inertial", "Measurement", "Sensors", "Joshi", "Sing", "Bing", "Kang", "C.", "Lawrence", "Zitnick", "Microsoft", "Research", "Bluetooth", "Radio", "3-axis", "Accelerometer", "Arduino", "Board", "SLR", "trigger", "Gyros", "Figure", "SLR", "Camera", "instrument", "we", "image", "deblurr", "attachment", "use", "inertial", "measurement", "sensor", "input", "image", "aid", "blind-deconvolution", "algorithm", "automatically", "deblur", "image", "spatially-varying", "blur", "-lrb-", "first", "two", "image", "-rrb-", "blurry", "input", "image", "-lrb-", "third", "image", "-rrb-", "result", "we", "method", "-lrb-", "fourth", "image", "-rrb-", "blur", "kernel", "each", "corner", "image", "show", "size", "we", "present", "deblurr", "algorithm", "use", "hardware", "attachment", "couple", "natural", "image", "prior", "deblur", "image", "from", "consumer", "camera", "we", "approach", "use", "combination", "inexpensive", "gyroscope", "accelerometer", "energy", "optimization", "framework", "estimate", "blur", "function", "from", "camera?s", "acceleration", "angular", "velocity", "during", "exposure", "we", "solve", "camera", "motion", "high", "sampling", "rate", "during", "exposure", "infer", "latent", "image", "use", "joint", "optimization", "we", "method", "completely", "automatic", "handle", "per-pixel", "spatially-varying", "blur", "out-perform", "current", "lead", "image-based", "method", "we", "experiment", "show", "handle", "large", "kernel", "up", "least", "100", "pixel", "typical", "size", "30", "pixel", "we", "also", "present", "method", "perform", "ground-truth", "measurement", "camera", "motion", "blur", "we", "use", "method", "validate", "we", "hardware", "deconvolution", "approach", "best", "we", "knowledge", "first", "work", "use", "dof", "inertial", "sensor", "dense", "per-pixel", "spatially-varying", "image", "deblurring", "first", "work", "gather", "dense", "ground-truth", "measurement", "camera-shake", "blur", "introduction", "intentional", "blur", "can", "use", "great", "artistic", "effect", "photography", "however", "many", "common", "imaging", "situation", "blur", "nuisance", "camera", "motion", "blur", "often", "occur", "light-limited", "situation", "one", "most", "common", "reason", "discard", "photograph", "blur", "function", "know", "image", "can", "improve", "deblurr", "non-blind", "deconvolution", "method", "however", "most", "image", "blur", "function", "unknown", "must", "recov", "ACM", "Reference", "Format", "Joshi", "N.", "Kang", "S.", "Zitnick", "C.", "Szeliski", "R.", "2010", "image", "deblurring", "use", "Inertial", "Measurement", "Sensors", "ACM", "Trans", "graph", "29", "Article", "30", "-lrb-", "July", "2010", "-rrb-", "page", "dous", "10.1145", "1778765.1778767", "http://doi.acm.org/10.1145/1778765.1778767", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2010", "ACM", "0730-0301/2010", "07-art30", "10.00", "DOI", "10.1145", "1778765.1778767", "http://doi.acm.org/10.1145/1778765.1778767", "Richard", "Szeliski", "ere", "recover", "both", "blur", "point-spread", "function", "-lrb-", "psf", "-rrb-", "desire", "deblurred", "image", "from", "single", "blur", "input", "-lrb-", "know", "blind-deconvolution", "problem", "-rrb-", "inherently", "ill-posed", "observe", "blur", "image", "provide", "only", "partial", "constraint", "solution", "prior", "knowledge", "about", "image", "kernel", "can", "disambiguate", "potential", "solution", "make", "deblurr", "more", "tractable", "-lsb-", "Fergus", "et", "al.", "2006", "-rsb-", "most", "current", "approach", "use", "image", "prior", "model", "from", "local", "image", "statistics", "while", "approach", "have", "show", "some", "promise", "have", "some", "limitation", "generally", "assume", "spatially", "invariant", "blur", "have", "long", "run", "time", "can", "run", "highresolution", "image", "often", "fail", "large", "image", "blur", "one", "most", "significant", "aspect", "camera-shake", "blur", "recent", "work", "have", "overlook", "blur", "usually", "spatially", "invariant", "can", "depth-dependent", "due", "camera", "translation", "depthindependent", "due", "camera", "rotation", "furthermore", "image-based", "method", "can", "always", "distinguish", "unintended", "camera-shake", "blur", "from", "intentional", "defocus", "blur", "e.g.", "when", "intentional", "shallow", "depth", "field", "many", "method", "treat", "all", "type", "blur", "equally", "intentional", "defocus", "blur", "may", "remove", "create", "oversharpen", "image", "we", "address", "some", "limitation", "combined", "hardware", "software-based", "approach", "we", "have", "present", "novel", "hardware", "attachment", "can", "affix", "any", "consumer", "camera", "device", "use", "inexpensive", "gyroscope", "accelerometer", "measure", "camera?s", "acceleration", "angular", "velocity", "during", "exposure", "datum", "use", "input", "novel", "aid", "blind-deconvolution", "algorithm", "compute", "spatially-varying", "image", "blur", "latent", "deblurred", "image", "we", "derive", "model", "handle", "spatially-varying", "blur", "due", "full", "6-dof", "camera", "motion", "spatially-varying", "scene", "depth", "however", "we", "system", "assume", "spatially", "invariant", "depth", "instrument", "camera", "inertial", "measurement", "sensor", "we", "can", "obtain", "relevant", "information", "about", "camera", "motion", "thus", "camera-shake", "blur", "however", "many", "challenge", "use", "information", "effectively", "Motion", "tracking", "use", "inertial", "sensor", "prone", "significant", "error", "when", "track", "over", "extended", "time", "error", "know", "drift", "occur", "due", "integration", "noisy", "measurement", "which", "lead", "increase", "inaccuracy", "track", "position", "over", "time", "we", "show", "we", "experiment", "use", "inertial", "sensor", "directly", "sufficient", "camera", "tracking", "deblurring", "instead", "we", "use", "inertial", "datum", "record", "blurry", "image", "together", "image", "prior", "novel", "aid", "blind-deconvolution", "method", "compute", "camera-induced", "motion", "blur", "latent", "deblurred", "image", "use", "energy", "minimization", "framework", "we", "consider", "algorithm", "aid", "blind-deconvolution", "since", "only", "give", "estimate", "psf", "from", "sensor", "we", "method", "completely", "automatic", "handle", "per-pixel", "spatially-varying", "blur", "out-performs", "current", "lead", "image-based", "method", "we", "experiment", "show", "handle", "large", "kernel", "up", "100", "pixel", "typical", "size", "30", "pixel", "second", "contribution", "we", "expand", "previous", "work", "develop", "validation", "method", "recover", "ground-truth", "per-pixel", "spatiallyvarying", "motion", "blur", "due", "camera-shake", "we", "use", "method", "validate", "we", "hardware", "blur", "estimation", "approach", "also", "use", "study", "property", "motion", "blur", "due", "camera", "shake", "specifically", "we", "work", "have", "follow", "contribution", "-lrb-", "-rrb-", "novel", "hardware", "attachment", "consumer", "camera", "measure", "camera", "motion", "-lrb-", "-rrb-", "novel", "aid", "blind-deconvolution", "algorithm", "combine", "natural", "image", "prior", "we", "sensor", "datum", "estimate", "spatially-varying", "psf", "-lrb-", "-rrb-", "deblurring", "method", "use", "novel", "spatially-varying", "image", "deconvolution", "method", "only", "remove", "camera-shake", "blur", "leave", "intentional", "artistic", "blur", "-lrb-", "i.e.", "shallow", "dof", "-rrb-", "intact", "-lrb-", "-rrb-", "method", "accurately", "measure", "spatiallyvary", "camera-induced", "motion", "blur", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "30:2", "N.", "Joshi", "et", "al.", "related", "work", "image", "deblurring", "have", "recently", "receive", "lot", "attention", "computer", "graphic", "vision", "community", "image", "deblurring", "combination", "two", "tightly", "couple", "sub-problem", "psf", "estimation", "non-blind", "image", "deconvolution", "problem", "have", "be", "address", "both", "independently", "jointly", "-lsb-", "Richardson", "1972", "-rsb-", "both", "longstanding", "problem", "computer", "graphic", "computer", "vision", "image", "processing", "thus", "entire", "body", "previous", "work", "area", "beyond", "what", "can", "cover", "here", "more", "depth", "review", "earlier", "work", "blur", "estimation", "we", "refer", "reader", "survey", "paper", "Kundur", "Hatzinakos", "-lsb-", "1996", "-rsb-", "Blind", "deconvolution", "inherently", "ill-posed", "problem", "due", "loss", "information", "during", "blur", "early", "work", "area", "significantly", "constrain", "form", "kernel", "while", "more", "recently", "researcher", "have", "put", "constraint", "underlie", "sharp", "image", "-lsb-", "Bascle", "et", "al.", "1996", "Fergus", "et", "al.", "2006", "Yuan", "et", "al.", "2007", "Joshi", "et", "al.", "2008", "-rsb-", "alternative", "approach", "those", "use", "additional", "hardware", "augment", "camera", "aid", "blur", "process", "-lsb-", "Ben-Ezra", "Nayar", "2004", "Tai", "et", "al.", "2008", "Park", "et", "al.", "2008", "-rsb-", "most", "common", "commercial", "approach", "reduce", "image", "blur", "image", "stabilization", "-lrb-", "-rrb-", "method", "use", "high-end", "lens", "now", "appear", "lower-end", "point", "shoot", "camera", "use", "mechanical", "means", "dampen", "camera", "motion", "offset", "lens", "element", "translate", "sensor", "method", "similar", "we", "work", "use", "inertial", "sensor", "reduce", "blur", "several", "significant", "difference", "fundamentally", "try", "dampen", "motion", "assume", "past", "motion", "predict", "future", "motion", "-lsb-", "Canon", "1993", "-rsb-", "however", "do", "counteract", "actual", "camera", "motion", "during", "exposure", "nor", "do", "actively", "remove", "blur", "only", "reduce", "blur", "contrast", "we", "method", "record", "actual", "camera", "motion", "remove", "blur", "from", "image", "further", "difference", "method", "can", "only", "dampen", "2d", "motion", "e.g.", "method", "handle", "camera", "roll", "while", "we", "method", "can", "handle", "six", "degree", "motion", "say", "we", "method", "could", "use", "conjunction", "image", "stabilization", "one", "be", "able", "obtain", "reading", "mechanical", "offset", "perform", "system", "recent", "research", "hardware-based", "approach", "image", "deblurr", "modify", "image", "capture", "process", "aid", "deblurring", "area", "we", "work", "most", "similar", "approach", "use", "hybrid", "camera", "-lsb-", "Ben-Ezra", "Nayar", "2004", "Tai", "et", "al.", "2008", "-rsb-", "which", "track", "camera", "motion", "use", "datum", "from", "video", "camera", "attach", "still", "camera", "work", "compute", "global", "frame-to-frame", "motion", "calculate", "2d", "camera", "motion", "during", "image-exposure", "window", "we", "work", "similar", "since", "we", "also", "track", "motion", "during", "exposure", "window", "however", "we", "use", "inexpensive", "small", "lightweight", "sensor", "instead", "second", "camera", "allow", "we", "measure", "more", "degree", "camera", "motion", "higher-rate", "lower", "cost", "another", "difficulty", "hybrid", "camera", "approach", "we", "avoid", "can", "difficult", "get", "high-quality", "properly", "expose", "image", "out", "video", "camera", "low", "light", "condition", "where", "image", "blur", "prevalent", "we", "do", "however", "use", "modified", "form", "Ben-Ezra", "Nayar?s", "work", "controlled", "situation", "help", "validate", "we", "estimate", "camera", "motion", "also", "similar", "we", "work", "Park", "et", "al.", "-lsb-", "2008", "-rsb-", "who", "use", "3-axis", "accelerometer", "measure", "motion", "blur", "main", "difference", "between", "we", "work", "theirs", "we", "additionally", "measure", "axis", "rotational", "velocity", "we", "discuss", "later", "we", "have", "find", "axis", "acceleration", "insufficient", "accurately", "measure", "motion", "blur", "rotation", "commonly", "significant", "part", "blur", "we", "work", "complementary", "hardware-based", "deblurring", "work", "Levin", "el", "al.", "-lsb-", "2008", "-rsb-", "who", "show", "move", "camera", "along", "parabolic", "arc", "one", "can", "create", "image", "1d", "blur", "due", "object", "scene", "can", "remove", "regardless", "speed", "direction", "motion", "we", "work", "also", "complementary", "Raskar", "et", "al.", "-lsb-", "2006", "-rsb-", "who", "develop", "fluttered", "camera", "shutter", "create", "image", "blur", "more", "easily", "invert", "deblurr", "use", "Inertial", "Sensors", "section", "we", "describe", "design", "challenge", "decision", "build", "we", "sense", "platform", "image", "deblurring", "we", "first", "review", "image", "blur", "process", "from", "perspective", "six", "degree", "motion", "camera", "next", "we", "give", "overview", "camera", "dynamics", "inertial", "sensor", "follow", "we", "deblurr", "approach", "3.1", "Camera", "Motion", "Blur", "spatially", "invariant", "image", "blur", "model", "convolution", "latent", "sharp", "image", "shift-invariant", "kernel", "plus", "noise", "which", "typically", "consider", "additive", "white", "gaussian", "noise", "specifically", "blur", "formation", "commonly", "model", "where", "blur", "kernel", "-lrb-", "-rrb-", "noise", "few", "exception", "most", "image", "deblurr", "work", "assume", "spatially", "invariant", "kernel", "however", "often", "do", "hold", "practice", "-lsb-", "Joshi", "et", "al.", "2008", "Levin", "et", "al.", "2009", "-rsb-", "fact", "many", "property", "camera", "scene", "can", "lead", "spatially-varying", "blur", "-lrb-", "-rrb-", "depth", "dependent", "defocus", "blur", "-lrb-", "-rrb-", "defocus", "blur", "due", "focal", "length", "variation", "over", "image", "plane", "-lrb-", "-rrb-", "depth", "dependent", "blur", "due", "camera", "translation", "-lrb-", "-rrb-", "camera", "roll", "motion", "-lrb-", "-rrb-", "camera", "yaw", "pitch", "motion", "when", "strong", "perspective", "effect", "work", "we", "goal", "handle", "only", "camera", "induce", "motion", "blur", "i.e.", "spatiallyvary", "blur", "due", "last", "three", "factor", "first", "let", "we", "consider", "image", "camera", "capture", "during", "its", "exposure", "window", "intensity", "light", "from", "scene", "point", "-lrb-", "-rrb-", "instantaneous", "time", "capture", "image", "plane", "location", "-lrb-", "-rrb-", "which", "function", "camera", "projection", "matrix", "homogenous", "coordinate", "can", "write", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "Sensor", "Data", "image", "deblurr", "use", "Inertial", "Measurement", "Sensors", "30:3", "blurry", "image", "Compute", "Camera", "Motion", "Sensor", "Data", "Figure", "we", "image", "deblurr", "algorithm", "First", "sensor", "datum", "use", "compute", "initial", "guess", "camera", "motion", "from", "we", "use", "image", "datum", "search", "small", "perturbation", "end", "point", "camera", "translation", "overcome", "drift", "use", "result", "we", "compute", "spatially-varying", "blur", "matrix", "deconvolve", "image", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "camera", "motion", "vary", "time", "function", "camera", "rotation", "translation", "cause", "fix", "point", "scene", "project", "different", "location", "each", "time", "integration", "project", "observation", "create", "blur", "image", "project", "trajectory", "each", "point", "image", "plane", "point?s", "point-spread", "function", "-lrb-", "psf", "-rrb-", "camera", "projection", "matrix", "can", "decompose", "k?e", "where", "intrinsic", "matrix", "canonical", "perspective", "projection", "matrix", "time", "dependent", "extrinsic", "matrix", "compose", "camera", "rotation", "translation", "case", "image", "blur", "necessary", "consider", "absolute", "motion", "camera", "only", "relative", "motion", "its", "effect", "image", "we", "model", "consider", "planar", "homography", "map", "initial", "projection", "point", "any", "other", "time", "i.e.", "reference", "coordinate", "frame", "coincident", "frame", "time", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lsb-", "-lrb-", "-rrb-", "-rsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "particular", "depth", "where", "unit", "vector", "orthogonal", "image", "plane", "thus", "give", "image", "time", "pixel", "value", "any", "subsequent", "image", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "image", "warp", "can", "re-written", "matrix", "form", "-lrb-", "-rrb-", "where", "column-vectorized", "image", "-lrb-", "-rrb-", "sparse", "re-sampling", "matrix", "implement", "image", "warping", "resampling", "due", "homography", "each", "row", "-lrb-", "-rrb-", "contain", "weight", "compute", "value", "pixel", "-lrb-", "-rrb-", "interpolation", "point", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "use", "bilinear", "interpolation", "thus", "four", "value", "per", "row", "we", "can", "now", "define", "alternative", "formulation", "image", "blur", "integration", "apply", "homography", "over", "time", "-lrb-", "-rrb-", "idt", "spatially", "invariant", "kernel", "equation", "now", "replace", "spatially-variant", "blur", "represent", "sparse-matrix", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "dt", "deblurred", "image", "drift", "Deblur", "Correction", "image", "we", "spatially-varying", "blur", "model", "give", "10", "-lrb-", "-rrb-", "N.", "thus", "camera-induced", "spatially-varying", "blur", "estimation", "process", "reduce", "estimate", "rotation", "translation", "time", "-lsb-", "...", "-rsb-", "scene", "depths", "camera", "intrinsic", "K.", "represent", "camera-shake", "blur", "six", "degree", "motion", "camera", "instead", "purely", "image", "plane", "number", "unknown", "reduce", "significantly", "six", "unknown", "per", "each", "time-step", "unknown", "depth", "per-pixel", "-lrb-", "unknown", "-rrb-", "camera", "intrinsic", "which", "focal", "length", "most", "important", "factor", "result", "6m", "wh", "unknown", "oppose", "image-based", "approach", "must", "recover", "kernel", "each", "pixel", "result", "wh", "unknown", "practice", "since", "we", "assume", "single", "depth", "scene", "unknown", "we", "system", "reduce", "6m", "value", "know", "image", "can", "deblurr", "use", "nonblind", "deconvolution", "we", "modify", "formulation", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "use", "we", "spatially-varying", "blur", "model", "we", "formulate", "image", "deconvolution", "use", "bayesian", "framework", "find", "most", "likely", "estimate", "sharp", "image", "give", "observe", "blur", "image", "blur", "matrix", "noise", "level", "use", "maximum", "posteriorus", "-lrb-", "map", "-rrb-", "technique", "we", "express", "maximization", "over", "probability", "distribution", "posterior", "use", "Bayes", "rule", "result", "minimization", "sum", "negative", "log", "likelihood", "3.2", "spatially-varying", "deconvolution", "11", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "12", "argmax", "-lrb-", "-rrb-", "argmin", "-lsb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rsb-", "problem", "deconvolution", "now", "reduce", "minimize", "negative", "log", "likelihood", "term", "give", "blur", "formation", "model", "-lrb-", "equation", "-rrb-", "datum", "negative", "log", "likelihood", "13", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "contribution", "we", "deconvolution", "approach", "new", "datum", "term", "use", "spatially-varying", "model", "derive", "previous", "section", "we", "image", "negative", "log", "likelihood", "same", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "sparse", "gradient", "penalty", "which", "enforce", "hyper-laplacian", "distribution", "-lrb-", "-rrb-", "0.8", "minimization", "perform", "use", "iteratively", "re-weighted", "least-square", "-lsb-", "Stewart", "1999", "-rsb-", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "30:4", "N.", "Joshi", "et", "al.", "3.3", "rigid", "Body", "Dynamics", "Inertial", "Sensors", "discuss", "previous", "section", "camera", "motion", "blur", "dependent", "rotation", "translation", "time", "-lsb-", "...", "-rsb-", "scene", "depths", "camera", "intrinsic", "section", "we", "discuss", "how", "recover", "camera", "rotation", "translation", "section", "4.2", "we", "address", "recover", "camera", "intrinsic", "any", "motion", "rigid", "body", "any", "point", "body", "can", "parameterize", "function", "six", "unknown", "three", "rotation", "three", "translation", "we", "now", "describe", "how", "recover", "quantity", "give", "inertial", "measurement", "from", "accelerometer", "gyroscope", "Accelerometers", "measure", "total", "acceleration", "give", "point", "along", "axis", "while", "gyroscope", "measure", "angular", "velocity", "give", "point", "around", "axis", "note", "move", "rigid", "body", "pure", "rotation", "all", "point", "same", "while", "translation", "all", "point", "same", "when", "body", "rotate", "before", "derive", "how", "compute", "camera", "motion", "from", "inertial", "measurement", "we", "first", "present", "we", "notation", "summarize", "Table", "symbol", "description", "initial", "current", "frame", "current", "angular", "po.", "vel.", "accel", "initial", "frame", "current", "angular", "vel", "current", "frame", "current", "po.", "vel", "accel", "initial", "frame", "Accel", "accelerometer", "current", "frame", "position", "accelerometer", "initial", "frame", "Vector", "from", "accelerometer", "center", "rotation", "gravity", "camera?s", "initial", "coordinate", "frame", "Table", "quantity", "bold", "indicate", "measure", "observe", "value", "note", "vector", "i.e.", "three", "axis", "quantity", "superscript", "character", "indicate", "coordinate", "system", "value", "subscript", "indicate", "value", "measure", "rigid", "body", "camera", "three", "axis", "accelerometer", "three", "axis", "gyroscope", "-lrb-", "three", "accelerometer", "gyroscope", "mount", "along", "single", "chip", "respectively", "-rrb-", "measure", "follow", "acceleration", "angular", "velocity", "15", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "15", "measure", "acceleration", "sum", "acceleration", "due", "translation", "camera", "centripetal", "acceleration", "due", "rotation", "tangential", "component", "angular", "acceleration", "gravity", "all", "rotate", "current", "frame", "camera", "measure", "angular", "velocity", "camera?s", "angular", "velocity", "also", "rotate", "current", "frame", "camera", "recover", "relative", "camera", "rotation", "necessary", "recover", "angular", "velocity", "each", "time-step", "coordinate", "system", "initial", "frame", "which", "can", "integrate", "get", "angular", "position", "recover", "relative", "camera", "translation", "we", "need", "first", "compute", "accelerometer", "position", "each", "time-step", "relative", "initial", "frame", "from", "we", "can", "recover", "camera", "translation", "camera", "rotation", "can", "recover", "sequentially", "integrate", "rotate", "measure", "angular", "velocity", "initial", "camera", "frame", "17", "=-lrb-_NN", "-rrb-", "angleaxist", "om", "-lrb-", "-rrb-", "17", "tt", "ii", "tt", "tt", "ii", "ii", "tt", "tt", "pc", "tt", "figure", "rigid-body", "dynamics", "we", "camera", "setup", "where", "angleaxist", "om", "convert", "angular", "position", "vector", "rotation", "matrix", "since", "we", "only", "concern", "relative", "rotation", "initial", "rotation", "zero", "18", "identity", "once", "rotation", "compute", "each", "time-step", "we", "can", "compute", "acceleration", "initial", "frame?s", "coordinate", "system", "19", "19", "integrate", "acceleration", "minus", "constant", "acceleration", "gravity", "get", "accelerometer?s", "relative", "position", "each", "timestep", "20", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "21", "-lrb-", "-rrb-", "0.5", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "concern", "relative", "position", "we", "set", "initial", "position", "zero", "we", "also", "assume", "initial", "velocity", "zero", "22", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lsb-", "-rsb-", "accelerometer", "translation", "-lrb-", "its", "position", "relative", "initial", "frame", "-rrb-", "term", "rigid", "body", "rotation", "translation", "23", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "give", "we", "can", "compute", "camera", "position", "time", "24", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "equation", "20", "necessary", "subtract", "value", "gravity", "initial", "frame", "camera", "we", "note", "however", "initial", "rotation", "camera", "relative", "world", "unknown", "gyroscope", "only", "measure", "velocity", "accelerometer", "can", "use", "estimate", "initial", "orientation", "camera", "initially", "have", "external", "force", "other", "than", "gravity", "we", "have", "find", "assumption", "unreliable", "so", "we", "instead", "make", "assumption", "measure", "acceleration", "normally", "distribute", "about", "constant", "force", "gravity", "we", "have", "find", "reliable", "when", "camera", "motion", "due", "high-frequency", "camera-shake", "thus", "we", "set", "direction", "mean", "acceleration", "vector", "direction", "gravity", "25", "mean", "-lrb-", "-lrb-", "-rrb-", "-lsb-", "...", "-rsb-", "-rrb-", "-rrb-", "summarize", "camera", "rotation", "translation", "recover", "integrate", "measure", "acceleration", "angular", "velocity", "rotate", "camera?s", "initial", "coordinate", "frame", "give", "we", "relative", "rotation", "translation", "over", "time", "which", "use", "compute", "spatially-varying", "psf", "matrix", "equation", "10", "measurement", "noise-free", "rotation", "motion", "information", "sufficient", "deblurr", "however", "practice", "sensor", "noise", "introduce", "significant", "error", "furthermore", "even", "camera", "motion", "know", "perfectly", "one", "still", "need", "know", "scene", "depth", "discuss", "section", "thus", "additional", "step", "need", "deblur", "image", "use", "inertial", "datum", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "image", "deblurring", "use", "Inertial", "Measurement", "Sensors", "30:5", "3.4", "Drift", "Compensation", "Deconvolution", "well", "know", "compute", "motion", "integrate", "differential", "sensor", "can", "lead", "drift", "computed", "result", "drift", "due", "noise", "present", "sensor", "reading", "integration", "noisy", "signal", "lead", "temporally", "increase", "deviation", "computed", "motion", "from", "true", "motion", "we", "have", "measure", "standard", "deviation", "we", "gyroscope?s", "noise", "0.5", "deg/s", "accelerometer", "noise", "0.006", "m/s", "use", "sample", "from", "when", "gyroscope", "accelerometer", "hold", "stationary", "-lrb-", "zero", "angular", "velocity", "constant", "acceleration", "respectively", "-rrb-", "we", "experiment", "significantly", "less", "drift", "rotation", "due", "need", "perform", "only", "single", "integration", "step", "gyroscope", "datum", "necessity", "integrate", "twice", "get", "positional", "datum", "from", "accelerometer", "cause", "more", "drift", "get", "high-quality", "deblurring", "result", "we", "must", "overcome", "drift", "we", "propose", "novel", "aid", "blind", "deconvolution", "algorithm", "compute", "camera-motion", "in-turn", "image", "blur", "function", "best", "match", "measure", "acceleration", "angular", "velocity", "while", "maximize", "likelihood", "deblurred", "latent", "image", "accord", "natural", "image", "prior", "we", "deconvolution", "algorithm", "compensate", "positional", "drift", "assume", "linear", "time", "which", "can", "estimate", "one", "know", "final", "end", "position", "camera", "we", "assume", "rotational", "drift", "minimal", "final", "camera", "position", "course", "unknown", "however", "we", "know", "drift", "bound", "thus", "correct", "final", "position", "should", "lie", "close", "we", "estimate", "from", "sensor", "datum", "thus", "contrast", "traditional", "blind-deconvolution", "algorithm", "solve", "each", "value", "kernel", "psf", "we", "algorithm", "only", "have", "solve", "few", "unknown", "we", "solve", "use", "energy", "minimization", "framework", "perform", "search", "small", "local", "neighborhood", "around", "initially", "compute", "end", "point", "we", "experiment", "we", "have", "find", "camera", "travel", "order", "couple", "millimeter", "during", "long", "exposure", "-lrb-", "longest", "we", "have", "try", "1/2", "second", "-rrb-", "we", "note", "few", "millimeter", "translation", "depth", "-lrb-", "-rrb-", "have", "little", "effect", "image", "lens", "common", "focal", "length", "thus", "drift", "only", "significant", "source", "error", "we", "set", "we", "optimization", "parameter", "search", "optimal", "end", "point", "within", "1mm", "radius", "initially", "compute", "end", "point", "subject", "constraint", "acceleration", "along", "recover", "path", "match", "measure", "acceleration", "best", "least-square", "sense", "optimal", "end", "point", "one", "result", "deconvolved", "image", "highest", "log-likelihood", "measure", "hyper-laplacian", "image", "prior", "-lrb-", "discuss", "section", "3.1", "-rrb-", "specifically", "let", "we", "define", "function", "give", "potential", "end", "point", "-lrb-", "-rrb-", "compute", "camera?s", "translational", "path", "which", "best", "match", "least", "square", "sense", "observe", "acceleration", "terminate", "-lrb-", "-rrb-", "26", "-lrb-", "-rrb-", "argmin", "-lrb-", "dt", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "Figure", "Ground", "Truth", "Blur", "Measurements", "we", "attach", "highspeed", "camera", "next", "slr", "capture", "high-speed", "video", "frame", "during", "camera", "exposure", "additional", "wide-baseline", "shot", "we", "perform", "3d", "reconstruction", "use", "bundle", "adjustment", "we", "show", "we", "high-speed", "camera", "attachment", "few", "image", "from", "highspeed", "camera", "-lrb-", "we", "take", "about", "hundred", "total", "process", "-rrb-", "notational", "convenience", "let", "define", "function", "form", "blur", "sampling", "matrix", "from", "camera", "intrinsic", "extrinsic", "scene", "depth", "use", "rigid-body", "dynamics", "temporal", "integration", "process", "discuss", "section", "3.1", "3.3", "27", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "drift-compensated", "blur", "matrix", "deconvolution", "equation", "28", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "29", "argmin", "-lsb-", "-lrb-", "-rrb-", "0.8", "-rsb-", "we", "search", "over", "space", "-lrb-", "-rrb-", "find", "-lrb-", "-rrb-", "result", "image", "have", "highest", "likelihood", "give", "observation", "image", "prior", "we", "perform", "energy", "minimization", "use", "Nelder-Mead", "simplex", "method", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.2", "use", "error", "function", "inner", "loop", "optimization", "we", "perform", "optimization", "1/10", "down-sampled", "version", "-lrb-", "we", "21", "mp", "image", "-rrb-", "result", "from", "we", "search", "process", "show", "plot", "camera", "motion", "Figure", "visually", "figure", "when", "use", "deblur", "image", "run", "time", "search", "method", "about", "minute", "0.75", "mp", "image", "search", "only", "need", "run", "once", "either", "entire", "image", "could", "run", "subsection", "image", "preferable", "once", "drift", "correct", "PSF", "more", "accurate", "entire", "image", "Computing", "Scene", "Depth", "note", "spatially", "invariant", "scene", "depth", "implicitly", "compute", "during", "drift", "compensation", "process", "scale", "end", "point", "equally", "dimension", "equivalent", "scale", "depth", "value", "specifically", "optimization", "over", "u/d", "v/d", "thus", "solve", "single", "depth", "value", "entire", "scene", "Deblurring", "System", "previous", "section", "we", "discuss", "how", "remove", "camera", "motion", "blur", "recover", "camera", "rotation", "translation", "from", "accelerometer", "gyroscope", "section", "we", "describe", "we", "hardware", "record", "accelerometer", "gyroscope", "datum", "implementation", "related", "concern", "challenge", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "30:6", "N.", "Joshi", "et", "al.", "Figure", "drift", "compensation", "angular", "translational", "position", "camera", "versus", "time", "show", "image", "blue", "box", "Figure", "ground-truth", "motion", "compute", "use", "structure", "from", "motion", "show", "solid", "line", "dash", "line", "motion", "from", "use", "raw", "sensor", "datum", "marked", "line", "result", "after", "drift", "compensation", "drift", "compensate", "result", "much", "closer", "ground-truth", "result", "note", "bottom", "right", "plot", "do", "show", "drift", "correct", "line", "we", "only", "compensate", "drift", "4.1", "Hardware", "Design", "since", "six", "unknown", "per", "time-step", "minimal", "configuration", "sensor", "six", "possible", "recover", "rotation", "translation", "use", "six", "accelerometer", "alone", "sense", "each", "axis", "pair", "three", "different", "point", "rigid-body", "however", "after", "experiment", "method", "we", "find", "accelerometer", "alone", "too", "noisy", "reliable", "computation", "rotation", "thus", "we", "prototype", "hardware", "system", "show", "Figure", "minimal", "configuration", "consist", "three-axis", "1.5", "mem", "accelerometer", "package", "three", "single", "axis", "150", "mem", "gyroscope", "wire", "Arduino", "controller", "board", "Bluetooth", "radio", "all", "part", "commodity", "off-theshelf", "component", "purchase", "online", "additionally", "slr?s", "hotshoe", "i.e.", "flash", "trigger", "signal", "wire", "Arduino", "board", "trigger", "signal", "from", "slr", "remain", "low", "entire", "length", "exposure", "high", "otherwise", "Arduino", "board", "interrupt", "drive", "when", "trigger", "signal", "from", "SLR", "fire", "accelerometer", "gyroscope", "poll", "200hz", "during", "exposure", "window", "each", "time", "sensor", "read", "value", "send", "over", "Bluetooth", "serial", "port", "interface", "additionally", "internal", "high-resolution", "counter", "read", "actual", "elapsed", "time", "between", "each", "reading", "sensor", "report", "sensor", "Arduino", "board", "mount", "laser-cut", "acrylic", "base", "secure", "board", "sensor", "battery", "pack", "acrylic", "mount", "tightly", "screw", "camera", "tripod", "mount", "only", "other", "connection", "camera", "flash", "trigger", "cable", "we", "hardware", "attachment", "have", "optional", "feature", "use", "calibration", "validation", "experiment", "mount", "hole", "point", "Grey", "high-speed", "camera", "when", "Arduino", "board", "sampling", "inertial", "sensor", "can", "also", "send", "100", "hz", "trigger", "high-speed", "camera", "we", "use", "high-speed", "datum", "help", "calibrate", "we", "sensor", "acquire", "datum", "get", "ground", "truth", "measurement", "motion", "blur", "Figure", "visualization", "ground-truth", "spatially-varying", "psf", "blurry", "image", "top", "we", "show", "sparsely", "sample", "visualization", "blur", "kernel", "across", "image", "plane", "quite", "significant", "variation", "across", "image", "plane", "demonstrate", "importance", "accounting", "spatially", "variance", "bottom", "row", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "non-corresponding", "area", "4.2", "calibration", "ground-truth", "measurement", "accurately", "compute", "camera", "motion", "from", "inertial", "sensor", "necessary", "calibrate", "several", "aspect", "we", "system", "we", "need", "accurately", "calibrate", "sensor", "response", "we", "have", "find", "they", "deviate", "from", "publish", "response", "range", "also", "necessary", "know", "position", "accelerometer", "relative", "camera?s", "optical", "center", "lastly", "we", "need", "calibrate", "camera", "intrinsic", "we", "calibrate", "value", "two", "stage", "first", "stage", "calibrate", "sensor", "response", "we", "do", "rotate", "gyroscope", "known", "constant", "angular", "velocity", "recover", "mapping", "from", "10-bit", "a/d", "output", "degrees/s", "we", "perform", "measurement", "several", "known", "angular", "velocity", "confirm", "gyroscope", "have", "linear", "response", "calibrate", "accelerometer", "we", "hold", "they", "stationary", "six", "orientation", "respect", "gravity", "-lrb-", "-rrb-", "which", "allow", "we", "map", "a/d", "output", "unit", "m/s", "calibrate", "we", "setup", "measure", "ground-truth", "measurement", "camera-shake", "we", "develop", "method", "accurately", "recover", "camera?s", "position", "during", "exposure", "use", "method", "inspire", "Ben-Ezra", "Nayar", "-lsb-", "2004", "-rsb-", "however", "instead", "track", "2d", "motion", "we", "track", "6d", "motion", "we", "attach", "high-speed", "camera", "-lrb-", "200", "fp", "pointgrey", "DragonFly", "Express", "-rrb-", "we", "sensor", "platform", "we", "Arduino", "micro-controller", "code", "set", "trigger", "high-speed", "camera", "100", "fp", "during", "slr?s", "exposure", "window", "lab", "setting", "we", "create", "scene", "significant", "amount", "texture", "take", "about", "10", "image", "exposure", "range", "from", "1/10", "1/2", "second", "each", "image", "accelerometer", "gyro", "datum", "record", "addition", "high-speed", "frame", "we", "take", "high-speed", "frame", "from", "shot", "acquire", "additional", "widebaseline", "shot", "slr", "high-speed", "camera", "use", "all", "datum", "we", "create", "3d", "reconstruction", "scene", "use", "bundle", "adjustment", "-lrb-", "we", "process", "use", "ransac", "feature", "matching", "compute", "sparse", "3d", "structure", "from", "motion", "compute", "camera", "focal", "length", "-rrb-", "Figure", "show", "we", "high-speed", "camera", "attachment", "few", "frame", "from", "high", "speed", "camera", "-lrb-", "we", "take", "about", "hundred", "total", "both", "camera", "process", "-rrb-", "reconstruction", "process", "give", "we", "collection", "sparse", "3d", "point", "camera", "rotation", "camera", "translation", "unknown", "global", "transformation", "scale", "ambiguity", "between", "camera", "depth", "scene", "depth", "we", "resolve", "scale", "ambiguity", "use", "calibration", "grid", "known", "size", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "image", "deblurring", "use", "Inertial", "Measurement", "Sensors", "30:7", "blurry", "image", "use", "psf", "from", "we", "final", "output", "raw", "sensor", "value", "figure", "deblurring", "result", "comparison", "here", "we", "show", "deblurr", "result", "crop", "portion", "scene", "show", "Figure", "section", "crop", "from", "11", "mega-pixel", "image", "result", "green", "box", "final", "output", "we", "method", "where", "sensor", "datum", "plus", "we", "drift", "compensation", "method", "use", "compute", "camera", "motion", "blur", "subtle", "difference", "psf", "before", "after", "drift", "compensation", "can", "have", "big", "result", "quality", "deconvolution", "result", "we", "now", "describe", "result", "we", "ground-truth", "camera-shake", "measurement", "compare", "result", "use", "we", "deblurring", "method", "ground-truth", "measurement", "we", "also", "compare", "we", "result", "those", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "use", "implementation", "author", "have", "available", "online", "we", "also", "show", "result", "we", "method", "run", "natural", "image", "acquire", "outside", "lab", "setup", "compare", "result", "use", "previous", "work", "well", "5.1", "lab", "experiment", "Camera-Shake", "Study", "Figure", "we", "show", "visualization", "ground-truth", "spatiallyvarying", "psf", "image", "from", "we", "lab", "setup", "image", "show", "some", "interesting", "property", "significant", "variation", "across", "image", "plane", "also", "kernel", "display", "fix", "depth", "thus", "all", "spatial", "variance", "due", "rotation", "demonstrate", "importance", "accounting", "spatial", "variance", "bottom", "row", "Figure", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "psf", "different", "non-corresponding", "area", "result", "quite", "interesting", "show", "some", "common", "assumption", "make", "image", "deconvolution", "do", "always", "hold", "most", "deconvolution", "work", "assume", "spatially", "invariant", "kernel", "which", "really", "only", "apply", "camera", "motion", "under", "orthographic", "model", "however", "typical", "imaging", "setup", "-lrb-", "we", "use", "40mm", "lens", "-rrb-", "perspective", "effect", "strong", "enough", "induce", "spatially-varying", "blur", "we", "also", "note", "often", "roll", "component", "blur", "something", "also", "model", "spatially", "invariant", "kernel", "lastly", "we", "observe", "translation", "thus", "depth", "dependent", "effect", "can", "significant", "which", "interesting", "often", "think", "most", "camera-shake", "blur", "due", "rotation", "please", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "example", "Figure", "use", "same", "scene", "above", "we", "show", "comparison", "deconvolution", "result", "use", "we", "method", "ground-truth", "two", "other", "image", "show", "lab", "calibration", "scene", "be", "take", "exposure", "from", "1/3", "1/10", "second", "each", "image", "we", "show", "input", "result", "deconvolve", "psf", "from", "initial", "motion", "estimate", "after", "perform", "drift", "correction", "compare", "deconvolution", "use", "psf", "from", "recover", "groundtruth", "motion", "psf", "recover", "use", "method", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "latter", "two", "comparison", "we", "make", "best", "effort", "adjust", "parameter", "recover", "best", "blur", "kernel", "possible", "make", "comparison", "fair", "all", "result", "be", "deblurr", "use", "exactly", "same", "deconvolution", "method", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "result", "figure", "show", "wide", "variety", "blur", "yet", "we", "method", "recover", "accurate", "kernel", "provide", "deconvolution", "result", "very", "close", "ground-truth", "all", "case", "we", "result", "better", "than", "those", "use", "Shan", "et", "al.", "Fergus", "et", "al.", "method", "use", "Shan", "et", "al.", "Fergus", "et", "al.", "Groundtruth", "Motion", "5.2", "real", "image", "after", "calibrate", "we", "hardware", "system", "use", "method", "discuss", "section", "4.2", "we", "take", "camera", "outside", "lab", "use", "laptop", "Bluetooth", "adapter", "capture", "inertial", "sensor", "datum", "Figure", "we", "show", "several", "result", "where", "we", "have", "deblurr", "image", "use", "we", "method", "Shan", "et", "al.", "method", "Fergus", "et", "al.", "method", "Shan", "et", "al.", "Fergus", "et", "al.", "result", "be", "deconvolve", "use", "Levin", "et", "al.", "method", "-lsb-", "2007", "-rsb-", "we", "result", "deconvolve", "use", "we", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.1", "all", "image", "we", "result", "show", "clear", "improvement", "over", "input", "blurry", "image", "still", "some", "residually", "ring", "unavoidable", "due", "frequency", "loss", "during", "blur", "Shan", "et", "al.", "result", "vary", "quality", "many", "show", "large", "ring", "artifact", "due", "kernel", "misestimation", "over-sharpening", "Fergus", "et", "al.", "result", "generally", "blurrier", "than", "ours", "all", "image", "show", "here", "be", "shoot", "1/2", "1/10", "second", "exposure", "40mm", "lens", "Canon", "1Ds", "Mark", "III", "additional", "result", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "30:8", "N.", "Joshi", "et", "al.", "input", "imu", "deblurring", "-lrb-", "we", "final", "output", "-rrb-", "figure", "natural", "image", "deblurr", "we", "setup", "all", "image", "we", "result", "show", "clear", "improvement", "over", "input", "blurry", "image", "blur", "kernel", "each", "corner", "image", "show", "size", "still", "some", "residual", "ringing", "unavoidable", "due", "frequency", "loss", "during", "blur", "Shan", "et", "al.", "result", "vary", "quality", "many", "show", "large", "ring", "artifact", "due", "kernel", "misestimation", "over-sharpening", "Fergus", "et", "al.", "result", "generally", "blurry", "than", "we", "result", "stone", "image", "intentionally", "defocused", "background", "stone", "sharpen", "we", "result", "discussion", "future", "work", "work", "we", "present", "aid", "blind", "deconvolution", "algorithm", "use", "hardware", "attachment", "conjunction", "corresponding", "blurry", "input", "image", "natural", "image", "prior", "compute", "perpixel", "spatially-varying", "blur", "deconvolve", "image", "produce", "sharp", "result", "several", "benefit", "we", "method", "over", "previous", "approach", "-lrb-", "-rrb-", "we", "method", "automatic", "have", "user-tuned", "parameter", "-lrb-", "-rrb-", "use", "inexpensive", "commodity", "hardware", "could", "easily", "build", "camera", "produce", "mass-market", "attachment", "more", "compact", "than", "we", "prototype", "we", "have", "show", "advantage", "over", "purely", "image-based", "method", "which", "some", "sense", "surprising?blind", "deconvolution", "inherently", "ill-posed", "problem", "thus", "extra", "information", "inertial", "measurement", "should", "helpful", "many", "challenge", "use", "datum", "properly", "many", "which", "we", "have", "address", "however", "we", "result", "also", "suggest", "several", "area", "future", "work", "biggest", "limitation", "we", "method", "sensor", "accuracy", "noise", "we", "method?s", "performance", "degrade", "under", "few", "case", "-lrb-", "-rrb-", "drift", "large", "enough", "search", "space", "we", "optimization", "process", "too", "large", "i.e.", "greater", "than", "couple", "mm", "-lrb-", "-rrb-", "we", "estimation", "initial", "camera", "rotation", "relative", "gravity", "incorrect", "similarly", "camera", "move", "way", "normally", "distribute", "about", "gravity", "vector", "-lrb-", "-rrb-", "significant", "depth", "variation", "scene", "camera", "undergo", "significant", "translation", "-lrb-", "-rrb-", "camera", "translate", "some", "initial", "constant", "velocity", "-lrb-", "-rrb-", "large", "image", "frequency", "information", "loss", "blur", "handle", "-lrb-", "-rrb-", "we", "interested", "use", "more", "sensor", "sensor", "inexpensive", "one", "could", "easily", "add", "sensor", "redundancy", "perform", "denoising", "average", "either", "analog", "digital", "domain", "-lrb-", "-rrb-", "one", "could", "consider", "add", "other", "sensor", "magnetometer", "get", "another", "measure", "orientation", "already", "common", "approach", "IMU", "base", "navigation", "system", "-lrb-", "-rrb-", "one", "could", "recover", "depth", "scene", "perform", "depth", "from", "motion", "blur", "algorithm", "similar", "depth", "from", "defocus", "we", "pursue", "problem", "however", "important", "note", "vary", "scene", "depth", "do", "always", "significantly", "affect", "blur", "typical", "situation", "we", "have", "find", "depth", "only", "need", "accurate", "deblurring", "object", "within", "meter", "from", "camera", "most", "often", "people", "take", "image", "where", "scene", "farther", "than", "distance", "-lrb-", "-rrb-", "we", "assume", "initial", "translation", "velocity", "zero", "we", "accelerometer", "give", "we", "measure", "while", "we", "currently", "only", "consider", "accelerometer", "datum", "during", "exposure", "we", "actually", "record", "all", "sensor", "datum", "before", "after", "each", "exposure", "well", "thus", "one", "way", "address", "issue", "try", "identify", "stationary", "before", "camera", "exposure", "track", "from", "get", "more", "accurate", "initial", "velocity", "estimate", "-lrb-", "-rrb-", "issue", "all", "deblurr", "method", "frequency", "loss", "cause", "unavoidable", "artifact", "during", "deconvolution", "could", "appear", "ringing", "banding", "over-smoothing", "depend", "deconvolution", "method", "would", "interesting", "combine", "we", "hardware", "Raskar", "et", "al.", "-lsb-", "2006", "-rsb-", "flutter", "shutter", "hardware", "reduce", "frequency", "loss", "during", "image", "capture", "Shan", "et", "al.", "Fergus", "et", "al.", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010", "image", "deblurring", "use", "Inertial", "Measurement", "Sensors", "30:9", "acknowledgement", "we", "would", "like", "thank", "anonymous", "SIGGRAPH", "reviewer", "we", "also", "thank", "Mike", "Sinclair", "Turner", "Whitted", "help", "hardware", "lab", "reference", "ascle", "B.", "LAKE", "a.", "isserman", "a.", "1996", "Motion", "deblurr", "super-resolution", "from", "image", "sequence", "eccv", "96", "Proceedings", "4th", "european", "conference", "computer", "Vision-Volume", "II", "Springer-Verlag", "London", "UK", "573", "582", "en", "zra", "m.", "ayar", "S.", "K.", "2004", "motion-based", "motion", "deblurring", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "26", "689", "698", "anon", "L.", "G.", "1993", "EF", "LENS", "work", "iii", "eye", "eo", "Canon", "Inc.", "ergus", "R.", "ingh", "B.", "ERTZMANN", "A.", "OWEIS", "S.", "T.", "reeman", "W.", "T.", "2006", "remove", "camera", "shake", "from", "single", "photograph", "ACM", "Trans", "graph", "25", "-lrb-", "July", "-rrb-", "787", "794", "OSHI", "N.", "zeliskus", "R.", "RIEGMAN", "D.", "J.", "2008", "psf", "estimation", "use", "sharp", "edge", "prediction", "Computer", "Vision", "Pattern", "recognition", "2008", "CVPR", "2008", "IEEE", "Conference", "UNDUR", "D.", "atzinako", "D.", "1996", "Blind", "image", "deconvolution", "signal", "Processing", "Magazine", "IEEE", "13", "43", "64", "evin", "a.", "ergus", "R.", "URAND", "F.", "reeman", "W.", "T.", "2007", "image", "depth", "from", "conventional", "camera", "code", "aperture", "ACM", "Trans", "graph", "26", "-lrb-", "July", "-rrb-", "Article", "70", "evin", "a.", "P.", "HO", "T.", "S.", "URAND", "F.", "reeman", "W.", "T.", "2008", "motion-invariant", "photography", "ACM", "Trans", "graph", "27", "-lrb-", "August", "-rrb-", "71:1", "71:9", "evin", "a.", "EISS", "Y.", "URAND", "F.", "reeman", "W.", "2009", "understand", "evaluate", "blind", "deconvolution", "algorithm", "Computer", "Vision", "Pattern", "recognition", "2009", "CVPR", "2009", "IEEE", "Conference", "IEEE", "Computer", "Society", "1964", "1971", "ark", "s.-y.", "ark", "e.-s.", "IM", "h.-i", "2008", "image", "deblurr", "use", "vibration", "information", "from", "3-axis", "accelerometer", "Journal", "Institute", "Electronics", "Engineers", "Korea", "sc", "System", "control", "45", "11", "askar", "R.", "GRAWAL", "a.", "umblin", "J.", "2006", "code", "exposure", "photography", "motion", "deblurr", "use", "flutter", "shutter", "ACM", "Trans", "graph", "25", "-lrb-", "July", "-rrb-", "795", "804", "ichardson", "W.", "H.", "1972", "bayesian-based", "iterative", "method", "image", "restoration", "Journal", "Optical", "Society", "America", "-lrb-", "1917-1983", "-rrb-", "62", "55", "59", "HAN", "Q.", "IA", "J.", "garwalum", "a.", "2008", "high-quality", "motion", "deblurr", "from", "single", "image", "ACM", "Trans", "graph", "27", "-lrb-", "August", "-rrb-", "73:1", "73:10", "tewart", "C.", "V.", "1999", "robust", "parameter", "estimation", "computer", "vision", "SIAM", "review", "41", "-lrb-", "September", "-rrb-", "513", "537", "aus", "y.-w.", "H.", "ROWN", "M.", "S.", "S.", "2008", "image/video", "deblurring", "use", "hybrid", "camera", "Computer", "Vision", "Pattern", "recognition", "2008", "CVPR", "2008", "IEEE", "Conference", "uan", "L.", "UN", "J.", "UAN", "L.", "hum", "h.-y", "2007", "image", "deblurr", "blurred/noisy", "image", "pair", "ACM", "Trans", "graph", "26", "-lrb-", "July", "-rrb-", "Article", "ACM", "transaction", "Graphics", "Vol", "29", "no.", "Article", "30", "publication", "date", "July", "2010" ],
  "content" : "\n  \n    4ceb827e6a483db80b16aa1b759c48a08f6b9cdcb12301eb1f9bcb7fc9f55681\n    mig\n    10.1145/1778765.1778767\n    Name identification was not possible. \n  \n  \n    \n      \n        Image Deblurring using Inertial Measurement Sensors\n      \n      \n        \n      \n      Joshi Sing Bing Kang C. Lawrence Zitnick Microsoft Research\n      Bluetooth Radio 3-axis Accelerometer\n      \n        \n      \n      Arduino Board SLR Trigger Gyros\n      \n        Figure 1: An SLR Camera instrumented with our image deblurring attachment that uses inertial measurement sensors and the input image in an ?aided blind-deconvolution? algorithm to automatically deblur images with spatially-varying blurs (first two images). A blurry input image (third image) and the result of our method (fourth image). The blur kernel at each corner of the image is shown at 2? size.\n        \n        \n      \n      We present a deblurring algorithm that uses a hardware attachment coupled with a natural image prior to deblur images from consumer cameras. Our approach uses a combination of inexpensive gyroscopes and accelerometers in an energy optimization framework to estimate a blur function from the camera?s acceleration and angular velocity during an exposure. We solve for the camera motion at a high sampling rate during an exposure and infer the latent image using a joint optimization. Our method is completely automatic, handles per-pixel, spatially-varying blur, and out-performs the current leading image-based methods. Our experiments show that it handles large kernels ? up to at least 100 pixels, with a typical size of 30 pixels. We also present a method to perform ?ground-truth? measurements of camera motion blur. We use this method to validate our hardware and deconvolution approach. To the best of our knowledge, this is the first work that uses 6 DOF inertial sensors for dense, per-pixel spatially-varying image deblurring and the first work to gather dense ground-truth measurements for camera-shake blur.\n    \n    \n      \n        1 Introduction\n      \n      Intentional blur can be used to great artistic effect in photography. However, in many common imaging situations, blur is a nuisance. Camera motion blur often occurs in light-limited situations and is one of the most common reason for discarding a photograph. If the blur function is known, the image can be improved by deblurring it with a non-blind deconvolution method. However, for most images, the blur function is unknown and must be recov-\n      \n        ACM Reference Format\n      \n      Joshi, N., Kang, S., Zitnick, C., Szeliski, R. 2010. Image Deblurring using Inertial Measurement Sensors. ACM Trans. Graph. 29, 4, Article 30 (July 2010), 9 pages. DOI = 10.1145/1778765.1778767 http://doi.acm.org/10.1145/1778765.1778767.\n      \n        Copyright Notice\n      \n      Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2010 ACM 0730-0301/2010/07-ART30 $10.00 DOI 10.1145/1778765.1778767 http://doi.acm.org/10.1145/1778765.1778767\n      Richard Szeliski\n      ered. Recovering both the blur or ?point-spread function? (PSF) and the desired deblurred image from a single blurred input (known as the blind-deconvolution problem) is inherently ill-posed, as the observed blurred image provides only a partial constraint on the solution. Prior knowledge about the image or kernel can disambiguate the potential solutions and make deblurring more tractable [Fergus et al. 2006]. Most current approaches use image priors modeled from local image statistics. While these approaches have shown some promise, they have some limitations: they generally assume spatially invariant blur, have long run times, cannot be run on highresolution images, and often fail for large image blurs. One of the most significant aspects of camera-shake blur that recent work has overlooked is that the blur is usually not spatially invariant. This can be depth-dependent due to camera translation, or depthindependent, due to camera rotation. Furthermore, image-based methods cannot always distinguish unintended camera-shake blur from intentional defocus blur, e.g., when there is an intentional shallow depth of field. As many methods treat all types of blur equally, intentional defocus blur may be removed, creating an oversharpened image. We address some of these limitations with a combined hardware and software-based approach. We have present a novel hardware attachment that can be affixed to any consumer camera. The device uses inexpensive gyroscopes and accelerometers to measure a camera?s acceleration and angular velocity during an exposure. This data is used as an input to a novel ?aided blind-deconvolution? algorithm that computes the spatially-varying image blur and latent deblurred image. We derive a model that handles spatially-varying blur due to full 6-DOF camera motion and spatially-varying scene depth; however, our system assumes spatially invariant depth. By instrumenting a camera with inertial measurement sensors, we can obtain relevant information about the camera motion and thus the camera-shake blur; however, there are many challenges in using this information effectively. Motion tracking using inertial sensors is prone to significant error when tracking over an extended period of time. This error, known as ?drift?, occurs due to the integration of the noisy measurements, which leads to increasing inaccuracy in the tracked position over time. As we will show in our experiments, using inertial sensors directly is not sufficient for camera tracking and deblurring.  Instead, we use the inertial data and the recorded blurry image together with an image prior in a novel ?aided blind-deconvolution? method that computes the camera-induced motion blur and the latent deblurred image using an energy minimization framework. We consider the algorithm to be ?aided blind-deconvolution?, since it is only given an estimate of the PSF from the sensors. Our method is completely automatic, handles per-pixel, spatially-varying blur, out-performs current leading image-based methods, and our experiments show it handles large kernels ? up to 100 pixels, with a typical size of 30 pixels. As a second contribution, we expand on previous work and develop a validation method to recover ?ground-truth?, per-pixel spatiallyvarying motion blurs due to camera-shake. We use this method to validate our hardware and blur estimation approach, and also use it to study the properties of motion blur due to camera shake. Specifically, our work has the following contributions: (1) a novel hardware attachment for consumer cameras that measures camera motion, (2) a novel aided blind-deconvolution algorithm that combines a natural image prior with our sensor data to estimate a spatially-varying PSF, (3) a deblurring method that using a novel spatially-varying image deconvolution method to only remove the camera-shake blur and leaves intentional artistic blurs (i.e., shallow DOF) intact, and (4) a method for accurately measuring spatiallyvarying camera-induced motion blur.\n      ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n      30:2 ? N. Joshi et al.\n      \n        2 Related Work\n        Image deblurring has recently received a lot of attention in the computer graphics and vision communities. Image deblurring is the combination of two tightly coupled sub-problems: PSF estimation and non-blind image deconvolution. These problems have been addressed both independently and jointly [Richardson 1972]. Both are longstanding problems in computer graphics, computer vision, and image processing, and thus the entire body of previous work in this area is beyond what can be covered here. For a more in depth review of earlier work in blur estimation, we refer the reader to the survey paper by Kundur and Hatzinakos [1996].  Blind deconvolution is an inherently ill-posed problem due to the loss of information during blurring. Early work in this area significantly constrained the form of the kernel, while more recently, researchers have put constraints on the underlying sharp image [Bascle et al. 1996; Fergus et al. 2006; Yuan et al. 2007; Joshi et al. 2008]. Alternative approaches are those that use additional hardware to augment a camera to aid in the blurring process [Ben-Ezra and Nayar 2004; Tai et al. 2008; Park et al. 2008]. The most common commercial approach for reducing image blur is image stabilization (IS). These methods, used in high-end lenses and now appearing in lower-end point and shoot cameras, use mechanical means to dampen camera motion by offsetting lens elements or translating the sensor. IS methods are similar to our work in that they use inertial sensors to reduce blur, but there are several significant differences. Fundamentally, IS tries to dampen motion by assuming that the past motion predicts the future motion [Canon 1993]; however, it does not counteract the actual camera motion during an exposure nor does it actively remove blur ? it only reduces blur. In contrast, our method records the actual camera motion and removes the blur from the image. A further difference is that IS methods can only dampen 2D motion, e.g., these methods will not handle camera roll, while our method can handle six degrees of motion. That said, our method could be used in conjunction with image stabilization, if one were able to obtain readings of the mechanical offsetting performed by the IS system. Recent research in hardware-based approaches to image deblurring modify the image capture process to aid in deblurring. In this area, our work is most similar to approaches that uses hybrid cameras [Ben-Ezra and Nayar 2004; Tai et al. 2008], which track camera motion using data from a video camera attached to a still camera. This work compute a global frame-to-frame motion to calculate the 2D camera motion during the image-exposure window. Our work is similar, since we also track motion during the exposure window; however, we use inexpensive, small, and lightweight sensors instead of a second camera. This allows us to measure more degrees of camera motion at a higher-rate and lower cost. Another difficulty of the hybrid camera approach that we avoid is that it can be difficult to get high-quality, properly exposed images out of the video camera in the low light conditions where image blur is prevalent. We do, however, use a modified form of Ben-Ezra and Nayar?s work in a controlled situation to help validate our estimated camera motions. Also similar to our work is that of Park et al. [2008], who use a 3-axis accelerometer to measure motion blur. The main difference between our work and theirs is that we additionally measure 3 axes of rotational velocity. As we discuss later, we have found 3 axes of acceleration insufficient for accurately measuring motion blur, as rotation is commonly a significant part of the blur. Our work is complementary to the hardware-based deblurring work of Levin el al.?s [2008], who show that by moving a camera along a parabolic arc, one can create an image such that 1D blur due to objects in the scene can be removed regardless of the speed or direction of motion. Our work is also complementary to that of Raskar et al. [2006], who developed a fluttered camera shutter to create images with blur that was more easily inverted.\n      \n      \n        3 Deblurring using Inertial Sensors\n        In this section, we describe the design challenges and decisions for building our sensing platform for image deblurring. We first review the image blur process from the perspective of the six degree motion of a camera. Next we give an overview of camera dynamics and inertial sensors followed by our deblurring approach.\n        \n          3.1 Camera Motion Blur\n          Spatially invariant image blur is modeled as the convolution of a latent sharp image with a shift-invariant kernel plus noise, which is typically considered to be additive white Gaussian noise. Specifically, blur formation is commonly modeled as:\n          \n            1\n            B = I ? K + N,\n          \n          where K is the blur kernel, N ? N (0, ? 2 ) is the noise. With a few exceptions, most image deblurring work assumes a spatially invariant kernel; however, this often does not hold in practice [Joshi et al. 2008; Levin et al. 2009]. In fact there are many properties of a camera and a scene that can lead to spatially-varying blur: (1) depth dependent defocus blur, (2) defocus blur due to focal length variation over the image plane, (3) depth dependent blur due to camera translation, (4) camera roll motion, and (5) camera yaw and pitch motion when there are strong perspective effects. In this work, our goal is to handle only camera induced motion blur, i.e., spatiallyvarying blur due to the last three factors. First, let us consider the image a camera captures during its exposure window. The intensity of light from a scene point (X, Y, Z) at an instantaneous time t is captured on the image plane at a location (u t , v t ), which is a function of the camera projection matrix P t . In homogenous coordinates, this can be written as:\n          ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n          \n            \n            \n            \n            \n          \n          Sensor Data\n          Image Deblurring using Inertial Measurement Sensors ? 30:3\n          Blurry Image\n          \n            \n          \n          Compute Camera Motion\n          \n            \n            \n          \n          Sensor Data\n          \n            Figure 2: Our image deblurring algorithm: First the sensor data is used to compute an initial guess for the camera motion. From this, we use the image data to search for a small perturbation of the x and y end points of the camera translation, to overcome drift. Using this result, we compute the spatially-varying blur matrix and deconvolve the image.\n          \n          \n            2\n            T T\n          \n          \n            2\n            (u t , v t , 1) = P t (X, Y, Z, 1) .\n          \n          If there is camera motion, P t varies with time as a function of camera rotation and translation causing fixed points in the scene to project to different locations at each time. The integration of these projected observations creates a blurred image, and the projected trajectory of each point on the image plane is that point?s point-spread function (PSF). The camera projection matrix can be\n          \n            3\n            decomposed as: P t = K?E t ,\n          \n          where K is the intrinsics matrix, ? is the canonical perspective projection matrix, and E t is the time dependent extrinsics matrix that is composed of the camera rotation R t and translation T t . In the case of image blur, it is not necessary to consider the absolute motion of the camera, only the relative motion and its effect on the image. We model this by considering the planar homography that maps the initial projection of points at t = 0 to any other time t, i.e., the reference coordinate frame is coincident with the frame at time t = 0:\n          (4)\n          \n            5\n            1 T ?1 H t (d) = [K(R t + T t N )K ] d T T (u t , v t , 1) = H t (d)(u 0 , v 0 , 1) ,\n          \n          for a particular depth d, where N is the unit vector that is orthogonal to the image plane. Thus given an image I at time t = 0, the pixel value of any subsequent image is:\n          \n            6\n            T I t (u t , v t ) = I(H t (d)(u 0 , v 0 , 1) ).\n          \n          This image warp can be re-written in matrix form as:\n          \n            7\n            I t = A t (d) I,\n          \n          where I t and I are column-vectorized images and A t (d) is a sparse re-sampling matrix that implements the image warping and resampling due to the homography. Each row of A t (d) contains the weights to compute the value at pixel (u t , v t ) as the interpolation of the point (u 0 , v 0 , 1) T = H t (d) ?1 (u t , v t , 1) T ? we use bilinear interpolation, thus there are four values per row. We can now define an alternative formulation for image blur as the integration of applying these homographies over time:\n          \n            8\n            Z s h i B = A t (d) Idt . 0\n          \n          The spatially invariant kernel in Equation 1 is now replaced by a spatially-variant blur represented by a sparse-matrix: Z s\n          \n            9\n            A(d) = A t (d)dt, 0\n          \n          Deblurred Image\n          Drift Deblur Correction Image\n          \n            \n          \n          our spatially-varying blur model is given by:\n          \n            10\n            B = A(d) I + N.\n          \n          2 Thus, the camera-induced, spatially-varying blur estimation process is reduced to estimating the rotations R and translations T for times [0...t], the scene depths d, and the camera intrinsics K. By representing the camera-shake blur in the six degrees of motion of the camera, instead of purely in the image plane, the number of unknowns is reduced significantly ? there are six unknowns per each of M time-steps, an unknown depth per-pixel (w ? h unknowns), and the camera intrinsics, of which the focal length is the most important factor. This results in 6M + wh + 1 unknowns as opposed to an image-based approach that must recover an k ? k kernel for each pixel, resulting in k 2 ? wh unknowns. In practice, since we assume a single depth for the scene, the unknowns in our system reduce to 6M + 2.  If these values are known, the image can be deblurred using nonblind deconvolution. We modify the formulation of Levin et al. [2007] to use our spatially-varying blur model. We formulate image deconvolution using a Bayesian framework and find the most likely estimate of the sharp image I, given the observed blurred image B, the blur matrix A, and noise level ? 2 using a maximum a posteriori (MAP) technique. We express this as a maximization over the probability distribution of the posterior using Bayes? rule. The result is a minimization of a sum of negative log likelihoods:\n          3.2 Spatially-Varying Deconvolution\n          \n            11\n            P (I|B, A) = P (B|I)P (I)/P (B)\n          \n          \n            12\n            argmax P (I|B) = argmin [L(B|I) + L(I)]. I I\n          \n          The problem of deconvolution is now reduced to minimizing the negative log likelihood terms. Given the blur formation model (Equation 1), the ?data? negative log likelihood is:\n          \n            13\n            L(B|I) = || B ? A(d) I|| 2 /? 2 .\n          \n          The contribution of our deconvolution approach is this new data term that uses the spatially-varying model derived in the previous section.  Our ?image? negative log likelihood is the same as Levin et al.?s [2007] sparse gradient penalty, which enforces a hyper-Laplacian distribution: L(I) = ?|| I|| 0.8 . The minimization is performed using iteratively re-weighted least-squares [Stewart 1999].\n          ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n          30:4 ? N. Joshi et al.\n        \n        \n          3.3 Rigid Body Dynamics and Inertial Sensors\n          As discussed in the previous section, camera motion blur is dependent on rotations R and translations T for times [0...t], the scene depths d, and camera intrinsics K. In this section, we discuss how to recover the camera rotations and translations, and in Section 4.2 we address recovering camera intrinsics. Any motion of a rigid body and any point on that body can be parameterized as a function of six unknowns, three for rotation and three for translation. We now describe how to recover these quantities given inertial measurements from accelerometers and gyroscopes. Accelerometers measure the total acceleration at a given point along an axis, while gyroscopes measure the angular velocity at a given point around an axis. Note that for a moving rigid body, the pure rotation at all points is the same, while the translations for all points is not the same when the body is rotating. Before deriving how to compute camera motion from inertial measurements, we first present our notation, as summarized in Table 1 .\n          Symbol Description t R i Initial to current frame ? t i , ? t i , ? t i Current angular pos., vel., and accel. in initial frame ? t t Current angular vel. in the current frame x i t ,v t i ,a t i Current pos., vel. and accel. in the initial frame a p t Accel. of the accelerometer in the current frame x i p Position of the accelerometer in the initial frame r p q Vector from the accelerometer to center of rotation g i Gravity in the camera?s initial coordinate frame\n          \n            Table 1: Quantities in bold indicate measured or observed values. Note that these are vectors, i.e., three axis quantities. The ?superscript? character indicates the coordinate system of a value and the ?subscript? indicates the value measured.\n          \n          A rigid body, such as a camera, with a three axis accelerometer and three axis gyroscope (three accelerometers and gyroscopes mounted along x, y, and z in a single chip, respectively) measures the following accelerations and angular velocities:\n          \n            15\n            ? t t = t R i ? ? t i a t = t R i ? a i + g i + (? i ? (? i ? r q )) + (? i ? r q ) ? .\n          \n          \n            15\n            p t t t p t p\n          \n          The measured acceleration is the sum of the acceleration due to translation of the camera, centripetal acceleration due to rotation, the tangential component of angular acceleration, and gravity, all rotated into the current frame of the camera. The measured angular velocity is the camera?s angular velocity also rotated in the current frame of the camera. To recover the relative camera rotation, it is necessary to recover the angular velocity for each time-step t in the coordinate system of the initial frame ? t i , which can be integrated to get the angular position. To recover relative camera translation, we need to first compute the accelerometer position for each time-step relative to the initial frame. From this, we can recover the camera translation. The camera rotation can be recovered by sequentially integrating and rotating the measured angular velocity into the initial camera frame.\n          \n            17\n            i i t?1 t?1 i ? t =( R ? t?1 )?t + ? t?1 t i i R =angleAxisT oM at( ? ),\n          \n          \n            17\n            t\n          \n          +Y tt +Y ii w y,tt i R t\n          w z,tt\n          \n            \n          \n          -Z ii\n          +X ii g i\n          -Z tt\n          w x,tt\n          x ? pc\n          +X tt\n          \n            \n            Figure 3: The rigid-body dynamics of our camera setup.\n          \n          where ?angleAxisT oM at? converts the angular position vector to a rotation matrix. Since we are only concerned with relative rotation, the initial rotation is zero:\n          \n            18\n            i t=0 i ? t=0 = 0, R = Identity.\n          \n          Once the rotations are computed for each time-step, we can compute the acceleration in the initial frame?s coordinate system:\n          \n            19\n            a p = R a p ,\n          \n          \n            19\n            i i t t\n          \n          and integrate the acceleration, minus the constant acceleration of gravity, to get the accelerometer?s relative position at each timestep:\n          \n            20\n            v p i (t) = (a i p (t ? 1) ? g i )?t + v p i (t ? 1)\n          \n          \n            21\n            x p i (t) = 0.5 ? (a i p (t ? 1) ? g i )?t 2 + v p i (t ? 1)?t + x i p (t ? 1).\n          \n          As we are concerned with relative position, we set the initial position to zero, and we also assume that the initial velocity is zero:\n          \n            22\n            i i x p (0) = v p (0) = [0, 0, 0].\n          \n          The accelerometers? translation (its position relative to the initial frame) in terms of the rigid body rotation and translation is:\n          \n            23\n            i t i i i x p (t) = R x p (0) + x t .\n          \n          Given this, we can compute the camera position at time t:\n          \n            24\n            x i t = t R i x p i (0) ? x p i (t).\n          \n          In Equation 20, it is necessary to subtract the value of gravity in the initial frame of the camera. We note, however, that the initial rotation of the camera relative to the world is unknown, as the gyroscopes only measure velocity. The accelerometers can be used to estimate the initial orientation if the camera initially has no external forces on it other than gravity. We have found this assumption unreliable, so we instead make the assumption that the measured acceleration is normally distributed about the constant force of gravity. We have found this reliable when the camera motion is due to high-frequency camera-shake. Thus we set the direction of mean acceleration vector as the direction of gravity:\n          \n            25\n            i i g = mean(a p (t), [0...T ])).\n          \n          To summarize, the camera rotation and translation are recovered by integrating the measured acceleration and angular velocities that  are rotated into the camera?s initial coordinate frame. This gives us the relative rotation and translation over time, which is used to compute the spatially-varying PSF matrix in Equation 10. If the measurements are noise-free, this rotation and motion information is sufficient for deblurring; however, in practice, the sensor noise introduces significant errors. Furthermore, even if the camera motion is known perfectly, one still needs to know the scene depth, as discussed in Section 3. Thus additional steps are needed to deblur an image using the inertial data.\n          ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n          Image Deblurring using Inertial Measurement Sensors ? 30:5\n        \n        \n          3.4 Drift Compensation and Deconvolution\n          It is well known that computing motion by integrating differential sensors can lead to drift in the computed result. This drift is due to the noise present in the sensor readings. The integration of a noisy signal leads to a temporally increasing deviation of the computed motion from the true motion. We have measured the standard deviation of our gyroscope?s noise to be 0.5deg/s and the accelerometer noise is 0.006m/s 2 , using samples from when the gyroscopes and accelerometers are held stationary (at zero angular velocity and constant acceleration, respectively) . In our experiments, there is significantly less drift in rotation, due to the need to perform only a single integration step on the gyroscope data. The necessity to integrate twice to get positional data from the accelerometers causes more drift. To get a high-quality deblurring results, we must overcome the drift. We propose a novel aided blind deconvolution algorithm that computes the camera-motion, and in-turn the image blur function, that best matches the measured acceleration and angular velocity while maximizing the likelihood of the deblurred latent image according to a natural image prior. Our deconvolution algorithms compensate for positional drift by assuming it is linear in time, which can be estimated if one knows the final end position of the camera. We assume the rotational drift is minimal. The final camera position is, of course, unknown; however, we know the drift is bounded and thus the correct final position should lie close to our estimate from the sensor data. Thus in contrast to a traditional blind-deconvolution algorithm that solves for each value of a kernel or PSF, our algorithm only has to solve for a few unknowns. We solve for these using an energy minimization framework that performs a search in a small local neighborhood around the initially computed end point. In our experiments, we have found that the camera travels on the order of a couple millimeters during a long exposure (the longest we have tried is a 1/2 second). We note that a few millimeter translation in depth (z) has little effect on the image for lenses of common focal lengths, thus the drift in x and y is the only significant source of error. We set our optimization parameters to search for the optimal end point within a 1mm radius of the initially computed end point, subject to the constraints that the acceleration along that recovered path matches the measured accelerations best in the least-squares sense. The optimal end point is the one that results in a deconvolved image with the highest log-likelihood as measured by the hyper-Laplacian image prior (discussed in Section 3.1). Specifically, let us define a function ? that given a potential end point (u, v) computes the camera?s translational path as that which best matches, in the least squares sense, the observed acceleration and terminates at (u, v):\n          \n            26\n            ?(a i , u, v) = argmin x i X t=0 T ( d dt 2 x 2 t i ? a t i ) 2 + (? x,T i ? u) 2 + (? y,T i ? v) 2 .\n          \n          \n            \n            Figure 4: Ground Truth Blur Measurements: We attach a highspeed camera next to an SLR and capture high-speed video frames during the camera exposure. With additional wide-baseline shots, we perform 3D reconstruction using bundle adjustment. We show our high-speed camera attachment and a few images from the highspeed camera (we took about a hundred total for the process).\n          \n          For notational convenience, let ? define a function that forms the blur sampling matrix from the camera intrinsics, extrinsics, and scene depth as using the rigid-body dynamics and temporal integration processes discussed in Section 3.1 and 3.3:\n          \n            27\n            i i A(d) = ?( ? , x , d, K)\n          \n          The drift-compensated blur matrix and deconvolution equations are:\n          \n            28\n            i i A(d, u, v) = ?(? , ?(a , u, v), d, K),\n          \n          \n            29\n            I = argmin [|| B ? A(d, u, v) I|| 2 /? 2 + ?|| I|| 0.8 ]. I,d,u,v\n          \n          We then search over the space of (u, v) to find the (u, v) that results in the image I that has the highest likelihood given the observation and image prior. We perform this energy minimization using the Nelder-Mead simplex method, and the spatially-varying deconvolution method discussed in Section 3.2 is used as the error function in the inner loop of the optimization. We perform the optimization on 1/10 down-sampled versions (of our 21 MP images).  The results from our search process are shown as plots of the camera motion in Figure 5 and visually in Figure 7 , when used to deblur images. The running time for this search method is about 5 minutes on a 0.75 MP image. The search only needs to be run once either for the entire image, or could be run on a subsection of the image if that is preferable. Once the drift is corrected for, the PSF is more accurate for the entire image. Computing Scene Depth: Note that a spatially invariant scene depth is implicitly computed during the drift compensation process, as scaling the end point equally in the x and y dimensions is equivalent to scaling the depth value. Specifically, the optimization is over u = u/d and v = v/d and thus solves for a single depth value for the entire scene.\n        \n      \n      \n        4 Deblurring System\n        In the previous section, we discussed how to remove camera motion blur by recovering the camera rotation and translation from accelerometers and gyroscopes. In this section, we describe our hardware for recording the accelerometer and gyroscope data and implementation related concerns and challenges.\n        ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n        30:6 ? N. Joshi et al.\n        \n          \n          Figure 5: Drift compensation: Angular and translational position of the camera versus time is shown for the image in the blue box in Figure 7 . The ?ground-truth? motion, computed using structure from motion, is shown as solid lines. The dashed lines are the motions from using the raw sensor data and the ?+? marked lines are the result after drift compensation. The drift compensated results are much closer to the ground-truth result. Note the bottom right plot does not show a drift corrected line as we only compensate for x and y drift.\n        \n        \n          4.1 Hardware Design\n          Since there are six unknowns per time-step, the minimal configuration of sensors is six. It is possible to recover rotation and translation using six accelerometers alone, by sensing each axis in pairs at three different points on a rigid-body; however, after experimenting with this method we found accelerometers alone to be too noisy for reliable computation of rotation. Thus our prototype hardware system, shown in Figure 1 , is a minimal configuration consisting of a three-axis ?1.5g MEMS accelerometer package and three single axis ?150 ? /s MEMS gyroscopes wired to an Arduino controller board with a Bluetooth radio. All parts are commodity, off-theshelf components purchased online. Additionally, the SLR?s hotshoe, i.e., flash trigger signal, is wired to the Arduino board. The trigger signal from the SLR remains low for the entire length of the exposure and is high otherwise. The Arduino board is interrupt driven such that when the trigger signal from the SLR fires, the accelerometers and gyroscopes are polled at 200Hz during the exposure window. Each time the sensors are read, the values are sent over the Bluetooth serial port interface. Additionally, an internal high-resolution counter is read and the actual elapsed time between each reading of the sensors is reported. The sensors and Arduino board are mounted to a laser-cut acrylic base that secures the board, the sensors, and a battery pack. The acrylic mount is tightly screwed into the camera tripod mount. The only other connection to the camera is the flash trigger cable. Our hardware attachment has an optional feature used for calibration and validation experiments: mounting holes for a Point Grey high-speed camera. When the Arduino board is sampling the inertial sensors, it can also send a 100 Hz trigger to the high-speed camera. We use the high-speed data to help calibrate our sensors and to acquire data to get ground truth measurements for motion blur.\n          \n            \n            Figure 6: Visualization of ground-truth spatially-varying PSFs: For the blurry image on top, we show a sparsely sampled visualization of the blur kernels across the image plane. There is quite\n          \n          a significant variation across the image plane. To demonstrate the importance of accounting for spatially variance, in the bottom row we show a result where we have deconvolved using the PSF for the correct part of the image and a non-corresponding area.\n        \n        \n          4.2 Calibration and Ground-truth Measurements\n          To accurately compute camera motion from inertial sensors, it is necessary to calibrate several aspects of our system. We need to accurately calibrate the sensor responses, as we have found them to deviate from the published response ranges. It is also necessary to know the position of the accelerometer relative to the camera?s optical center. Lastly, we need to calibrate the camera intrinsics.  We calibrate these values in two stages. The first stage is to calibrate the sensors? responses. We do this by rotating the gyroscopes at a known constant angular velocity to recover the mapping from the 10-bit A/D output to degrees/s. We performed this measurement at several known angular velocities to confirm that the gyroscopes have a linear response. To calibrate the accelerometers, we held them stationary in six orientations with respect to gravity, (?x, ?y, ?z), which allows us to map the A/D output to units of m/s 2 . To calibrate our setup and to measure ground-truth measurements for camera-shake, we developed a method to accurately recover a camera?s position during an exposure using a method inspired by Ben-Ezra and Nayar [2004]. However, instead of tracking 2D motion, we track 6D motion. We attached a high-speed camera (200 FPS PointGrey DragonFly Express) to our sensor platform, and our Arduino micro-controller code is set to trigger the high-speed camera at 100 FPS during the SLR?s exposure window. In a lab setting, we created a scene with a significant amount of texture and took about 10 images with exposures ranging from 1/10 to 1/2 of a second. For each of these images, accelerometer and gyro data was recorded in addition to high-speed frames. We took the high-speed frames from these shots and acquired additional widebaseline shots with the SLR and high-speed camera. Using all of this data, we created a 3D reconstruction of the scene using bundle adjustment (our process uses RANSAC for feature matching, computes sparse 3D structure from motion, and computes the camera focal length). Figure 4 shows our high-speed camera attachment and a few frames from the high speed cameras (we took about a hundred total with both cameras for the process). This reconstruction process gives us a collection of sparse 3D points, camera rotations, and camera translations, with an unknown global transformation and a scale ambiguity between camera depth and scene depth. We resolve the scale ambiguity using a calibration grid of known size.\n          ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n          Image Deblurring using Inertial Measurement Sensors ? 30:7\n          \n            \n          \n          Blurry Image Using PSFs from Our Final Output the raw sensor values\n          \n            Figure 7: Deblurring results and comparisons: Here we show deblurring results for a cropped portion of the scene shown in Figure 6 . These sections are cropped from an 11 mega-pixel image. The results in the green box are the final output of our method, where the sensor data plus our drift compensation method are used to compute the camera motion blur. Subtle differences in the PSF before and after drift compensation can have a big result on the quality of the deconvolution.\n          \n        \n      \n      \n        5 Results\n        We will now describe the results of our ground-truth camera-shake measurements and compare results using our deblurring method to the ground-truth measurements. We also compare our results to those of Shan et al. [2008] and Fergus et al. [2006], using the implementations the authors have available online. We also show results of our methods running on natural images acquired outside of a lab setup and compare these to results using previous work as well.\n        \n          5.1 Lab Experiments and Camera-Shake Study\n          In Figure 6 , we show visualizations of the ground-truth spatiallyvarying PSFs for an image from our lab setup. This image shows some interesting properties. There is a significant variation across the image plane. Also, the kernels displayed are for a fixed depth, thus all the spatial variance is due to rotation. To demonstrate the importance of accounting for spatial variance, on the bottom row of Figure 6 , we show a result where we have deconvolved using the PSF for the correct part of the image and the PSF for a different, non-corresponding area. These results are quite interesting as they show that some of the common assumptions made in image deconvolution do not always hold. Most deconvolution work assumes spatially invariant kernels, which really only applies for camera motion under an orthographic model; however, with a typical imaging setup (we use a 40mm lens), the perspective effects are strong enough to induce a spatially-varying blur. We also note that there is often a roll component to the blur, something that is also not modeled by spatially invariant kernels. Lastly, we observe that translation, and thus depth dependent effects can be significant, which is interesting as it is often thought that most camera-shake blur is due to rotation. Please visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/ for examples. In Figure 7 , using the same scene as above, we show comparisons of deconvolution results using our method, ground-truth, and two others. The images shown of the lab calibration scene were taken at exposures from 1/3 to 1/10 of a second. For each image we show the input, the result of deconvolving with PSFs from the initial motion estimate and after performing drift correction, and compare these to a deconvolution using PSFs from the recovered groundtruth motions, and PSFs recovered using the methods of Shan et al. [2008] and Fergus et al. [2006]. For these latter two comparisons, we made a best effort to adjust the parameters to recover the best blur kernel possible. To make the comparison fair, all results were deblurred using exactly the same deconvolution method, that of Levin et al. [2007]. The results in Figure 7 , show a wide variety of blurs, yet our method recovers an accurate kernel and provides deconvolution results that are very close to that of the ground-truth. In all cases, our results are better than those using Shan et al.?s and Fergus et al.?s methods.\n          Using Shan et al. Fergus et al. Groundtruth Motion\n        \n        \n          5.2 Real Images\n          After calibrating our hardware system using the method discussed in Section 4.2, we took the camera outside of the lab, using a laptop with a Bluetooth adapter to capture the inertial sensor data. In Figure 8 , we show several results where we have deblurred images using our method, Shan et al.?s method, and Fergus et al.?s method. The Shan et al. and Fergus et al. results were deconvolved using Levin et al.?s method [2007], and our results are deconvolved using our spatially-varying deconvolution method discussed in Section 3.1. For all the images, our results show a clear improvement over the input blurry image. There is still some residually ringing that is unavoidable due to frequency loss during blurring. The Shan et al. results are of varying quality, and many show large ringing artifacts that are due to kernel misestimation and over-sharpening; the Fergus et al. results are generally blurrier than ours. All the images shown here were shot with 1/2 to 1/10 second exposures with a 40mm lens on a Canon 1Ds Mark III. For additional results, visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/.\n          ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n          30:8 ? N. Joshi et al.\n          \n            \n          \n          Input IMU Deblurring (Our Final Output)\n          \n            Figure 8: Natural images deblurred with our setup: For all the images our results show a clear improvement over the input blurry image. The blur kernel at each corner of the image is shown at 2? size. There is still some residual ringing that is unavoidable due to frequency loss during blurring. The Shan et al. results are of varying quality, and many show large ringing artifacts that are due to kernel misestimation and over-sharpening; the Fergus et al. results are generally blurry than our results. With the stones image, the intentionally defocused background stones are not sharpened in our result.\n          \n        \n      \n      \n        6 Discussion and Future Work\n        In this work, we presented an aided blind deconvolution algorithm that uses a hardware attachment in conjunction with a corresponding blurry input image and a natural image prior to compute perpixel, spatially-varying blur and that deconvolves an image to produce a sharp result. There are several benefits to our method over previous approaches: (1) our method is automatic and has no user-tuned parameters and (2) it uses inexpensive commodity hardware that could easily be built into a camera or produced as a mass-market attachment that is more compact than our prototype. We have shown advantages over purely image-based methods, which in some sense is not surprising?blind deconvolution is an inherently ill-posed problem,  thus the extra information of inertial measurements should be helpful. There are many challenges to using this data properly, many of which we have addressed; however, our results also suggest several areas for future work. The biggest limitation of our method is sensor accuracy and noise. Our method?s performance will degrade under a few cases: (1) if the drift is large enough that the search space for our optimization process is too large, i.e., greater than a couple mm, (2) if our estimation of the initial camera rotation relative to gravity is incorrect or similarly, if the camera moves in a way that is not normally distributed about the gravity vector, (3) if there is significant depth variation in the scene and the camera undergoes significant translation, (4) if the camera is translating at some initial, constant velocity, and (5) if there is large image frequency information loss to blurring. To handle (1), we are interested in using more sensors. The sensors are inexpensive and one could easily add sensors for redundancy and perform denoising by averaging either in the analog or digital domain. For (2) one could consider adding other sensors, such as a magnetometer, to get another measure of orientation ? this is already a common approach in IMU based navigation systems. For (3) one could recover the depth in the scene by performing a ?depth from motion blur? algorithm, similar to depth from defocus. We are pursuing this problem; however, it is important to note that varying scene depth does not always significantly affect the blur. For typical situations, we have found that depth is only needed for accurate deblurring of objects within a meter from the camera. Most often people take images where the scene is farther than this distance. For (4) we assume the initial translation velocity is zero and our accelerometer gives us no measure of this. While we currently only consider the accelerometer data during an exposure, we actually record all the sensors? data before and after each exposure as well. Thus one way to address this issue is to try to identify a stationary period before the camera exposure and track from there to get a more accurate initial velocity estimate. (5) is an issue with all deblurring methods. Frequency loss will cause unavoidable artifacts during deconvolution that could appear as ringing, banding, or over-smoothing depending on the deconvolution method. It would be interesting to combine our hardware with the Raskar et al. [2006] flutter shutter hardware to reduce frequency loss during image capture.\n        Shan et al. Fergus et al.\n        ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n        Image Deblurring using Inertial Measurement Sensors ? 30:9\n      \n      \n        Acknowledgements\n        We would like to thank the anonymous SIGGRAPH reviewers. We also thank Mike Sinclair and Turner Whitted for their help in the hardware lab.\n      \n      \n        References\n        \n          B ASCLE , B., B LAKE , A., AND Z ISSERMAN , A. 1996. Motion deblurring and super-resolution from an image sequence. In ECCV ?96: Proceedings of the 4th European Conference on Computer Vision-Volume II, Springer-Verlag, London, UK, 573?582.\n          B EN -E ZRA , M., AND N AYAR , S. K. 2004. Motion-based motion deblurring. IEEE Trans. Pattern Anal. Mach. Intell. 26, 6, 689? 698.\n          C ANON , L. G. 1993. EF LENS WORK III, The Eyes of EOS. Canon Inc.\n          F ERGUS , R., S INGH , B., H ERTZMANN , A., R OWEIS , S. T., AND F REEMAN , W. T. 2006. Removing camera shake from a single photograph. ACM Trans. Graph. 25 (July), 787?794.\n          J OSHI , N., S ZELISKI , R., AND K RIEGMAN , D. J. 2008. Psf estimation using sharp edge prediction. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1?8.\n          K UNDUR , D., AND H ATZINAKOS , D. 1996. Blind image deconvolution. Signal Processing Magazine, IEEE 13, 3, 43?64.\n          L EVIN , A., F ERGUS , R., D URAND , F., AND F REEMAN , W. T. 2007. Image and depth from a conventional camera with a coded aperture. ACM Trans. Graph. 26 (July), Article 70.\n          L EVIN , A., S AND , P., C HO , T. S., D URAND , F., AND F REEMAN , W. T. 2008. Motion-invariant photography. ACM Trans. Graph. 27 (August), 71:1?71:9.\n          L EVIN , A., W EISS , Y., D URAND , F., AND F REEMAN , W. 2009. Understanding and evaluating blind deconvolution algorithms. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, IEEE Computer Society, 1964?1971.\n          P ARK , S.-Y., P ARK , E.-S., AND K IM , H.-I. 2008. Image deblurring using vibration information from 3-axis accelerometer. Journal of the Institute of Electronics Engineers of Korea. SC, System and control 45, 3, 1?11.\n          R ASKAR , R., A GRAWAL , A., AND T UMBLIN , J. 2006. Coded exposure photography: motion deblurring using fluttered shutter. ACM Trans. Graph. 25 (July), 795?804.\n          R ICHARDSON , W. H. 1972. Bayesian-based iterative method of image restoration. Journal of the Optical Society of America (1917-1983) 62, 55?59.\n          S HAN , Q., J IA , J., AND A GARWALA , A. 2008. High-quality motion deblurring from a single image. ACM Trans. Graph. 27 (August), 73:1?73:10.\n          S TEWART , C. V. 1999. Robust parameter estimation in computer vision. SIAM Reviews 41, 3 (September), 513?537.\n          T AI , Y.-W., D U , H., B ROWN , M. S., AND L IN , S. 2008. Image/video deblurring using a hybrid camera. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, 1?8.\n          Y UAN , L., S UN , J., Q UAN , L., AND S HUM , H.-Y. 2007. Image deblurring with blurred/noisy image pairs. ACM Trans. Graph. 26 (July), Article 1.\n        \n        ACM Transactions on Graphics, Vol. 29, No. 4, Article 30, Publication date: July 2010.\n      \n    \n  ",
  "resources" : [ ]
}