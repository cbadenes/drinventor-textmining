{
  "uri" : "sig2014-a35-xu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a35-xu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Organizing Heterogeneous Scene Collections through Contextual Focal Points",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Kai Xu-null",
      "name" : "Kai Xu",
      "surname" : null
    }, {
      "uri" : "http://drinventor/Rui-Ma",
      "name" : "Rui",
      "surname" : "Ma"
    }, {
      "uri" : "http://drinventor/Hao Zhang-null",
      "name" : "Hao Zhang",
      "surname" : null
    }, {
      "uri" : "http://drinventor/Chenyang-Zhu",
      "name" : "Chenyang",
      "surname" : "Zhu"
    }, {
      "uri" : "http://drinventor/Ariel-Shamir",
      "name" : "Ariel",
      "surname" : "Shamir"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Hui Huang-null",
      "name" : "Hui Huang",
      "surname" : null
    } ]
  },
  "bagOfWords" : [ "24c70539c86adcbaf4aa3fbfbf1996264c3c405bc67a28638b5a214e75811a2d", "p93", "10.1145", "2601097.2601109", "name", "identification", "possible", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "Kai", "Xu", "1,3", "Rui", "Ma", "Hao", "Zhang", "Chenyang", "Zhu", "Ariel", "Shamir", "Daniel", "Cohen-Or", "Hui", "Huang", "Shenzhen", "VisuCA", "Key", "Lab", "siat", "Simon", "Fraser", "University", "hpcl", "National", "University", "Defense", "Technology", "Interdisciplinary", "Center", "Tel", "Aviv", "University", "we", "introduce", "focal", "point", "characterize", "compare", "organize", "collection", "complex", "heterogeneous", "datum", "apply", "concept", "algorithm", "develop", "collection", "3d", "indoor", "scene", "we", "represent", "each", "scene", "graph", "its", "constituent", "object", "define", "focal", "point", "representative", "substructure", "scene", "collection", "organize", "heterogeneous", "scene", "collection", "we", "cluster", "scene", "base", "set", "extract", "focal", "point", "scene", "cluster", "closely", "connect", "when", "view", "from", "perspective", "representative", "focal", "point", "cluster", "key", "concept", "representativity", "require", "focal", "point", "occur", "frequently", "cluster", "result", "compact", "cluster", "hence", "problem", "focal", "point", "extraction", "intermix", "problem", "clustering", "group", "scene", "base", "representative", "focal", "point", "we", "present", "co-analysis", "algorithm", "which", "interleave", "frequent", "pattern", "mining", "subspace", "clustering", "extract", "set", "contextual", "focal", "point", "which", "guide", "clustering", "scene", "collection", "we", "demonstrate", "advantage", "focal-centric", "scene", "comparison", "organization", "over", "exist", "approach", "particularly", "deal", "hybrid", "scene", "scene", "consist", "element", "which", "suggest", "membership", "different", "semantic", "category", "cr", "category", "i.", "3.5", "-lsb-", "Computing", "Methodologies", "-rsb-", "Computer", "Graphics?Computational", "Geometry", "Object", "Modeling", "Keywords", "3d", "indoor", "scene", "contextual", "focal", "point", "heterogeneous", "collection", "scene", "organization", "retrieval", "exploration", "Links", "dl", "pdf", "EB", "IDEO", "ODE", "can", "think", "better", "expression", "characterize", "similarity", "than", "family", "resemblance", "various", "resemblance", "between", "member", "family", "build", "feature", "colour", "eye", "gait", "temperament", "etc.", "etc.", "overlap", "criss-cross", "same", "way", "Ludwig", "Wittgenstein", "-lsb-", "1953", "-rsb-", "introduction", "recent", "work", "organize", "explore", "3d", "visual", "datum", "have", "mostly", "be", "devote", "object", "collection", "-lsb-", "Ovsjanikov", "et", "al.", "2011", "Jain", "et", "al.", "2012", "Kim", "et", "al.", "2012", "van", "Kaick", "et", "al.", "2013", "Huang", "et", "al.", "2013b", "-rsb-", "paper", "we", "interested", "analyze", "organize", "visual", "datum", "larger", "scope", "namely", "3d", "indoor", "scene", "even", "moderately", "complex", "indoor", "scene", "would", "contain", "ten", "hundred", "object", "compare", "individual", "object", "therein", "scene", "more", "complex", "looser", "structural", "spatial", "relation", "among", "its", "component", "more", "diverse", "mixture", "functional", "substructure", "latter", "point", "attest", "hybrid", "scene", "which", "contain", "element", "reminiscent", "different", "semantic", "category", "example", "middle", "scene", "Figure", "partly", "bedroom", "partly", "living", "room", "greater", "intra-class", "variability", "richer", "characteristic", "scene", "datum", "motivate", "we", "work", "go", "beyond", "provide", "only", "holistic", "singular", "view", "scene", "scene", "collection", "we", "introduce", "use", "focal", "point", "characterize", "compare", "organize", "collection", "complex", "datum", "apply", "concept", "algorithm", "develop", "3d", "indoor", "scene", "particular", "we", "interested", "organize", "scene", "heterogeneous", "collection", "i.e.", "scene", "belong", "multiple", "semantic", "category", "analyze", "complex", "heterogeneous", "datum", "difficult", "without", "reference", "certain", "point", "attention", "focus", "i.e.", "focal", "point", "example", "compare", "New", "York", "City", "Paris", "whole", "unlikely", "yield", "useful", "answer", "comparison", "lot", "more", "meaningful", "focus", "particular", "aspect", "city", "e.g.", "architectural", "style", "fashion", "trend", "one", "natural", "consequence", "focal", "point", "drive", "datum", "view", "scene", "comparison", "may", "yield", "different", "similarity", "distance", "depend", "focal", "point", "see", "Figure", "illustration", "well", "accompany", "video", "we", "represent", "indoor", "scene", "graph", "its", "constituent", "object", "ACM", "Reference", "Format", "Xu", "K.", "Ma", "R.", "Zhang", "H.", "Zhu", "C.", "Shamir", "a.", "cohen-or", "D.", "Huang", "H.", "2014", "Organizing", "Heterogenous", "Scene", "Collection", "through", "Contextural", "Focal", "Points", "ACM", "Trans", "graph", "33", "Article", "35", "-lrb-", "July", "2014", "-rrb-", "12", "page", "dous", "10.1145", "2601097.2601109", "http://doi.acm.org/10.1145/2601097.2601109", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "copyright", "ACM", "0730-0301/14", "07-art35", "15.00", "DOI", "http://doi.acm.org/10.1145/2601097.2601109", "figure", "we", "analyze", "organize", "3d", "indoor", "scene", "heterogeneous", "collection", "from", "perspective", "focal", "point", "-lrb-", "sub-scene", "color", "-rrb-", "scene", "comparison", "may", "yield", "different", "similarity", "distance", "-lrb-", "left", "-rrb-", "depend", "focal", "point", "figure", "focal", "point", "-lrb-", "marked", "red", "scene", "-rrb-", "contextual", "depend", "scene", "composition", "collection", "more", "bedroom", "-lrb-", "-rrb-", "more", "living", "room", "-lrb-", "-rrb-", "different", "focal", "be", "extract", "hybrid", "scene", "pull", "towards", "one", "cluster", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:2", "K.", "Xu", "et", "al.", "Figure", "focal-driven", "scene", "clustering", "produce", "overlap", "cluster", "exploratory", "path", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "through", "overlap", "which", "often", "contain", "hybrid", "scene", "-lrb-", "-rrb-", "possess", "multiple", "focal", "can", "smoothly", "transition", "between", "scene", "cluster", "scene", "cluster", "often", "characterize", "meaningful", "scene", "category", "example", "transition", "from", "bedroom", "scene", "office", "focal", "point", "focal", "short", "substructure", "scene", "correspond", "subgraph", "however", "we", "interested", "all", "sub-scene", "key", "premise", "we", "work", "meaningful", "focal", "should", "determine", "contextually", "set", "-lrb-", "figure", "-rrb-", "through", "co-analysis", "illustrate", "probably", "too", "many", "notable", "aspect", "about", "Paris", "when", "put", "London", "Paris", "together", "one?s", "focus", "narrow", "down", "e.g.", "european", "capital", "we", "throw", "New", "York", "Milan", "mix", "most", "people", "first", "remind", "four", "city", "fashion", "capital", "world", "work", "we", "interested", "extract", "contextual", "focal", "point", "representative", "give", "scene", "collection", "focal", "representative", "must", "occur", "sufficiently", "frequently", "however", "frequency", "analysis", "alone", "insufficient", "example", "chair", "likely", "find", "almost", "all", "scene", "can", "hardly", "regard", "representative", "any", "meaningful", "scene", "group", "e.g.", "bedroom", "live", "room", "we", "stipulate", "representativity", "also", "tie", "notion", "coherence", "compactness", "group", "scene", "focal", "point", "represent", "characterize", "therefore", "frequency", "analysis", "focal", "extraction", "intermix", "clustering", "which", "compute", "compact", "group", "scene", "where", "scene", "each", "cluster", "closely", "connect", "when", "view", "from", "perspective", "representative", "focal", "cluster", "once", "again", "representative", "focal", "occur", "frequently", "cluster", "must", "also", "induce", "compact", "cluster", "solve", "two", "couple", "problem", "simultaneously", "we", "develop", "co-analysis", "algorithm", "which", "interleave", "frequent", "pattern", "mining", "-lsb-", "Han", "et", "al.", "2007", "-rsb-", "subspace", "clustering", "-lsb-", "Vidal", "2011", "-rsb-", "focal", "point", "play", "key", "role", "we", "organization", "heterogeneous", "scene", "collection", "first", "we", "define", "compactness", "cluster", "base", "focal-centric", "scene-to-scene", "similarity", "which", "build", "rooted", "walk", "graph", "kernel", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "assign", "higher", "weight", "walk", "which", "originate", "from", "representative", "focal", "cluster", "secondly", "scene", "organization", "give", "clustering", "scene", "base", "representative", "focal", "extract", "some", "scene", "may", "contain", "multiple", "focal", "thus", "belong", "multiple", "cluster", "scene", "typically", "hybrid", "nature", "provide", "linkage", "gateway", "between", "scene", "cluster", "allow", "exploration", "scene", "organization", "naturally", "transition", "between", "meaningful", "scene", "category", "illustrate", "Figure", "we", "main", "contribution", "focal-driven", "analysis", "organization", "heterogeneous", "datum", "collection", "while", "we", "only", "consider", "3d", "indoor", "scene", "paper", "we", "aware", "previous", "work", "coanalysis", "organization", "heterogeneous", "scene", "collection", "analysis", "general", "confine", "scene", "datum", "important", "characteristic", "we", "work", "which", "set", "apart", "from", "previous", "approach", "organize", "datum", "collection", "include", "datum", "compare", "holistically", "without", "discrimination", "we", "develop", "focal-centric", "scene", "descriptor", "scene", "com", "parison", "which", "support", "scene", "analysis", "perspective", "similarity", "distance", "between", "two", "scene", "may", "non-unique", "i.e.", "base", "focal", "designate", "comparison", "multiple", "view", "scene", "datum", "depend", "focal", "point", "lead", "overlap", "clustering", "scene", "collection", "rather", "than", "partition", "result", "organization", "particularly", "suit", "retrieve", "explore", "complex", "hybrid", "scene", "we", "show", "advantage", "focal-centric", "scene", "comparison", "organization", "over", "exist", "approach", "particularly", "deal", "hybrid", "scene", "we", "also", "demonstrate", "new", "capability", "offer", "new", "datum", "organization", "scene", "retrieval", "exploration", "related", "work", "background", "conceptual", "level", "we", "work", "can", "see", "realization", "notion", "family", "resemblance", "from", "seminal", "work", "Wittgenstein", "-lsb-", "1953", "-rsb-", "scene", "collection", "form", "family", "extract", "focal", "represent", "resemblance", "which", "overlap", "criss-cross", "among", "scene", "Works", "from", "cognitive", "psychology", "particular", "those", "Rosch", "-lsb-", "1975", "-rsb-", "provide", "evidence", "perceptual", "semantic", "category", "naturally", "form", "term", "focal", "point", "prototype", "-lrb-", "see", "account", "-lsb-", "Tversky", "1977", "-rsb-", "-rrb-", "though", "so-called", "cognitive", "reference", "point", "she", "work", "refer", "whole", "representative", "category", "instead", "featured", "substructure", "role", "context", "measure", "datum", "similarity", "have", "long", "be", "study", "various", "field", "e.g.", "-lsb-", "Biberman", "1994", "Jeh", "Widom", "2002", "-rsb-", "we", "work", "present", "algorithm", "identify", "conceptual", "focal", "which", "serve", "reference", "point", "compare", "scene", "heterogeneous", "collection", "scene", "analysis", "most", "familiar", "environment", "human", "indoor", "scene", "ubiquitous", "graphic", "application", "virtual", "reality", "gaming", "design", "much", "research", "vision", "graphic", "have", "be", "devote", "recognize", "classify", "retrieve", "indoor", "scene", "e.g.", "-lsb-", "Rasiwasia", "Vasconcelos", "2008", "Quattoni", "Torralba", "2009", "Fisher", "et", "al.", "2011", "Juneja", "et", "al.", "2013", "Xu", "et", "al.", "2013", "Zhao", "et", "al.", "2014", "-rsb-", "among", "other", "we", "work", "recognize", "difficulty", "compare", "complex", "scene", "globally", "e.g.", "via", "classic", "graph", "kernel", "-lsb-", "Fisher", "et", "al.", "2011", "-rsb-", "we", "propose", "extract", "utilize", "focal", "substructure", "scene", "analysis", "relevance", "work", "which", "extract", "distinctive", "region", "-lsb-", "shilane", "Funkhouser", "2007", "Juneja", "et", "al.", "2013", "-rsb-", "representative", "semantic", "category", "focal", "we", "extract", "mean", "scene", "recognition", "organization", "one", "focal", "may", "share", "scene", "from", "different", "category", "object", "collection", "co-analysis", "have", "be", "grow", "body", "work", "unsupervised", "co-analysis", "-lsb-", "xu", "et", "al.", "2012", "Huang", "et", "al.", "2012", "van", "Kaick", "et", "al.", "2013", "Huang", "et", "al.", "2013a", "Zheng", "et", "al.", "2013", "-rsb-", "organization", "3d", "object", "collection", "-lsb-", "Ovsjanikov", "et", "al.", "2011", "Jain", "et", "al.", "2012", "Kim", "et", "al.", "2012", "Huang", "et", "al.", "2013b", "-rsb-", "similar", "work", "exist", "image", "collection", "e.g.", "image", "co-salience", "detection", "-lsb-", "Cheng", "et", "al.", "2014", "-rsb-", "most", "case", "co-analysis", "operate", "object", "belong", "same", "semantic", "category", "exception", "recent", "work", "Huang", "et", "al.", "-lsb-", "2013b", "-rsb-", "which", "perform", "qualitative", "analysis", "heterogeneous", "object", "collection", "however", "object", "comparison", "employ", "global", "shape", "descriptor", "while", "still", "result", "unique", "qualitative", "distance", "term", "number", "hop", "tree", "representation", "between", "object", "another", "recent", "work", "co-hierarchical", "analysis", "van", "Kaick", "et", "al.", "-lsb-", "2013", "-rsb-", "also", "employ", "clustering", "approach", "clustering", "partition", "set", "shape", "different", "mode", "structural", "variation", "while", "hierarchical", "model", "offer", "flexibility", "account", "structural", "variation", "still", "provide", "only", "single", "view", "each", "shape", "we", "representation", "allow", "multiple", "view", "scene", "model", "each", "which", "may", "see", "from", "perspective", "particular", "focal", "point", "moreover", "we", "analysis", "produce", "overlap", "cluster", "which", "characterize", "underlie", "datum", "larger", "granularity", "contextual", "analysis", "part-in-whole", "object-in-scene", "type", "retrieval", "have", "be", "study", "semantic", "analysis", "3d", "object", "indoor", "scene", "Shapira", "et", "al.", "-lsb-", "2009", "-rsb-", "define", "context", "shape", "part", "within", "extract", "part", "hierarchy", "series", "work", "from", "Fisher", "et", "al.", "rely", "spatial", "semantic", "relation", "among", "scene", "object", "context-based", "object", "search", "-lsb-", "Fisher", "Hanrahan", "2010", "Fisher", "et", "al.", "2011", "-rsb-", "object", "replacement", "scene", "synthesis", "-lsb-", "Fisher", "et", "al.", "2012", "-rsb-", "all", "work", "substructure", "scene", "provide", "context", "characterize", "individual", "object", "therein", "we", "treat", "substructure", "explicit", "scene", "feature", "i.e.", "potential", "focal", "perform", "contextual", "analysis", "larger", "scope", "one", "possible", "way", "find", "salient", "substructure", "scene", "collection", "extract", "object", "group", "base", "co-occurrence", "object", "category", "like", "work", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "contrast", "we", "group", "scene", "object", "rather", "than", "object", "category", "form", "focal", "furthermore", "group", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "base", "frequency", "analysis", "only", "while", "we", "perform", "both", "frequent", "pattern", "mining", "subspace", "clustering", "focal", "point", "extraction", "Singh", "et", "al.", "-lsb-", "2012", "-rsb-", "detect", "mid-level", "discriminative", "patch", "from", "set", "unlabeled", "image", "alternate", "between", "clustering", "training", "discriminative", "classifier", "similar", "idea", "apply", "extract", "from", "large", "repository", "geo-tagged", "imagery", "visual", "feature", "which", "both", "frequently", "occur", "geographically", "distinctive", "under", "weak", "supervision", "-lsb-", "Doersch", "et", "al.", "2012", "-rsb-", "we", "co-analysis", "unsupervised", "drive", "novel", "cluster", "compactness", "objective", "both", "focal", "selection", "focal-induced", "clustering", "frequent", "pattern", "mining", "frequent", "pattern", "mining", "have", "be", "extensively", "study", "topic", "datum", "mining", "-lsb-", "Han", "et", "al.", "2007", "-rsb-", "most", "relevant", "work", "those", "design", "frequent", "subgraph", "mining", "e.g.", "-lsb-", "Yan", "Han", "2002", "-rsb-", "which", "primarily", "base", "subgraph", "isomorphism", "testing", "directly", "adapt", "method", "we", "problem", "set", "infeasible", "since", "relation", "among", "object", "we", "input", "graph", "loose", "possibly", "uncertain", "we", "adopt", "inexact", "subgraph", "matching", "formulate", "graph", "edit", "distance", "-lsb-", "Riesen", "et", "al.", "2010", "-rsb-", "where", "edit", "cost", "define", "base", "spatial", "arrangement", "between", "scene", "object", "also", "worth", "note", "frequency", "occurrence", "only", "criterion", "focal", "point", "selection", "subsequent", "cluster", "analysis", "further", "adjust", "extract", "focal", "subspace", "clustering", "subspace", "clustering", "cluster", "highdimensional", "datum", "multiple", "subspace", "each", "model", "subset", "feature", "-lsb-", "Vidal", "2011", "-rsb-", "high", "level", "clustering", "problem", "we", "face", "have", "similar", "setting", "subspace", "clustering", "where", "focal", "act", "feature", "subset", "characterize", "subspace", "contain", "cluster", "scene", "subspace", "analysis", "via", "spectral", "clustering", "have", "be", "one", "most", "effective", "approach", "subspace", "clustering", "-lsb-", "Wang", "et", "al.", "2011a", "-rsb-", "however", "spectral", "clustering", "always", "produce", "partition", "we", "work", "we", "perform", "cluster", "attachment", "reveal", "cluster", "overlap", "base", "representative", "focal", "make", "obtain", "cluster", "better", "reflect", "complexity", "heterogeneity", "datum", "collection", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "35:3", "Figure", "overview", "we", "algorithm", "input", "heterogeneous", "collection", "3d", "indoor", "scene", "we", "represent", "each", "scene", "structural", "graph", "-lrb-", "-rrb-", "co-analysis", "algorithm", "iterative", "between", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "each", "iteration", "involve", "interleave", "optimization", "consist", "focal", "point", "detection", "-lrb-", "-rrb-", "focal-induced", "scene", "clustering", "-lrb-", "-rrb-", "after", "set", "contextual", "focal", "obtain", "entire", "scene", "collection", "can", "organize", "focal", "serve", "interlink", "between", "scene", "from", "various", "cluster", "-lrb-", "-rrb-", "overview", "input", "we", "algorithm", "heterogeneous", "collection", "3d", "indoor", "scene", "collect", "from", "public", "repository", "scene", "typically", "come", "semantic", "label", "object", "scene", "themselves", "we", "analysis", "use", "object", "label", "never", "scene", "label", "we", "goal", "extract", "set", "contextual", "focal", "well", "clustering", "scene", "base", "focal", "see", "Figure", "each", "scene", "structural", "graph", "construct", "which", "encode", "two", "type", "relationship", "between", "scene", "object", "support", "proximity", "we", "main", "algorithm", "consist", "couple", "optimization", "whose", "objective", "maximize", "overall", "compactness", "scene", "cluster", "while", "ensure", "focal", "represent", "respective", "cluster", "effectively", "key", "each", "representative", "focal", "sufficiently", "discriminative", "so", "frequent", "only", "within", "cluster", "represent", "characterize", "optimization", "iterative", "where", "each", "iteration", "interleave", "between", "cluster-guided", "focal", "point", "mining", "focal-induced", "subspace", "clustering", "scene", "see", "Figure", "first", "initial", "phase", "optimization", "extract", "frequent", "substructure", "focal", "from", "input", "structural", "graph", "via", "subgraph", "mining", "-lrb-", "section", "4.1", "-rrb-", "rather", "than", "rely", "subgraph", "isomorphism", "we", "perform", "inexact", "graph", "matching", "which", "insist", "consistency", "node", "labeling", "edge", "connection", "latter", "account", "loose", "relation", "between", "corresponding", "object", "across", "large", "heterogeneous", "scene", "collection", "matching", "relation", "base", "layout", "similarity", "measure", "between", "spatial", "arrangement", "object", "matching", "confine", "scene", "group", "result", "from", "most", "recent", "clustering", "phase", "specifically", "subgraph", "matching", "weight", "so", "substructure", "find", "frequent", "only", "within", "cluster", "characterize", "second", "phase", "base", "extract", "focal", "we", "perform", "subspace", "clustering", "-lrb-", "section", "4.2", "-rrb-", "scene", "structural", "graph", "cluster", "so", "each", "cluster", "characterize", "subset", "current", "focal", "generally", "representative", "focal", "cluster", "unique", "clustering", "step", "seek", "maximize", "compactness", "all", "cluster", "where", "compactness", "define", "scene-to-scene", "similarity", "base", "focal-centric", "graph", "kernel", "-lrb-", "fcgk", "-rrb-", "we", "define", "fcgk", "base", "work", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "which", "utilize", "rooted", "walk", "graph", "kernel", "however", "instead", "weighting", "equally", "walk", "from", "all", "source", "we", "weigh", "more", "heavily", "those", "walk", "which", "originate", "from", "representative", "focal", "graph", "maximization", "base", "iteratively", "reweight", "subspace", "clustering", "scheme", "we", "develop", "which", "gradually", "increase", "cluster", "compactness", "finally", "once", "cluster", "focal", "determine", "optimization", "we", "perform", "cluster", "attachment", "focal", "join", "-lrb-", "section", "4.3", "-rrb-", "some", "cluster", "share", "scene", "contain", "multiple", "focal", "each", "characterize", "different", "cluster", "cluster", "naturally", "attach", "shared", "scene", "within", "cluster", "multiple", "local", "substructure", "may", "occur", "concurrently", "across", "all", "most", "scene", "substructure", "naturally", "join", "form", "non-local", "focal", "note", "non-local", "focal", "could", "detect", "via", "subgraph", "mining", "since", "only", "spatially", "close", "object", "connect", "graph", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:4", "K.", "Xu", "et", "al.", "-lrb-", "-rrb-", "mining", "-lrb-", "-rrb-", "clustering", "-lrb-", "-rrb-", "mining", "Figure", "illustration", "iterative", "optimization", "pipeline", "scene", "depict", "grey", "box", "enclose", "several", "substructure", "represent", "circle", "square", "diamond", "triangle", "etc.", "initialize", "interleave", "optimization", "we", "first", "detect", "set", "frequent", "substructure", "show", "middle", "-lrb-", "-rrb-", "base", "subspace", "clustering", "lead", "incorrect", "cluster", "-lrb-", "mark", "red", "-rrb-", "due", "trivial", "substructure", "-lrb-", "circle", "-rrb-", "occur", "most", "scene", "we", "perform", "cluster-guided", "weighted", "mining", "which", "eliminate", "trivial", "substructure", "follow", "more", "accurate", "clustering", "result", "obtain", "-lrb-", "-rrb-", "base", "new", "set", "discriminant", "substructure", "-lrb-", "-rrb-", "finally", "we", "perform", "cluster", "attachment", "reveal", "overlap", "cluster", "-lrb-", "red", "yellow", "cluster", "-lrb-", "-rrb-", "-rrb-", "well", "focal", "join", "discover", "non-local", "focal", "point", "-lrb-", "mark", "red", "-lrb-", "-rrb-", "-rrb-", "focal-driven", "scene", "co-analysis", "each", "input", "scene", "we", "construct", "structural", "graph", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "whose", "node", "scene", "object", "edge", "encode", "spatial", "relationship", "support", "proximity", "between", "object", "see", "algorithm", "both", "node", "edge", "label", "object", "semantic", "label", "relationship", "type", "-lrb-", "support", "proximity", "-rrb-", "respectively", "we", "first", "detect", "all", "support", "relationship", "between", "object", "testing", "vertical", "contact", "between", "shape", "geometry", "second", "we", "add", "proximity", "edge", "from", "any", "object", "connect", "support", "edge", "object", "which", "have", "strongest", "connection", "where", "connection", "strength", "-lrb-", "equation", "-rrb-", "define", "part", "layout", "similarity", "Third", "we", "ensure", "any", "group", "symmetric", "object", "have", "symmetric", "connection", "other", "object", "any", "we", "detect", "all", "group", "mutually", "symmetric", "object", "examine", "each", "group", "all", "outside", "object", "connect", "group", "more", "than", "two", "symmetric", "object", "group", "have", "similar", "spatial", "arrangement", "-lrb-", "equation", "-rrb-", "respect", "outside", "object", "we", "ensure", "all", "connect", "outside", "object", "edge", "same", "type", "depend", "relationship", "against", "outside", "object", "detect", "mutually", "symmetric", "object", "i.e.", "object", "possess", "similar", "geometry", "we", "adopt", "registration", "method", "describe", "-lsb-", "Wang", "et", "al.", "2011b", "-rsb-", "finally", "we", "detect", "connected", "component", "current", "graph", "connect", "component", "proximity", "edge", "make", "sure", "entire", "scene", "represent", "connected", "graph", "we", "co-analysis", "operate", "structural", "graph", "main", "algorithm", "involve", "couple", "optimization", "both", "focal", "point", "mining", "scene", "clustering", "objective", "optimization", "max", "-lrb-", "-rrb-", "where", "-lcb-", "-rcb-", "set", "focal", "point", "-lcb-", "-rcb-", "...", "-lrb-", "-rrb-", "clustering", "-lrb-", "-rrb-", "cluster", "attachment", "-lrb-", "-rrb-", "focal", "join", "set", "cluster", "denote", "compactness", "cluster", "base", "FCGK", "size", "cluster", "we", "optimize", "iteratively", "iteration", "continue", "until", "overall", "compactness", "cluster", "converge", "specifically", "when", "change", "objective", "function", "less", "than", "1.0", "10", "follow", "section", "we", "detail", "we", "co-analysis", "algorithm", "4.1", "focal", "extraction", "via", "graph", "mining", "substructure", "scene", "consist", "group", "nearby", "object", "along", "spatial", "arrangement", "subgraph", "we", "could", "define", "focal", "substructure", "occur", "frequently", "across", "large", "number", "semantically", "related", "scene", "e.g.", "bedroom", "however", "since", "scene", "label", "can", "unknown", "ambiguous", "especially", "hybrid", "scene", "we", "do", "use", "they", "instead", "we", "couple", "focal", "detection", "identification", "meaningful", "cluster", "substructure", "occur", "scene", "we", "say", "scene", "support", "substructure", "notion", "occurrence", "quickly", "relax", "inexact", "graph", "matching", "which", "enable", "similarity", "measure", "spatial", "layout", "between", "substructure", "scene", "layout", "similarity", "we", "define", "layout", "similarity", "between", "two", "substructure", "examine", "pair-wise", "spatial", "arrangement", "orient", "bound", "box", "-lrb-", "obb", "-rrb-", "object", "substructure", "suppose", "we", "give", "two", "substructure", "represent", "two", "subgraph", "structural", "graph", "two", "scene", "vertex", "support", "edge", "proximity", "edge", "each", "group", "symmetric", "object", "each", "outside", "object", "do", "symmetric", "connection", "algorithm", "Structural", "Graph", "Construction", "Input", "scene", "-lcb-", "-rcb-", "output", "structural", "graph", "-lcb-", "-rcb-", "supportedge", "-lrb-", "-rrb-", "ProximityEdge", "-lrb-", "-rrb-", "detectsymgroup", "-lrb-", "-rrb-", "foreach", "do", "foreach", "do", "arr", "-lrb-", "-rrb-", "0.1", "foreach", "do", "-lcb-", "-rcb-", "10", "connectcomponent", "-lrb-", "-rrb-", "11", "return", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "35:5", "Figure", "structural", "graph", "-lrb-", "-rrb-", "input", "scene", "-lrb-", "-rrb-", "encode", "two", "type", "relationship", "support", "-lrb-", "red", "-rrb-", "proximity", "-lrb-", "blue", "-rrb-", "-lrb-", "-rrb-", "plot", "layout", "similarity", "object", "pair", "after", "spectral", "embedding", "layout", "dissimilarity", "between", "they", "define", "layout", "-lrb-", "-rrb-", "arr", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-lcb-", "-rcb-", "-lcb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rcb-", "where", "-lrb-", "-rrb-", "corresponding", "object", "correspondence", "can", "determine", "during", "subgraph", "mining", "describe", "below", "arr", "measure", "spatial", "arrangement", "dissimilarity", "between", "two", "pair", "object", "which", "define", "base", "two", "factor", "first", "connection", "strength", "between", "object", "-lrb-", "obb", "-lrb-", "-rrb-", "obb", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "dl", "-lrb-", "-rrb-", "dl", "-lrb-", "-rrb-", "where", "Hausdorff", "distance", "obb", "-lrb-", "-rrb-", "obb", "object", "dl", "-lrb-", "-rrb-", "diagonal", "length", "obb", "-lrb-", "-rrb-", "second", "factor", "angle", "between", "upright", "vector", "vector", "between", "-lrb-", "-rrb-", "angle", "-lrb-", "dir", "-lrb-", "-rrb-", "upright", "-rrb-", "where", "dir", "-lrb-", "-rrb-", "vector", "from", "larger", "object", "two", "smaller", "one", "upright", "upright", "vector", "dissimilarity", "spatial", "arrangement", "between", "two", "object", "pair", "define", "arr", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "??", "-lrb-", "??", "max", "-rrb-", "normalize", "connection", "strength", "where", "0.4", "maximum", "value", "max", "find", "all", "pair", "object", "normalize", "similarly", "we", "use", "0.6", "we", "implementation", "Figure", "-lrb-", "-rrb-", "show", "few", "example", "similar", "layout", "frequent", "substructure", "mining", "frequent", "subgraph", "mining", "extract", "from", "set", "input", "graph", "-lcb-", "-rcb-", "set", "subgraph", "-lcb-", "-rcb-", "which", "frequently", "occur", "-lrb-", "more", "than", "give", "threshold", "value", "min", "-rrb-", "input", "graph", "base", "subgraph", "isomorphism", "we", "define", "-lcb-", "ik", "min", "-rcb-", "where", "ik", "-lrb-", "-rrb-", "indicator", "function", "subgraph", "isomorphism", "-lcb-", "ik", "-rcb-", "supporter", "set", "directly", "apply", "frequent", "subgraph", "mining", "structural", "graph", "ineffective", "since", "proximity", "relationship", "necessarily", "consistent", "across", "different", "scene", "e.g.", "see", "Figure", "-lrb-", "-rrb-", "one", "may", "resort", "inexact", "graph", "matching", "e.g.", "base", "graph", "edit", "distance", "-lsb-", "Riesen", "et", "al.", "2010", "-rsb-", "however", "large", "search", "space", "inexact", "subgraph", "mining", "make", "approach", "prohibitive", "we", "propose", "two-step", "scheme", "frequent", "substructure", "mining", "-lrb-", "algorithm", "-rrb-", "which", "carry", "out", "inexact", "graph", "match", "efficiently", "we", "first", "perform", "frequent", "subgraph", "mining", "base", "exact", "subgraph", "isomorphism", "use", "gspan", "-lsb-", "yan", "Han", "2002", "-rsb-", "relatively", "low", "minimal", "support", "threshold", "-lrb-", "line", "algorithm", "-rrb-", "second", "step", "we", "employ", "inexact", "subgraph", "matching", "-lsb-", "Riesen", "et", "al.", "2010", "-rsb-", "match", "frequent", "subgraph", "mine", "previous", "step", "against", "all", "graph", "set", "expand", "support", "-lrb-", "Lines", "2-6", "-rrb-", "note", "both", "step", "matching", "graph", "node", "exact", "base", "only", "node", "label", "create", "tolerance", "different", "proximity", "connection", "graph", "structure", "we", "use", "error", "correction", "subgraph", "introduce", "three", "edit", "operation", "graph", "edge", "insertion", "deletion", "proximitytype", "edge", "well", "substitution", "between", "two", "proximity", "edge", "edit", "cost", "each", "operation", "define", "spatial", "arrangement", "dissimilarity", "-lrb-", "equation", "-rrb-", "between", "two", "pair", "object", "involve", "total", "edit", "cost", "-lrb-", "-rrb-", "match", "less", "than", "0.1", "we", "add", "supporter", "set", "frequent", "subgraph", "we", "have", "obtain", "its", "embedding", "any", "its", "supporter", "graph", "during", "mining", "step", "denote", "-lrb-", "-rrb-", "however", "embedding", "its", "supporter", "may", "have", "different", "layout", "since", "exact", "mining", "step", "layout-oblivious", "e.g.", "show", "figure", "-lrb-", "-rrb-", "we", "locate", "remove", "weak", "-lrb-", "outlier", "-rrb-", "supporter", "which", "embedded", "subgraph", "have", "significantly", "different", "layout", "from", "those", "other", "supporter", "-lrb-", "line", "7-10", "-rrb-", "specifically", "give", "supporter", "we", "compute", "average", "dissimilarity", "between", "its", "corresponding", "embedding", "those", "all", "other", "supporter", "filter", "out", "supporter", "value", "exceed", "threshold", "0.3", "finally", "we", "remove", "those", "subgraph", "whose", "number", "supporter", "fall", "below", "minimal", "support", "threshold", "min", "-lrb-", "line", "12", "-rrb-", "cluster-guided", "weighted", "mining", "we", "goal", "detect", "representative", "focal", "point", "characterize", "meaningful", "clustering", "input", "scene", "substructure", "which", "frequent", "over", "entire", "collection", "therefore", "instead", "rely", "frequency", "criterion", "equation", "-lrb-", "-rrb-", "we", "base", "we", "substructure", "mining", "current", "cluster", "perform", "weighted", "subgraph", "mining", "-lsb-", "tsuda", "Kudo", "2006", "-rsb-", "each", "cluster", "we", "define", "support", "weight", "-lrb-", "-rrb-", "figure", "scene", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "have", "same", "sub-scene", "represent", "different", "subgraph", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "have", "same", "subgraph", "while", "layout", "corresponding", "sub-scene", "different", "-lrb-", "-rrb-", "layout", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:6", "K.", "Xu", "et", "al.", "Extended", "frequent", "substructure", "mining", "structural", "graph", "-lcb-", "-rcb-", "minimal", "support", "frequent", "substructure", "-lcb-", "-rcb-", "expand", "support", "Algorithm", "Input", "min", "output", "-lcb-", "-rcb-", "MineSubgraph", "-lrb-", "min", "-rrb-", "foreach", "do", "foreach", "do", "-lrb-", "-rrb-", "ErrorCorrectMatch", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lcb-", "-rcb-", "filter", "support", "10", "11", "12", "13", "foreach", "do", "foreach", "do", "-lrb-", "-rrb-", "-lcb-", "-rcb-", "min", "-lcb-", "-rcb-", "return", "measure", "support", "any", "substructure", "substructure", "detect", "frequent", "its", "weighted", "sum", "support", "denote", "discriminant", "score", "greater", "than", "threshold", "-lcb-", "-rcb-", "where", "-lrb-", "2x", "ik", "-rrb-", "use", "positive", "weight", "belong", "negative", "otherwise", "discriminant", "score", "favor", "substructure", "which", "frequent", "cluster", "penalize", "its", "frequency", "other", "cluster", "therefore", "mine", "substructure", "frequent", "mainly", "within", "cluster", "specifically", "we", "set", "1/n", "where", "-lrb-", "-rrb-", "n/n", "we", "fix", "0.1", "we", "algorithm", "final", "set", "focal", "point", "take", "union", "per-cluster", "discriminant", "substructure", "where", "number", "cluster", "achieve", "weighted", "mining", "we", "evaluate", "discriminant", "score", "individual", "substructure", "which", "efficiently", "enumerate", "gspan", "identify", "discriminative", "one", "base", "current", "cluster", "we", "perform", "support", "expand", "filter", "extract", "substructure", "first", "iteration", "when", "clustering", "miss", "we", "use", "unweighted", "frequent", "substructure", "mining", "focal", "extract", "we", "perform", "subspace", "clustering", "group", "input", "scene", "accord", "extract", "focal", "share", "i.e.", "scene", "contain", "support", "same", "focal", "each", "scene", "we", "build", "high-dimensional", "feature", "vector", "clustering", "feature", "define", "set", "all", "extract", "focal", "most", "current", "focal", "mining", "step", "-lrb-", "section", "4.1", "-rrb-", "each", "entry", "feature", "vector", "indicator", "support", "scene", "corresponding", "focal", "form", "Bag-of-Words", "-lrb-", "bow", "-rrb-", "feature", "-lrb-", "ik", "-rrb-", "subspace", "clustering", "perform", "over", "all", "input", "datum", "represent", "feature", "space", "-lsb-", "-rsb-", "d?n", "extract", "cluster", "characterize", "low-dimensional", "subspace", "subspace", "clustering", "we", "adopt", "method", "Wang", "et", "al.", "-lsb-", "2011a", "-rsb-", "subspace", "segmentation", "via", "quadratic", "programming", "-lrb-", "ssqp", "-rrb-", "state-of-the-art", "spectral", "clustering", "base", "approach", "basic", "idea", "ssqp", "express", "each", "datum", "linear", "combination", "all", "other", "datum", "dataset", "ij", "while", "implicitly", "enforce", "coefficient", "ij", "zero", "all", "which", "belong", "different", "subspace", "from", "learn", "coefficient", "matrix", "n?n", "solve", "follow", "constrain", "optimization", "4.2", "focal-induced", "scene", "clustering", "algorithm", "iteratively", "reweight", "Subspace", "Clustering", "Input", "structural", "graph", "-lcb-", "-rcb-", "bow", "feature", "-lsb-", "-rsb-", "-lrb-", "-lrb-", "ik", "-rrb-", "-rrb-", "weight", "-lsb-", "-rsb-", "-lrb-", "-lrb-", "ik", "-rrb-", "-rrb-", "subspace", "cluster", "-lcb-", "-rcb-", "update", "weight", "output", "do", "repeat", "-lcb-", "-rcb-", "subspaceclustering", "-lrb-", "-rrb-", "do", "representativefocalset", "-lrb-", "-rrb-", "compactness", "-lrb-", "-rrb-", "foreach", "do", "foreach", "do", "10", "ik", "11", "do", "12", "13", "do", "14", "ik", "overall", "compactness", "do", "improve", "15", "until", "16", "return", "-lcb-", "-rcb-", "problem", "min", "-lrb-", "-rrb-", "xz", "s.t.", "diag", "-lrb-", "-rrb-", "where", "Frobenius", "norm", "diag", "-lrb-", "-rrb-", "diagonal", "vector", "matrix", "regularization", "term", "enforce", "sparsity", "solution", "lead", "feature", "selection", "subspace", "clustering", "problem", "linear", "constrain", "quadratic", "programming", "which", "can", "solve", "efficiently", "result", "coefficient", "matrix", "form", "affinity", "matrix", "base", "which", "spectral", "clustering", "apply", "obtain", "clustering", "result", "automatically", "determine", "number", "cluster", "we", "employ", "self-tuning", "spectral", "clustering", "-lsb-", "zelnik-manor", "Perona", "2004", "-rsb-", "practice", "cluster", "count", "relatively", "stable", "throughout", "iteration", "since", "structure", "bow", "feature", "matrix", "do", "change", "significantly", "besides", "clustering", "result", "we", "need", "identify", "representative", "focal", "which", "characterize", "cluster", "each", "cluster", "we", "identify", "set", "representative", "focal", "denote", "we", "rank", "importance", "all", "focal", "support", "any", "structural", "graph", "cluster", "base", "discriminant", "score", "see", "equation", "-lrb-", "-rrb-", "top", "rank", "focal", "select", "representative", "one", "we", "select", "top", "focal", "from", "list", "until", "i-th", "one", "when", "over", "80", "structural", "graph", "cluster", "which", "support", "top", "focals", "simultaneously", "we", "ultimate", "goal", "maximize", "compactness", "all", "cluster", "base", "scene-to-scene", "similarity", "emphasize", "representative", "focal", "point", "subspace", "clustering", "above", "base", "indicator", "feature", "which", "capture", "occurrence", "focal", "sufficiently", "informative", "reflect", "actual", "scene", "similarity", "directly", "incorporate", "focal-centric", "scene", "similarity", "subspace", "clustering", "infeasible", "since", "representative", "focal", "unknown", "before", "feature", "selective", "clustering", "perform", "therefore", "we", "propose", "iteratively", "reweight", "subspace", "clustering", "process", "gradually", "produce", "more", "compact", "cluster", "where", "compactness", "measure", "base", "focal-centric", "graph", "kernel", "-lrb-", "fcgk", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "35:7", "-lrb-", "-rrb-", "initial", "weighted", "-lrb-", "-rrb-", "subspace", "clustering", "bow", "feature", "characterize", "set", "-lrb-", "-rrb-", "re-weighting", "-lrb-", "-rrb-", "re-clustering", "figure", "mini-experiment", "reweight", "subspace", "clustering", "weighted", "bow", "feature", "shaded", "grey", "level", "-lrb-", "dark", "large", "light", "small", "-rrb-", "from", "initial", "bow", "feature", "-lrb-", "-rrb-", "subspace", "clustering", "produce", "three", "cluster", "-lrb-", "color", "-rrb-", "along", "representative", "focal", "-lrb-", "mark", "corresponding", "color", "-rrb-", "color", "number", "indicate", "compactness", "value", "cluster", "discriminant", "appear", "across", "three", "cluster", "-lrb-", "-rrb-", "so", "-lrb-", "-rrb-", "corresponding", "weight", "set", "weight", "decrease", "due", "low", "compactness", "blue", "cluster", "next", "clustering", "group", "green", "cluster", "representative", "focal", "point", "-lrb-", "-rrb-", "focal-centric", "graph", "kernel", "give", "cluster", "its", "compactness", "define", "average", "distance", "between", "all", "pair", "structural", "graph", "belong", "measure", "FCGK", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "weighted", "p-th", "order", "walk", "graph", "kernel", "10", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "r?g", "s?g", "-lrb-", "-rrb-", "p-th", "order", "rooted-walk", "graph", "kernel", "-lsb-", "Fisher", "et", "al.", "2011", "-rsb-", "which", "we", "briefly", "review", "below", "completeness", "compare", "node", "graph", "respectively", "compare", "all", "walk", "length", "whose", "first", "node", "against", "all", "walk", "length", "whose", "first", "node", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "r1", "...", "ep", "rp", "-rrb-", "gi", "-lrb-", "-rrb-", "-lrb-", "s1", "...", "fp", "sp", "-rrb-", "Gj", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "set", "all", "walk", "length", "originate", "from", "graph", "node", "kernel", "take", "both", "geometry", "label", "comparison", "account", "similar", "-lsb-", "Fisher", "et", "al.", "2011", "-rsb-", "except", "we", "use", "single", "label", "each", "object", "instead", "series", "semantic", "tag", "edge", "kernel", "we", "use", "similarity", "spatial", "arrangement", "-lrb-", "equation", "-rrb-", "instead", "binary", "comparison", "edge", "type", "walk", "kernel", "focal-centric", "we", "set", "higher", "weight", "those", "root", "walk", "which", "originate", "from", "node", "representative", "focal", "cluster", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "otherwise", "where", "scaling", "factor", "we", "algorithm", "we", "set", "100", "which", "fairly", "high", "emphasize", "more", "role", "focal", "scene", "characterization", "than", "overall", "scene", "similarity", "iteratively", "reweight", "subspace", "clustering", "structural", "graph", "we", "weight", "individual", "dimension", "its", "bow", "feature", "vector", "weight", "vector", "-lrb-", "ik", "-rrb-", "solve", "weigh", "subspace", "clustering", "which", "minimize", "error", "linear", "approximation", "equation", "-lrb-", "-rrb-", "under", "weighted", "Frobenius", "norm", "specifically", "we", "replace", "first", "term", "equation", "-lrb-", "-rrb-", "11", "xz", "ik", "-lsb-", "-lrb-", "xz", "-rrb-", "ik", "ik", "-rsb-", "weight", "allow", "we", "tune", "importance", "individual", "dimension", "when", "seek", "subspace", "can", "utilize", "iteratively", "shift", "clustering", "result", "example", "one", "can", "increase", "weight", "correspond", "dimension", "span", "subspace", "cluster", "obtain", "last", "round", "reinforce", "cluster", "current", "clustering", "we", "case", "we", "encourage", "reoccurrence", "compact", "cluster", "next", "iteration", "increase", "weight", "dimension", "correspond", "its", "representative", "focal", "point", "deprecate", "incompact", "cluster", "decrease", "corresponding", "weight", "initially", "weight", "set", "uniformly", "each", "iteration", "we", "perform", "weighted", "subspace", "clustering", "update", "base", "compactness", "cluster", "which", "belong", "see", "Algorithm", "each", "member", "cluster", "we", "compute", "weight", "dimension", "correspond", "representative", "focal", "cluster", "base", "cluster", "compactness", "focal", "point", "discriminant", "score", "-lrb-", "line", "10", "-rrb-", "focal", "representative", "one", "any", "cluster", "we", "set", "corresponding", "dimension", "weight", "vector", "all", "structural", "graph", "-lrb-", "line", "11-14", "-rrb-", "stop", "criterion", "iterative", "process", "same", "one", "use", "during", "interleave", "optimization", "i.e.", "change", "overall", "cluster", "compactness", "Figure", "demonstrate", "process", "reweight", "subspace", "clustering", "mini-experiment", "structural", "graph", "focal", "experiment", "after", "obtain", "subspace", "clustering", "along", "representative", "focal", "weight", "correspond", "focal", "point", "decrease", "due", "low", "discriminant", "score", "low", "cluster", "compactness", "respectively", "update", "weight", "which", "originally", "cluster", "blue", "cluster", "due", "now", "group", "green", "one", "characterize", "because", "play", "major", "role", "clustering", "after", "deprecate", "after", "reweight", "weighted", "feature", "vector", "some", "structural", "graph", "may", "decrease", "-lrb-", "close", "-rrb-", "vector", "-lrb-", "e.g.", "Figure", "-rrb-", "since", "clustering", "structural", "graph", "quite", "unpredictable", "we", "choose", "leave", "they", "out", "when", "weight", "vector", "vanish", "make", "iterative", "clustering", "converge", "faster", "structural", "graph", "later", "introduce", "back", "beginning", "next", "round", "interleave", "optimization", "cluster", "attachment", "spectral", "clustering", "produce", "partition", "input", "dataset", "which", "do", "reflect", "potential", "cluster", "overlap", "due", "scene", "which", "exist", "multiple", "cluster", "general", "structural", "graph", "input", "scene", "which", "support", "multiple", "focal", "may", "belong", "multiple", "cluster", "have", "other", "different", "representative", "focal", "we", "simply", "attach", "cluster", "respect", "shared", "scene", "which", "can", "easily", "identify", "reveal", "overlap", "focal", "join", "subgraph", "mining", "perform", "structural", "graph", "whose", "node", "connection", "only", "capture", "local", "proximity", "unable", "return", "large-scale", "non-local", "substructure", "issue", "have", "be", "observe", "recent", "work", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "which", "base", "structure", "group", "detection", "over", "structural", "graph", "we", "work", "frequent", "substructure", "detection", "couple", "subspace", "clustering", "enable", "we", "combine", "extract", "focal", "form", "larger", "non-local", "substructure", "through", "analyze", "cluster", "characterize", "suppose", "both", "representative", "focal", "some", "cluster", "supporter", "set", "denote", "overlap", "sufficiently", "i.e.", "0.9", "min", "-lcb-", "-rcb-", "we", "join", "they", "union", "node", "form", "larger", "substructure", "12", "representative", "focal", "4.3", "cluster", "attachment", "focal", "join", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:8", "K.", "Xu", "et", "al.", "collection", "#f", "#nlf", "min", "avg", "max", "mf", "Stanford", "24", "50.4", "Tsinghua", "34", "46.1", "Collection", "#f", "#nlf", "min", "avg", "max", "mf", "Stanford", "24", "50.4", "Tsinghua", "34", "46.1", "Table", "statistics", "focal", "point", "extraction", "#f", "denote", "total", "number", "focal", "#nlf", "non-local", "one", "minimum", "average", "maximum", "number", "object", "extract", "focal", "denote", "min", "avg", "max", "mf", "percentage", "multifocal", "scene", "over", "whole", "collection", "result", "we", "present", "result", "obtain", "we", "algorithm", "focal", "point", "drive", "analysis", "indoor", "scene", "collection", "scene", "retrieval", "we", "compare", "we", "result", "those", "obtain", "from", "state-of-the-art", "method", "both", "through", "precision-recall", "curve", "preliminary", "user", "study", "target", "hybrid", "scene", "more", "extensive", "result", "accompany", "video", "can", "find", "supplementary", "material", "dataset", "dataset", "we", "experiment", "be", "provide", "Stanford", "repository", "-lsb-", "Fisher", "et", "al.", "2012", "-rsb-", "Tsinghua", "repository", "-lsb-", "xu", "et", "al.", "2013", "-rsb-", "both", "dataset", "contain", "semantic", "tag", "object", "originally", "collect", "from", "Google", "-lrb-", "now", "Trimble", "-rrb-", "3D", "Warehouse", "since", "tag", "from", "two", "dataset", "inconsistent", "we", "run", "we", "test", "each", "dataset", "separately", "each", "scene", "we", "remove", "wall", "focus", "only", "interior", "scene", "object", "Stanford", "collection", "consist", "132", "scene", "461", "object", "encompass", "78", "object", "category", "five", "labeled", "scene", "category", "Tsinghua", "dataset", "consist", "792", "scene", "13", "365", "object", "encompass", "119", "object", "category", "six", "labeled", "scene", "category", "Tsinghua", "dataset", "contain", "102", "hybrid", "scene", "which", "compose", "many", "subscene", "each", "represent", "room", "parameter", "statistics", "key", "parameter", "we", "algorithm", "include", "minimum", "support", "min", "use", "frequent", "substructure", "mining", "first", "iteration", "rooted", "path", "combination", "weight", "use", "compute", "graph", "kernel", "all", "result", "report", "paper", "be", "obtain", "same", "parameter", "setting", "min", "40", "Tsinghua", "dataset", "min", "20", "Stanford", "dataset", "parameter", "graph", "kernel", "use", "optimal", "one", "available", "from", "publish", "work", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "value", "all", "other", "parameter", "fix", "throughout", "describe", "section", "statistics", "timing", "Table", "show", "some", "statistics", "from", "focal", "point", "extraction", "scene", "clustering", "timing", "wise", "take", "10.5", "minute", "process", "whole", "Tsinghua", "dataset", "-lrb-", "792", "scene", "-rrb-", "3.2", "minute", "Stanford", "scene", "collection", "-lrb-", "132", "scene", "-rrb-", "over", "iteration", "compactness", "evaluation", "-lrb-", "include", "FCGK", "computation", "-rrb-", "take", "60", "time", "spectral", "clustering", "30", "inexact", "frequent", "pattern", "mining", "note", "first", "two", "part", "be", "both", "implement", "Matlab", "could", "see", "significant", "speed-up", "code", "c/c", "timing", "measure", "quad-core", "2.80", "GHz", "Intel", "Core", "CPU", "12gb", "RAM", "focal", "point", "extraction", "Figure", "show", "several", "cluster", "representative", "focal", "point", "extract", "from", "Tsinghua", "collection", "complete", "set", "result", "focal", "extraction", "can", "find", "supplementary", "material", "we", "can", "observe", "hybrid", "scene", "contain", "multiple", "focal", "point", "which", "fairly", "typical", "result", "cluster", "overlap", "also", "worth", "note", "extraction", "non-local", "focal", "which", "compose", "relatively", "distant", "object", "group", "e.g.", "-lcb-", "tv", "tv-stand", "table", "sofa", "-rcb-", "etc.", "Table", "give", "number", "non-local", "focal", "extract", "both", "dataset", "see", "also", "last", "two", "row", "Figure", "effect", "focal", "join", "iterative", "clustering", "Figure", "10", "plot", "how", "normalize", "compactness", "cluster", "change", "iterative", "clustering", "algorithm", "progress", "while", "change", "strictly", "monotone", "evident", "iteration", "generally", "improve", "cluster", "quality", "over", "time", "final", "cluster", "count", "two", "set", "respectively", "precision-recall", "scene", "retrieval", "Figure", "11", "compare", "we", "method", "two", "other", "method", "scene", "retrieval", "19.5", "115.0", "19.0", "105.0", "18.5", "95.0", "compactness", "17.5", "18.0", "compactness", "85.0", "75.0", "17.0", "16.5", "65.0", "16.0", "55.0", "12", "16", "20", "24", "28", "12", "15", "18", "21", "-lrb-", "-rrb-", "Stanford", "-lrb-", "-rrb-", "Tsinghua", "Figure", "10", "plot", "show", "change", "compactness", "cluster", "obtain", "we", "interleave", "optimization", "progress", "Stanford", "Tsinghua", "dataset", "respectively", "red", "dot", "represent", "switching", "point", "from", "outer", "loop", "-lrb-", "mining", "-rrb-", "inner", "loop", "-lrb-", "clustering", "-rrb-", "optimization", "take", "interleave", "iteration", "converge", "two", "dataset", "respectively", "GK", "graph", "kernel", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "measure", "similarity", "between", "whole", "scene", "since", "we", "be", "unable", "obtain", "author", "code", "we", "code", "up", "we", "own", "implementation", "two", "major", "difference", "original", "work", "first", "we", "use", "we", "structural", "graph", "which", "only", "encode", "two", "type", "relationship", "-lrb-", "support", "proximity", "-rrb-", "do", "consider", "hierarchical", "scene", "graph", "second", "computation", "node", "edge", "kernel", "slightly", "different", "see", "section", "4.2", "both", "GK", "FCGK", "scheme", "node", "edge", "kernel", "estimation", "graph", "kernel", "normalization", "well", "all", "parameter", "same", "original", "work", "bow", "baseline", "method", "where", "we", "use", "bag-of-words", "feature", "focal", "point", "only", "scene-to-scene", "similarity", "fcgk", "-lrb-", "sg", "-rrb-", "Tsinghua", "dataset", "we", "also", "apply", "we", "fcgk", "similarity", "scene", "where", "focal", "we", "use", "212", "structural", "group", "detect", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "when", "apply", "we", "method", "which", "use", "fcgk", "scene", "similarity", "we", "show", "result", "three", "setting", "-rrb-", "use", "initial", "set", "focal", "after", "only", "one", "step", "frequent", "pattern", "mining", "-rrb-", "use", "intermediate", "set", "focal", "-rrb-", "use", "final", "set", "focal", "extract", "Tsinghua", "dataset", "ground", "truth", "evaluate", "scene", "retrieval", "give", "scene", "labels/categories", "which", "come", "dataset", "since", "dataset", "contain", "many", "hybrid", "scene", "we", "separate", "subset", "simple", "scene", "remain", "hybrid", "-lrb-", "complex", "-rrb-", "scene", "report", "result", "each", "combination", "since", "Stanford", "collection", "do", "come", "scene", "label", "we", "provide", "we", "own", "label", "obtain", "manually", "which", "admittedly", "could", "introduce", "evaluation", "bias", "potentially", "more", "reliable", "method", "vote", "from", "multiple", "user", "could", "employ", "from", "precision-recall", "curve", "we", "see", "we", "focal-centric", "similarity", "base", "final", "set", "focal", "best", "all", "four", "case", "moreover", "performance", "gain", "more", "prominent", "hybrid", "scene", "result", "demonstrate", "only", "merit", "utilize", "focal", "scene", "comparison", "also", "merit", "we", "focal", "extraction", "scheme", "seem", "evident", "retrieval", "performance", "improve", "we", "iterative", "algorithm", "progress", "comparison", "GK", "Figure", "12", "show", "explicit", "comparison", "between", "GK", "FCGK", "scene", "similarity", "attest", "effectiveness", "utilize", "focal", "we", "experiment", "we", "also", "observe", "matching", "performance", "GK", "tend", "negatively", "affect", "presence", "many", "small/trivial", "object", "example", "when", "scene", "contain", "shelf", "support", "many", "small", "object", "GK", "count", "rooted", "walk", "from", "all", "object", "which", "would", "influence", "similarity", "between", "more", "prominent", "object", "fcgk", "more", "discriminative", "trivial", "object", "less", "likely", "have", "be", "choose", "focal", "user", "evaluation", "retrieval", "hybrid", "scene", "may", "difficult", "assign", "unambiguous", "category", "label", "ground", "truth", "use", "retrieval", "scene", "may", "unreliable", "thus", "instead", "rely", "scene", "category", "ground", "truth", "we", "let", "human", "user", "judge", "scene", "similarity", "base", "prior", "knowledge", "second", "comparative", "study", "scene", "retrieval", "we", "focus", "exclusively", "retrieval", "where", "query", "hybrid", "scene", "we", "present", "user", "10", "query", "each", "query", "top", "return", "from", "three", "compare", "method", "-lrb-", "gk", "bow", "fcgk", "-rrb-", "present", "user", "user", "ask", "choose", "which", "three", "most", "similar", "query", "we", "repeat", "total", "102", "query", "hybrid", "scene", "Tsinghua", "dataset", "against", "GK", "we", "obtain", "win", "percentage", "70.2", "against", "BOW", "we", "obtain", "73.9", "result", "statistically", "significant", "-lrb-", "0.01", "-rrb-", "study", "each", "scene", "have", "be", "render", "three", "random", "bird?s", "eye", "view", "image", "be", "present", "randomly", "among", "43", "participant", "80", "computer", "science", "researcher", "age", "20", "50", "rest", "frequent", "computer", "user", "vary", "background", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "35:9", "Figure", "several", "cluster", "representative", "focal", "-lrb-", "highlight", "color", "-rrb-", "extract", "from", "Tsinghua", "scene", "collection", "top", "row", "show", "intermediate", "result", "two", "cluster", "middle", "row", "show", "final", "result", "relevant", "cluster", "bottom", "row", "show", "final", "result", "other", "cluster", "note", "multi-focal", "hybrid", "scene", "cluster", "overlap", "-lrb-", "mark", "red", "dash", "box", "-rrb-", "non-local", "focal", "point", "combo", "-lcb-", "tv", "tv-stand", "table", "sofa", "-rcb-", "-lcb-", "bed", "nightstand", "dresser", "mirror", "-rcb-", "last", "two", "row", "fcgk", "-lrb-", "final", "-rrb-", "fcgk", "-lrb-", "final", "-rrb-", "0.9", "fcgk", "-lrb-", "Inter", "-rrb-", "0.9", "fcgk", "-lrb-", "Inter", "-rrb-", "0.8", "fcgk", "-lrb-", "init", "-rrb-", "0.8", "fcgk", "-lrb-", "init", "-rrb-", "gk", "fcgk", "-lrb-", "sg", "-rrb-", "0.7", "bow", "0.7", "GK", "BOW", "0.6", "0.6", "precision", "0.5", "0.4", "precision", "0.4", "0.5", "0.3", "0.3", "0.2", "0.2", "0.1", "0.1", "0.2", "0.4", "recall", "0.6", "0.8", "0.2", "0.4", "recall", "0.6", "0.8", "-lrb-", "-rrb-", "Stanford", "-lrb-", "-rrb-", "tsinghua", "-lrb-", "simple", "-rrb-", "fcgk", "-lrb-", "final", "-rrb-", "fcgk", "-lrb-", "final", "-rrb-", "0.9", "fcgk", "-lrb-", "Inter", "-rrb-", "0.9", "fcgk", "-lrb-", "Inter", "-rrb-", "fcgk", "-lrb-", "init", "-rrb-", "fcgk", "-lrb-", "init", "-rrb-", "0.8", "0.8", "fcgk", "-lrb-", "sg", "-rrb-", "fcgk", "-lrb-", "sg", "-rrb-", "0.7", "GK", "0.7", "GK", "BOW", "BOW", "0.6", "0.6", "precision", "0.4", "0.5", "precision", "0.5", "0.4", "0.3", "0.3", "0.2", "0.2", "0.1", "0.1", "0.2", "0.4", "recall", "0.6", "0.8", "0.2", "0.4", "recall", "0.6", "0.8", "-lrb-", "-rrb-", "tsinghua", "-lrb-", "hybrid", "-rrb-", "-lrb-", "-rrb-", "tsinghua", "-lrb-", "all", "-rrb-", "figure", "11", "precision-recall", "curve", "scene", "retrieval", "-lrb-", "-rrb-", "Stanford", "scene", "collection", "-lrb-", "-rrb-", "tsinghua", "collection", "simple", "scene", "-lrb-", "-rrb-", "tsinghua", "hybrid", "scene", "-lrb-", "-rrb-", "Tsinghua", "all", "scene", "80", "fcgk", "70.9", "GK", "60", "40", "20", "5.9", "160", "142.8", "fcgk", "120", "GK", "80", "40", "13", "Figure", "12", "compare", "GK", "FCGK", "scene", "similarity", "top", "row", "two", "scene", "same", "category", "GK", "return", "large", "distance", "between", "they", "due", "dissimilar", "surround", "object", "bottom", "row", "two", "scene", "belong", "different", "category", "while", "gk", "return", "small", "distance", "also", "attribute", "surround", "object", "e.g.", "nearby", "bookshelf", "contrast", "focal-centric", "view", "we", "method", "give", "more", "meaningful", "distance", "two", "pair", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:10", "K.", "Xu", "et", "al.", "Figure", "13", "Comprehensive", "retrieval", "take", "query", "scene", "return", "scene", "group", "well-matched", "focal", "query", "each", "group", "return", "rank", "fcgk", "base", "corresponding", "focal", "example", "query", "have", "two", "focal", "-lrb-", "color", "yellow", "red", "-rrb-", "match", "from", "scene", "organization", "three", "rank", "list", "return", "correspond", "two", "focal", "-lrb-", "first", "two", "row", "-rrb-", "join", "focal", "-lrb-", "bottom", "row", "-rrb-", "show", "application", "we", "scene", "organization", "allow", "classical", "scene", "query", "thus", "suitable", "any", "application", "which", "utilize", "retrieval", "result", "before", "e.g.", "-lsb-", "Fisher", "et", "al.", "2012", "Xu", "et", "al.", "2013", "-rsb-", "section", "we", "discuss", "several", "new", "capability", "afford", "we", "focal-based", "datum", "organization", "scene", "retrieval", "exploration", "Comprehensive", "retrieval", "classical", "retrieval", "single", "query", "would", "fetch", "single", "rank", "list", "datum", "item", "we", "focal-centric", "similarity", "pre-computed", "set", "focal", "we", "scene", "organization", "support", "classical", "query", "also", "support", "part-in-whole", "type", "query", "where", "user", "specify", "region", "interest", "-lrb-", "rous", "-rrb-", "query", "scene", "demonstrate", "exploration", "tool", "which", "we", "describe", "below", "interesting", "new", "feature", "enable", "we", "scene", "organization", "what", "we", "call", "comprehensive", "retrieval", "here", "query", "do", "have", "specify", "focal", "however", "available", "focal", "organization", "match", "query", "scene", "instead", "return", "single", "rank", "list", "scene", "comprehensive", "retrieval", "return", "multiple", "rank", "list", "each", "which", "correspond", "well-matched", "focal", "Figure", "13", "show", "result", "note", "vertical", "order", "table", "have", "clear", "meaning", "since", "three", "-lrb-", "horizontal", "-rrb-", "list", "retrieve", "base", "different", "set", "focal", "put", "all", "result", "together", "however", "one", "can", "expect", "those", "retrieve", "multiple", "focal", "should", "rank", "higher", "since", "have", "more", "focal", "substructure", "receive", "higher", "weight", "refer", "equation", "-lrb-", "-rrb-", "focal-to-scene", "matching", "we", "utilize", "efficient", "subgraph", "match", "approach", "describe", "-lsb-", "Riesen", "et", "al.", "2010", "-rsb-", "which", "focal", "subgraph", "pre-compile", "hierarchical", "representation", "accelerate", "online", "matching", "average", "query", "time", "960m", "Tsinghua", "collection", "140m", "Stanford", "set", "multi-query", "retrieval", "application", "example-based", "scene", "synthesis", "-lsb-", "Fisher", "et", "al.", "2012", "-rsb-", "one", "may", "form", "query", "consist", "multiple", "semantically", "related", "scene", "wish", "retrieve", "more", "scene", "same", "multi-query", "retrieval", "wellsupport", "we", "scene", "organization", "indeed", "since", "query", "scene", "related", "likely", "share", "meaningful", "substructure", "make", "they", "suitable", "focal-based", "scene", "comparison", "give", "query", "set", "we", "extract", "frequent", "substructure", "from", "set", "match", "they", "against", "extract", "focal", "scene", "organization", "we", "retrieve", "scene", "from", "organization", "use", "fcgk", "base", "match", "focal", "Figure", "14", "show", "one", "result", "query", "set", "four", "hybrid", "scene", "comparison", "we", "also", "show", "rank", "list", "return", "base", "gk", "similarity", "measure", "against", "any", "scene", "query", "set", "one", "would", "expect", "focal-based", "retrieval", "produce", "more", "discernable", "result", "more", "useful", "result", "user", "select", "four", "query", "scene", "all", "contain", "bednightstand", "combo", "desk-chair", "combo", "likely", "he/she", "seek", "scene", "contain", "similar", "substructure", "scene", "exploration", "we", "develop", "exploration", "tool", "base", "extract", "focal", "which", "enable", "user", "browse", "through", "heterogeneous", "scene", "collection", "focal", "point", "primary", "means", "search", "navigation", "Figure", "15", "show", "GUI", "we", "tool", "user", "can", "select", "few", "focal", "from", "focal", "point", "list", "panel", "-lrb-", "bottom", "-rrb-", "we", "tool", "automatically", "select", "set", "scene", "share", "similar", "focal", "list", "they", "scene", "list", "panel", "-lrb-", "right", "-rrb-", "user", "can", "browse", "list", "view", "scene", "main", "viewer", "-lrb-", "middle", "-rrb-", "any", "time", "user", "can", "click", "select", "focal", "view", "its", "embedding", "current", "scene", "term", "navigation", "show", "Figure", "user", "can", "traverse", "from", "one", "scene", "another", "one", "scene", "cluster", "another", "through", "focal", "which", "interlink", "they", "accompany", "video", "contain", "full", "session", "interactive", "exploration", "addition", "we", "provide", "interface", "user", "paint", "region", "interest", "-lrb-", "rous", "-rrb-", "search", "scene", "which", "contain", "sub-scene", "similar", "surroundings", "rous", "when", "user", "select", "rous", "scene", "we", "system", "first", "find", "focal", "point", "scene", "which", "overlap", "most", "rous", "add", "focal", "select", "list", "retrieve", "new", "list", "scene", "base", "update", "list", "select", "focal", "explore", "database", "focal", "point", "around", "rous", "instead", "only", "rous", "can", "provide", "more", "relevant", "result", "example", "user", "select", "only", "chair", "model", "rous", "naive", "partial", "matching", "would", "simply", "return", "all", "scene", "contain", "chair", "contrast", "we", "tool", "search", "scene", "share", "same", "fo", "cal", "around", "chair", "return", "result", "more", "context-aware", "note", "rooted", "walk", "graph", "kernel", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "could", "also", "support", "contextual", "part-in-whole", "query", "however", "perform", "subgraph", "search", "likely", "too", "time", "consume", "online", "retrieval", "pre-analysis", "result", "focal-based", "scene", "organization", "we", "tool", "can", "support", "efficient", "context-aware", "partial", "matching", "over", "large", "heterogeneous", "scene", "collection", "Figure", "14", "multi-query", "retrieval", "take", "query", "set", "-lrb-", "left", "-rrb-", "return", "rank", "list", "scene", "-lrb-", "bottom-right", "-rrb-", "via", "focal-based", "scene", "comparison", "fcgk", "similarity", "use", "measure", "base", "focal", "-lrb-", "color", "red", "yellow", "-rrb-", "well-match", "frequent", "substructure", "query", "set", "return", "base", "global", "scene", "similarity", "compute", "GK", "also", "show", "-lrb-", "top-right", "-rrb-", "introduce", "bias", "color", "focal", "gk", "return", "we", "also", "color", "any", "object", "whose", "tag", "match", "object", "one", "focal", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "Organizing", "heterogeneous", "scene", "collection", "through", "Contextual", "Focal", "Points", "35:11", "Figure", "15", "GUI", "we", "exploration", "tool", "compose", "four", "part", "focal", "point", "list", "panel", "-lrb-", "red", "box", "-rrb-", "select", "focal", "list", "panel", "-lrb-", "green", "-rrb-", "scene", "list", "panel", "-lrb-", "blue", "-rrb-", "main", "scene", "viewer", "user", "can", "pick", "select", "focal", "view", "its", "embedding", "current", "scene", "she", "can", "also", "select", "region", "interest", "-lrb-", "rous", "-rrb-", "viewer", "explore", "more", "scene", "via", "focal", "around", "rous", "discussion", "future", "work", "core", "datum", "organization", "problem", "mechanism", "compare", "datum", "traditional", "approach", "rely", "holistic", "datum", "view", "unique", "distance", "define", "between", "datum", "item", "group", "clustering", "however", "when", "datum", "become", "complex", "multifaceted", "fix", "global", "view", "datum", "similarity", "can", "hardly", "express", "rich", "characteristic", "datum", "we", "advocate", "use", "focal", "point", "compare", "organize", "complex", "heterogeneous", "datum", "use", "3d", "indoor", "scene", "prototype", "demonstrate", "its", "feasibility", "performance", "gain", "e.g.", "retrieval", "new", "approach", "seem", "particularly", "apt", "deal", "complex", "hybrid", "scene", "perhaps", "its", "most", "compelling", "feature", "ability", "process", "large", "heterogeneous", "collection", "scene", "organize", "they", "interlink", "well-connected", "cluster", "formation", "which", "facilitate", "scene", "exploration", "FCGK", "vs.", "GK", "while", "we", "retrieval", "experiment", "show", "superior", "performance", "fcgk", "over", "GK", "one", "should", "realize", "direct", "comparison", "between", "two", "exactly", "fair", "GK", "standalone", "graph", "similarity", "measure", "where", "only", "two", "graph", "compare", "need", "fcgk-based", "comparison", "come", "higher", "cost", "require", "set", "graph", "co-analysis", "focal", "extraction", "say", "scene", "collection", "available", "we", "would", "still", "suggest", "use", "fcgk", "its", "better", "performance", "modest", "processing", "cost", "comparison", "structural", "group", "we", "work", "focal", "point", "consist", "group", "scene", "object", "derive", "via", "structural", "scene", "analysis", "name", "alone", "suggest", "similarity", "structural", "group", "compute", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "however", "major", "difference", "first", "structural", "group", "category", "group", "while", "we", "focal", "object", "group", "more", "importantly", "group", "extraction", "involve", "only", "frequent", "pattern", "mining", "through", "local", "proximity", "base", "search", "latter", "imply", "method", "unlikely", "return", "non-local", "structural", "group", "part", "evidence", "much", "higher", "number", "group", "-lrb-", "212", "-rrb-", "obtain", "vs.", "34", "focal", "we", "obtain", "same", "scene", "collection", "-lrb-", "tsinghua", "792", "scene", "-rrb-", "retrieval", "result", "Figure", "11", "seem", "suggest", "nonlocal", "focal", "extract", "via", "mining", "clustering", "provide", "better", "perspective", "meaningful", "scene", "comparison", "non-unique", "distance", "retrieval", "experiment", "use", "fcgk", "seem", "suggest", "we", "method", "assign", "unique", "distance", "between", "any", "two", "scene", "true", "once", "set", "focal", "fix", "fcgk", "compute", "base", "those", "focal", "clustering", "result", "however", "non-uniqueness", "focal-centric", "distance", "well", "utilize", "other", "setting", "include", "comprehensive", "retrieval", "multi-query", "retrieval", "roi-driven", "scene", "exploration", "where", "relevant", "focal", "query", "scene", "all", "determine", "on-demand", "Limitations", "we", "current", "algorithm", "depend", "semantic", "labeling", "scene", "object", "remain", "see", "whether", "work", "effectively", "noisy", "incomplete", "label", "base", "pure", "geometry", "analysis", "example", "interesting", "test", "we", "method", "input", "various", "level", "label", "noise", "however", "would", "hard", "quantitatively", "evaluate", "robustness", "against", "noisy", "label", "since", "may", "difficult", "reproduce", "realistic", "labeling", "noise", "introduce", "human", "nevertheless", "two", "dataset", "we", "use", "do", "contain", "some", "incorrect", "label", "which", "do", "seem", "affect", "overall", "performance", "perhaps", "more", "than", "desirable", "number", "parameter", "algorithm", "whose", "value", "be", "determine", "experimentally", "from", "technical", "stand", "point", "improvement", "possible", "various", "component", "algorithm", "example", "we", "layout", "similarity", "operate", "obb", "only", "which", "may", "unsuitable", "object", "complex", "geometry", "spatial", "arrangement", "structural", "graph", "model", "scene", "only", "flat", "arrangement", "object", "hierarchical", "organization", "may", "potentially", "advantageous", "future", "work", "one", "obvious", "pursuit", "apply", "we", "focal-driven", "approach", "other", "dataset", "e.g.", "large", "heterogeneous", "collection", "annotated", "image", "interesting", "technical", "question", "whether", "we", "scene", "organization", "can", "update", "additional", "set", "scene", "without", "recompute", "everything", "also", "rather", "than", "replace", "one", "object", "time", "scene", "synthesis", "like", "previous", "work", "we", "scene", "organization", "focal-based", "partial", "scene", "retrieval", "may", "allow", "substitute", "sub-scene", "synthesis", "task", "we", "conclude", "paper", "question", "what", "best", "way", "compare", "complex", "scene", "??", "work", "along", "other", "before", "assume", "compare", "attribute", "graph", "define", "semantic", "tag", "object", "arrangement", "best", "way", "however", "we", "observe", "visually", "many", "retrieval", "result", "do", "look", "so", "compelling", "even", "best", "method", "date", "one", "take", "away", "coloring", "Figure", "14", "contrast", "between", "GK", "FCGK", "would", "salient", "hence", "focal-centric", "view", "we", "advocate", "offer", "perspective", "worth", "consider", "general", "question", "also", "one", "attribute", "complex", "datum", "beyond", "those", "indoor", "scene", "should", "perhaps", "answer", "user", "application", "intent", "mind", "acknowledgment", "we", "thank", "all", "reviewer", "comment", "feedback", "we", "grateful", "author", "-lsb-", "Fisher", "et", "al.", "2011", "-rsb-", "-lsb-", "Xu", "et", "al.", "2013", "-rsb-", "provide", "dataset", "work", "support", "part", "nsfc", "-lrb-", "61202333", "61379090", "61272327", "-rrb-", "NSERC", "Canada", "-lrb-", "611370", "-rrb-", "National", "863", "program", "China", "-lrb-", "2012aa011802", "-rrb-", "Shenzhen", "Innovation", "Program", "-lrb-", "CXB201104220029A", "kqcx20120807104901791", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014", "35:12", "K.", "Xu", "et", "al.", "jsgg20130624154940238", "jcyj20120617114842361", "-rrb-", "CPSF", "China", "-lrb-", "2012m520392", "-rrb-", "Israel", "Science", "Foundation", "reference", "iberman", "Y.", "1994", "context", "similarity", "measure", "machine", "Learning", "784", "49", "63", "heng", "m.-m.", "itra", "N.", "J.", "UANG", "X.", "s.-m", "2014", "SalientShape", "group", "saliency", "image", "collection", "Visual", "Computer", "30", "443", "453", "oersch", "C.", "ingh", "S.", "UPTA", "a.", "ivic", "J.", "FROS", "A.", "A.", "2012", "what", "make", "paris", "look", "like", "Paris", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "31", "101:1", "ISHER", "M.", "ANRAHAN", "P.", "2010", "context-based", "search", "3d", "model", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "Asia", "-rrb-", "29", "182:1", "10", "ISHER", "M.", "AVVA", "M.", "ANRAHAN", "P.", "2011", "characterize", "structural", "relationship", "scene", "use", "graph", "kernel", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "30", "34:1", "11", "ISHER", "M.", "ITCHIE", "D.", "AVVA", "M.", "unkhouser", "T.", "ANRAHAN", "P.", "2012", "example-based", "synthesis", "3d", "object", "arrangement", "ACM", "Trans", "graph", "31", "135:1", "11", "J.", "HENG", "H.", "D.", "X.", "2007", "frequent", "pattern", "mining", "current", "status", "future", "direction", "datum", "mining", "knowledge", "Discovery", "15", "55", "86", "uang", "Q.", "hang", "G.", "ao", "L.", "S.", "USTCHER", "a.", "uiba", "l.", "2012", "optimization", "approach", "extract", "encode", "consistent", "map", "shape", "collection", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "Asia", "-rrb-", "31", "167:1", "11", "uang", "Q.", "H.", "UIBAS", "L.", "2013", "fine-grained", "semisupervised", "labeling", "large", "shape", "collection", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "Asia", "-rrb-", "32", "190:1", "10", "uang", "s.-s.", "hamir", "a.", "hen", "c.-h.", "hang", "H.", "hef", "fer", "a.", "s.-m.", "ohen", "D.", "2013", "qualitative", "organization", "collection", "shape", "via", "quartet", "analysis", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "32", "71:1", "10", "AIN", "a.", "HORM", "AHLEN", "T.", "ITSCHEL", "T.", "EIDEL", "h.-p", "2012", "explore", "shape", "variation", "3d-model", "decomposition", "part-based", "recombination", "Computer", "Graphics", "Forum", "-lrb-", "special", "issue", "eurographic", "-rrb-", "31", "631", "640", "EH", "G.", "IDOM", "J.", "2002", "SimRank", "measure", "structuralcontext", "similarity", "Proc", "ACM", "SIGKDD", "538", "543", "UNEJA", "M.", "EDALDI", "A.", "AWAHAR", "C.", "V.", "isserman", "a.", "2013", "block", "shout", "distinctive", "part", "scene", "classification", "Proc", "IEEE", "Conf", "Comp", "Vis", "Pat", "Rec.", "923", "930", "IM", "V.", "G.", "W.", "ITRA", "N.", "ERDI", "S.", "unkhouser", "t.", "2012", "explore", "collection", "3d", "model", "use", "fuzzy", "correspondence", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "31", "54:1", "11", "vsjanikov", "m.", "W.", "UIBAS", "L.", "itra", "N.", "J.", "2011", "exploration", "continuous", "variability", "collection", "3d", "shape", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "30", "33:1", "10", "UATTONI", "a.", "orralba", "a.", "2009", "recognize", "indoor", "scene", "Proc", "IEEE", "Conf", "Comp", "Vis", "Pat", "Rec.", "413", "420", "asiwasium", "N.", "ASCONCELOS", "N.", "2008", "scene", "classification", "low-dimensional", "semantic", "space", "weak", "supervision", "Proc", "IEEE", "Conf", "Comp", "Vis", "Pat", "Rec.", "iesen", "K.", "IANG", "X.", "UNKE", "H.", "2010", "exact", "inexact", "graph", "matching", "methodology", "application", "manage", "mining", "Graph", "Data", "40", "217", "247", "osch", "E.", "1975", "cognitive", "reference", "point", "cognitive", "psychology", "532", "547", "hapira", "L.", "HALOM", "S.", "hamir", "a.", "ohen", "D.", "hang", "H.", "2009", "contextual", "part", "analogy", "3d", "object", "int", "J.", "Comp", "Vis", "89", "2-3", "309", "326", "hilane", "P.", "unkhouser", "t.", "2007", "distinctive", "region", "3d", "surface", "ACM", "Trans", "graph", "26", "7:1", "15", "ingh", "S.", "UPTA", "a.", "fro", "a.", "2012", "unsupervised", "discovery", "mid-level", "discriminative", "patch", "Proc", "Euro", "Conf", "Comp", "Vis.", "73", "86", "suda", "K.", "UDO", "T.", "2006", "cluster", "graph", "weighted", "substructure", "mining", "Proc", "Intl", "Conf", "Machine", "Learning", "-lrb-", "icml", "-rrb-", "953", "960", "versky", "a.", "1977", "feature", "similarity", "psychological", "review", "84", "327", "352", "VAN", "AICK", "O.", "K.", "HANG", "H.", "ang", "Y.", "UN", "S.", "hamir", "a.", "ohen", "D.", "2013", "co-hierarchical", "analysis", "shape", "structure", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "32", "69:1", "10", "idal", "R.", "2011", "subspace", "clustering", "IEEE", "Signal", "Processing", "Magazine", "28", "52", "68", "ang", "S.", "uan", "X.", "ao", "T.", "S.", "hen", "J.", "2011", "efficient", "subspace", "segmentation", "via", "quadratic", "programming", "AAAI", "519", "524", "ang", "Y.", "K.", "J.", "HANG", "H.", "hamir", "a.", "iu", "L.", "HENG", "Z.", "iong", "Y.", "2011", "symmetry", "hierarchy", "man-made", "object", "Computer", "Graphics", "Forum", "-lrb-", "special", "issue", "eurographic", "-rrb-", "30", "287", "296", "ITTGENSTEIN", "L.", "1953", "philosophical", "investigation", "New", "York", "Macmillan", "K.", "HANG", "H.", "OHEN", "D.", "hen", "B.", "2012", "fit", "diverse", "set", "evolution", "inspiring", "3d", "shape", "gallery", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "31", "57:1", "10", "K.", "HEN", "K.", "H.", "UN", "W.-L.", "s.-m", "2013", "sketch2scene", "sketch-based", "co-retrieval", "co-placement", "3d", "model", "ACM", "transaction", "Graphics", "32", "123:1", "10", "X.", "J.", "2002", "gSpan", "graph-based", "substructure", "pattern", "mining", "Proc", "int", "Conf", "Data", "Mining", "721", "724", "elnik", "anor", "L.", "erona", "P.", "2004", "self-tuning", "spectral", "clustering", "Proc", "advance", "Neural", "Information", "Processing", "Systems", "-lrb-", "NIPS", "-rrb-", "vol", "17", "1601", "1608", "hao", "X.", "ang", "H.", "OMURA", "T.", "2014", "indexing", "3d", "scene", "use", "interaction", "bisector", "surface", "ACM", "Trans", "Graph", "appear", "heng", "Y.", "OHEN", "D.", "itra", "N.", "J.", "2013", "Smart", "variation", "functional", "substructure", "part", "compatibility", "Computer", "Graphics", "Forum", "-lrb-", "special", "issue", "eurographic", "-rrb-", "32", "195", "204", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "35", "publication", "date", "July", "2014" ],
  "content" : "\n  \n    24c70539c86adcbaf4aa3fbfbf1996264c3c405bc67a28638b5a214e75811a2d\n    p93\n    10.1145/2601097.2601109\n    Name identification was not possible. \n  \n  \n    \n      \n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n      \n      Kai Xu 1,3 Rui Ma 2 Hao Zhang 2 Chenyang Zhu 3 Ariel Shamir 4 Daniel Cohen-Or 5 Hui Huang 1 1 Shenzhen VisuCA Key Lab / SIAT 2 Simon Fraser University 3 HPCL, National University of Defense Technology 4 The Interdisciplinary Center 5 Tel Aviv University\n      We introduce focal points for characterizing, comparing, and organizing collections of complex and heterogeneous data and apply the concepts and algorithms developed to collections of 3D indoor scenes. We represent each scene by a graph of its constituent objects and define focal points as representative substructures in a scene collection. To organize a heterogeneous scene collection, we cluster the scenes based on a set of extracted focal points: scenes in a cluster are closely connected when viewed from the perspective of the representative focal points of that cluster. The key concept of representativity requires that the focal points occur frequently in the cluster and that they result in a compact cluster. Hence, the problem of focal point extraction is intermixed with the problem of clustering groups of scenes based on their representative focal points. We present a co-analysis algorithm which interleaves frequent pattern mining and subspace clustering to extract a set of contextual focal points which guide the clustering of the scene collection. We demonstrate advantages of focal-centric scene comparison and organization over existing approaches, particularly in dealing with hybrid scenes, scenes consisting of elements which suggest membership in different semantic categories. CR Categories: I.3.5 [Computing Methodologies]: Computer Graphics?Computational Geometry and Object Modeling; Keywords: 3D indoor scenes, contextual focal point, heterogeneous collection, scene organization, retrieval, exploration\n      Links:\n      \n        \n      \n      DL PDF W\n      \n        \n        \n      \n      EB\n      \n        \n      \n      V IDEO\n      \n        \n      \n      C ODE\n    \n    \n      \n        ?I can think of no better expression to characterize these similarities than ?family resemblances?; for the various resemblances between members of a family: build, features, colour of eyes, gait, temperament, etc. etc. overlap and criss-cross in the same way.?\n        ? Ludwig Wittgenstein [1953]\n      \n      \n        1 Introduction\n      \n      Recent works on organizing and exploring 3D visual data have mostly been devoted to object collections [Ovsjanikov et al. 2011; Jain et al. 2012; Kim et al. 2012; van Kaick et al. 2013; Huang et al. 2013b]. In this paper, we are interested in analyzing and organizing visual data at a larger scope, namely, 3D indoor scenes. Even a moderately complex indoor scene would contain tens to hundreds of objects. Compared to the individual objects therein, a scene is  more complex with looser structural and spatial relations among its components and a more diverse mixture of functional substructures. The latter point is attested by hybrid scenes which contain elements reminiscent of different semantic categories. For example, the middle scene in Figure 1 is partly a bedroom and partly a living room. The greater intra-class variabilities and richer characteristics in scene data motivate our work to go beyond providing only a holistic and singular view of a scene or a scene collection. We introduce the use of focal points for characterizing, comparing, and organizing collections of complex data and apply the concepts and algorithms developed to 3D indoor scenes. In particular, we are interested in organizing scenes in a heterogeneous collection, i.e., scenes belonging to multiple semantic categories. Analyzing complex and heterogeneous data is difficult without references to certain points of attention or focus, i.e., the focal points. For example, comparing New York City to Paris as a whole will unlikely yield a useful answer. The comparison is a lot more meaningful if it is focused on particular aspects of the cities, e.g., architectural style or fashion trends. One of the natural consequences of the focal point driven data view is that scene comparison may yield different similarity distances depending on the focal points; see Figure 1 for an illustration, as well as the accompanying video. We represent an indoor scene by a graph of its constituent objects.\n      ACM Reference Format Xu, K., Ma, R., Zhang, H., Zhu, C., Shamir, A., Cohen-Or, D., Huang, H. 2014. Organizing Heterogenous Scene Collection through Contextural Focal Points. ACM Trans. Graph. 33, 4, Article 35 (July 2014), 12 pages. DOI = 10.1145/2601097.2601109 http://doi.acm.org/10.1145/2601097.2601109. Copyright Notice Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . Copyright ? ACM 0730-0301/14/07-ART35 $15.00. DOI: http://doi.acm.org/10.1145/2601097.2601109\n      \n        \n        Figure 1: We analyze and organize 3D indoor scenes in a heterogeneous collection from the perspective of focal points (sub-scenes in color). Scene comparisons may yield different similarity distances (left) depending on the focal points.\n      \n      \n        \n        Figure 2: Focal points (marked red in the scenes) are contextual and depend on scene composition in a collection. With more bedrooms (a) or more living rooms (b), different focals were extracted and hybrid scenes are pulled towards one of the clusters.\n      \n      ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n      35:2\n      ?\n      K. Xu et al.\n      \n        \n        Figure 3:\n      \n      Focal-driven scene clustering produces overlapping clusters. An exploratory path, (a) to (e), through an overlap, which often contains hybrid scenes (c) possessing multiple focals, can smoothly transition between the scene clusters. These scene clusters often characterize meaningful scene categories. In this example, the transition is from bedroom scenes to offices.\n      A focal point, or focal, for short, is a substructure in a scene and corresponds to a subgraph. However, we are not interested in all sub-scenes. A key premise of our work is that meaningful focals should be determined contextually, in a set ( Figure 2 ), and through a co-analysis. To illustrate, there are probably too many notable aspects about Paris. When putting London and Paris together, one?s focuses narrow down to, e.g., European capitals. If we throw New York and Milan into the mix, then most people are first reminded that the four cities are the fashion capitals of the world. In this work, we are interested in extracting contextual focal points that are representative in a given scene collection. For a focal to be representative, it must occur sufficiently frequently. However, frequency analysis alone is insufficient. For example, chairs are likely to be found in almost all scenes, but they can hardly be regarded as representative of any meaningful scene groups, e.g., bedrooms or living rooms. We stipulate that representativity is also tied to a notion of coherence or compactness of the group of scenes the focal point is to represent or characterize. Therefore, frequency analysis for focal extraction is intermixed with clustering, which computes compact groups of scenes, where the scenes in each cluster are closely connected when viewed from the perspective of the representative focals of the cluster. Once again, the representative focals occur frequently in the cluster and they must also induce a compact cluster. To solve the two coupled problems simultaneously, we develop a co-analysis algorithm which interleaves frequent pattern mining [Han et al. 2007] and subspace clustering [Vidal 2011]. Focal points play a key role in our organization of a heterogeneous scene collection. First, we define compactness of a cluster based on a focal-centric scene-to-scene similarity, which builds on the rooted walk graph kernels of Fisher et al. [2011] and assigns higher weights to walks which originate from the representative focals of that cluster. Secondly, the scene organization is given by the clustering of scenes based on the representative focals extracted. Some scenes may contain multiple focals, thus belong to multiple clusters. Such scenes, typically of a hybrid nature, provide linkages or gateways between scene clusters, allowing an exploration of the scene organization to naturally transition between meaningful scene categories, as illustrated in Figure 3 . Our main contribution is a focal-driven analysis and organization of heterogeneous data collections. While we only consider 3D indoor scenes in this paper and we are not aware of previous works on coanalysis and organization of heterogeneous scene collections, the analysis is general and not confined to scene data. Important characteristics of our work which set it apart from previous approaches to organizing data collections include: ? Data are not compared holistically without discrimination. We develop a focal-centric scene descriptor for scene com- parison, which supports scene analysis in perspective. ? Similarity distance between two scenes may be non-unique, i.e., it is based on the focals designated for comparison. ? Multiple views on scene data depend on focal points, leading to overlapping clustering of a scene collection, rather than a partition. The resulting organization is particularly suited for retrieving and exploring complex and hybrid scenes. We show advantages of focal-centric scene comparison and organization over existing approaches, particularly in dealing with hybrid scenes. We also demonstrate new capabilities offered by the new data organization for scene retrieval and exploration.\n      \n        2 Related work\n        Background. At a conceptual level, our work can be seen as a realization of the notion of ?family resemblances? from the seminal work of Wittgenstein [1953]. A scene collection forms the ?family?, and the extracted focals represent the resemblances which ?overlap and criss-cross? among the scenes. Works from cognitive psychology, in particular those by Rosch [1975], provided evidences that perceptual and semantic categories are naturally formed in terms of focal points or prototypes (see account in [Tversky 1977]), though the so-called ?cognitive reference points? in her work referred to whole representatives of a category instead of featured substructures. The role of context in measuring data similarity has long been studied in various fields, e.g., [Biberman 1994; Jeh and Widom 2002]. Our work presents an algorithm for identifying conceptual focals which serve as reference points for comparing scenes in a heterogeneous collection.  Scene analysis. As the most familiar environments to humans, indoor scenes are ubiquitous in graphics applications such as virtual reality, gaming, and design. Much research in vision and graphics has been devoted to recognizing, classifying, and retrieving indoor scenes, e.g., [Rasiwasia and Vasconcelos 2008; Quattoni and Torralba 2009; Fisher et al. 2011; Juneja et al. 2013; Xu et al. 2013; Zhao et al. 2014], among others. Our work recognizes the difficulty in comparing complex scenes globally, e.g., via the classic graph kernels [Fisher et al. 2011]. We propose extracting and utilizing focal substructures for scene analysis. Of relevance are works which extract distinctive regions [Shilane and Funkhouser 2007; Juneja et al. 2013] that are representative of a semantic category. The focals we extract are not meant for scene recognition but organization; one focal may be shared by scenes from different categories. Object collections and co-analysis. There have been a growing body of work on unsupervised co-analysis [Xu et al. 2012; Huang et al. 2012; van Kaick et al. 2013; Huang et al. 2013a; Zheng et al. 2013] and organization of 3D object collections [Ovsjanikov et al. 2011; Jain et al. 2012; Kim et al. 2012; Huang et al. 2013b]. Similar works exist on image collections, e.g., for image co-salience detection [Cheng et al. 2014]. In most cases, co-analysis operates on objects belonging to the same semantic category. An exception is the recent work of Huang et al. [2013b] which performs qualitative analysis on heterogeneous object collections. However, their object comparison employs global shape descriptors while still resulting in unique qualitative distances, in terms of number of ?hops? in a tree representation, between objects. Another recent work, the co-hierarchical analysis of van Kaick et al. [2013], also employs a clustering approach and the clustering partitions a set of shapes into different modes of structural variation. While hierarchical models offer the flexibility to account for structural variations, they still provide only a single view on each shape. Our representation allows multiple views of a scene model, each of which may be seen as from the perspective of a particular focal point. Moreover, our analysis produces overlapping clusters which characterize the underlying data with larger granularity. Contextual analysis. Part-in-whole or object-in-scene types of retrievals have been studied in semantic analysis of 3D objects or indoor scenes. Shapira et al. [2009] define the context for a shape part within an extracted part hierarchy. The series of work from Fisher et al. rely on spatial and semantic relations among the scene objects for context-based object search [Fisher and Hanrahan 2010; Fisher et al. 2011] or object replacement for scene synthesis [Fisher et al. 2012]. In all of these works, substructures in a scene provide the contexts for characterizing individual objects therein. We treat the substructures as explicit scene features, i.e., potential focals, and perform contextual analysis in a larger scope. One possible way to find salient substructures in a scene collection is to extract object groups based on co-occurrences of object categories, like in the work of Xu et al. [2013]. In contrast, we group scene objects, rather than object categories, to form focals. Furthermore, the grouping in Xu et al. [2013] is based on frequency analysis only, while we perform both frequent pattern mining and subspace clustering for focal point extraction. Singh et al. [2012] detect mid-level discriminative patches from a set of unlabeled images by alternating between clustering and training discriminative classifiers. A similar idea is then applied to extract, from a large repository of geo-tagged imagery, visual features which are both frequently occurring and geographically distinctive under weak supervision [Doersch et al. 2012]. Our co-analysis is unsupervised, driven by a novel cluster compactness objective for both focal selection and focal-induced clustering. Frequent pattern mining. Frequent pattern mining has been an extensively studied topic in data mining [Han et al. 2007]. The most relevant works are those designed for frequent subgraph mining, e.g., [Yan and Han 2002], which are primarily based on subgraph isomorphism testing. Directly adapting these methods to our problem setting is infeasible since the relations among objects in our input graphs are loose and possibly uncertain. We adopt inexact subgraph matching formulated by graph edit distances [Riesen et al. 2010] where the edit cost is defined based on spatial arrangements between scene objects. It is also worth noting that frequency of occurrence is not the only criterion for focal point selection. The subsequent cluster analysis further adjusts the extracted focals. Subspace clustering. Subspace clustering clusters highdimensional data into multiple subspaces, each modeled by a subset of features [Vidal 2011]. At a high level, the clustering problem we face has a similar setting as subspace clustering, where focals act as the feature subsets and characterize the subspaces that contain the clusters of scenes. Subspace analysis via spectral clustering has been one of the most effective approaches to subspace clustering [Wang et al. 2011a]. However, spectral clustering always produces a partition. In our work, we perform cluster attachment to reveal cluster overlap based on their representative focals, making the obtained clusters better reflect the complexity and heterogeneity of the data collection.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n        ?\n        35:3\n        \n          \n          Figure 4: An overview of our algorithm. The input is a heterogeneous collection of 3D indoor scenes. We represent each scene by a structural graph (a). The co-analysis algorithm is iterative, between (b) and (c). Each iteration involves an interleaving optimization consisting of focal point detection (b) and focal-induced scene clustering (c). After the set of contextual focals are obtained, the entire scene collection can be organized with the focals serving as the interlinks between scenes from various clusters (d).\n        \n      \n      \n        3 Overview\n        The input to our algorithm is a heterogeneous collection of 3D indoor scenes collected from public repositories. Such scenes typically come with semantic labels for the objects and the scenes themselves. Our analysis uses the object labels but never the scene labels. Our goal is to extract a set of contextual focals, as well as a clustering of the scenes based on these focals; see Figure 4 . For each scene, a structural graph is constructed which encodes two types of relationships between scene objects: support and proximity. Our main algorithm consists of a coupled optimization whose objective is to maximize the overall compactness of the scene clusters while ensuring that the focals represent their respective clusters effectively. A key is that each representative focal is sufficiently discriminative so that it is frequent only within the cluster it represents or characterizes. The optimization is iterative, where each iteration interleaves between cluster-guided focal point mining and focal-induced subspace clustering of the scenes; see Figure 5 . The first and initial phase of the optimization is to extract frequent substructures as focals from the input structural graphs, via subgraph mining (Section 4.1). Rather than relying on subgraph isomorphism, we perform inexact graph matching which insists on consistency of node labeling but not edge connection. The latter is to account for loose relations between corresponding objects across a large heterogeneous scene collection. The matching of such relations is based on a layout similarity measure between spatial arrangements of objects. This matching is confined by scene grouping resulting from the most recent clustering phase. Specifically, the subgraph matching is weighted so that the substructures found are frequent only within the clusters they characterize. In the second phase, based on the extracted focals, we perform subspace clustering (Section 4.2) on the scenes. The structural graphs are clustered so that each cluster is characterized by a subset of current focals. Generally, the representative focals for a cluster are not unique. The clustering step seeks to maximize the compactness of all clusters, where compactness is defined by a scene-to-scene similarity based on focal-centric graph kernels (FCGK). We define FCGK based on the work of Fisher et al. [2011] which utilizes rooted walk graph kernels. However, instead of weighting equally walks from all sources, we weigh more heavily those walks which originate from representative focals in the graphs. The maximization is based an iteratively reweighted subspace clustering scheme we develop, which gradually increases cluster compactness. Finally, once the clusters and focals are determined by the optimization, we perform cluster attachment and focal joining (Section 4.3). Some clusters share scenes containing multiple focals, each characterizing a different cluster. These clusters are naturally attached at the shared scenes. Within a cluster, multiple local substructures may occur concurrently across all or most scenes. These substructures are naturally joined to form non-local focals. Note that such non-local focals could not be detected via subgraph mining since only spatially close objects are connected in the graphs.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        35:4\n        ?\n        K. Xu et al.\n        \n          \n        \n        (a) Mining 1 (b) Clustering 1 (c) Mining 2\n        \n          Figure 5: Illustration of iterative optimization pipeline. A scene is depicted with a grey box enclosing several substructures represented by circles, squares, diamonds, triangles, etc. To initialize the interleaving optimization, we first detect a set frequent substructure shown in the middle in (a). Based on that, subspace clustering leads to incorrect clusters (marked in red) due to the trivial substructure (circle) occurring in most scenes. Then we perform cluster-guided weighted mining which eliminates the trivial substructure. Following that, a more accurate clustering result is obtained in (d) based on the new set of discriminant substructures in (c). Finally, we perform cluster attachment to reveal overlapping clusters (the red and yellow clusters in (e)), as well as focal joining to discover non-local focal points (marked in red in (f)).\n        \n      \n      \n        4 Focal-driven scene co-analysis\n        For each input scene, we construct a structural graph ( Figure 6(b) ) whose nodes are scene objects and edges encode spatial relationship, support and proximity, between objects; see Algorithm 1. Both nodes and edges are labeled, by object semantic labels and relationship types (support or proximity), respectively. We first detect all support relationship between objects by testing vertical contacts between their shape geometries. Second, we add a proximity edge from any object that is not connected by a support edge, to the object which has the strongest connection with it, where connection strength (Equation 3) is defined as a part of layout similarity. Third, we ensure that any group of symmetric objects has symmetric connections to other objects, if any. We detect all groups of mutually symmetric objects and examine for each group all outside objects connecting to that group. If more than two symmetric objects in the group have similar spatial arrangement (Equation 5) with respect to an outside object, we ensure they all connect to the outside object with edges of the same type, depending on their relationship against the outside object. To detect mutually symmetric objects, i.e., objects possessing similar geometry, we adopt the registration method described in [Wang et al. 2011b]. Finally, we detect the connected components in the current graph, and connect the components with proximity edges to make sure the entire scene is represented by a connected graph. Our co-analysis operates on these structural graphs. The main algorithm involves a coupled optimization for both focal point mining and scene clustering. The objective of the optimization is c\n        \n          1\n          max n ? (F, ?) F ,? =1\n        \n        where F = {F k } n k=1 are the set of focal points, and ? = {C } c =1\n        ...\n        \n          \n          \n        \n        (d) Clustering 2 (e) Cluster attachment (f) Focal joining\n        the set of clusters. ? denotes the compactness of cluster C based on FCGK, and n is the size of cluster C . We optimize iteratively with the iterations continuing until the overall compactness of the clusters converges, specifically, when the change of the objective function is less than 1.0 ? 10 ?6 . In the following sections, we detail our co-analysis algorithm.\n        \n          4.1 Focal extraction via graph mining\n          A substructure of a scene consists of a group of nearby objects along with their spatial arrangement; it is a subgraph. We could define focals as substructures that occur frequently across a large number of semantically related scenes, e.g., bedrooms. However, since scene labels can be unknown or ambiguous, especially for hybrid scenes, we do not use them. Instead, we couple focal detection with the identification of meaningful clusters. If a substructure occurs in a scene, we say that the scene supports that substructure. The notion of occurrence will be quickly relaxed by inexact graph matching, which is enabled by a similarity measure of spatial layout between substructures of scenes.  Layout similarity. We define a layout similarity between two substructures by examining the pair-wise spatial arrangement of oriented bounding boxes (OBBs) of the objects in the substructures. Suppose we are given two substructures represented by two subgraphs in the structural graphs of two scenes: S a ? G A and\n        \n      \n      \n        vertices support edges proximity edges for each group of symmetric objects for each outside object do symmetric connection\n        Algorithm 1: Structural Graph Construction Input : scene C = {O i } i Output: structural graph G = V, E 1 ?O i ? C, V ? V ? {v i } ; // 2 E ? E? SupportEdge(C) ; // 3 E ? E? ProximityEdge(C, E) ; // 4 U ? DetectSymGroup(C); 5 foreach U ? U do // 6 foreach v ? V ? U do // 7 if ?s, t ? U ; v, s , v, t ? E; d arr ( v, s , v, t ) < 0.1 then 8 foreach u ? U do // 9 E ? E ? { u, v };\n        10 E ? E? ConnectComponents(V, E); 11 return G;\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n        ?\n        35:5\n        \n          \n          Figure 6: The structural graph (b) of the input scene (a) encodes two types of relationship: support (red) and proximity (blue). (c) plots the layout similarity of object pairs after spectral embedding.\n        \n        S b ? G B . The layout dissimilarity between them is defined as:\n        \n          2\n          D layout (S a , S b ) = d arr ( p, q , ?(p), ?(q) ), {p,q}?S a , {?(p),?(q)}?S b\n        \n        where ?(p) ? G B is the corresponding object of p ? G A . Such correspondences can be determined during subgraph mining, as described below. d arr measures the spatial arrangement dissimilarity between two pairs of objects which is defined based on two factors. The first is the connection strength between objects p and q:\n        \n          3\n          d H (obb(p), obb(q)) ?(p, q) = , dl(p) + dl(q)\n        \n        where d H is Hausdorff distance, obb(p) the OBB of object p, and dl(p) the diagonal length of obb(p). The second factor is the angle between the upright vector and the vector between p and q:\n        \n          4\n          ?(p, q) = angle(v dir (p, q), v upright ),\n        \n        where v dir (p, q) is the vector from the larger object of the two to the smaller one and v upright the upright vector. The dissimilarity of spatial arrangement between two object pairs p, q and s, t is then defined as:\n        \n          5\n          d arr ( p, q , s, t ) = ?| ?(p, q) ? ?(s, t)| + (1 ? ?)| ?(p, q) ? ?(s, t)|.\n        \n        ? = e ?? 2 /(?? max ) 2 is normalized connection strength where ? = 0.4 and the maximum value ? max is found for all pairs of objects. ? is normalized similarly. We use ? = 0.6 in our implementation. Figure 6(c) shows a few examples of similar layouts. Frequent substructure mining. Frequent subgraph mining extracts from a set of input graphs G = {G i } n i=1 , a set of subgraphs F = {F k } k=1 d , which frequently occur (more than a given threshold value s min ) in the input graphs based on subgraph isomorphism. We define: n\n        \n          6\n          F = {F k | |S k | = x ik > s min } i=1\n        \n        where x ik = I(F k ? G i ) is an indicator function for subgraph isomorphism and S k = {G i | x ik = 1} is the supporter set of F k . Directly applying frequent subgraph mining to structural graphs is ineffective since the the proximity relationships are not necessarily consistent across different scenes, e.g., see Figure 7(a ,b). One may then resort to inexact graph matching, e.g., based on graph edit distance [Riesen et al. 2010]. However, the large search space of inexact subgraph mining makes such approaches prohibitive. We propose a two-step scheme for frequent substructure mining (Algorithm 2) which carries out inexact graph matching efficiently. We first perform frequent subgraph mining based on exact subgraph isomorphism, using gSpan [Yan and Han 2002], with a relatively low minimal support threshold (Line 1 in Algorithm 2). Then, in the second step, we employ inexact subgraph matching [Riesen et al. 2010] to match the frequent subgraphs mined in the previous step against all graphs in the set, to expand their support (Lines 2-6). Note that in both steps, the matching of graph nodes is exact and based only on node labels. To create tolerance for different proximity connection graph structure, we use error correction of the subgraphs by introducing three edit operations on graph edges: insertion and deletion of proximitytype edges, as well as substitution between two proximity edges. The edit cost of each operation is defined as the spatial arrangement dissimilarity (Equation 5) between the two pairs of objects involved. If the total edit cost ?(G i , F k ) for matching F k and G i is less than ? t = 0.1, we add G i to F k ?s supporter set. For a frequent subgraph F k , we have obtained its embedding in any of its supporter graphs during the mining step, denoted as G i (F k ) ? G i , G i ? S k . However, the embedding of F k in its supporters may have different layouts since the exact mining step is layout-oblivious, e.g., as shown in Figure 7(c ,d). We locate and remove weak (or outlier) supporters in which the embedded subgraph has significantly different layout from those in the other supporters (Lines 7-10). Specifically, given a supporter G i ? S k of F k , we compute the average dissimilarity between its corresponding embedding and those in all other supporters, and filter out this supporter if the value exceeds a threshold ? t = 0.3|S k |. Finally, we remove those subgraphs whose number of supporters falls below the minimal support threshold s min (Line 12). Cluster-guided weighted mining. Our goal is to detect representative focal points characterizing a meaningful clustering of the input scenes, and not substructures which are frequent over the entire collection. Therefore, instead of relying on the frequency criterion in Equation (6), we base our substructure mining on the current clusters and perform weighted subgraph mining [Tsuda and Kudo 2006]. For each cluster C , we define supporting weights ( i ) i=1 n\n        \n          \n          Figure 7: Scenes (a) and (b) have the same sub-scenes represented with different subgraphs. (c) and (d) have the same subgraphs while the layouts of the corresponding sub-scenes are different.\n        \n        ?(G i , F k ) = D layout (G i (F k ), G j (F k )), G j ?S k ,i=j\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        35:6\n        ?\n        K. Xu et al.\n        2: Extended Frequent Substructure Mining : structural graphs G = {G i } i , minimal support frequent substructures F = { F k , S k } k\n      \n      \n        expand support ,\n        1 2 3 4 5 6\n        Algorithm Input s min Output: F = { F k , S k } k ? MineSubgraph(G, s min ); foreach G i ? G do // foreach F k , S k ? F do ?(G i , F k ) ? ErrorCorrectMatch(F k G i ); if ?(G i , F k ) < ? t then S k ? S k ? {G i };\n        //\n      \n      \n        filter support\n        7 8 9 10\n        11 12\n        13\n        foreach F k , S k ? F do foreach G i ? S k do if ?(G i , F k ) > ? t then S k ? S k ? {G i }; if |S k | < s min then F ? F ? { F k , S k }; return F;\n        as a measure of support of G i to any substructure. A substructure is detected as frequent if its weighted sum of support, denoted by discriminant score ? k , is greater than a threshold ? t :\n        \n          7\n          n F = {F k | ? k > ? t } where ? k = i (2x ik ? 1) . i=1\n        \n        By using positive weights i , if G i belongs to C , and negative otherwise, the discriminant score favors a substructure which is frequent in cluster C and penalizes its frequency in other clusters. Therefore, the mined substructures in F are frequent mainly within cluster C . Specifically, we set i = x i /n ? 1/n, where x i = I(G i ? C ), and ? t = ?n/n . We fix ? = 0.1 in our algorithm. The final set of focal points takes the union of per-cluster discriminant substructures: F = c =1 F , where c is the number of clusters. To achieve weighted mining, we evaluate the discriminant score of the individual substructures, which are efficiently enumerated by gSpan, and identify the discriminative ones based on the current clusters. Then we perform support expanding and filtering for the extracted substructures. In the first iteration, when clustering is missing, we use unweighted frequent substructure mining.  With the focals extracted, we perform subspace clustering to group the input scenes according to the extracted focals that they ?share?, i.e., the scenes contain and support the same focal. For each scene, we build a high-dimensional feature vector for clustering. The feature is defined by the set of all extracted focals in the most current focal mining step (Section 4.1). Each entry of the feature vector is an indicator of support of the scene to the corresponding focal, forming a Bag-of-Words (BoW) feature: x i = (x ik ) k=1 d . Subspace clustering is then performed over all input data represented in the feature space, X = [x i ] i=1 n ? R d?n , to extract clusters characterized by a low-dimensional subspace. For subspace clustering, we adopt the method of Wang et al. [2011a] on subspace segmentation via quadratic programming (SSQP), a state-of-the-art spectral clustering based approach. The basic idea of SSQP is to express each datum x i as a linear combination of all other data in the dataset, x i = j=i z ij x j , while implicitly enforcing the coefficients z ij to be zero for all x j which belongs to different subspace from x i . To learn such a coefficient matrix Z ? R n?n , it solves the following constrained optimization\n        4.2 Focal-induced scene clustering\n        Algorithm 3: Iteratively Reweighted Subspace Clustering Input : structural graphs G = {G i } n i=1 , BoW features: X = [x i ] n i=1 , (x i = (x ik ) d k=1 ) weights: W = [w i ] i=1 n , (w i = (w ik ) k=1 d ) subspace clusters {C } c\n      \n      \n        update weights\n        Output: =1 1 for i = 1 to n do 2 w i ? 1; 3 repeat 4 {C } c =1 ? SubspaceClustering(G, X, W); 5 for = 1 to c do // 6 R ? RepresentativeFocalSet(C ); 7 ? ? Compactness(C , R ); 8 foreach G i ? C do 9 foreach F k ? R do 10 w ik ? n ? ? ? ? k\n        ; 11 for k = 1 to d do 12 if F k ? / c =1 R then 13 for i = 1 to n do 14 w ik ? 0;\n      \n      \n        the overall compactness does not improve;\n        c n ? =1\n        15 until 16 return {C } c =1\n        ; problem:\n        \n          8\n          min f (Z) = XZ ? X F + ? Z Z 1 Z s.t. Z > 0; diag(Z) = 0,\n        \n        where ? F is the Frobenius norm and diag(Z) the diagonal vector of matrix Z. The 1 -regularization term enforces sparsity of the solution, leading to feature selection for subspace clustering. The problem is a linear constrained quadratic programming which can be solved efficiently. The resulting coefficient matrix then forms an affinity matrix, |Z + Z T |/2, based on which spectral clustering is applied to obtain the clustering result. To automatically determine the number of clusters, we employ self-tuning spectral clustering [Zelnik-Manor and Perona 2004]. In practice, the cluster count is relatively stable throughout the iterations since the structure of the BOW feature matrix does not change significantly.  Besides the clustering result, we need to identify the representative focals which characterize the clusters. For each cluster C , we identify a set of representative focals, denoted as R . We rank the importance of all focals supported by any structural graph in the cluster based on their discriminant score ? k ; see Equation (7). The top ranked focal is selected as the representative one. We select the top focals from the list until the i-th one, when there are over p c = 80% of the structural graphs in the cluster which support these top i focals simultaneously. Our ultimate goal is to maximize the compactness of all clusters based on a scene-to-scene similarity emphasizing their representative focal points. The subspace clustering above is based on indicator features, which capture the occurrence of the focals but are not sufficiently informative to reflect the actual scene similarity. Directly incorporating focal-centric scene similarity into the subspace clustering is infeasible since the representative focals are unknown before the feature selective clustering is performed. Therefore, we propose an iteratively reweighted subspace clustering process to gradually produce more compact clusters where the compactness is measured based on the focal-centric graph kernel (FCGK).\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n        ?\n        35:7\n        \n          \n          \n        \n        F 1 F 2 F 3 F 4 F 5 F 1 F 2 F 3 F 4 F 5 F 1 F 2 F 3 F 4 F 5 F 1 F 2 F 3 F 4 F 5 G 1 G 1 G 1 G 1 G 2 G 5 G 5 G 5 G 3 G 8 G 8 G 8 G 4 G 3 G 3 G 3 G 5 G 6 G 6 G 6 G 6 G 2 G 2 G 2 G 7 G 4 G 4 G 4 G 8 G 7 G 7 G 7 (a) Initial weighted (b) Subspace clustering BOW features & characterizing sets (c) Re-weighting (d) Re-clustering\n        \n          Figure 8: An mini-experiment on reweighted subspace clustering. The weighted BoW features are shaded in grey level (dark=large; light=small). From the initial BoW features (a), subspace clustering produces three clusters (colored) along with their representative focals (marked in corresponding color). The colored numbers indicate the compactness values of clusters. F 3 is not discriminant\n        \n        as it appears across three clusters (b) so in (c) the corresponding weights are set to 0. The weights for F 4 are decreased due to the low compactness of the blue cluster. The next clustering groups G 2 into the green cluster with F 5 as the representative focal point (d).\n        Focal-centric graph kernel. Given a cluster C , its compactness is defined as the average distance between all pairs of structural graphs belonging to it, measured by the FCGK:\n        \n          9\n          1 p ? = n 2 k G (G i , G j ), G i ,G j ?C\n        \n        where k G p (?, ?) is the\n      \n      \n        weighted\n        p-th order walk graph kernel:\n        \n          10\n          p p k G (G i , G j ) = ? r,s k R (G i , G j , r, s). r?G i ,s?G j\n        \n        p k R (G i , G j , r, s) is the p-th order rooted-walk graph kernel [Fisher et al. 2011] which we briefly review below for completeness. It compares nodes r and s, in graphs G i and G j , respectively, by comparing all walks of length p whose first node is r against all walks of length p whose first node is s: p k R (G i , G j , r, s) = p?1 k n (r p , s p ) k n (r i , s i )k e (e i , f i ), (r1 ,e 1 ,...,ep?1 ,rp )?W Gi p (r) i=1 p (s1 ,f 1 ,...,fp?1 ,sp )?W Gj (s) p where W G (r) is the set of all walks of length p originated from r in graph G. The node kernel k n takes both geometry and label comparison into account, similar to [Fisher et al. 2011], except that we used a single label for each object, instead of a series of semantic tags. For edge kernel k e , we use the similarity of spatial arrangement (Equation 2), instead of a binary comparison of edge types. For the walk kernel ? to be focal-centric, we set higher weight for those rooted walks which originates from a node in a representative focal of cluster C :\n        1+ ? ? ? k if r ? G i (F k ), s ? G j (F k ) and F k ? R ? r,s = 1 otherwise\n        where ? is a scaling factor. In our algorithm, we set ? = 100 which is fairly high and emphasizes more the role of focals in scene characterization than the overall scene similarity.\n        Iteratively reweighted subspace clustering. For a structural graph G i , we weight the individual dimensions of its BoW feature\n        vector by a weight vector w i = (w ik ) k=1 d and solve a weighed subspace clustering which minimizes the error of linear approximation in Equation (8) under a weighted Frobenius norm. Specifically, we replace the first term in Equation (8) by:\n        \n          11\n          n d XZ ? X W,F 2 = w ik 2 [(XZ) ik ? X ik ] 2 . i=1 k=1\n        \n        The weights allow us to tune the importance of the individual dimensions when seeking subspaces and can be utilized to iteratively shift clustering results. For example, one can increase the weights corresponding to the dimensions spanning the subspace of a cluster obtained in the last round, to reinforce the cluster in the current clustering. In our case, we encourage the reoccurrence of the compact clusters in the next iteration by increasing the weights of the dimensions corresponding to its representative focal points, and deprecate incompact clusters by decreasing their corresponding weights. Initially, the weights in w i are set uniformly to 1. In each iteration, we perform the weighted subspace clustering and then update w i based on the compactness of the cluster to which G i belongs; see Algorithm 3. For each member of a cluster, we compute the weights of the dimensions corresponding to the representative focals of the cluster based on cluster compactness and focal point discriminant score (Line 10). If a focal is not a representative one for any cluster, we set a 0 for the corresponding dimension of the weight vector for all structural graphs (Line 11-14). The stopping criteria for this iterative process is the same as the one used during the interleaving optimization, i.e., the change of overall cluster compactness. Figure 8 demonstrates the process of reweighted subspace clustering with a mini-experiment on 8 structural graphs with 5 focals. In the experiment, after obtaining the subspace clustering along with the representative focals, the weights corresponding to focal point F 3 and F 4 are decreased, due to low discriminant score and low cluster compactness, respectively. With the updated weights, G 2 , which was originally clustered into the blue cluster due to F 4 , is now grouped into the green one characterized by F 5 . This is because F 5 plays the major role in clustering G 2 after F 4 is deprecated. After reweighting, the weighted feature vector of some structural graphs may decrease to (or close to) 0 vector (e.g., G 4 and G 7 in Figure 8 ). Since the clustering of these structural graphs is quite unpredictable, we choose to leave them out when their weight vector vanishes, to make the iterative clustering converge faster. These structural graphs are later introduced back in the beginning of the next round of interleaving optimization. Cluster attachment. Spectral clustering produces a partition of an input dataset, which does not reflect potential cluster overlapping due to scenes which exist in multiple clusters. In general, a structural graph for an input scene which support multiple focals may belong to multiple clusters that have other different representative focals. We simply attach such clusters with respect to the shared scenes, which can be easily identified, to reveal the overlap. Focal joining. As subgraph mining is performed on structural graphs whose node connections only capture local proximity, it is unable to return large-scale and non-local substructures. This issue has been observed in the recent work of Xu et al. [2013] which is based on structure group detection over the structural graphs. In our work, frequent substructure detection is coupled with subspace clustering. This enables us to combine the extracted focals to form a larger and non-local substructure, through analyzing the clusters they characterize. Suppose that F 1 and F 2 are both representative focals for some cluster C . If their supporter sets in C , denoted as S 1 and S 2 , overlap sufficiently, i.e., |S 1 ? S 2 | > 0.9 min{|S 1 |, |S 2 |}, we join them, by a union of their nodes, to form a larger substructure F 12 as a representative focal for C .\n        4.3 Cluster attachment and focal joining\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        35:8\n        ?\n        K. Xu et al.\n        \n          \n            \n              \n                \n                   Collection\n                   #f\n                   #nlf\n                   f min\n                   f avg\n                   f max\n                   %mf\n                \n              \n              \n                \n                   Stanford\n                   24\n                   4\n                   2\n                   3\n                   6\n                   50.4%\n                \n                \n                   Tsinghua\n                   34\n                   7\n                   2\n                   3\n                   5\n                   46.1%\n                \n              \n            \n          \n          Collection #f #nlf f min f avg f max %mf Stanford 24 4 2 3 6 50.4% Tsinghua 34 7 2 3 5 46.1%\n          Table 1: Statistics for focal point extraction. #f denotes the total number of focals and #nlf that of non-local ones. The minimum, average, and maximum number of objects in an extracted focal is denoted by f min , f avg , and f max . %mf is the percentage of multifocal scenes over the whole collection.\n        \n      \n      \n        5 Results\n        We present results obtained by our algorithm for focal point driven analysis of indoor scene collections. For scene retrieval, we compare our results to those obtained from state-of-the-art methods both through precision-recall curves and a preliminary user study, targeted for hybrid scenes. More extensive results and an accompanying video can be found in the supplementary material.  Datasets. The datasets we experiment on were provided by the Stanford repository [Fisher et al. 2012] and the Tsinghua repository [Xu et al. 2013]. Both datasets contain semantic tags with the objects originally collected from Google (now Trimble) 3D Warehouse. Since the tags from the two datasets are inconsistent, we run our test on each dataset separately. For each scene, we remove the walls and focus only on the interior scene objects. The Stanford collection consists of 132 scenes and 3, 461 objects, encompassing 78 object categories and five labeled scene categories. The Tsinghua dataset consists of 792 scenes and 13, 365 objects, encompassing 119 object categories and six labeled scene categories. The Tsinghua dataset contains 102 hybrid scenes which is composed of many subscenes, each representing a room. Parameters and statistics. The key parameters of our algorithm include: the minimum support s min used for frequent substructure mining in the first iteration, and the rooted paths combination weights used in computing graph kernel. All the results reported in the paper were obtained with the same parameter setting: s min = 40 for Tsinghua dataset and s min = 20 for the Stanford dataset. The parameters for graph kernel use the optimal ones available from the published work of Fisher et al. [2011]. Values for all other parameters are fixed throughout and described in Section 4. Statistics and timing. Table 1 shows some statistics from focal point extraction and scene clustering. Timing wise, it took 10.5 minutes to process the whole Tsinghua dataset (792 scenes) and 3.2 minutes for the Stanford scene collection (132 scenes). Over an iteration, compactness evaluation (including FCGK computation) takes ~60% of the time, with spectral clustering ~30%, and inexact frequent pattern mining ~5%. Note that the first two parts were both implemented in Matlab and could see significant speed-up if coded in C/C++. Timing is measured on a 4 quad-core 2.80GHz Intel Core CPU with 12GB RAM. Focal point extraction. Figure 9 shows several clusters and their representative focal points extracted from the Tsinghua collection; the complete set of results for focal extraction can be found in the supplementary material. We can observe hybrid scenes containing multiple focal points, which is fairly typical and results in cluster overlap. Also worth noting is the extraction of non-local focals, which are composed of relatively distant object groups, e.g., {TV, TV-stand, table, sofa}, etc. Table 1 gives the number of non-local focals extracted for both datasets. See also the last two rows in Figure 9 for the effect of focal joining. Iterative clustering. Figure 10 plots how the normalized compactness of the clusters change as the iterative clustering algorithm progresses. While the change is not strictly monotone, it is evident that the iteration generally improves cluster quality over time. The final cluster counts for the two sets are 5 and 9, respectively. Precision-recall on scene retrieval. Figure 11 compares our method to two other methods for scene retrieval:\n        19.5 115.0 19.0 105.0 18.5 95.0 compactness 17.5 18.0 compactness 85.0 75.0 17.0 16.5 65.0 16.0 ? i t e r a t i o n 55.0 ? i t e r a t i o n 0 4 8 12 16 20 24 28 0 3 6 9 12 15 18 21 (a) Stanford (b) Tsinghua\n        \n          Figure 10: The plots show the change of the compactness of the clusters obtained as our interleaving optimization progresses, for Stanford and Tsinghua datasets respectively. The red dots represent the switching points from outer loop (mining) to inner loop (clustering). The optimization takes 5 and 6 interleaving iterations to converge on the two datasets, respectively.\n        \n        1. GK: Graph kernels of Fisher et al. [2011] to measure similarity between whole scenes. Since we were unable to obtain the authors? code, we coded up our own implementation with two major differences to the original work. First, we use our structural graphs which only encode two types of relationships (support and proximity) and do not consider hierarchical scene graphs. Second, the computation of node and edge kernels are slightly different; see Section 4.2. For both GK and FCGK, the schemes for node and edge kernel estimation and graph kernel normalization, as well as all the parameters, are the same as the original work. 2. BOW: A baseline method where we use bag-of-words features on the focal points only as a scene-to-scene similarity. 3. FCGK (SG): On the Tsinghua dataset, we also apply our FCGK similarity on the scenes where as focals, we use the 212 structural groups detected by Xu et al. [2013].\n        When applying our method, which uses FCGK for scene similarity, we show results in three settings: 1) using the initial set of focals after only one step of frequent pattern mining; 2) using an intermediate set of focals; 3) using the final set of focals extracted.  For the Tsinghua dataset, the ground truth for evaluating scene retrieval is given by the scene labels/categories which come with the dataset. Since this dataset contains many hybrid scenes, we separate it into a subset of simple scenes and the remaining hybrid (complex) scenes and report results on each and their combination. Since the Stanford collection does not come with scene labels, we provide our own labels obtained manually, which, admittedly, could introduce an evaluation bias. A potentially more reliable method, such as voting from multiple users, could be employed. From the precision-recall curves, we see that our focal-centric similarity based on the final set of focals is the best in all four cases. Moreover, the performance gain is more prominent for hybrid scenes. These results demonstrate not only the merit of utilizing focals for scene comparison but also the merit of our focal extraction scheme, as it seems evident that retrieval performance improves as our iterative algorithm progresses. Comparison to GK. Figure 12 shows an explicit comparison between GK and FCGK on scene similarity, attesting to the effectiveness of utilizing focals. In our experiment, we also observed that the matching performance of GK tends to be negatively affected by the presence of many small/trivial objects. For example, when a scene contains a shelf supporting many small objects, GK counts rooted walks from all these objects, which would influence the similarity between more prominent objects. FCGK is more discriminative and trivial objects are less likely to have been chosen as focals. User evaluation on retrieval. For a hybrid scene, it may be difficult to assign an unambiguous category label. The ground truth used for retrieval on such scenes may be unreliable. Thus instead of relying on scene categories as ground truth, we let human users judge scene similarity based on their prior knowledge. In this second comparative study on scene retrieval, we focus exclusively on retrieval where the query is a hybrid scene. We present a user with 10 queries. For each query, the top return from the three compared methods (GK, BOW and FCGK) are presented to the user and the user is asked to choose which of the three is most similar to the query. We repeat this for a total of 102 queries for the hybrid scenes in the Tsinghua dataset. Against GK, we obtain a winning percentage of 70.2% and against BOW, we obtain 73.9%. The results are statistically significant (with p = 0.01). In the studies, each scene has been rendered in three random bird?s eye views and the images were presented randomly. Among the 43 participants, 80% are computer science researchers, with ages 20 to 50. The rest are frequent computer users with varying backgrounds.\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n        ?\n        35:9\n        \n          \n          Figure 9: Several clusters and their representative focals (highlighted in colors) extracted from the Tsinghua scene collection. Top row shows an intermediate result for two clusters and the middle row shows the final result for the relevant clusters. Bottom rows show the final result for other clusters. Note multi-focal hybrid scenes, cluster overlap (marked with the red dashed box), and non-local focal points, such as the combos of {TV, TV-stand, table, sofa} and {bed, nightstands, dresser, mirror} in the last two rows.\n        \n        1 1 FCGK (Final) FCGK (Final) 0.9 FCGK (Inter) 0.9 FCGK (Inter) 0.8 FCGK (Init) 0.8 FCGK (Init) GK FCGK (SG) 0.7 BOW 0.7 GK BOW 0.6 0.6 precision 0.5 0.4 precision 0.4 0.5 0.3 0.3 0.2 0.2 0.1 0.1 0 0 0 0.2 0.4 recall 0.6 0.8 1 0 0.2 0.4 recall 0.6 0.8 1 (a) Stanford (b) Tsinghua (simple) 1 1 FCGK (Final) FCGK (Final) 0.9 FCGK (Inter) 0.9 FCGK (Inter) FCGK (Init) FCGK (Init) 0.8 0.8 FCGK (SG) FCGK (SG) 0.7 GK 0.7 GK BOW BOW 0.6 0.6 precision 0.4 0.5 precision 0.5 0.4 0.3 0.3 0.2 0.2 0.1 0.1 0 0 0 0.2 0.4 recall 0.6 0.8 1 0 0.2 0.4 recall 0.6 0.8 1 (c) Tsinghua (hybrid) (d) Tsinghua (all)\n        \n          Figure 11: Precision-recall curves for scene retrieval. (a) Stanford scene collection. (b) Tsinghua collection, simple scenes. (c) Tsinghua, hybrid scenes. (d) Tsinghua, all scenes.\n        \n        80 FCGK 70.9 GK 60 40\n        \n          \n        \n        20 5.9 0\n        \n          \n          \n        \n        160 142.8 FCGK 120 GK 80 40 13 0\n        \n          \n          Figure 12: Comparing GK and FCGK on scene similarity. Top row: two scenes in the same category, but GK returns a large distance between them due to the dissimilar surrounding objects. Bottom row: two scenes belonging to different categories while GK returns a small distance also attributing to surrounding objects, e.g., the nearby bookshelves. In contrast, with a focal-centric view, our method gives more meaningful distances on the two pairs.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        35:10\n        ?\n        K. Xu et al.\n        \n          \n          Figure 13: Comprehensive retrieval takes a query scene and returns scenes grouped by well-matched focals with the query. In each group, the returns are ranked by FCGK based on the corresponding focal. In this example, the query has two focals (colored yellow and red) matched from the scene organization. Three ranked lists of returns corresponding to the two focals (first two rows) and to the joined focal (bottom row) are shown.\n        \n      \n      \n        6 Applications\n        Our scene organization allows classical scene queries and is thus suitable for any application which utilized retrieval results as before, e.g., [Fisher et al. 2012; Xu et al. 2013]. In this section, we discuss several new capabilities afforded by our focal-based data organization for scene retrieval and exploration.  Comprehensive retrieval. In classical retrieval, a single query would fetch a single ranked list of data items. With our focal-centric similarity and pre-computed set of focals, our scene organization supports such classical queries. It also supports part-in-whole type of queries, where the user specifies a region of interest (ROI) in the query scene. This is demonstrated with the exploration tool which we describe below. The interesting new feature enabled by our scene organization is what we call comprehensive retrieval. Here the query does not have a specified focal. However, the available focals in the organization are matched with the query scene. Instead of returning a single ranked list of scenes, the comprehensive retrieval returns multiple ranked lists, each of which corresponds to a well-matched focal. Figure 13 shows such a result. Note that the vertical order in the table has no clear meaning since the three (horizontal) lists are retrieved based on different sets of focals. If putting all the results together, however, one can expect that those retrieved with multiple focals should be ranked higher since they have more focal substructures receiving higher weights; refer to Equation (9). For focal-to-scene matching, we utilize the efficient subgraph matching approach described in [Riesen et al. 2010], by which the focal subgraphs are pre-compiled into a hierarchical representation to accelerate the online matching. The average query time is 960ms for the Tsinghua collection and 140ms for the Stanford set. Multi-query retrieval. In applications such as example-based scene synthesis [Fisher et al. 2012], one may form queries consisting of multiple semantically related scenes and wish to retrieve more scenes ?of the same?. Such multi-query retrievals are wellsupported by our scene organization. Indeed, since the query scenes are related, they likely share meaningful substructures, making them suitable for focal-based scene comparisons. Given a query set, we extract frequent substructures from the set and match them against the extracted focals in the scene organization. We then retrieve scenes from the organization using FCGK based on the matched focals. Figure 14 shows one such result with a query set of four hybrid scenes. For comparison, we also show a ranked list of returns based on GK similarity measured against any scene in the query set. As one would expect, the focal-based retrieval produces more discernable results, and more useful results. If the user selected four query scenes all containing a bednightstand combo and a desk-chair combo, then it is likely that he/she was seeking scenes that contain similar substructures. Scene exploration. We develop an exploration tool, based on the extracted focals, which enables a user to browse through a heterogeneous scene collection. Focal points are the primary means for search and navigation. Figure 15 shows the GUI of our tool. The user can select a few focals from the focal point list panel (bottom), and our tool automatically selects a set of scenes sharing similar focals and lists them in the scene list panel (right). The user can browse the list and view the scenes in the main viewer (middle). At any time, the user can click on a selected focal to view its embedding in the current scene. In terms of navigation, as shown in Figure 3, the user can traverse from one scene to another, and one scene cluster to another, through focals which interlink them. The accompanying video contains full sessions of interactive exploration. In addition, we provide an interface for the user to paint a region of interest (ROI) and search for scenes which contain sub-scenes that are similar to the surroundings of the ROI. When the user selects an ROI in a scene, our system first finds a focal point in the scene which overlaps most with the ROI and adds the focal to the selected list. It then retrieves a new list of scenes based on the updated list of selected focals. Exploring the database with focal points around an ROI, instead of with only the ROI, can provide more relevant results. For example, if the user selects only a chair model as ROI, naive partial matching would simply return all scenes containing a chair. In contrast, our tool searches for scenes sharing the same fo- cal around the chair, returning results that are more context-aware. Note that the rooted walk graph kernels of Fisher et al. [2011] could also support contextual part-in-whole queries. However, performing subgraph search is likely too time consuming for online retrieval. With pre-analysis resulting a focal-based scene organization, our tool can support efficient context-aware partial matching over a large heterogeneous scene collection.\n        \n          \n          Figure 14: Multi-query retrieval takes a query set (left) and returns a ranked list of scenes (bottom-right) via focal-based scene comparison. FCGK similarity is used and measured based on focals (colored red and yellow) that well-match frequent substructures in the query set. Returns based on global scene similarity computed by GK are also shown (top-right). To not introduce a bias by coloring of the focals, in the GK returns, we also color any object whose tag matches that of an object in one of the focals.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        Organizing Heterogeneous Scene Collections through Contextual Focal Points\n        ?\n        35:11\n        \n          \n          Figure 15: GUI for our exploration tool is composed of four parts: the focal point list panel (red box), the selected focal list panel (green), the scene list panel (blue), and the main scene viewer. The user can pick a selected focal to view its embedding in the current scene. She can also select a region of interest (ROI) in the viewer to explore more scenes via the focals around the ROI.\n        \n      \n      \n        7 Discussion and future work\n        At the core of the data organization problem is the mechanism for comparing data. Traditional approaches rely on holistic data views and unique distances defined between data items for grouping or clustering. However, when the data become complex and multifaceted, a fixed and global view on data similarity can hardly express the rich characteristics in the data. We advocate the use of focal points for comparing and organizing complex and heterogeneous data and use 3D indoor scenes as a prototype to demonstrate its feasibility and performance gains, e.g., in retrieval. The new approach seems particularly apt at dealing with complex and hybrid scenes. Perhaps its most compelling feature is the ability to process large and heterogeneous collections of scenes and to organize them into an interlinked and well-connected cluster formation, which facilitates scene exploration.  FCGK vs. GK. While our retrieval experiment showed superior performance of FCGK over GK, one should realize that a direct comparison between the two is not exactly fair. GK is a standalone graph similarity measure, where only two graphs to compare are needed. FCGK-based comparison comes with a higher cost as it requires a set of graphs and a co-analysis for focal extraction. That said, if a scene collection is available, we would still suggest using FCGK for its better performance and modest processing costs. Comparison to structural groups. In our work, a focal point consists of a group of scene objects and it is derived via structural scene analysis. By name alone, this suggests similarity to the structural groups computed by Xu et al. [2013]. There are however major differences. First, their structural groups are category groups, while our focals are object groups. More importantly, their group extraction involves only frequent pattern mining through local proximity based search. The latter implies that their method is unlikely to return non-local structural groups. This is in part evidenced by the much higher number of groups (212) they obtain vs. the 34 focals we obtain, on the same scene collection (Tsinghua, 792 scenes). The retrieval results in Figure 11 seem to suggest that nonlocal focals extracted via mining and clustering provide the better perspectives for meaningful scene comparison. Non-unique distance. The retrieval experiment using FCGK seems to suggest that our method assigns a unique distance between any two scenes. This is true once the set of focals is fixed and FCGK is to be computed based on those focals and the clustering result. However, the non-uniqueness of focal-centric distances is well utilized in other settings including comprehensive retrieval, multi-query retrieval, and ROI-driven scene exploration, where the relevant focals in the query scenes are all determined on-demand. Limitations. Our current algorithm depends on semantic labeling of scene objects. It remains to be seen whether it works effectively with noisy or incomplete labels, based on pure geometry analysis. For example, it is interesting to test our method on inputs with various levels of label noise. However, it would be hard to quantitatively evaluate the robustness against noisy labels since it may be difficult to reproduce realistic labeling noise introduced by humans. Nevertheless, the two datasets we used do contain some incorrect labels, which did not seem to affect the overall performance. There are perhaps more than a desirable number of parameters in the algorithm, whose values were determined experimentally. From a technical stand point, improvements are possible in various components of the algorithm. For example, our layout similarity operates on OBBs only, which may be unsuitable for objects with complex geometry and spatial arrangements. The structural graphs model the scenes only as flat arrangements of objects. Hierarchical organization may be potentially advantageous. Future work. One obvious pursuit is to apply our focal-driven approach to other datasets, e.g., large and heterogeneous collections of annotated images. An interesting technical question is whether our scene organization can be updated with an additional set of scenes without recomputing everything. Also, rather than replacing one object at a time for scene synthesis like in previous works, our scene organization and focal-based partial scene retrieval, may allow for substituting sub-scenes for the synthesis task. We conclude the paper with a question: ?what is the best way to compare complex scenes?? This work, along with others before it, assume that comparing attributed graphs defined by semantic tags and object arrangements is the best way. However, we observe that visually, many retrieval results do not look so compelling even with the best method to date. If one takes away the colorings in Figure 14, then the contrast between GK and FCGK would not be as salient. Hence, the focal-centric view we advocate offers a perspective worth considering. The general question, also one that is attributed to complex data beyond those of indoor scenes, should perhaps be answered with user and application intent in mind.\n      \n      \n        Acknowledgments\n        We thank all the reviewers for their comments and feedback. We are grateful to the authors of [Fisher et al. 2011] and [Xu et al. 2013] for providing their datasets. This work was supported in part by NSFC (61202333, 61379090 and 61272327), NSERC Canada (611370), National 863 Program of China (2012AA011802), Shenzhen Innovation Program (CXB201104220029A, KQCX20120807104901791,\n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n        35:12\n        ?\n        K. Xu et al.\n        JSGG20130624154940238, JCYJ20120617114842361), CPSF China (2012M520392), and Israel Science Foundation.\n      \n      \n        References\n        \n          B IBERMAN , Y. 1994. A context similarity measure. Machine Learning 784, 49?63.\n          C HENG , M.-M., M ITRA , N. J., H UANG , X., AND H U , S.-M. 2014. SalientShape: group saliency in image collections. The Visual Computer 30, 4, 443?453.\n          D OERSCH , C., S INGH , S., G UPTA , A., S IVIC , J., AND E FROS , A. A. 2012. What makes paris look like Paris? ACM Trans. on Graph (Proc. of SIGGRAPH) 31, 4, 101:1?9.\n          F ISHER , M., AND H ANRAHAN , P. 2010. Context-based search for 3D models. ACM Trans. on Graph (Proc. of SIGGRAPH Asia) 29, 6, 182:1?10.\n          F ISHER , M., S AVVA , M., AND H ANRAHAN , P. 2011. Characterizing structural relationships in scenes using graph kernels. ACM Trans. on Graph (Proc. of SIGGRAPH) 30, 4, 34:1?11.\n          F ISHER , M., R ITCHIE , D., S AVVA , M., F UNKHOUSER , T., AND H ANRAHAN , P. 2012. Example-based synthesis of 3D object arrangements. ACM Trans. on Graph 31, 6, 135:1?11.\n          H AN , J., C HENG , H., X IN , D., AND Y AN , X. 2007. Frequent pattern mining: current status and future directions. Data Mining and Knowledge Discovery 15, 1, 55?86.\n          H UANG , Q., Z HANG , G., G AO , L., H U , S., B USTCHER , A., AND G UIBAS , L. 2012. An optimization approach for extracting and encoding consistent maps in a shape collection. ACM Trans. on Graph (Proc. of SIGGRAPH Asia) 31, 6, 167:1?11.\n          H UANG , Q., S U , H., AND G UIBAS , L. 2013. Fine-grained semisupervised labeling of large shape collections. ACM Trans. on Graph (Proc. of SIGGRAPH Asia) 32, 6, 190:1?10.\n          H UANG , S.-S., S HAMIR , A., S HEN , C.-H., Z HANG , H., S HEF FER , A., H U , S.-M., AND C OHEN -O R , D. 2013. Qualitative organization of collections of shapes via quartet analysis. ACM Trans. on Graph (Proc. of SIGGRAPH) 32, 4, 71:1?10.\n          J AIN , A., T HORM AHLEN  ? , T., R ITSCHEL , T., AND S EIDEL , H.-P. 2012. Exploring shape variations by 3D-model decomposition and part-based recombination. Computer Graphics Forum (Special Issue of Eurographics) 31, 2, 631?640.\n          J EH , G., AND W IDOM , J. 2002. SimRank: a measure of structuralcontext similarity. In Proc. of ACM SIGKDD, 538?543.\n          J UNEJA , M., V EDALDI , A., J AWAHAR , C. V., AND Z ISSERMAN , A. 2013. Blocks that shout: Distinctive parts for scene classification. In Proc. IEEE Conf. on Comp. Vis. and Pat. Rec., 923?930.\n          K IM , V. G., L I , W., M ITRA , N., D I V ERDI , S., AND F UNKHOUSER , T. 2012. Exploring collections of 3D models using fuzzy correspondences. ACM Trans. on Graph (Proc. of SIGGRAPH) 31, 54:1?11.\n          O VSJANIKOV , M., L I , W., G UIBAS , L., AND M ITRA , N. J. 2011. Exploration of continuous variability in collections of 3D shapes. ACM Trans. on Graph (Proc. of SIGGRAPH) 30, 4, 33:1?10.\n          Q UATTONI , A., AND T ORRALBA , A. 2009. Recognizing indoor scenes. In Proc. IEEE Conf. on Comp. Vis. and Pat. Rec., 413? 420.\n          R ASIWASIA , N., AND V ASCONCELOS , N. 2008. Scene classification with low-dimensional semantic spaces and weak supervision. In Proc. IEEE Conf. on Comp. Vis. and Pat. Rec., 1?6.\n          R IESEN , K., J IANG , X., AND B UNKE , H. 2010. Exact and inexact graph matching: Methodology and applications. Managing and Mining Graph Data 40, 217?247.\n          R OSCH , E. 1975. Cognitive reference points. Cognitive Psychology 7, 4, 532?547.\n          S HAPIRA , L., S HALOM , S., S HAMIR , A., C OHEN -O R , D., AND Z HANG , H. 2009. Contextual part analogies in 3D objects. Int. J. Comp. Vis. 89, 2-3, 309?326.\n          S HILANE , P., AND F UNKHOUSER , T. 2007. Distinctive regions of 3D surfaces. ACM Trans. on Graph 26, 2, 7:1?15.\n          S INGH , S., G UPTA , A., AND E FROS , A. 2012. Unsupervised discovery of mid-level discriminative patches. In Proc. Euro. Conf. on Comp. Vis., 73?86.\n          T SUDA , K., AND K UDO , T. 2006. Clustering graphs by weighted substructure mining. In Proc. Intl Conf on Machine Learning (ICML), 953?960.\n          T VERSKY , A. 1977. Features of similarity. Psychological Review 84, 4, 327?352.\n          VAN K AICK , O., X U , K., Z HANG , H., W ANG , Y., S UN , S., S HAMIR , A., AND C OHEN -O R , D. 2013. Co-hierarchical analysis of shape structures. ACM Trans. on Graph (Proc. of SIGGRAPH) 32, 4, 69:1?10.\n          V IDAL , R. 2011. Subspace clustering. IEEE Signal Processing Magazine 28, 3, 52?68.\n          W ANG , S., Y UAN , X., Y AO , T., Y AN , S., AND S HEN , J. 2011. Efficient subspace segmentation via quadratic programming. In AAAI, 519?524.\n          W ANG , Y., X U , K., L I , J., Z HANG , H., S HAMIR , A., L IU , L., C HENG , Z., AND X IONG , Y. 2011. Symmetry hierarchy of man-made objects. Computer Graphics Forum (Special Issue of Eurographics) 30, 2, 287?296.\n          W ITTGENSTEIN , L. 1953. Philosophical investigations. New York: Macmillan.\n          X U , K., Z HANG , H., C OHEN -O R , D., AND C HEN , B. 2012. Fit and diverse: Set evolution for inspiring 3D shape galleries. ACM Trans. on Graph (Proc. of SIGGRAPH) 31, 4, 57:1?10.\n          X U , K., C HEN , K., F U , H., S UN , W.-L., AND H U , S.-M. 2013. Sketch2Scene: Sketch-based co-retrieval and co-placement of 3D models. ACM Transactions on Graphics 32, 4, 123:1?10.\n          Y AN , X., AND H AN , J. 2002. gSpan: graph-based substructure pattern mining. In Proc. Int. Conf. on Data Mining, 721?724.\n          Z ELNIK -M ANOR , L., AND P ERONA , P. 2004. Self-tuning spectral clustering. In Proc. Advances in Neural Information Processing Systems (NIPS), vol. 17, 1601?1608.\n          Z HAO , X., W ANG , H., AND K OMURA , T. 2014. Indexing 3d scenes using the interaction bisector surface. ACM Trans. on Graph, to appear.\n          Z HENG , Y., C OHEN -O R , D., AND M ITRA , N. J. 2013. Smart variations: Functional substructures for part compatibility. Computer Graphics Forum (Special Issue of Eurographics) 32, 2, 195?204.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 4, Article 35, Publication Date: July 2014\n      \n    \n  ",
  "resources" : [ ]
}