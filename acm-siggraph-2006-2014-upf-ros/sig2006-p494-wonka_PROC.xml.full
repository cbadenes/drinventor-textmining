{
  "uri" : "sig2006-p494-wonka_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2006/p494-wonka_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Guided Visibility Sampling",
    "published" : "2006",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Peter-Wonka",
      "name" : "Peter",
      "surname" : "Wonka"
    }, {
      "uri" : "http://drinventor/Michael-Wimmer",
      "name" : "Michael",
      "surname" : "Wimmer"
    }, {
      "uri" : "http://drinventor/Kaichi-Zhou",
      "name" : "Kaichi",
      "surname" : "Zhou"
    }, {
      "uri" : "http://drinventor/Stefan-Maierhofer",
      "name" : "Stefan",
      "surname" : "Maierhofer"
    }, {
      "uri" : "http://drinventor/Gerd-Hesina",
      "name" : "Gerd",
      "surname" : "Hesina"
    }, {
      "uri" : "http://drinventor/Alexander-Reshetov",
      "name" : "Alexander",
      "surname" : "Reshetov"
    } ]
  },
  "bagOfWords" : [ "d91f1c62fdfea1a2d9c92bb0334e9bc54715216df99270fc855d072746536108", "mhe", "http://dx.doi.org/10.1145/1141911.1141914", "name", "identification", "possible", "guide", "visibility", "sample", "Peter", "Wonka", "Michael", "Wimmer", "Kaichi", "Zhou", "Stefan", "Maierhofer", "Gerd", "Hesina", "Arizona", "State", "University", "Vienna", "University", "Technology", "VRVis", "Research", "Center", "Figure", "visualization", "sampling", "strategy", "-lrb-", "white", "pixel", "show", "subset", "actual", "sample", "miss", "geometry", "marked", "red", "-rrb-", "leave", "urban", "input", "scene", "view", "cell", "-lrb-", "yellow", "-rrb-", "visibility", "sampling", "middle", "previous", "visibility", "sampling", "algorithm", "repeatedly", "sample", "same", "triangle", "foreground", "while", "miss", "many", "smaller", "triangle", "distant", "geometry", "right", "we", "solution", "guide", "scene", "visibility", "therefore", "quickly", "find", "most", "visible", "triangle", "while", "require", "drastically", "fewer", "sample", "than", "previous", "method", "paper", "address", "problem", "compute", "triangle", "visible", "from", "region", "space", "propose", "aggressive", "visibility", "solution", "base", "stochastic", "ray", "shooting", "can", "take", "any", "triangular", "model", "input", "we", "do", "rely", "connectivity", "information", "volumetric", "occluder", "availability", "large", "occluder", "can", "therefore", "process", "any", "give", "input", "scene", "propose", "algorithm", "practically", "memoryless", "thereby", "alleviate", "large", "memory", "consumption", "problem", "prevalent", "several", "previous", "algorithm", "strategy", "we", "algorithm", "use", "ray", "mutation", "ray", "space", "cast", "ray", "likely", "sample", "new", "triangle", "we", "algorithm", "improve", "sampling", "efficiency", "previous", "work", "over", "two", "order", "magnitude", "cr", "category", "i.", "3.7", "-lsb-", "Computer", "Graphics", "-rsb-", "three-dimensional", "graphic", "realism?visible", "line/surface", "algorithm", "Keywords", "visibility", "occlusion", "culling", "pv", "visibility", "sampling", "introduction", "visibility", "fundamental", "problem", "computer", "graphic", "visibility", "computation", "necessary", "occlusion", "culling", "shadow", "generation", "inside-outside", "classification", "image-based", "rendering", "motion", "-lcb-", "peter.wonka", "kaichi.zhou", "-rcb-", "@asu", "edu", "Tempe", "AZ", "85287-0112", "wimmer@cg.tuwien.ac.at", "1040", "Vienna", "Austria", "-lcb-", "sm", "hesina", "-rcb-", "@vrvis", "1220", "Vienna", "Austria", "alexander.reshetov@intel.com", "Santa", "Clara", "CA", "95054", "planning", "navigation", "name", "just", "few", "example", "while", "visibility", "from", "single", "viewpoint", "can", "calculate", "quite", "easily", "many", "application", "require", "potentially", "visible", "set", "-lrb-", "pv", "-rrb-", "region", "space", "which", "unfortunately", "much", "more", "complicated", "number", "excellent", "from-region", "visibility", "algorithm", "exist", "most", "they", "only", "applicable", "limited", "range", "scene", "require", "complex", "computation", "sometimes", "significant", "amount", "memory", "therefore", "sampling-based", "solution", "have", "become", "very", "popular", "practical", "application", "due", "robustness", "general", "applicability", "ease", "implementation", "paper", "we", "improve", "upon", "previous", "sampling-based", "algorithm", "significantly", "improve", "sampling", "efficiency", "i.e.", "number", "sample", "require", "detect", "certain", "set", "visible", "polygon", "motivate", "we", "design", "choice", "we", "look", "two", "key", "aspect", "any", "visibility", "algorithm", "behavior", "algorithm", "ray", "space", "datum", "structure", "use", "store", "acquire", "visibility", "information", "figure", "illustrate", "concept", "ray", "space", "2d", "give", "view", "cell", "show", "edge", "parameterize", "scene", "object", "show", "grey", "we", "can", "compute", "visibility", "consider", "all", "ray", "from", "view", "cell", "plane", "behind", "scene", "parameterize", "t.", "2d", "scene", "2d", "set", "ray", "3d", "scene", "4d", "set", "ray", "set", "ray", "sample", "densely", "enough", "we", "have", "good", "visibility", "solution", "inefficiency", "pure", "regular", "sampling", "approach", "show", "figure", "same", "surface", "sample", "over", "over", "again", "-lrb-", "note", "definition", "regular", "depend", "parameterization", "ray", "space", "-rrb-", "therefore", "would", "beneficial", "we", "could", "only", "sample", "area", "have", "be", "sample", "before", "show", "Figure", "where", "after", "initial", "orthogonal", "sampling", "only", "few", "additional", "ray", "need", "find", "all", "visible", "surface", "total", "little", "more", "than", "1d", "subspace", "2d", "ray", "space", "need", "explore", "example", "due", "spatial", "coherence", "visibility", "paper", "we", "exploit", "coherence", "start", "from", "stochastically", "sample", "point", "we", "grow", "lower-dimensional", "subspace", "ray", "space", "use", "newly", "introduce", "strategy", "adaptive", "border", "sampling", "reverse", "sampling", "which", "guide", "property", "scene", "visibility", "second", "key", "aspect", "visibility", "algorithm", "what", "datum", "structure", "use", "store", "visibility", "information", "most", "complete", "also", "complex", "way", "store", "4d", "ray", "space", "large", "scene", "entail", "prohibitive", "level", "memory", "consumption", "conservative", "algorithm", "often", "store", "shadow", "volume", "whereas", "sampling", "algorithm", "use", "volume", "3d", "space", "have", "be", "sample", "yet", "-lrb-", "so-called", "void", "volume", "Figure", "-rrb-", "datum", "structure", "still", "require", "several", "time", "memory", "take", "scene", "description", "itself", "alternatively", "boundary", "void", "volume", "-lrb-", "void", "surface", "-lsb-", "Pito", "1999", "-rsb-", "-rrb-", "can", "use", "which", "easy", "sample", "from", "one", "point", "space", "difficult", "manipulate", "paper", "we", "do", "store", "visibility", "information", "beyond", "pv", "all", "rely", "we", "new", "reverse", "sampling", "approach", "penetrate", "void", "surface", "base", "current", "sample", "only", "key", "contribution", "paper", "intelligent", "sampling", "algorithm", "drastically", "improve", "performance", "previous", "sampling", "approach", "combine", "random", "sampling", "deterministic", "exploration", "phase", "algorithm", "require", "little", "memory", "simple", "implement", "accept", "any", "triangular", "test", "scene", "input", "can", "use", "general", "purpose", "visibility", "tool", "copyright", "2006", "Association", "Computing", "Machinery", "Inc.", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "first", "page", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "Permissions", "Dept", "ACM", "Inc.", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "e-mail", "Alexander", "Reshetov", "Intel", "Corporation", "permissions@acm.org", "2006", "ACM", "0730-0301/06", "0700-0494", "5.00", "494", "Object", "Space", "Object", "Space", "Ray", "Space", "figure", "sample", "object", "ray", "space", "leave", "scene", "set", "object", "view", "cell", "show", "line", "segment", "parameterize", "we", "interested", "all", "ray", "intersect", "view", "cell", "second", "line", "segment", "parameterize", "t.", "middle", "show", "subset", "possible", "ray", "one", "ray", "highlight", "blue", "right", "depiction", "discrete", "ray", "space", "any", "ray", "middle", "figure", "correspond", "point", "ray", "space", "blue", "point", "correspond", "blue", "ray", "middle", "figure", "Object", "Space", "Object", "Space", "Ray", "Space", "figure", "leave", "scene", "sample", "orthogonally", "middle", "additional", "sample", "capture", "oblique", "surface", "right", "ray", "use", "sample", "scene", "show", "corresponding", "color", "overview", "2.1", "Problem", "Statement", "we", "consider", "visibility", "problem", "pose", "follow", "first", "input", "we", "take", "three-dimensional", "scene", "consist", "set", "triangle", "we", "do", "rely", "connectivity", "information", "volumetric", "object", "large", "polygon", "potential", "occluder", "-lrb-", "set", "triangle", "often", "call", "triangle", "soup", "-rrb-", "second", "input", "we", "consider", "subset", "ray", "space", "usually", "define", "ray", "emanate", "within", "3d", "polyhedron", "call", "view", "cell", "intersect", "bound", "box", "scene", "ray", "can", "define", "start", "point", "direction", "use", "we", "can", "define", "visibility", "function", "so", "each", "ray", "map", "triangle", "intersect", "first", "exact", "solution", "visibility", "problem", "range", "function", "-lrb-", "-rrb-", "also", "call", "exact", "visible", "set", "ev", "we", "algorithm", "aggressive", "-lrb-", "-lsb-", "Nirenstein", "et", "al.", "2002", "-rsb-", "-rrb-", "i.e.", "calculate", "potentially", "visible", "set", "pv", "ev", "we", "algorithm", "can", "use", "solve", "visibility", "problem", "different", "application", "-lrb-", "see", "Section", "5.6", "-rrb-", "usage", "scenario", "keep", "mind", "follow", "exposition", "visibility", "preprocessing", "system", "real-time", "rendering", "view", "space", "-lrb-", "set", "possible", "observer", "location", "-rrb-", "partition", "view", "cell", "preprocessing", "step", "we", "algorithm", "use", "calculate", "store", "pv", "each", "view", "cell", "-lrb-", "note", "only", "its", "boundary", "polygon", "take", "account", "since", "any", "ray", "leave", "view", "cell", "can", "represent", "ray", "boundary", "-rrb-", "runtime", "view", "cell", "correspond", "current", "observer", "location", "determine", "only", "object", "associate", "pv", "send", "graphic", "hardware", "lead", "significant", "savings", "render", "time", "point", "sample", "void", "volume", "void", "surface", "figure", "represent", "visibility", "from", "single", "point", "leave", "independent", "sample", "middle", "void", "volume", "right", "void", "surface", "2.2", "Algorithm", "Overview", "algorithm", "paper", "base", "ray", "shooting", "assume", "capability", "trace", "ray", "compute", "its", "first", "intersection", "scene", "triangle", "i.e.", "compute", "triangle", "-lrb-", "-rrb-", "-lrb-", "fast", "ray", "tracer", "include", "openrt", "-lsb-", "Wald", "et", "al.", "2003", "-rsb-", "recently", "present", "mlrta", "-lsb-", "Reshetov", "et", "al.", "2005", "-rsb-", "-rrb-", "idea", "sampling", "solution", "select", "sequence", "ray", "trace", "ray", "add", "triangle", "-lrb-", "-rrb-", "visibility", "set", "pv", "paper", "we", "address", "problem", "how", "sample", "efficiently", "how", "improve", "chance", "find", "new", "triangle", "we", "one", "most", "popular", "sampling", "strategy", "random", "sampling", "-lrb-", "section", "3.1", "-rrb-", "we", "show", "how", "use", "visibility", "information", "from", "previous", "sample", "construct", "intelligent", "sampling", "strategy", "base", "ray", "mutation", "complement", "random", "sampling", "adaptive", "border", "sampling", "algorithm", "quickly", "find", "nearby", "triangle", "sampling", "along", "border", "triangle", "previously", "find", "visible", "-lrb-", "section", "3.2", "-rrb-", "reverse", "sampling", "algorithm", "sample", "region", "space", "likely", "near", "boundary", "visible", "invisible", "space", "i.e.", "void", "surface", "-lrb-", "section", "3.3", "-rrb-", "section", "3.4", "we", "show", "how", "combine", "different", "sampling", "algorithm", "order", "obtain", "guide", "visibility", "sampling", "complete", "hybrid", "random", "deterministic", "sampling", "algorithm", "algorithm", "call", "guide", "because", "both", "sampling", "strategy", "guide", "visibility", "information", "scene", "-lrb-", "see", "Section", "5.4", "more", "detailed", "discussion", "-rrb-", "495", "visibility", "sample", "all", "ray", "scene", "form", "5d", "space", "ray", "have", "start", "point", "-lrb-", "3d", "-rrb-", "direction", "dir", "-lrb-", "2d", "-rrb-", "typical", "visibility", "query", "give", "region", "3d", "space", "ask", "what", "visible", "along", "ray", "leave", "region", "-lrb-", "view", "cell", "-rrb-", "while", "define", "5d", "set", "ray", "we", "only", "need", "consider", "4d", "set", "ray", "practice", "ray", "start", "boundary", "view", "region", "additionally", "all", "triangle", "intersect", "classify", "visible", "3.1", "Random", "Sample", "Generator", "random", "-lrb-", "pseudo-random", "-rrb-", "sampling", "algorithm", "select", "sequence", "random", "sample", "from", "scene", "probability", "distribution", "each", "new", "sample", "-lrb-", "-rrb-", "independent", "all", "previous", "sample", "...", "question", "sampling", "uniformity", "random", "sampling", "have", "be", "explore", "context", "form-factor", "computation", "-lsb-", "Sbert", "1993", "-rsb-", "we", "sample", "position", "ray", "direction", "uniformly", "use", "follow", "formula", "??", "arcsin", "where", "independent", "Halton", "sequence", "-lsb-", "Niederreiter", "1992", "-rsb-", "-lrb-", "-rrb-", "normalize", "coordinate", "view", "cell", "face", "while", "random", "sampling", "alone", "suffer", "from", "similar", "inefficiency", "regular", "sampling", "-lrb-", "see", "section", "-rrb-", "use", "seed", "more", "efficient", "strategy", "describe", "next", "3.2", "adaptive", "border", "sample", "sampling", "algorithm", "deterministic", "ray", "mutation", "strategy", "cover", "most", "ground", "work", "make", "we", "system", "successful", "strategy", "leave", "ray", "start", "point", "view", "cell", "fix", "while", "cover", "adjacent", "triangle", "object", "space", "practically", "construct", "local", "visibility", "map", "-lsb-", "Bittner", "2002", "-rsb-", "from", "select", "view", "cell", "point", "key", "idea", "sampling", "strategy", "adapt", "sampling", "rate", "geometric", "detail", "surface", "-lrb-", "see", "Figure", "-rrb-", "therefore", "unlikely", "subpixel", "triangle", "miss", "which", "problem", "method", "sample", "object", "regularly", "method", "perform", "especially", "well", "most", "frequent", "case", "connected", "mesh", "do", "assume", "use", "any", "connectivity", "information", "connected", "region", "discover", "random", "sampling", "step", "-lrb-", "therefore", "scene", "many", "small", "disconnect", "mesh", "like", "tree", "remain", "challenge", "approach", "-rrb-", "algorithm", "proceed", "follow", "triangle", "-lrb-", "-rrb-", "hit", "first", "time", "sample", "ray", "-lrb-", "dir", "-rrb-", "we", "enlarge", "small", "amount", "obtain", "enlarged", "polygon", "adaptively", "sample", "along", "its", "edge", "-lrb-", "figure", "-rrb-", "each", "edge", "we", "use", "two", "ray", "corresponding", "sample", "hit", "-lrb-", "-rrb-", "hit", "-lrb-", "-rrb-", "world", "space", "ray", "hit", "different", "triangle", "we", "recursively", "subdivide", "edge", "up", "give", "threshold", "point", "we", "also", "detect", "depth", "discontinuity", "between", "new", "sample", "original", "sample", "triangle", "which", "already", "part", "reverse", "sampling", "describe", "next", "section", "actual", "method", "use", "border", "enlargement", "deserve", "attention", "order", "miss", "any", "adjacent", "triangle", "border", "polygon", "should", "tight", "possible", "other", "hand", "too", "tight", "hit", "again", "due", "numerical", "precision", "ray", "shooting", "enlargement", "be", "do", "object", "space", "would", "happen", "near", "edge-on", "very", "distant", "triangle", "we", "therefore", "enlarge", "ray", "space", "rotate", "ray", "vertex", "new", "position", "small", "angle", "more", "robust", "because", "depend", "neither", "distance", "triangle", "nor", "its", "orientation", "only", "numerical", "precision", "ray", "representation", "practice", "mean", "each", "vertex", "new", "vertex", "put", "plane", "perpendicular", "ray", "shape", "choose", "so", "ray", "space", "distance", "fairly", "constant", "possible", "only", "vertex", "since", "sliver", "triangle", "would", "lead", "singularity", "we", "therefore", "choose", "polygon", "vertex", "each", "vertex", "three", "vertex", "generate", "two", "vertex", "generate", "each", "vector", "perpendicular", "ray", "one", "adjacent", "edge", "respectively", "third", "midpoint", "other", "two", "push", "away", "from", "along", "angle", "bisector", "surface", "1,2", "1,2", "1,1", "1,3", "1,1", "1,3", "figure", "adaptive", "border", "sampling", "Top", "we", "hit", "new", "surface", "we", "sample", "nearby", "point", "border", "polygon", "bottom", "adaptive", "subdivision", "edge", "+1", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "+1", "-rrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "+1", "-rrb-", "+1", "else", "-lrb-", "-lrb-", "-rrb-", "+1", "-lrb-", "-rrb-", "-rrb-", "where", "-lrb-", "-rrb-", "vector", "normalization", "operator", "choose", "numerically", "robust", "backface", "triangle", "need", "invert", "adaptive", "border", "sampling", "efficiently", "explore", "connected", "visible", "area", "input", "model", "from", "single", "viewpoint", "along", "1d", "curve", "ray", "space", "-lrb-", "see", "Section", "5.4", "-rrb-", "however", "can", "penetrate", "gap", "visible", "only", "from", "other", "portion", "ray", "space", "handle", "reverse", "sampling", "496", "predict", "-lrb-", "-rrb-", "hit", "-lrb-", "-rrb-", "view", "cell", "figure", "reverse", "sampling", "leave", "initial", "hit", "triangle", "t.", "middle", "new", "ray", "predict", "-lrb-", "-rrb-", "block", "much", "closer", "triangle", "right", "reverse", "sampling", "mutate", "start", "point", "view", "cell", "so", "ray", "pass", "through", "new", "-lrb-", "yellow", "-rrb-", "reach", "predict", "-lrb-", "-rrb-", "3.3", "reverse", "sample", "algorithm", "deterministic", "mutation", "strategy", "allow", "penetrate", "yet", "uncovered", "region", "space", "note", "can", "do", "perfectly", "find", "actual", "void", "volume", "equivalent", "original", "visibility", "problem", "however", "adaptive", "sampling", "process", "give", "good", "candidate", "location", "further", "sampling", "ray", "namely", "discontinuity", "location", "strategy", "work", "change", "start", "point", "ray", "instead", "its", "direction", "discontinuity", "detect", "during", "adaptive", "sampling", "edge", "compare", "distance", "ray", "origin", "actual", "hitpoint", "hit", "-lrb-", "-rrb-", "distance", "predict", "hitpoint", "predict", "-lrb-", "-rrb-", "predict", "hitpoint", "just", "intersection", "ray", "plane", "original", "triangle", "new", "hitpoint", "considerably", "-lrb-", "-rrb-", "closer", "i.e.", "ray", "obviously", "occlude", "closer", "triangle", "note", "we", "do", "check", "reverse", "case", "-lrb-", "jump", "from", "closer", "farther", "triangle", "-rrb-", "detect", "when", "do", "adaptive", "border", "sampling", "farther", "triangle", "we", "calculate", "mutate", "ray", "from", "different", "view", "cell", "position", "predict", "hitpoint", "so", "pass", "occlude", "triangle", "plane", "-lrb-", "hit", "-lrb-", "-rrb-", "hit", "-lrb-", "old", "-rrb-", "-rrb-", "intersect", "newly", "find", "triangle", "-lrb-", "old", "previous", "ray", "from", "which", "generate", "-rrb-", "intersect", "line", "we", "select", "point", "new", "which", "lie", "just", "outside", "new", "triangle", "mutate", "ray", "now", "construct", "new", "dir", "predict", "-lrb-", "-rrb-", "new", "direction", "vector", "new", "intersect", "-lrb-", "viewcell", "line", "-lrb-", "new", "predict", "-lrb-", "-rrb-", "-rrb-", "origin", "-lrb-", "see", "Figure", "-rrb-", "new", "ray", "contain", "ray", "space", "-lrb-", "i.e.", "do", "intersect", "view", "cell", "-rrb-", "however", "discard", "new", "ray", "new", "now", "treat", "independent", "ray", "triangle", "intersect", "add", "adaptive", "border", "sampling", "like", "any", "other", "triangle", "time", "new", "view", "cell", "origin", "2d", "example", "Figure", "reverse", "sampling", "correspond", "horizontal", "movement", "ray", "space", "predict", "-lrb-", "-rrb-", "hit", "-lrb-", "-rrb-", "3.4", "combine", "different", "sampling", "algorithm", "sampling", "strategy", "present", "so", "far", "can", "combine", "extremely", "efficient", "guide", "visibility", "sampling", "algorithm", "its", "two", "main", "component", "sample", "generator", "explore", "ray", "space", "independent", "random", "sample", "sampling", "queue", "propagate", "ray", "use", "adaptive", "border", "sampling", "reverse", "sampling", "algorithm", "describe", "follow", "pseudocode", "3.5", "termination", "criterion", "depend", "application", "requirement", "several", "option", "regard", "when", "stop", "cast", "ray", "view", "cell", "-rrb-", "fix", "criterion", "allocate", "number", "ray", "amount", "time", "computation", "each", "view", "cell", "-rrb-", "adaptive", "criterion", "terminate", "number", "newly", "find", "triangle", "per", "certain", "number", "sample", "fall", "below", "threshold", "most", "preferably", "-rrb-", "combination", "both", "typical", "example", "criterion", "stop", "iteration", "when", "more", "than", "50", "new", "triangle", "find", "1m", "ray", "when", "total", "10m", "ray", "have", "be", "shoot", "whichever", "come", "first", "result", "4.1", "Overview", "compare", "efficiency", "we", "algorithm", "previous", "work", "we", "use", "follow", "algorithm", "gv", "we", "guide", "visibility", "sampling", "algorithm", "adaptive", "border", "sampling", "-lrb-", "ab", "-rrb-", "reverse", "sampling", "-lrb-", "r", "-rrb-", "RAND", "random", "sampling", "-lrb-", "GVS", "value", "epsilon", "5e-5", "use", "enlarge", "triangle", "-rrb-", "we", "have", "dedicate", "separate", "subsection", "comparison", "nir", "main", "other", "exist", "visibility", "sampling", "method", "publish", "Nirenstein", "Blake", "-lsb-", "2004", "-rsb-", "-lrb-", "mainly", "because", "algorithm", "have", "slightly", "different", "goal", "than", "gv", "-rrb-", "EXACT", "Bittner?s", "-lsb-", "2003", "-rsb-", "exact", "visibility", "algorithm", "test", "scene", "select", "-lrb-", "see", "Figure", "Table", "-rrb-", "PPLANT", "complete", "UNC", "Power", "Plant", "model", "CITY", "city", "model", "ancient", "city", "Pompeii", "generate", "use", "cityengine", "-lsb-", "m?ller", "et", "al.", "2006", "-rsb-", "CANYON", "dataset", "Grand", "Canyon", "CUBES", "simple", "scene", "random", "cube", "test", "be", "conduct", "Intel", "pentium4", "3.2", "ghz", "4gb", "main", "memory", "graphic", "card", "NIR", "NVIDIA", "GeForce", "7900GTX", "512mb", "497", "scene", "triangle", "size", "vc", "CANYON", "2,242,504", "10x5x3", "km", "140x72", "city", "5,646,041", "320x312x9", "15x2", ".2", "pplant", "12,748,510", "200x61x81", "2x1", ".3", "CUBES", "24,000", "100x100x100", "1.5", "x1", ".5", "scene", "triangle", "size", "vc", "CANYON", "2,242,504", "10x5x3", "km", "140x72", "city", "5,646,041", "320x312x9", "15x2", ".2", "pplant", "12,748,510", "200x61x81", "2x1", ".3", "CUBES", "24,000", "100x100x100", "1.5", "x1", ".5", "Table", "statistics", "all", "scene", "vc", "denote", "average", "size", "view", "cell", "use", "model", "Figure", "top", "left", "city", "top", "right", "CUBES", "bottom", "leave", "PPLANT", "bottom", "right", "CANYON", "Inlays", "view", "from", "view", "cell", "gv", "rand", "we", "use", "intel?s", "multi-level", "ray", "tracer", "-lrb-", "mlrta", "-lsb-", "Reshetov", "et", "al.", "2005", "-rsb-", "-rrb-", "which", "allow", "sampling", "rate", "between", "about", "800k/s", "1200k/s", "peak", "up", "several", "million", "samples/s", "sampling", "rate", "depend", "scene", "type", "-lrb-", "so", "much", "size?pplant", "have", "higher", "sampling", "rate", "than", "CANYON", "example", "-rrb-", "coherence", "ray", "-lrb-", "random", "sample", "ab", "sample", "be", "faster", "depend", "again", "scene", "-rrb-", "overhead", "sampling", "selection", "process", "vary", "between", "15", "depend", "relative", "distribution", "random", "ab", "r", "ray", "4.2", "asymptotic", "behavior", "we", "first", "analyze", "theoretical", "property", "algorithm", "term", "sampling", "behavior", "i.e.", "sample-by-sample", "basis", "since", "only", "comparison", "do", "depend", "individual", "implementation", "since", "we", "do", "have", "exact", "visibility", "algorithm", "run", "reasonable", "time", "larger", "scene", "we", "can", "only", "study", "asymptotic", "behavior", "small", "number", "view", "cell", "Figure", "provide", "detailed", "analysis", "CANYON", "scene", "graph", "pixel", "error", "-lrb-", "calculate", "count", "false", "pixel", "large", "number", "random", "rendering", "-lsb-", "nirenstein", "Blake", "2004", "-rsb-", "-rrb-", "number", "triangle", "find", "over", "number", "sample", "gv", "rand", "top", "left", "image", "show", "gv", "converge", "linearly", "long", "deterministic", "strategy", "-lrb-", "ab", "r", "-rrb-", "can", "use", "most", "triangle", "black", "dot", "each", "view", "cell", "curve", "show", "when", "we", "termination", "criterion", "terminate", "pv", "search", "-lrb-", "we", "use", "50", "less", "new", "triangle", "find", "per", "1m", "sample", "-rrb-", "can", "see", "happen", "fairly", "well", "converged", "state", "already", "graph", "also", "show", "behavior", "very", "similar", "all", "view", "cell", "length", "linear", "segment", "only", "depend", "final", "pv", "size", "300", "300", "triangle", "200", "250", "gvs/canyon", "triangle", "250", "200", "view", "cell", "150", "150", "rand/canyon", "thousand", "100", "50", "thousand", "100", "50", "view", "cell", "11", "13", "15", "11", "13", "15", "million", "ray", "million", "ray", "100", "pixel", "error", "40", "60", "80", "gvs/canyon", "view", "cell", "triangle", "120Â 100Â 140", "80", "60", "GVS", "CANYON", "avg", "20", "thousand", "20", "40", "rand", "11", "13", "15", "50Â 100Â 150Â 200", "million", "ray", "million", "ray", "figure", "detailed", "asymptotic", "analysis", "view", "cell", "CANYON", "model", "-lrb-", "see", "text", "detail", "-rrb-", "pixel", "error", "measure", "1000x1000", "screen", "equivalent", "10", "plot", "lower", "right", "image", "show", "blue", "view", "cell", "from", "other", "image", "top", "right", "figure", "show", "rand", "comparison", "convergence", "rand", "look", "mainly", "logarithmic", "have", "very", "quick", "falloff", "after", "initial", "strong", "phase", "especially", "noteworthy", "even", "15m", "sample", "when", "GVS", "have", "already", "long", "converge", "rand", "still", "50k", "triangle", "behind", "gv", "most", "view", "cell", "bottom", "right", "figure", "analyze", "behavior", "even", "larger", "scale", "dark", "blue", "view", "cell", "from", "other", "graph", "figure", "confirm", "quick", "convergence", "gv", "show", "even", "after", "200m", "sample", "rand", "still", "several", "thousand", "triangle", "behind", "gv", "can", "conclude", "would", "take", "rand", "several", "order", "magnitude", "longer", "find", "pv", "GVS", "can", "find", "about", "7M", "8m", "sample", "finally", "bottom", "left", "figure", "prove", "PVS", "size", "correlate", "strongly", "average", "pixel", "error", "termination", "criterion", "discuss", "above", "work", "well", "practice", "bring", "average", "pixel", "error", "below", "30", "pixel", "1000x1000", "screen", "due", "better", "distribution", "initial", "sample", "rand", "show", "lower", "average", "pixel", "error", "phase", "where", "gv", "search", "mainly", "deterministically", "however", "reach", "same", "pixel", "error", "provide", "gv", "converged", "state", "rand", "have", "calculate", "similar", "number", "triangle", "PVS", "lead", "same", "observation", "before", "similar", "pixel", "error", "require", "order", "magnitude", "more", "sample", "than", "gv", "4.3", "Practical", "result", "next", "we", "demonstrate", "finding", "generalize", "larger", "number", "scene", "provide", "practical", "analysis", "include", "run", "time", "Table", "summarize", "we", "finding", "we", "use", "same", "convergence", "criterion", "50", "triangle", "per", "1m", "sample", "gv", "constant", "150m", "ray", "rand", "can", "see", "result", "very", "similar", "average", "maximum", "error", "both", "algorithm", "however", "run", "time", "differ", "more", "than", "order", "magnitude", "which", "reflect", "good", "convergence", "behavior", "gv", "respect", "rand", "show", "above", "table", "also", "list", "result", "NIR", "which", "discuss", "follow", "subsection", "addition", "error", "we", "also", "give", "size", "pv", "term", "whole", "model", "size", "-lrb-", "ev", "available", "reasonable", "time", "-rrb-", "higher", "value", "mean", "more", "accurate", "solution", "498", "alg", "avg.err", "max.err", "time", "pv", "gv", "35", "239", "7.9", "6.7", "rand", "67", "828", "183", "6.3", "nir512", "2,191", "8,215", "6.8", "5.8", "nir1024", "519", "2480", "11.6", "6.3", "gv", "22", "230", "6.1", "1.1", "rand", "70", "625", "69", "0.4", "nir512", "1,292", "8,655", "5.4", "0.2", "nir1024", "631", "8,965", "8", "0.4", "gv", "23", "211", "30s", "0.8", "rand", "69", "825", "129", "0.5", "nir512", "3,225", "17,169", "24", "0.4", "nir1024", "1,549", "8,317", "25.9", "0.6", "alg", "avg.err", "max.err", "time", "pv", "gv", "35", "239", "7.9", "6.7", "rand", "67Â 828Â 183", "6.3", "nir512", "2,191", "8,215", "6.8", "5.8", "nir1024", "519", "2480", "11.6", "6.3", "gv", "22", "230", "6.1", "1.1", "rand", "70", "625", "69", "0.4", "nir512", "1,292", "8,655", "5.4", "0.2", "nir1024", "631", "8,965", "8", "0.4", "gv", "23", "211", "30s", "0.8", "rand", "69Â 825Â 129", "0.5", "nir512", "3,225", "17,169", "24", "0.4", "nir1024", "1,549", "8,317", "25.9", "0.6", "Table", "statistics", "all", "scene", "average", "over", "number", "view", "cell", "we", "use", "threshold", "50", "less", "triangle", "find", "per", "1m", "sample", "cut", "off", "computation", "gv", "rand", "we", "shoot", "150m", "ray", "each", "test", "error", "number", "false", "pixel", "1000x1000", "screen", "which", "correspond", "10", "result", "NIR", "give", "512x512", "1024x1024", "resolution", "intrinsic", "parameter", "have", "adjust", "each", "scene", "obtain", "reasonable", "result", "-lrb-", "see", "comment", "section", "4.4", "-rrb-", "last", "column", "show", "average", "size", "calculate", "pv", "percentage", "whole", "model", "4.4", "comparison", "hardware", "sampling", "Nirenstein", "Blake", "-lsb-", "2004", "-rsb-", "recently", "publish", "interesting", "adaptive", "regular", "sampling", "algorithm", "which", "use", "graphic", "hardware", "adaptively", "sample", "hemi-cube", "view", "cell", "difficult", "directly", "compare", "NIR", "GVS", "one", "hand", "both", "base", "same", "atomic", "operation?taking", "visibility", "sample", "because", "sampling", "graphic", "hardware", "ray", "tracer", "functionally", "practically", "equivalent", "due", "available", "sub-pixel", "precision", "-lrb-", "usually", "12", "bit", "-rrb-", "current", "graphic", "hardware", "time", "complexity", "however", "differ", "significantly", "between", "two", "algorithm", "time", "complexity", "ray", "casting", "linear", "number", "ray", "due", "spatial", "datum", "structure", "logarithmic", "number", "object", "practice", "we", "have", "also", "observe", "strong", "dependence", "type", "scene", "implementation", "ray", "tracer", "which", "make", "general", "prediction", "scalability", "respect", "scene", "size", "very", "hard", "graphic", "hardware", "basic", "operation", "item-buffer", "render", "depend", "whether", "particular", "view", "mostly", "fill", "geometry", "limited", "resolution", "item", "buffer", "have", "more", "less", "impact", "render", "time", "we", "implementation", "NIR", "render", "model", "from", "multiple", "vertex", "buffer", "store", "directly", "video", "memory", "which", "provide", "triangle", "throughput", "near", "theoretical", "maximum", "card", "we", "use", "-lrb-", "between", "130", "190m", "triangles/s", "depend", "how", "many", "vertex", "be", "share", "model?note", "some", "vertex", "have", "duplicated", "allow", "item", "buffer", "rendering", "-rrb-", "only", "CANYON", "model", "do", "we", "observe", "fill", "rate", "limitation", "-lrb-", "vs.", "12", "hemicubes/s", "512", "vs.", "1024", "resolution", "-rrb-", "whereas", "city", "pplant", "be", "geometry", "limited", "-lrb-", "hemicubes/s", "-rrb-", "efficient", "acceleration", "algorithm", "exist", "both", "architecture", "certain", "amount", "preprocessing", "tolerate", "particular", "importance", "visibility", "processing", "complexity", "scene", "can", "handle", "ray", "trace", "limit", "only", "available", "storage", "space", "ray", "caster", "can", "work", "efficiently", "out", "core", "-lrb-", "e.g.", "Wald", "et", "al.", "-lsb-", "2004", "-rsb-", "have", "demonstrate", "350", "million", "polygon", "model", "can", "ray", "cast", "2-3", "frame", "per", "second", "-rrb-", "furthermore", "should", "point", "out", "rasterization", "benefit", "from", "hardware", "acceleration", "whereas", "ray", "trace", "still", "run", "software", "recent", "advance", "hardware", "ray", "trace", "-lsb-", "Woop", "et", "al.", "2005", "-rsb-", "promise", "huge", "po", "tential", "improve", "speed", "sampling-based", "algorithm", "like", "gv", "even", "further", "once", "technology", "become", "more", "commonplace", "however", "main", "difference", "between", "algorithm", "principal", "goal", "NIR", "aim", "increase", "render", "speed", "aggressively", "cull", "more", "object", "than", "actually", "occluded", "rationale", "be", "large", "gain", "render", "speed", "can", "obtain", "error", "final", "image", "tolerate", "indeed", "nir", "consistently", "underestimate", "pv", "show", "Table", "-lrb-", "note", "even", "error", "threshold", "significant", "rest-error", "report", "nir", "-lsb-", "nirenstein", "Blake", "2004", "-rsb-", "-rrb-", "while", "approach", "valuable", "application", "like", "quick", "preview", "etc.", "where", "resolution", "can", "fix", "average", "example", "1000", "false", "pixel", "tolerable", "many", "application", "require", "more", "accurate", "pv", "where", "GVS", "excel", "gv", "algorithm", "aim", "provide", "most", "accurate", "pv", "possible", "minimum", "number", "sample", "therefore", "performance", "metric", "gv", "total", "percentage", "culled", "object", "degree", "which", "actual", "pv", "can", "approximate", "we", "result", "show", "gv", "use", "limited", "number", "sample", "consistently", "find", "largest", "pv", "result", "average", "pixel", "error", "below", "0.005", "important", "any", "visualization", "application", "rely", "visibility", "preprocessing", "-lrb-", "especially", "antialiasing", "use", "output", "resolution", "fix", "advance", "-rrb-", "also", "number", "other", "application", "where", "reliable", "-lrb-", "practically", "exact", "-rrb-", "visibility", "require", "e.g.", "computational", "geometry", "gi", "robotic", "should", "note", "result", "Table", "NIR", "result", "derive", "through", "pv", "subdivision", "threshold", "which", "work", "differently", "from", "method", "use", "gv", "rand", "can", "therefore", "compare", "directly", "we", "find", "threshold", "very", "sensitive", "type", "scene", "have", "tune", "so", "lead", "excessive", "subdivision", "too", "early", "termination", "each", "scene", "separately", "-lrb-", "example", "once", "case", "error", "1024", "resolution", "significantly", "worse", "than", "512", "due", "premature", "termination", "-rrb-", "reason", "nir?s", "inability", "pick", "up", "complete", "pv", "lie", "both", "regular", "sampling", "strategy", "which", "force", "very", "fine", "subdivision", "view", "cell", "order", "pick", "up", "sub-pixel", "triangle", "thresholding", "adaptive", "subdivision", "which", "can", "prematurely", "terminate", "subdivision", "4.5", "comparison", "exact", "visibility", "we", "compare", "we", "algorithm", "EXACT", "CUBES", "scene", "from", "view", "cell", "about", "1.5", "x1", ".5", "m.", "EXACT", "take", "19", "piv", "1.7", "GHz", "PC", "find", "3,743", "visible", "triangle", "find", "same", "number", "triangle", "gv", "require", "about", "3", "gv", "screenspace", "error", "0.001", "already", "report", "after", "2", "more", "interesting", "however", "fact", "both", "gv", "rand", "find", "significantly", "more", "visible", "triangle", "than", "EXACT", "give", "enough", "sample", "example", "3,850", "triangle", "be", "find", "after", "only", "15s", "gv", "note", "EXACT", "use", "basis?better", "result", "could", "certainly", "achieve", "tuning", "numerical", "threshold", "intrinsic", "method", "show", "clearly", "accuracy", "visibility", "algorithm", "even", "exact", "one", "ultimately", "limit", "numerical", "issue", "related", "work", "discussion", "application", "large", "volume", "research", "have", "be", "devote", "visibility", "problem", "due", "importance", "computer", "graphic", "computer", "vision", "robotic", "other", "field", "section", "compare", "various", "pect", "propose", "visibility", "sampling", "algorithm", "wider", "class", "from-region", "visibility", "algorithm", "general", "overview", "we", "can", "recommend", "excellent", "survey", "visibility", "problem", "algorithm", "-lsb-", "Durand", "1999", "Cohen-Or", "et", "al.", "2003", "-rsb-", "from-region", "visibility", "algorithm", "usually", "classify", "exact", "-lrb-", "potentially", "visible", "set", "pv", "exact", "visible", "set", "ev", "-rrb-", "conservative", "-lrb-", "pv", "ev", "-rrb-", "aggressive", "-lrb-", "pv", "ev", "-rrb-", "approximate", "-lrb-", "pv", "ev", "-rrb-", "499", "5.1", "exact", "visibility", "exact", "solution", "compute", "visibility", "from", "region", "space", "have", "be", "rare", "-lsb-", "Duguet", "Drettakis", "2002", "Durand", "1999", "-rsb-", "recently", "two", "algorithm", "have", "be", "publish", "-lsb-", "Nirenstein", "et", "al.", "2002", "Bittner", "2003", "-rsb-", "further", "improve", "upon", "-lsb-", "Haumont", "et", "al.", "2005", "Mora", "et", "al.", "2005", "-rsb-", "both", "exact", "work", "general", "scene", "while", "exact", "algorithm", "have", "be", "holy", "grail", "visibility", "community", "long", "time", "two", "algorithm", "show", "complexity", "inherent", "visibility", "problem", "may", "obstacle", "make", "exact", "visibility", "widely", "applicable", "high", "run", "time", "high", "complexity", "implementation", "critical", "numerical", "robustness", "issue", "can", "actually", "make", "solution", "approximate", "sampling-based", "strategy", "-lrb-", "see", "-lsb-", "Bittner", "2003", "-rsb-", "-rrb-", "we", "believe", "sampling-based", "method", "exact", "method", "complement", "each", "other", "have", "different", "strength", "weakness", "5.2", "conservative", "visibility", "several", "author", "stress", "importance", "conservative", "visibility", "computation", "i.e.", "never", "underestimate", "visible", "set", "since", "problem", "almost", "hard", "exact", "visibility", "problem", "practically", "all", "publish", "conservative", "from-region", "algorithm", "simplify", "problem", "impose", "certain", "restriction", "scene", "typical", "restriction", "limitation", "2.5", "visibility", "-lsb-", "Wonka", "et", "al.", "2000", "Bittner", "et", "al.", "2001", "Koltun", "et", "al.", "2001", "-rsb-", "architectural", "scene", "-lsb-", "Airey", "et", "al.", "1990", "Teller", "S?quin", "1991", "-rsb-", "restriction", "volumetric", "occluder", "-lsb-", "Schaufler", "et", "al.", "2000", "-rsb-", "restriction", "larger", "occluder", "close", "view", "cell", "-lsb-", "Leyvand", "et", "al.", "2003", "Durand", "et", "al.", "2000", "-rsb-", "last", "restriction", "imply", "nature", "datum", "structure", "use", "store", "visibility", "information", "while", "can", "argue", "larger", "occluder", "can", "synthesize", "from", "smaller", "one", "-lsb-", "Andujar", "et", "al.", "2000", "-rsb-", "possible", "general", "guarantee", "include", "all", "visible", "geometry", "pv", "may", "important", "some", "application", "ultimately", "sampling-based", "method", "can", "much", "more", "successful", "oppose", "publish", "conservative", "algorithm", "do", "make", "any", "assumption", "about", "scene", "allow", "they", "handle", "much", "larger", "variety", "scene", "due", "ease", "implementation", "robustness", "nonconservative", "algorithm", "more", "practical", "commercial", "product", "computer", "game", "-lsb-", "Aila", "Miettinen", "2004", "-rsb-", "already", "use", "context", "numerical", "issue", "often", "make", "conservative", "algorithm", "nonconservative", "practice", "5.3", "aggressive", "visibility", "since", "visibility", "fundamental", "problem", "general", "robust", "practical", "tool", "important", "complement", "specialize", "algorithm", "discuss", "before", "tool", "almost", "universally", "base", "sampling", "two", "most", "popular", "solution", "randomly", "select", "large", "number", "ray", "sample", "visibility", "-lsb-", "Schaufler", "et", "al.", "2000", "Airey", "et", "al.", "1990", "Shade", "et", "al.", "1998", "-rsb-", "first", "sample", "boundary", "view", "cell", "point", "sample", "visibility", "from", "each", "point", "-lsb-", "Levoy", "Hanrahan", "1996", "Stuerzlinger", "1999", "-rsb-", "context", "view", "planning", "laser", "range", "scanner", "sampling", "algorithm", "exist", "store", "void", "surface", "void", "volume", "compute", "next-best", "view", "-lsb-", "Pito", "1999", "-rsb-", "similar", "algorithm", "also", "use", "generation", "textured", "depth", "mesh", "-lsb-", "Wilson", "Manocha", "2003", "-rsb-", "another", "option", "shoot", "ray", "from", "scene", "triangle", "towards", "view", "cell", "-lsb-", "Gotsman", "et", "al.", "1999", "-rsb-", "which", "lead", "oversampling", "ray", "space", "most", "scene", "Nirenstein", "Blake", "-lsb-", "2004", "-rsb-", "be", "first", "realize", "full", "potential", "sampling", "visibility", "computation", "propose", "new", "approach", "which", "use", "graphic", "hardware", "sampling", "discuss", "section", "4.4", "algorithm", "aim", "reduce", "render", "time", "cull", "even", "visible", "triangle", "long", "do", "result", "significant", "render", "error", "oppose", "we", "algorithm", "which", "always", "try", "find", "best", "possible", "approximation", "exact", "visible", "set", "5.4", "Algorithm", "Analysis", "Ray", "space", "analysis", "introduction", "Figure", "we", "have", "argue", "desirable", "sample", "ray", "space", "regularly", "right", "image", "figure", "show", "only", "approximately", "1d", "subspace", "ray", "need", "consider", "simple", "2d", "example", "we", "new", "algorithm", "sample", "ray", "space", "more", "intelligently", "random", "sampling", "place", "initial", "seed", "point", "ray", "space", "stochastically", "search", "region", "ray", "space", "have", "be", "explore", "yet", "continue", "example", "2d", "figure", "adaptive", "border", "sampling", "correspond", "vertical", "expansion", "2d", "ray", "space", "-lrb-", "since", "viewpoint", "remain", "fixed", "-rrb-", "which", "only", "proceed", "yet", "unexplored", "area", "particular", "advantage", "adaptive", "border", "sampling", "method", "sampling", "rate", "adapt", "geometric", "complexity", "visible", "surface", "reverse", "sampling", "other", "hand", "movement", "horizontal", "direction", "-lrb-", "since", "hitpoint", "remain", "fixed", "-rrb-", "case", "where", "movement", "promise", "lead", "yet", "explore", "region", "full", "3d", "case", "instructive", "study", "we", "algorithm", "term", "visibility", "complex", "-lsb-", "Durand", "1999", "-rsb-", "visibility", "complex", "describe", "partition", "4d", "ray", "space", "4d", "region", "ray", "hit", "same", "object", "-lrb-", "note", "ray", "space", "strictly", "4d", "because", "we", "only", "interested", "ray", "start", "from", "view", "cell", "-rrb-", "3d", "boundary", "partition", "call", "tangency", "volume", "consist", "ray", "tangent", "scene", "object", "sample", "place", "along", "object", "border", "therefore", "correspond", "sample", "near", "tangency", "volume", "object", "dual", "space", "since", "we", "keep", "viewpoint", "-lrb-", "degree", "freedom", "-rrb-", "fix", "during", "deterministic", "ab", "exploration", "phase", "we", "need", "sample", "1d", "set", "only", "without", "ab", "we", "would", "ignore", "tangency", "volume", "have", "sample", "whole", "2d", "subset", "ray", "space", "define", "choose", "viewpoint", "reverse", "sampling", "other", "hand", "look", "line", "tangent", "two", "scene", "edge", "ray", "space", "line", "near", "intersection", "two", "tangency", "volume", "intersection", "call", "bitangent", "only", "2d", "reverse", "sampling", "viewpoint", "allow", "move", "along", "plane", "-lrb-", "1d", "-rrb-", "so", "total", "r", "also", "sample", "1d", "set", "combined", "ab", "r", "strategy", "therefore", "correspond", "exploration", "4d", "ray", "space", "along", "those", "1d", "curve", "most", "likely", "reveal", "new", "object", "explain", "high", "efficiency", "gv", "algorithm", "another", "useful", "interpretation", "ab", "sampling", "strategy", "3d", "base", "visibility", "map", "-lsb-", "Bittner", "2002", "-rsb-", "visibility", "map", "structure", "contain", "all", "visible", "line", "segment", "give", "view", "segment", "can", "characterize", "mainly", "flat", "corner", "-lrb-", "interior", "edge", "mesh", "-rrb-", "shadow", "-lrb-", "depth", "discontinuity", "-rrb-", "ab", "sampling", "strategy", "place", "sample", "all", "edge", "visibility", "map", "-lrb-", "without", "explicitly", "construct", "-rrb-", "sample", "interior", "edge", "mesh", "serve", "find", "connected", "set", "mesh", "-lrb-", "trivially", "adjacent", "region", "visibility", "complex", "-rrb-", "sample", "shadow", "edge", "serve", "discover", "depth", "discontinuity", "where", "object", "partly", "occlude", "other", "object", "Shadow", "edge", "where", "r", "sampling", "strategy", "use", "refine", "sampling", "-lrb-", "find", "bitangent", "visibility", "complex", "-rrb-", "accuracy", "term", "conservative", "-lrb-", "even", "exact", "-rrb-", "visibility", "actually", "quite", "misleading", "most", "algorithm", "though", "conservative", "theory", "conservative", "practice", "due", "numerical", "robustness", "problem", "especially", "true", "algorithm", "rely", "graphic", "hardware", "furthermore", "complex", "algorithm", "prone", "implementation", "problem", "due", "much", "improved", "sampling", "efficiency", "magnitude", "error", "introduce", "we", "algorithm", "comparable", "other", "error", "source", "error", "usually", "tolerate", "conservative", "algorithm", "-lrb-", "see", "Section", "-rrb-", "other", "algorithm", "often", "use", "conjunction", "visibility", "processing", "like", "level-of-detail", "algorithm", "shadow", "mapping", "additional", "source", "error", "scene", "complexity", "one", "distinguish", "feature", "we", "samplingbased", "algorithm", "can", "handle", "arbitrary", "type", "scene", "high", "overall", "visual", "complexity", "do", "rely", "occluder", "synthesis", "depend", "mostly", "size", "visible", "set", "total", "scene", "complexity", "500", "5.5", "Limitations", "Future", "Work", "although", "guide", "visibility", "sampling", "generally", "find", "major", "part", "PVS", "very", "quickly", "fact", "stochastic", "one", "hand", "guide", "visibility", "scene", "other", "hand", "make", "final", "accuracy", "dependent", "structure", "scene", "therefore", "we", "can", "give", "any", "hard", "guarantee", "pixel", "error", "calculate", "pv", "also", "ability", "explore", "connected", "ray", "space", "subset", "far", "distance", "limit", "numerical", "precision", "ray", "direction", "vector", "ab", "mean", "triangle", "have", "solid", "angle", "less", "than", "double", "precision", "accuracy", "when", "see", "from", "ray", "origin", "most", "likely", "miss", "worst", "case", "scene", "complexity", "scene", "consist", "large", "set", "small", "disconnect", "triangle", "forest", "scene", "synthetic", "scene", "random", "triangle", "visibility", "scene", "so", "complex", "even", "sampling-based", "solution", "either", "have", "high", "error", "take", "long", "time", "compute", "still", "important", "point", "out", "sampling-based", "algorithm", "only", "one", "able", "even", "process", "scene", "respect", "avenue", "future", "work", "incorporate", "geometric", "lod", "sampling", "framework", "similar", "vlod", "system", "propose", "Chhugani", "et", "al.", "-lsb-", "2005", "-rsb-", "geometric", "lod", "could", "potentially", "increase", "speed", "ray", "tracer", "make", "intersection", "computation", "more", "robust", "because", "small", "triangle", "distance", "get", "replace", "larger", "one", "however", "robust", "geometric", "lod", "available", "all", "scene", "integrate", "lod", "ray", "tracer", "current", "topic", "research", "furthermore", "error", "metric", "use", "create", "lod", "impact", "accuracy", "visibility", "algorithm", "therefore", "usable", "output", "resolution", "5.6", "application", "one", "important", "strength", "sampling-based", "method", "ease", "application", "we", "discuss", "number", "application", "scenario", "we", "algorithm", "visibility", "preprocessing", "real-time", "rendering", "game", "scenario", "already", "describe", "overview", "one", "most", "important", "application", "gv", "example", "scene", "current", "computer", "game", "become", "increasingly", "general", "so", "special", "purpose", "algorithm", "-lrb-", "cell", "portal", "2.5", "solution", "-rrb-", "can", "use", "anymore", "while", "exact", "algorithm", "difficult", "implement", "error-prone", "gv", "can", "use", "all", "stage", "game", "development", "during", "level", "design", "number", "ray", "can", "limit", "so", "coarse", "solution", "can", "provide", "almost", "instantaneously", "final", "production", "pv", "can", "calculate", "high", "accuracy", "very", "important", "create", "pv", "close", "EVS", "possible", "dependent", "particular", "output", "resolution", "since", "resolution", "application", "run", "know", "advance", "addition", "antialiasing", "method", "-lrb-", "supersampling", "multisampling", "-rrb-", "use", "information", "from", "subpixel", "triangle", "so", "virtual", "resolution", "even", "higher", "note", "although", "scene", "computer", "game", "inherently", "dynamic", "major", "part", "scene", "still", "static", "so", "huge", "gain", "render", "speed", "can", "obtain", "furthermore", "gv", "work", "arbitrary", "polyhedral", "view", "cell", "so", "view", "space", "can", "choose", "freely", "online", "networked", "visibility", "show", "result", "reasonable", "approximation", "ev", "low", "pixel", "error", "can", "find", "second", "less", "therefore", "gv", "can", "use", "online", "visibility", "culling", "run", "separate", "processor", "over", "network", "describe", "instant", "visibility", "system", "-lsb-", "Wonka", "et", "al.", "2001", "-rsb-", "case", "transmit", "pv", "per-object", "basis", "improve", "result", "because", "suffice", "one", "triangle", "object", "find", "gv", "order", "classify", "whole", "object", "visible", "furthermore", "small", "modification", "gv", "make", "algorithm", "better", "suitable", "progressive", "evaluation", "instead", "interleave", "ab", "random", "sample", "from", "beginning", "create", "certain", "number", "-lrb-", "e.g.", "1m", "-rrb-", "random", "sample", "startup", "phase", "only", "use", "those", "seed", "ABS", "ray", "give", "better", "distribution", "sample", "initial", "phase", "algorithm", "since", "ab", "systematically", "flood", "fill", "PVS", "around", "its", "seed", "point", "take", "some", "time", "until", "all", "image", "region", "have", "be", "reach", "impostor", "generation", "many", "scene", "visibility", "culling", "sufficient", "guarantee", "high", "frame", "rate", "everywhere", "model", "therefore", "image-based", "method", "can", "use", "replace", "complex", "scene", "part", "so-called", "impostor", "however", "since", "impostor", "trade", "render", "speed", "against", "memory", "consumption", "important", "find", "exact", "visible", "part", "scene", "avoid", "waste", "impostor", "memory", "invisible", "geometry", "-lsb-", "Jeschke", "et", "al.", "2005", "-rsb-", "gv", "ideally", "suit", "purpose", "since", "provide", "accurate", "per-triangle", "visibility", "information", "so", "only", "those", "object", "part", "actually", "visible", "need", "store", "impostor", "visibility", "decision", "basis", "many", "practical", "application", "require", "accurate", "visibility", "information", "part", "decision", "make", "process", "example", "include", "visibility", "analysis", "urban", "planning", "-lrb-", "do", "new", "skyscraper", "impact", "old", "town", "-rrb-", "military", "application", "-lrb-", "line", "sight", "culling", "tactical", "battlefield", "management", "-lsb-", "McDermott", "Gelsey", "1987", "-rsb-", "-rrb-", "telecommunications", "-lrb-", "visibility", "emitter", "-rrb-", "robotic", "many", "more", "gv", "advantageous", "problem", "because", "general", "purpose", "do", "have", "any", "parameter", "tweak", "do", "depend", "any", "special", "property", "scene", "501", "conclusion", "we", "have", "present", "visibility", "sampling", "algorithm", "compute", "full", "3d", "visibility", "solution", "from", "region", "space", "propose", "algorithm", "improve", "efficiency", "previous", "sampling", "strategy", "over", "two", "order", "magnitude", "thereby", "allow", "visibility", "solution", "negligible", "error", "compute", "reasonable", "time", "propose", "algorithm", "work", "arbitrary", "so-called", "polygon", "soup", "do", "require", "any", "memory", "beyond", "use", "ray", "caster", "due", "new", "sampling", "strategy", "employ", "algorithm", "its", "accuracy", "competitive", "even", "exact", "conservative", "approach", "while", "also", "extremely", "simple", "implement", "we", "have", "provide", "evidence", "guided", "visibility", "sample", "close", "important", "gap", "visibility", "research", "combine", "speed", "ease", "implementation", "sampling-based", "special-purpose", "conservative", "algorithm", "most", "accuracy", "exact", "solution", "thus", "gv", "can", "use", "general", "purpose", "visibility", "tool", "acknowledgement", "we", "thank", "Jiri", "Bittner", "fruitful", "discussion", "research", "also", "support", "EU", "scope", "GameTools", "project", "-lrb-", "ist-2-004363", "-rrb-", "NGA", "grant", "hm1582-05-1-2004", "reference", "ilum", "T.", "IETTINEN", "V.", "2004", "dPVS", "occlusion", "culling", "system", "massive", "dynamic", "environment", "IEEE", "Computer", "Graphics", "application", "24", "irey", "J.", "M.", "OHLF", "J.", "H.", "ROOKS", "F.", "P.", "1990", "towards", "image", "realism", "interactive", "update", "rate", "complex", "virtual", "building", "environment", "Computer", "Graphics", "-lrb-", "1990", "Symposium", "interactive", "3d", "graphic", "-rrb-", "vol", "24", "41", "50", "ndujar", "C.", "aona", "C.", "AVAZO", "I.", "2000", "lod", "visibility", "culling", "occluder", "synthesis", "computer", "aid", "design", "32", "13", "773", "783", "ittner", "J.", "ONKA", "P.", "IMMER", "M.", "2001", "visibility", "preprocessing", "urban", "scene", "use", "line", "space", "subdivision", "Proc", "Pacific", "Graphics", "2001", "276", "284", "ittner", "J.", "2002", "efficient", "construction", "visibility", "map", "use", "approximate", "occlusion", "sweep", "SCCG", "02", "Proceedings", "18th", "spring", "conference", "computer", "graphic", "167", "175", "ittner", "J.", "2003", "hierarchical", "technique", "visibility", "computation", "phd", "thesis", "Czech", "Technical", "University", "Prague", "hhuganus", "J.", "urnomo", "B.", "RISHNAN", "S.", "OHEN", "J.", "ENKATA", "SUBRAMANIAN", "S.", "OHNSON", "D.", "S.", "2005", "vLOD", "high-fidelity", "walkthrough", "large", "virtual", "environment", "IEEE", "Trans", "visualization", "computer", "graphic", "11", "35", "47", "ohen", "D.", "HRYSANTHOU", "Y.", "L.", "ILVA", "C.", "T.", "URAND", "F.", "2003", "survey", "visibility", "walkthrough", "application", "IEEE", "Trans", "visualization", "computer", "graphic", "412", "431", "uguet", "F.", "RETTAKIS", "G.", "2002", "robust", "epsilon", "visibility", "Proc", "ACM", "SIGGRAPH", "2002", "567", "575", "urand", "F.", "RETTAKIS", "G.", "hollot", "J.", "uech", "C.", "2000", "conservative", "visibility", "preprocessing", "use", "extended", "projection", "Proc", "ACM", "SIGGRAPH", "2000", "239", "248", "urand", "F.", "1999", "3d", "visibility", "Analytical", "Study", "application", "phd", "thesis", "Universite", "Joseph", "fourier", "Grenoble", "France", "otsman", "C.", "udarsky", "O.", "AYMAN", "J.", "1999", "optimize", "occlusion", "culling", "use", "five-dimensional", "subdivision", "computer", "graphic", "23", "645", "654", "AUMONT", "D.", "AKINEN", "O.", "IRENSTEIN", "S.", "2005", "low", "dimensional", "framework", "exact", "polygon-to-polygon", "occlusion", "query", "Proc", "Eurographics", "Symposium", "Rendering", "211", "222", "ESCHKE", "S.", "IMMER", "M.", "CHUMANN", "H.", "urgathofer", "W.", "2005", "Automatic", "impostor", "placement", "guarantee", "frame", "rate", "low", "memory", "requirement", "Proc", "ACM", "SIGGRAPH", "Symp", "interactive", "3d", "graphic", "Games", "103", "110", "OLTUN", "V.", "HRYSANTHOU", "Y.", "ohen", "c.-o", "2001", "hardware-accelerated", "from-region", "visibility", "use", "dual", "ray", "space", "render", "technique", "2001", "205", "216", "evoy", "M.", "ANRAHAN", "P.", "1996", "light", "field", "render", "Proc", "ACM", "SIGGRAPH", "96", "31", "42", "eyvand", "T.", "orkine", "O.", "ohen", "D.", "2003", "Ray", "space", "factorization", "from-region", "visibility", "ACM", "transaction", "Graphics", "22", "595", "604", "ermott", "D.", "elsey", "a.", "1987", "terrain", "analysis", "tactical", "situation", "assessment", "Proceedings", "Spatial", "reasoning", "multi-sensor", "fusion", "420", "429", "os", "F.", "VENEAU", "L.", "RIAUX", "M.", "2005", "coherent", "exact", "polygon-to-polygon", "visibility", "Proceedings", "Winter", "School", "Computer", "Graphics", "2005", "87", "94", "uller", "P.", "ONKA", "P.", "AGLER", "S.", "LMER", "A.", "OOL", "L.", "V.", "2006", "procedural", "modeling", "building", "ACM", "transaction", "Graphics", "25", "iederreiter", "H.", "1992", "Random", "Number", "Generation", "Quasi-Monte", "Carlo", "method", "SIAM", "Philadelphia", "irenstein", "S.", "LAKE", "E.", "2004", "hardware", "accelerate", "visibility", "preprocessing", "use", "adaptive", "sampling", "render", "technique", "2004", "207", "216", "irenstein", "S.", "LAKE", "E.", "AIN", "J.", "2002", "exact", "from-region", "visibility", "culling", "render", "technique", "2002", "191", "202", "ito", "R.", "1999", "solution", "next", "best", "view", "problem", "automate", "surface", "acquisition", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "21", "10", "1016", "1030", "eshetov", "a.", "oupikov", "a.", "urley", "J.", "2005", "multi-level", "ray", "trace", "algorithm", "ACM", "Trans", "Graphics", "24", "1176", "1185", "BERT", "M.", "1993", "integral", "geometry", "method", "fast", "form", "factor", "computation", "Computer", "Graphics", "Forum", "12", "c409?c420", "chaufler", "G.", "ORSEY", "J.", "ECORET", "X.", "ILLION", "F.", "2000", "conservative", "volumetric", "visibility", "occluder", "fusion", "Proc", "ACM", "SIGGRAPH", "2000", "229", "238", "hade", "J.", "ORTLER", "S.", "WEI", "L.", "ZELISKI", "R.", "1998", "layered", "depth", "image", "Proc", "ACM", "SIGGRAPH", "98", "231", "242", "tuerzlinger", "W.", "1999", "Imaging", "all", "visible", "surface", "Proc", "Graphics", "Interface", "1999", "115", "122", "eller", "S.", "J.", "QUIN", "C.", "H.", "1991", "visibility", "preprocessing", "interactive", "walkthrough", "Computer", "Graphics", "-lrb-", "Proc", "ACM", "SIGGRAPH", "91", "-rrb-", "25", "61", "69", "ALD", "I.", "urcell", "T.", "J.", "chmittler", "J.", "ENTHIN", "C.", "LUSALLEK", "P.", "2003", "Realtime", "ray", "trace", "its", "use", "interactive", "global", "illumination", "Eurographics", "State", "Art", "Reports", "ALD", "I.", "IETRICH", "a.", "lusallek", "P.", "2004", "interactive", "outof-core", "render", "framework", "visualize", "massively", "complex", "model", "render", "technique", "2004", "81", "92", "ILSON", "a.", "anocha", "D.", "2003", "simplify", "complex", "environment", "use", "incremental", "textured", "depth", "mesh", "ACM", "transaction", "Graphics", "22", "678", "688", "ONKA", "P.", "IMMER", "M.", "CHMALSTIEG", "D.", "2000", "visibility", "preprocess", "occluder", "fusion", "urban", "walkthrough", "render", "technique", "2000", "71", "82", "ONKA", "P.", "IMMER", "M.", "ILLION", "F.", "2001", "instant", "visibility", "Computer", "Graphics", "Forum", "20", "411", "421", "OOP", "S.", "chmittler", "J.", "LUSALLEK", "P.", "2005", "RPU", "programmable", "ray", "processing", "unit", "realtime", "ray", "trace", "ACM", "transaction", "Graphics", "24", "434", "444", "502" ],
  "content" : "\n  \n    d91f1c62fdfea1a2d9c92bb0334e9bc54715216df99270fc855d072746536108\n    mhe\n    http://dx.doi.org/10.1145/1141911.1141914\n    Name identification was not possible. \n  \n  \n    \n      \n        Guided Visibility Sampling\n      \n      Peter Wonka ? Michael Wimmer ? Kaichi Zhou ? Stefan Maierhofer ? Gerd Hesina ? ? Arizona State University ? Vienna University of Technology ? VRVis Research Center\n      \n        \n        Figure 1: Visualization of sampling strategies (white pixels show a subset of the actual samples, missed geometry is marked red). Left: An urban input scene and a view cell (in yellow) for visibility sampling. Middle: Previous visibility sampling algorithms repeatedly sample the same triangles in the foreground while missing many smaller triangles and distant geometry. Right: Our solution is guided by scene visibility and therefore quickly finds most visible triangles while requiring drastically fewer samples than previous methods.\n      \n      This paper addresses the problem of computing the triangles visible from a region in space. The proposed aggressive visibility solution is based on stochastic ray shooting and can take any triangular model as input. We do not rely on connectivity information, volumetric occluders, or the availability of large occluders, and can therefore process any given input scene. The proposed algorithm is practically memoryless, thereby alleviating the large memory consumption problems prevalent in several previous algorithms. The strategy of our algorithm is to use ray mutations in ray space to cast rays that are likely to sample new triangles. Our algorithm improves the sampling efficiency of previous work by over two orders of magnitude. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism?Visible line/surface algorithms Keywords: visibility, occlusion culling, PVS, visibility sampling\n    \n    \n      \n        1 Introduction\n      \n      Visibility is a fundamental problem in computer graphics: visibility computations are necessary for occlusion culling, shadow generation, inside-outside classifications, image-based rendering, motion ? {peter.wonka|kaichi.zhou}@asu.edu , Tempe, AZ 85287-0112 ? wimmer@cg.tuwien.ac.at , 1040 Vienna, Austria ? {sm|hesina}@vrvis.at , 1220 Vienna, Austria ? alexander.reshetov@intel.com , Santa Clara, CA 95054 planning, and navigation, to name just a few examples. While visibility from a single viewpoint can be calculated quite easily, many applications require the potentially visible set (PVS) for a region in space, which is, unfortunately, much more complicated. A number of excellent from-region visibility algorithms exist, but most of them are only applicable to a limited range of scenes, require complex computations, and sometimes significant amounts of memory. Therefore, sampling-based solutions have become very popular for practical applications due to their robustness, general applicability, and ease of implementation. In this paper we will improve upon previous sampling-based algorithms by significantly improving the sampling efficiency, i.e., the number of samples required to detect a certain set of visible polygons. To motivate our design choices, we will look at two key aspects of any visibility algorithm: the behavior of the algorithm in ray space, and the data structure used to store and acquire visibility information. Figure 2 illustrates the concept of ray space in 2D. Given a view cell, shown as edge parameterized with s, and a scene with objects shown in grey, we can compute visibility by considering all rays from the view cell to a plane behind the scene, parameterized with t. For a 2D scene, this is a 2D set of rays; for a 3D scene this is a 4D set of rays. If this set of rays is sampled densely enough, we will have a good visibility solution. The inefficiency of a pure regular sampling approach as shown in Figure 2 is that the same surfaces are sampled over and over again (note that the definition of regular depends on the parameterization of ray space!). Therefore, it would be beneficial if we could only sample areas that have not been sampled before. This is shown in Figure 3 , where after an initial orthogonal sampling, only few additional rays are needed to find all visible surfaces. In total, little more than a 1D subspace of the 2D ray space needs to be explored in this example. This is due to the spatial coherence of visibility. In this paper, we exploit this coherence: starting from stochastically sampled points, we grow lower-dimensional subspaces of ray space using the newly introduced strategies of adaptive border sampling and reverse sampling, which are guided by the properties of scene visibility. The second key aspect of a visibility algorithm is what data structure is used to store visibility information. The most complete, but also complex, way is to store 4D ray space. For large scenes, this entails prohibitive levels of memory consumption. Conservative algorithms often store the shadow volume, whereas sampling algorithms use the volume of 3D space that has not been sampled yet (the so-called void volume, Figure 4 ); but these data structures still require several times the memory taken by the scene description itself. Alternatively, the boundary of the void volume (the void surface [Pito 1999]) can be used, which is easy to sample from one point in space, but difficult to manipulate. In this paper, we do not store visibility information beyond the PVS at all, relying on our new reverse sampling approach to penetrate the void surface based on the current sample only. The key contribution of this paper is an intelligent sampling algorithm that drastically improves the performance of previous sampling approaches by combining random sampling with deterministic exploration phases. The algorithm requires little memory, is simple to implement, accepts any triangular test scene as input, and can be used as a general purpose visibility tool.\n      Copyright ? 2006 by the Association for Computing Machinery, Inc. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions Dept, ACM Inc., fax +1 (212) 869-0481 or e-mail\n      Alexander Reshetov ? ? Intel Corporation\n       permissions@acm.org . ? 2006 ACM 0730-0301/06/0700-0494 $5.00\n      494\n      Object Space Object Space Ray Space 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 t t t 8 7 6 5 4 3 2 s s 1 s 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8\n      \n        Figure 2: Sampling in object and ray space. Left: a scene with a set of objects. A view cell is shown as a line segment parameterized with s. We are interested in all rays that intersect the view cell and a second line segment parameterized with t. Middle: Shows a subset of the possible rays. One ray is highlighted in blue. Right: A depiction of the discrete ray space. Any ray in the middle figure corresponds to a point in ray space. The blue point corresponds to the blue ray in the middle figure.\n      \n      Object Space Object Space Ray Space 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 t t t 8 7 6 5 4 3 2 s s 1 s 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8\n      \n        Figure 3: Left: The scene sampled orthogonally. Middle: Additional samples to capture oblique surfaces. Right: The rays used to sample the scene are shown in corresponding colors.\n      \n      \n        2 Overview\n        \n          2.1 Problem Statement\n          We consider visibility problems that are posed as follows: As first input we take a three-dimensional scene consisting of a set of triangles, T S. We do not rely on connectivity information, volumetric  objects, or large polygons as potential occluders (such a set of triangles is often called triangle soup). As second input we consider a subset of ray space ?, usually defined by the rays emanating within a 3D polyhedron called view cell and intersecting the bounding box of the scene. A ray can be defined by a starting point and a direction. Using T S and ?, we can define a visibility function v : ? ? T S, so that each ray in ? maps to the triangle in T S that it intersects first. The exact solution of the visibility problem is the range of this function, v(?) ? T S, also called exact visible set EVS. Our algorithm is aggressive ([Nirenstein et al. 2002]), i.e., it calculates a potentially visible set PV S ? EV S. Our algorithm can be used to solve the visibility problem in different applications (see Section 5.6). A usage scenario to keep in mind for the following exposition is a visibility preprocessing system for real-time rendering: the view space (set of possible observer locations) is partitioned into view cells. In a preprocessing step, our algorithm is used to calculate and store a PVS for each view cell (note that only its boundary polygons are taken into account, since any ray leaving the view cell can be represented by a ray on the boundary). At runtime, the view cell corresponding to the current observer location is determined, and only the objects in the associated PVS are sent to the graphics hardware, leading to significant savings in rendering time.\n          Point Sample Void Volume Void Surface\n          \n            Figure 4: Representing visibility from a single point. Left: independent samples. Middle: the void volume. Right: the void surface.\n          \n        \n        \n          2.2 Algorithm Overview\n          The algorithms in this paper are based on ray shooting and assume the capability to trace a ray x and compute its first intersection with a scene triangle t ? T S, i.e., to compute the triangle t = v(x) (fast ray tracers include OpenRT [Wald et al. 2003] and the recently presented MLRTA [Reshetov et al. 2005]). The idea of a sampling solution is to select a sequence of rays X = x i , trace the ray and add the triangle v(x i ) to the visibility set PV S. In this paper, we will address the problem on how to sample efficiently, that is how to improve the chances of finding new triangles. We will start with one of the most popular sampling strategies, random sampling (Section 3.1). Then we will show how to use visibility information from previous samples to construct intelligent sampling strategies based on ray mutation to complement random sampling: Adaptive Border Sampling is an algorithm to quickly find nearby triangles by sampling along the borders of triangles previously found to be visible (Section 3.2). Reverse Sampling is an algorithm to sample into regions in space that are likely to be near the boundaries of visible and invisible space, i.e., the void surface (Section 3.3).  In Section 3.4, we will show how to combine the different sampling algorithms in order to obtain guided visibility sampling, a complete hybrid random and deterministic sampling algorithm. The algorithm is called guided because both sampling strategies are guided by visibility information in the scene (see Section 5.4 for a more detailed discussion).\n          495\n        \n      \n      \n        3 Visibility Sampling\n        All rays in the scene form a 5D space. A ray x has a starting point x p (3D) and a direction x dir (2D). A typical visibility query is to give a region R in 3D space and ask what is visible along the rays leaving the region (view cell). While this defines a 5D set of rays, we only need to consider a 4D set of rays in practice; the rays starting at the boundary ? R of the viewing region. Additionally, all triangles intersecting R are classified as visible.\n        \n          3.1 Random Sample Generator\n          The random (or pseudo-random) sampling algorithm selects a sequence of random samples X = x i from the scene. The probability distribution for each new sample p(x i ) is independent of all previous samples x 1 , ..., x n?1 . The question of sampling uniformity for random sampling has been explored in the context of form-factor computation [Sbert 1993]. We sample the position and ray direction uniformly using the following formulae:\n          u = ? 1 , v = ? 2 , ? = 2 ?? 3 , ? = arcsin ? 4 ,\n          where the ? i are independent Halton sequences [Niederreiter 1992], and (u, v) are the normalized coordinates on a view cell face. While random sampling alone suffers from similar inefficiencies as regular sampling (see Section 1), it will be used to seed the more efficient strategies described next.\n        \n        \n          3.2 Adaptive Border Sampling\n          This sampling algorithm is a deterministic ray mutation strategy that covers most of the ground work to make our system successful. This strategy leaves the ray starting point x p on the view cell fixed while covering adjacent triangles in object space, practically constructing a local visibility map [Bittner 2002] from the selected view cell point. The key idea of this sampling strategy is that it adapts the sampling rate to the geometric detail of the surface (see Figure 5 ). Therefore, it is unlikely that subpixel triangles are missed, which is a problem for methods that sample objects regularly. The method performs especially well for the most frequent case of a connected mesh, but does not assume or use any connectivity information. The connected regions are discovered in the random sampling step (therefore, scenes with many small disconnected meshes like trees remain a challenge for the approach). The algorithm proceeds as follows. If a triangle t = (p 1 , p 2 , p 3 ) is hit for the first time by a sample ray x = (x p , x dir ), we enlarge t by a small amount to obtain an enlarged polygon t , and adaptively sample along its edges ( Figure 5 ). For each edge, we use two rays x l and x r , and the corresponding samples hit(x l ) and hit(x r ) in world space. If the rays x l and x r hit different triangles, we recursively subdivide the edge, up to a given threshold. At this point, we also detect depth discontinuities between the new samples and the original sample on the triangle, which is already a part of reverse sampling as described in the next section. The actual method used for border enlargement deserves attention. In order not to miss any adjacent triangles, the border polygon t should be as tight as possible. On the other hand, if it is too tight, t will be hit again due to the numerical precision of ray shooting. If the enlargement were done in object space, this would happen for near edge-on or very distant triangles. We therefore enlarge t in ray space by rotating rays to the vertices of t to their new positions on t by a small angle. This is more robust because it depends neither on the distance of the triangle nor on its orientation, but only on the numerical precision of the ray representation. In practice, this means that for each vertex, the new vertices are put on a plane perpendicular to the ray. The shape of t is chosen so that the ray space distance to t is fairly constant. This is not possible with only 3 vertices, since sliver triangles would lead to singularities. We therefore chose t to be a polygon of 9 vertices. For each vertex p i of t, three vertices p i, j on t are generated. Two vertices are generated each on a vector d i, j perpendicular to the ray and to one of the adjacent edges, respectively. The third is the midpoint of the other two, pushed away from t along the angle bisector d i,i :\n          t' surface t t' d 1,2 x 1,2 t p 1 d d 1,1 1,3 x x 1,1 1,3\n          \n            \n            \n          \n          x p\n          \n            \n            \n            Figure 5: Adaptive border sampling: Top: If we hit a new surface, we sample nearby points on the border polygon t . Bottom: Adaptive subdivision of an edge of t .\n          \n          d i,i+1 = N((p i ? x p ) ? (p i+1 ? p i )) d i,i?1 = N((p i ? x p ) ? (p i ? p i?1 )) d i,i = N(d i,i?1 + d i,i+1 ) if d i,i?1 ? d i,i+1 > 0, else: N((p i ? x p ) ? d i,i?1 + d i,i+1 ? (p i ? x p )) p i, j = p i + ? ? |p i ? x p | ? d i, j\n          where N(v) is the vector normalization operator. d i,i is chosen to be numerically robust. For backfacing triangles, the d i, j need to be inverted. Adaptive border sampling efficiently explores connected visible areas of the input model from a single viewpoint along a 1D curve in ray space (see Section 5.4). However, it cannot penetrate into gaps  visible only from other portions of ray space. This is handled by reverse sampling.\n          496\n          predicted(x) hit(x) x p view cell\n          \n            Figure 6: Reverse Sampling: Left: initial hit on triangle t. Middle: the new ray to predicted(x) is blocked by a much closer triangle t . Right: Reverse sampling mutates the starting point on the view cell so that the ray passes through p new (yellow) and reaches predicted(x).\n          \n        \n        \n          3.3 Reverse Sampling\n          This algorithm is a deterministic mutation strategy that allows penetrating into as yet uncovered regions of space. Note that this cannot be done perfectly: finding the actual void volume is equivalent to the original visibility problem. However, the adaptive sampling process gives good candidate locations for further sampling rays, namely at discontinuity locations. This strategy works by changing the starting point of the ray instead of its direction.  A discontinuity is detected during the adaptive sampling of an edge by comparing the distance of the ray origin to the actual hitpoint |hit(x) ? x p | with the distance to a ?predicted? hitpoint |predicted(x) ? x p |. The predicted hitpoint is just the intersection of the ray x with the plane of the original triangle t. If the new hitpoint is considerably (?) closer, i.e., the ray is obviously occluded by a closer triangle. Note that we do not check the reverse case (jump from closer to farther triangle) as this will be detected when doing adaptive border sampling for the farther triangle. We calculate a mutated ray from a different view cell position to the predicted hitpoint so that it passes by the occluding triangle. For this, the plane p = (x p , hit(x), hit(x old )) is intersected with the newly found triangle (x old is the previous ray from which x was generated). On the intersecting line, we select a point p new which lies just outside of the new triangle. The mutated ray is now constructed with x new,dir = predicted(x) ? p new as direction vector, and x new,p = intersect(viewcell, line(p new , predicted(x)) as origin (see Figure 6 ). If the new ray is not contained in the ray space ? (i.e., it does not intersect the view cell), however, it is discarded. The new ray x new is now treated as independent ray, and the triangle it intersects will be added for adaptive border sampling like any other triangle, but this time with the new view cell origin. For the 2D example in Figure 3 , reverse sampling corresponds to a horizontal movement in ray space.\n          |predicted(x) ? x p | ? |hit(x) ? x p | > ?,\n        \n        \n          3.4 Combining the Different Sampling Algorithms\n          The sampling strategies presented so far can be combined into an extremely efficient guided visibility sampling algorithm. Its two main components are a sample generator for exploring the ray space with independent random samples, and a sampling queue for propagating the ray using adaptive border sampling and reverse sampling. The algorithm is described by the following pseudocode:\n        \n        \n          3.5 Termination criteria\n          Depending on the application requirements, there are several options regarding when to stop casting rays for a view cell: a) a fixed criterion, allocating a number of rays or an amount of time for the computation of each view cell, or b) an adaptive criterion, terminating if the number of newly found triangles per a certain number of samples falls below a threshold, or most preferably, c) a combination of both. A typical example for such a criterion is: stop the iteration when not more than 50 new triangles are found for 1M rays, or when a total of 10M rays has been shot, whichever comes first.\n        \n      \n      \n        4 Results\n        \n          4.1 Overview\n          To compare the efficiency of our algorithm to previous work, we use the following algorithms: GVS, our guided visibility sampling algorithm with adaptive border sampling (ABS) and reverse sampling (RS); and RAND, random sampling (in GVS, a value of epsilon of 5e-5 was used for enlarging triangles). We have dedicated separate subsections to the comparisons with NIR, the main other existing visibility sampling method published by Nirenstein and Blake [2004] (mainly because this algorithm has a slightly different goal than GVS); and EXACT, Bittner?s [2003] exact visibility algorithm. The test scenes selected are (see Figure 7 and Table 1 ): PPLANT, the complete UNC Power Plant model; CITY, a city model of the ancient city of Pompeii generated using the CityEngine [M?ller et al. 2006]; CANYON, a dataset of the Grand Canyon; and CUBES, a simple scene of random cubes. The tests were conducted on an Intel Pentium4 3.2GHz with 4GB of main memory. The graphics card for NIR was an NVIDIA GeForce 7900GTX 512MB.\n          497\n          \n            \n              \n                \n                  \n                     Scene\n                     triangles\n                     size\n                     vc\n                  \n                \n                \n                  \n                     CANYON\n                     2,242,504\n                     10x5x3 km\n                     140x72 m\n                  \n                  \n                     CITY\n                     5,646,041\n                     320x312x9 m\n                     15x2.2 m\n                  \n                  \n                     PPLANT\n                     12,748,510\n                     200x61x81 m\n                     2x1.3 m\n                  \n                  \n                     CUBES\n                     24,000\n                     100x100x100 m\n                     1.5x1.5 m\n                  \n                \n              \n            \n            Scene triangles size vc CANYON 2,242,504 10x5x3 km 140x72 m CITY 5,646,041 320x312x9 m 15x2.2 m PPLANT 12,748,510 200x61x81 m 2x1.3 m CUBES 24,000 100x100x100 m 1.5x1.5 m\n            Table 1: Statistics for all scenes. vc denotes the average size of view cells used in the model.\n          \n          \n            \n            Figure 7:\n          \n          Top left: CITY. Top right: CUBES. Bottom left: PPLANT. Bottom right: CANYON. Inlays: view from a view cell.\n          For GVS and RAND, we used Intel?s multi-level ray tracer (MLRTA [Reshetov et al. 2005]), which allowed sampling rates between about 800K/s and 1200K/s, with peaks up to several million samples/s. The sampling rate depends on the scene type (not so much on the size?PPLANT had a higher sampling rate than CANYON, for example), and on the coherence of the rays (with random samples and ABS samples being faster depending again on the scene). The overhead of the sampling selection process varied between 5 and 15%, depending on the relative distribution of random, ABS and RS rays.\n        \n        \n          4.2 Asymptotic behavior\n          We first analyze the theoretical properties of the algorithms in terms of their sampling behavior, i.e., on a sample-by-sample basis, since this is the only comparison that does not depend on the individual implementation. Since we do not have an exact visibility algorithm that runs in reasonable time on larger scenes, we can only study their asymptotic behavior on a small number of view cells. Figure 8 provides a detailed analysis of the CANYON scene, graphing the pixel error (calculated by counting the false pixels in a large number of random renderings [Nirenstein and Blake 2004]) and the number of triangles found over the number of samples for GVS and RAND. The top left image shows that GVS converges linearly as long as the deterministic strategies (ABS and RS) can be used for most triangles. The black dot on each view cell curve shows when our termination criteria terminates the PVS search (we used 50 or less new triangles found per 1M samples). It can be seen that this happens in a fairly well converged state already. The graph also shows that the behavior is very similar for all view cells. The length of the linear segment only depends on the final PVS size.\n          300 300 triangles 200 250 GVS/CANYON triangles 250 200 5 view cells 150 150 RAND/CANYON thousand 100 50 thousand 100 50 5 view cells 0 0 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 million rays million rays 100 pixel error 40 60 80 5 GVS/CANYON view cells triangles 120 100 140 80 60 GVS CANYON avg 20 thousand 20 40 RAND 0 0 1 3 5 7 9 11 13 15 0 50 100 150 200 million rays million rays\n          \n            Figure 8: Detailed asymptotic analysis for 5 view cells for the CANYON model (see text for details). The pixel errors are measured on a 1000x1000 screen, equivalent to 10 ?4 %. The plots in\n          \n          the lower right image show the blue view cell from the other images.\n          The top right figure shows RAND in comparison. The convergence of RAND looks mainly logarithmic and has a very quick falloff after an initial strong phase. It is especially noteworthy that even at 15M samples, when GVS has already long converged, RAND is still 50K triangles behind GVS for most view cells. The bottom right figure analyzes this behavior on an even larger scale for the dark blue view cell from the other graphs. This figure confirms the quick convergence of GVS, and shows that even after 200M samples, RAND is still several thousand triangles behind GVS. It can be concluded that it would take RAND several orders of magnitude longer to find a PVS that GVS can find with about 7M to 8M samples. Finally, the bottom left figure proves that the PVS size correlates strongly to average pixel error, and that the termination criterion discussed above works well in practice, bringing the average pixel error below 30 pixels on a 1000x1000 screen. Due to the better distribution of initial samples, RAND shows lower average pixel error in the phase where GVS searches mainly deterministically. However, to reach the same pixel errors as provided by GVS in a converged state, RAND has to calculate a similar number of triangles in the PVS, leading to the same observation as before, that similar pixel error requires orders of magnitude more samples than with GVS.\n        \n        \n          4.3 Practical results\n          Next, we demonstrate that these findings generalize to a larger number of scenes, and provide a practical analysis including running times. Table 2 summarizes our findings. We used the same convergence criterion of 50 triangles per 1M samples for GVS, and a constant 150M rays for RAND. It can be seen that this results in very similar average and maximum errors for both algorithms. However, the running times differ by more than an order of magnitude, which reflects the good convergence behavior of GVS with respect to RAND shown above. The table also lists results for NIR, which are discussed in the following subsection. In addition to the error we also give the size of the PVS in terms of the whole model size (an EVS was not available in reasonable time). A higher value means a more accurate solution.\n          498\n          \n            \n              \n                \n                  \n                     Alg.\n                     Avg.Err.\n                     Max.Err.\n                     time\n                     PVS\n                  \n                \n                \n                  \n                     GVS\n                     35\n                     239\n                     7.9s\n                     6.7%\n                  \n                  \n                     RAND\n                     67\n                     828\n                     183s\n                     6.3%\n                  \n                  \n                     NIR512\n                     2,191\n                     8,215\n                     6.8s\n                     5.8%\n                  \n                  \n                     NIR1024\n                     519\n                     2480\n                     11.6s\n                     6.3%\n                  \n                  \n                     GVS\n                     22\n                     230\n                     6.1s\n                     1.1%\n                  \n                  \n                     RAND\n                     70\n                     625\n                     69s\n                     0.4%\n                  \n                  \n                     NIR512\n                     1,292\n                     8,655\n                     5.4s\n                     0.2%\n                  \n                  \n                     NIR1024\n                     631\n                     8,965\n                     8s\n                     0.4%\n                  \n                  \n                     GVS\n                     23\n                     211\n                     30s\n                     0.8%\n                  \n                  \n                     RAND\n                     69\n                     825\n                     129s\n                     0.5%\n                  \n                  \n                     NIR512\n                     3,225\n                     17,169\n                     24s\n                     0.4%\n                  \n                  \n                     NIR1024\n                     1,549\n                     8,317\n                     25.9s\n                     0.6%\n                  \n                \n              \n            \n            Alg. Avg.Err. Max.Err. time PVS GVS 35 239 7.9s 6.7% RAND 67 828 183s 6.3% NIR512 2,191 8,215 6.8s 5.8% NIR1024 519 2480 11.6s 6.3% GVS 22 230 6.1s 1.1% RAND 70 625 69s 0.4% NIR512 1,292 8,655 5.4s 0.2% NIR1024 631 8,965 8s 0.4% GVS 23 211 30s 0.8% RAND 69 825 129s 0.5% NIR512 3,225 17,169 24s 0.4% NIR1024 1,549 8,317 25.9s 0.6%\n            Table 2: Statistics for all scenes averaged over a number of view cells. We used a threshold of 50 or less triangles found per 1M samples to cut off computation for GVS. For RAND, we shot 150M rays for each test. Errors are number of false pixels on a 1000x1000 screen, which corresponds to 10 ?4 %. Results for NIR are given at\n          \n          512x512 and 1024x1024 resolution. The intrinsic parameters had to be adjusted for each scene to obtain reasonable results (see the comments in Section 4.4). The last column shows the average size of the calculated PVS as a percentage of the whole model.\n        \n        \n          4.4 Comparison to hardware sampling\n          Nirenstein and Blake [2004] recently published an interesting adaptive regular sampling algorithm which uses graphics hardware to adaptively sample hemi-cubes on the view cell. It is difficult to directly compare NIR and GVS. On the one hand, they are both based on the same atomic operation?taking a visibility sample. This is because sampling with graphics hardware and with a ray tracer is functionally practically equivalent due to the available sub-pixel precision (usually 12 bit) in current graphics hardware. The time complexity, however, differs significantly between the two algorithms. The time complexity of ray casting is linear in the number of rays and, due to spatial data structures, logarithmic in the number of objects. In practice, we have also observed a strong dependence on the type of the scene and the implementation of the ray tracer, which makes general predictions on the scalability with respect to scene size very hard. For graphics hardware, the basic operation is an item-buffer render. Depending on whether a particular view is mostly fill or geometry limited, the resolution of this item buffer has more or less impact on the rendering time. Our implementation of NIR rendered models from multiple vertex buffers stored directly in video memory, which provided triangle throughput near the theoretical maximum on the card we used (between 130 and 190M triangles/s, depending on how many vertices were shared in the model?note that some vertices had to be duplicated to allow item buffer rendering). Only on the CANYON model did we observe a fill rate limitation (9 vs. 12 hemicubes/s for 512 vs. 1024 resolutions), whereas CITY and PPLANT were geometry limited (7 and 2 hemicubes/s). Efficient acceleration algorithms exist for both architectures, if a certain amount of preprocessing is tolerated. Of particular importance for visibility processing is that the complexity of scenes that can be handled by ray tracing is limited only by the available storage space, as ray casters can work efficiently out of core (e.g., Wald et al. [2004] have demonstrated that a 350 million polygon model can be ray cast at 2-3 frames per second). Furthermore, it should be pointed out that rasterization benefits from hardware acceleration, whereas ray tracing is still run in software. Recent advances in hardware for ray tracing [Woop et al. 2005] promise a huge po tential for improving the speed of sampling-based algorithms like GVS even further, once this technology becomes more commonplace. However, the main difference between the algorithms is the principal goal. NIR aims to increase rendering speed by aggressively culling more objects than are actually occluded, the rationale being that large gains in rendering speed can be obtained if errors in the final image are tolerated. Indeed, NIR consistently underestimates the PVS, as shown in Table 2 (note that even for an error threshold of 0, a significant rest-error is reported for NIR [Nirenstein and Blake 2004]). While this approach is valuable for applications like quick previewing etc., where a resolution can be fixed, and an average of, for example, 1000 false pixels is tolerable, many applications require a more accurate PVS. This is where GVS excels. The GVS algorithm aims to provide the most accurate PVS possible with a minimum number of samples. Therefore, the performance metric for GVS is not the total percentage of culled objects, but the degree to which the actual PVS can be approximated. Our results show that GVS, using a limited number of samples, consistently finds the largest PVS, resulting in average pixel errors below 0.005%. This is important for any visualization application that relies on visibility preprocessing (especially if antialiasing is used or the output resolution is not fixed in advance), but also for a number of other applications where reliable (and practically exact) visibility is required, e.g., computational geometry, GI, and robotics. It should be noted for the results in Table 2 that NIR results are derived through a PVS subdivision threshold, which works differently from the method used in GVS and RAND and can therefore not be compared directly. We found that this threshold was very sensitive to the type of the scene and had to be tuned so as not to lead to excessive subdivision or too early termination in each scene separately (for example, in once case the error for the 1024 resolution was significantly worse than for 512, due to premature termination). The reason for NIR?s inability to pick up the complete PVS lies both in the regular sampling strategy, which forces a very fine subdivision on the view cell in order to pick up sub-pixel triangles, and in the thresholding for the adaptive subdivision, which can prematurely terminate the subdivision.\n        \n        \n          4.5 Comparison to exact visibility\n          We compared our algorithm to EXACT on the CUBES scene, from a view cell of about 1.5x1.5m. EXACT took 19s on a PIV 1.7GHz PC to find 3,743 visible triangles. To find the same number of triangles, GVS required about 3s. For GVS, a screenspace error of 0.001% was already reported after 2s. More interesting, however, is the fact that both GVS and RAND found significantly more visible triangles than EXACT if given enough samples. For example, 3,850 triangles were found after only 15s by GVS. Note that EXACT was used on an ?as is? basis?better results could certainly be achieved by tuning numerical thresholds intrinsic to the method. This shows clearly that the accuracy of visibility algorithms, even exact ones, is ultimately limited by numerical issues.\n        \n      \n      \n        5 Related Work, Discussion and Applications\n        A large volume of research has been devoted to visibility problems due to their importance in computer graphics, computer vision, robotics and other fields. This section compares various as pects of the proposed visibility sampling algorithm to a wider class of from-region visibility algorithms. For a general overview, we can recommend excellent surveys of visibility problems and algorithms [Durand 1999; Cohen-Or et al. 2003]. From-region visibility algorithms are usually classified as exact (potentially visible set PVS = exact visible set EVS), conservative (PVS ? EVS), aggressive (PVS ? EVS), or approximate (PVS ? EVS).\n        499\n        \n          5.1 Exact Visibility\n          Exact solutions to compute visibility from a region in space have been rare [Duguet and Drettakis 2002; Durand 1999], but recently, two algorithms have been published [Nirenstein et al. 2002; Bittner 2003] and further improved upon [Haumont et al. 2005; Mora et al. 2005] that are both exact and work for general scenes. While exact algorithms have been the holy grail of the visibility community for a long time, these two algorithms show that the complexity inherent in the visibility problem may be an obstacle to make exact visibility widely applicable. The high running times and high complexity of implementation are critical, and numerical robustness issues can actually make the solution as approximate as a sampling-based strategy (see [Bittner 2003]). We believe that sampling-based methods and exact methods complement each other, as they have different strengths and weaknesses.\n        \n        \n          5.2 Conservative Visibility\n          Several authors stress the importance of conservative visibility computations, i.e., never underestimating the visible set. Since this problem is almost as hard as the exact visibility problem, practically all published conservative from-region algorithms simplify the problem by imposing certain restrictions on the scene. Typical restrictions are the limitation to 2.5D visibility [Wonka et al. 2000; Bittner et al. 2001; Koltun et al. 2001], architectural scenes [Airey et al. 1990; Teller and S?quin 1991], the restriction to volumetric occluders [Schaufler et al. 2000], or the restriction to larger occluders close to the view cell [Leyvand et al. 2003; Durand et al. 2000]?this last restriction is implied by the nature of the data structures used to store visibility information. While it can be argued that larger occluders can be synthesized from smaller ones [Andujar et al. 2000], this is not possible in general. The guarantee to include all visible geometry in the PVS may be important for some applications, but ultimately, sampling-based methods can be much more successful: 1. As opposed to the published conservative algorithms, they do not make any assumptions about the scene, allowing them to handle a much larger variety of scenes. 2. Due to their ease of implementation and robustness, nonconservative algorithms are more practical for commercial products such as computer games [Aila and Miettinen 2004], and are already used in this context. 3. Numerical issues often make conservative algorithms nonconservative in practice.\n        \n        \n          5.3 Aggressive Visibility\n          Since visibility is such a fundamental problem, general, robust and practical tools are important to complement the specialized algorithms discussed before. These tools are almost universally based  on sampling. The two most popular solutions are to randomly select a large number of rays to sample visibility [Schaufler et al. 2000; Airey et al. 1990; Shade et al. 1998], or to first sample the boundary of the view cell with points and then sample visibility from each of these points [Levoy and Hanrahan 1996; Stuerzlinger 1999]. In the context of view planning for laser range scanners, sampling algorithms exist that store the void surface or the void volume to compute the next-best view [Pito 1999]. A similar algorithm was also used for the generation of textured depth meshes [Wilson and Manocha 2003]. Another option is to shoot rays from the scene triangles towards the view cell [Gotsman et al. 1999], which leads to oversampling of ray space for most scenes. Nirenstein and Blake [2004] were the first to realize the full potential of sampling for visibility computation. They proposed a new approach which uses graphics hardware for sampling. As discussed in Section 4.4, this algorithm aims to reduce the rendering time by culling even visible triangles as long as this does not result in significant rendering error. This is opposed to our algorithm, which always tries to find the best possible approximation of the exact visible set.\n        \n        \n          5.4 Algorithm Analysis\n          Ray space analysis. In the introduction in Figure 3 , we have argued that it is desirable not to sample the ray space regularly. The right image in this figure shows that only an approximately 1D subspace of rays needs to be considered in this simple 2D example. Our new algorithm samples ray space more intelligently: random sampling places initial seed points in ray space to stochastically search for regions in ray space that have not been explored yet. To continue the example for 2D as in the figure, adaptive border sampling corresponds to a vertical expansion in 2D ray space (since the viewpoint remains fixed) which only proceeds into yet unexplored areas. A particular advantage of the adaptive border sampling method is that the sampling rate is adapted to the geometric complexity of the visible surfaces. Reverse sampling, on the other hand, is a movement in the horizontal direction (since the hitpoint remains fixed) in cases where these movements promise to lead to not yet explored regions. For the full 3D case, it is instructive to study our algorithm in terms of the visibility complex [Durand 1999]. The visibility complex describes a partition of the 4D ray space into 4D regions of rays that hit the same object (note that ray space is strictly 4D because we are only interested in rays starting from the view cell). The 3D boundaries of this partition are called tangency volume and consist of rays tangent to scene objects. Samples placed along the object borders therefore correspond to samples near the tangency volume of the object in dual space. Since we keep the viewpoint (2 degrees of freedom) fixed during the deterministic ABS exploration phase, we need to sample a 1D set only. Without ABS, we would ignore the tangency volumes and have to sample the whole 2D subset of ray space defined by the chosen viewpoint. Reverse sampling, on the other hand, looks for lines tangent to two scene edges. In ray space, these lines are near intersections of two tangency volumes. These intersections are called bitangents and are only 2D. For reverse sampling, the viewpoint is allowed to move along a plane (1D), so in total RS also samples a 1D set. The combined ABS and RS strategies therefore correspond to explorations of the 4D ray space along those 1D curves that are most likely to reveal new objects. This explains the high efficiency of the GVS algorithm. Another useful interpretation of the ABS sampling strategy in 3D is based on the visibility map [Bittner 2002]. The visibility map is a structure that contains all visible line segments in a given view. These segments can be characterized mainly as flat and corner (interior edges of a mesh), or shadow (depth discontinuities). The ABS sampling strategy places samples at all edges of the visibility map (without explicitly constructing it). Samples on interior edges of a mesh serve to find connected sets of a mesh (trivially adjacent regions in the visibility complex). Samples at the shadow edges serve to discover depth discontinuities, where objects are partly occluded by other objects. Shadow edges are where the RS sampling strategy is used to refine the sampling (by finding the bitangents in the visibility complex). Accuracy. The term conservative (or even exact) visibility is actually quite misleading. Most algorithms, though conservative in theory, are not conservative in practice due to numerical robustness problems. This is especially true for algorithms relying on graphics hardware. Furthermore, complex algorithms are prone to implementation problems. Due to the much improved sampling efficiency, the magnitude of error introduced by our algorithm is comparable to that of other error sources. Such errors are usually tolerated for conservative algorithms (see Section 4). Other algorithms that are often used in conjunction with visibility processing, like level-of-detail algorithms or shadow mapping, are an additional source of errors. Scene complexity. One distinguishing feature of our samplingbased algorithm is that it can handle arbitrary types of scenes with high overall and visual complexity. It does not rely on occluder synthesis, and depends mostly on the size of the visible set, not on the total scene complexity.\n          500\n        \n        \n          5.5 Limitations and Future Work\n          Although guided visibility sampling generally finds the major part of the PVS very quickly, the fact that it is stochastic on the one hand and guided by the visibility in the scene on the other hand makes the final accuracy dependent on the structure of the scene. Therefore, we cannot give any hard guarantees for the pixel error of the calculated PVS. Also, the ability to explore connected ray space subsets in the far distance is limited by the numerical precision of the ray direction vector. For ABS, this means that triangles that have a solid angle of less than double precision accuracy when seen from the ray origin will most likely be missed.  The worst case of scene complexity is in scenes that consist of a large set of small disconnected triangles, such as forest scenes or synthetic scenes of random triangles. The visibility of such scenes is so complex that even sampling-based solutions will either have high error or take a long time to compute. Still, it is important to point out that sampling-based algorithms are the only ones that are able to even process these scenes. In this respect, an avenue of future work is to incorporate geometric LOD into the sampling framework, similar to the vLOD system proposed by Chhugani et al. [2005]. Geometric LODs could potentially increase the speed of the ray tracer, and make intersection computations more robust because small triangles in the distance get replaced by larger ones. However, robust geometric LOD is not available for all scenes, and integrating LODs into ray tracers is a current topic of research. Furthermore, the error metric used to create the LODs impacts the accuracy of the visibility algorithm and therefore the usable output resolutions.\n        \n        \n          5.6 Applications\n          One important strength of sampling-based methods is their ease of application. We will discuss a number of application scenarios for our algorithm.  Visibility preprocessing for real-time rendering and games. This is the scenario already described in the overview, and one of the most important applications for GVS. For example, the scenes of current computer games are becoming increasingly general, so that special purpose algorithms (cells and portals, and 2.5D solutions) cannot be used anymore, while exact algorithms are difficult to implement and error-prone. GVS can be used in all stages of game development: During level design, the number of rays can be limited so that a coarse solution can be provided almost instantaneously. For the final production, the PVS can be calculated with high accuracy. It is very important to create a PVS that is as close to the EVS as possible and not dependent on a particular output resolution, since the resolution the application will be run at is not known in advance. In addition, antialiasing methods (supersampling and multisampling) use information from subpixel triangles, so that the virtual resolution is even higher. Note that although scenes in computer games are inherently dynamic, the major part of the scene is still static, so huge gains in rendering speeds can be obtained. Furthermore, GVS works on arbitrary polyhedral view cells, so that the view space can be chosen freely. Online and networked visibility. As shown in the results, a reasonable approximation to the EVS with low pixel error can be found in a second or less. Therefore, GVS can be used for online visibility culling by running it on a separate processor or over the network, as described in the Instant Visibility system [Wonka et al. 2001]. In this case, transmitting the PVS on a per-object basis will improve results because it suffices for one triangle of an object to be found by GVS in order to classify the whole object as visible. Furthermore, a small modification to GVS makes the algorithm better suitable to progressive evaluation: instead of interleaving ABS and random samples from the beginning, create a certain number (e.g., 1M) of random samples in a startup phase, and only then use those to seed the ABS rays. This will give a better distribution of samples in the initial phase of the algorithm, since ABS systematically ?flood fills? the PVS around its seed point, and it takes some time until all image regions have been reached. Impostor generation. In many scenes, visibility culling is not sufficient to guarantee a high frame rate everywhere in the model. Therefore, image-based methods can be used to replace complex scene parts by so-called impostors. However, since impostors trade rendering speed against memory consumption, it is important to find the exact visible parts of the scene to avoid wasting impostor memory on invisible geometry [Jeschke et al. 2005]. GVS is ideally suited for this purpose since it provides accurate per-triangle visibility information, so that only those object parts that are actually visible need to be stored in an impostor. Visibility as decision basis. Many practical applications require accurate visibility information as part of a decision making process. Examples include visibility analysis in urban planning (does the new skyscraper impact old town?), military applications (line of sight culling, tactical battlefield management [McDermott and Gelsey 1987]), telecommunications (visibility of emitters), robotics and many more. GVS is advantageous for these problems because it is general purpose and does not have any parameters to tweak, and does not depend on any special properties of the scene.\n          501\n        \n      \n      \n        6 Conclusion\n        We have presented a visibility sampling algorithm to compute a full 3D visibility solution from a region in space. The proposed algorithm improves the efficiency of previous sampling strategies by over two orders of magnitude, thereby allowing visibility solutions with negligible error to be computed in reasonable time. The proposed algorithm works on arbitrary so-called polygon soups and does not require any memory beyond that used by the ray caster. Due to the new sampling strategies employed in the algorithm, its accuracy is competitive even with exact and conservative approaches, while it is also extremely simple to implement. We have provided evidence that Guided Visibility Sampling closes an important gap in visibility research. It combines the speed and ease of implementation of sampling-based and special-purpose conservative algorithms with most of the accuracy of exact solutions. Thus, GVS can be used as a general purpose visibility tool.\n      \n      \n        Acknowledgements\n        We thank Jiri Bittner for fruitful discussions. This research was also supported by the EU in the scope of the GameTools project (IST-2-004363), and by the NGA, grant no. HM1582-05-1-2004.\n      \n      \n        References\n        \n          A ILA , T., AND M IETTINEN , V. 2004. dPVS: An occlusion culling system for massive dynamic environments. IEEE Computer Graphics & Applications 24, 2.\n          A IREY , J. M., R OHLF , J. H., AND B ROOKS , J R ., F. P. 1990. Towards image realism with interactive update rates in complex virtual building environments. In Computer Graphics (1990 Symposium on Interactive 3D Graphics), vol. 24, 41?50.\n          A NDUJAR , C., S AONA , C., AND N AVAZO , I. 2000. Lod visibility culling and occluder synthesis. Computer Aided Design 32, 13, 773?783.\n          B ITTNER , J., W ONKA , P., AND W IMMER , M. 2001. Visibility preprocessing for urban scenes using line space subdivision. In Proc. of Pacific Graphics 2001, 276?284.\n          B ITTNER , J. 2002. Efficient construction of visibility maps using approximate occlusion sweep. In SCCG ?02: Proceedings of the 18th spring conference on Computer graphics, 167?175.\n          B ITTNER , J. 2003. Hierarchical Techniques for Visibility Computations. PhD thesis, Czech Technical University in Prague.\n          C HHUGANI , J., P URNOMO , B., K RISHNAN , S., C OHEN , J., V ENKATA SUBRAMANIAN , S., AND J OHNSON , D. S. 2005. vLOD: High-fidelity walkthrough of large virtual environments. IEEE Trans. on Visualization and Computer Graphics 11, 1, 35?47.\n          C OHEN -O R , D., C HRYSANTHOU , Y. L., S ILVA , C. T., AND D URAND , F. 2003. A survey of visibility for walkthrough applications. IEEE Trans. on Visualization and Computer Graphics 9, 3, 412?431.\n          D UGUET , F., AND D RETTAKIS , G. 2002. Robust epsilon visibility. In Proc. ACM SIGGRAPH 2002, 567?575.\n          D URAND , F., D RETTAKIS , G., T HOLLOT , J., AND P UECH , C. 2000. Conservative visibility preprocessing using extended projections. In Proc. ACM SIGGRAPH 2000, 239?248.\n          D URAND , F. 1999. 3D Visibility: Analytical Study and Applications. PhD thesis, Universite Joseph Fourier, Grenoble, France.\n          G OTSMAN , C., S UDARSKY , O., AND F AYMAN , J. 1999. Optimized occlusion culling using five-dimensional subdivision. Computers and Graphics 5, 23, 645?654.\n          H AUMONT , D., M AKINEN  ? , O., AND N IRENSTEIN , S. 2005. A low dimensional framework for exact polygon-to-polygon occlusion queries. In Proc. Eurographics Symposium on Rendering, 211?222.\n          J ESCHKE , S., W IMMER , M., S CHUMANN , H., AND P URGATHOFER , W. 2005. Automatic impostor placement for guaranteed frame rates and low memory requirements. In Proc. of ACM SIGGRAPH Symp. on Interactive 3D Graphics and Games, 103?110.\n          K OLTUN , V., C HRYSANTHOU , Y., AND C OHEN -O R , C.-O. 2001. Hardware-accelerated from-region visibility using a dual ray space. In Rendering Techniques 2001, 205?216.\n          L EVOY , M., AND H ANRAHAN , P. 1996. Light field rendering. In Proc. ACM SIGGRAPH 96, 31?42.\n          L EYVAND , T., S ORKINE , O., AND C OHEN -O R , D. 2003. Ray space factorization for from-region visibility. ACM Transactions on Graphics 22, 3, 595?604.\n          M C D ERMOTT , D., AND G ELSEY , A. 1987. Terrain analysis for tactical situation assessment. In Proceedings Spatial Reasoning and Multi-Sensor Fusion, 420?429.\n          M ORA , F., A VENEAU , L., AND M ? RIAUX , M. 2005. Coherent and exact polygon-to-polygon visibility. In Proceedings of Winter School on Computer Graphics 2005, 87?94.\n          M ULLER  ? , P., W ONKA , P., H AGLER  ? , S., U LMER , A., AND G OOL , L. V. 2006. Procedural modeling of buildings. ACM Transactions on Graphics 25, 3.\n          N IEDERREITER , H. 1992. Random Number Generation and Quasi-Monte Carlo Methods. SIAM Philadelphia.\n          N IRENSTEIN , S., AND B LAKE , E. 2004. Hardware accelerated visibility preprocessing using adaptive sampling. In Rendering Techniques 2004, 207?216.\n          N IRENSTEIN , S., B LAKE , E., AND G AIN , J. 2002. Exact from-region visibility culling. In Rendering Techniques 2002, 191?202.\n          P ITO , R. 1999. A solution to the next best view problem for automated surface acquisition. IEEE Trans. Pattern Anal. Mach. Intell. 21, 10, 1016? 1030.\n          R ESHETOV , A., S OUPIKOV , A., AND H URLEY , J. 2005. Multi-level ray tracing algorithm. ACM Trans. on Graphics 24, 3, 1176?1185.\n          S BERT , M. 1993. An integral geometry method for fast form factor computation. Computer Graphics Forum 12, 3, C409?C420.\n          S CHAUFLER , G., D ORSEY , J., D ECORET , X., AND S ILLION , F. 2000. Conservative volumetric visibility with occluder fusion. In Proc. ACM SIGGRAPH 2000, 229?238.\n          S HADE , J., G ORTLER , S., WEI H E , L., AND S ZELISKI , R. 1998. Layered depth images. In Proc. ACM SIGGRAPH 98, 231?242.\n          S TUERZLINGER , W. 1999. Imaging all visible surfaces. In Proc. Graphics Interface 1999, 115?122.\n          T ELLER , S. J., AND S ? QUIN , C. H. 1991. Visibility preprocessing for interactive walkthroughs. Computer Graphics (Proc. ACM SIGGRAPH 91) 25, 61?69.\n          W ALD , I., P URCELL , T. J., S CHMITTLER , J., B ENTHIN , C., AND S LUSALLEK , P. 2003. Realtime ray tracing and its use for interactive global illumination. In Eurographics State of the Art Reports.\n          W ALD , I., D IETRICH , A., AND S LUSALLEK , P. 2004. An interactive outof-core rendering framework for visualizing massively complex models. In Rendering Techniques 2004, 81?92.\n          W ILSON , A., AND M ANOCHA , D. 2003. Simplifying complex environments using incremental textured depth meshes. ACM Transactions on Graphics 22, 3, 678?688.\n          W ONKA , P., W IMMER , M., AND S CHMALSTIEG , D. 2000. Visibility preprocessing with occluder fusion for urban walkthroughs. In Rendering Techniques 2000, 71?82.\n          W ONKA , P., W IMMER , M., AND S ILLION , F. 2001. Instant visibility. Computer Graphics Forum 20, 3, 411?421.\n          W OOP , S., S CHMITTLER , J., AND S LUSALLEK , P. 2005. RPU: a programmable ray processing unit for realtime ray tracing. ACM Transactions on Graphics 24, 3, 434?444.\n        \n        502\n      \n    \n  ",
  "resources" : [ ]
}