{
  "uri" : "sig2012-a51-lehtinen_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a51-lehtinen_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Reconstructing the Indirect Light Field for Global Illumination",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Jaakko-Lehtinen",
      "name" : "Jaakko",
      "surname" : "Lehtinen"
    }, {
      "uri" : "http://drinventor/Timo-Aila",
      "name" : "Timo",
      "surname" : "Aila"
    }, {
      "uri" : "http://drinventor/Samuli-Laine",
      "name" : "Samuli",
      "surname" : "Laine"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "when", "generate", "digital", "image", "simulate", "light", "transport", "each", "pixel", "compute", "integral", "multidimensional", "radiance", "function", "whose", "dimension", "range", "over", "image", "xy", "lens", "-lrb-", "depth", "field", "-rrb-", "time", "-lrb-", "motion", "blur", "-rrb-", "incident", "angle", "-lrb-", "indirect", "illumination", "glossy", "reflection", "-rrb-", "light", "source", "-lrb-", "area", "lighting", "-rrb-", "integral", "typically", "compute", "use", "Monte", "Carlo", "sampling", "evaluate", "indirect", "illumination", "entail", "sampling", "radiance", "over", "hemisphere", "each", "visible", "point", "notoriously", "prone", "noise", "due", "high", "variance", "integrand", "cause", "visibility", "texture", "illumination", "we", "seek", "generate", "high-quality", "image", "feature", "glossy", "reflection", "indirect", "illumination", "intelligently", "reuse", "sample", "draw", "stochastic", "renderer", "only", "few", "sample", "per", "pixel", "we", "cast", "problem", "reconstruct", "indirect", "light", "field", "shade", "point", "base", "sample", "record", "during", "initial", "coarse", "sampling", "step", "reconstructed", "light", "field", "use", "standard", "brdf", "sampling", "step", "generate", "final", "image", "we", "assume", "proper", "importance", "sampling", "apply", "require", "control", "over", "how", "sample", "generate", "throughout", "paper", "we", "assume", "direct", "illumination", "compute", "use", "other", "technique", "we", "build", "recent", "work", "have", "provide", "solid", "understanding", "local", "anisotropy", "exhibit", "temporal", "light", "field", "-lsb-", "Durand", "et", "al.", "2005", "Ramamoorthi", "et", "al.", "2007", "Egan", "et", "al.", "2009", "Soler", "et", "al.", "2009", "Egan", "et", "al.", "2011b", "-rsb-", "while", "previous", "sampling", "scheme", "anisotropic", "reconstruction", "filter", "can", "capture", "essence", "Input", "-lrb-", "spp", "-rrb-", "we", "reconstruction", "path", "trace", "-lrb-", "512", "spp", "-rrb-", "integrand", "better", "than", "naive", "point", "sampling", "situation", "motion", "blur", "depth", "field", "area", "lighting", "case", "general", "indirect", "illumination", "remain", "unsolved", "due", "complex", "visibility", "highly", "irregular", "input", "sampling", "glossy", "material", "we", "contribute", "method", "adapt", "sampling", "rate", "nonuniform", "sparse", "uncontrolled", "input", "both", "space", "angle", "spatio-angular", "anisotropic", "reconstruction", "method", "filter", "radiance", "from", "sparse", "sample", "robust", "method", "reasoning", "about", "occlusion", "base", "only", "input", "ray", "contrast", "large", "body", "previous", "work", "aim", "reduce", "noise", "global", "illumination", "we", "do", "utilize", "scene", "geometry", "reconstruction", "pass", "give", "we", "crucial", "advantage", "performance", "only", "weakly", "dependent", "scene", "sense", "we", "algorithm", "can", "see", "advanced", "image", "filter", "we", "algorithm", "produce", "megapixel", "image", "glossy", "reflection", "global", "illumination", "ambient", "occlusion", "few", "minute", "result", "dramatically", "higher", "quality", "than", "possible", "use", "earlier", "reconstruction", "method", "comparable", "use", "hundred", "path", "per", "pixel", "-lrb-", "figure", "-rrb-", "both", "visually", "term", "psnr", "anisotropic", "reconstruction", "image", "synthesis", "involve", "integrate", "high-dimensional", "function", "across", "image", "auxiliary", "dimension", "-lrb-", "time", "lens", "incident", "angle", "etc.", "-rrb-", "body", "recent", "work", "concentrate", "anisotropy", "i.e.", "fact", "signal", "often", "vary", "slowly", "along", "certain", "non-axis-aligned", "subspace", "fourier", "analysis", "elegantly", "reveal", "signal", "locally", "effectively", "lower", "dimension", "case", "-lsb-", "Durand", "et", "al.", "2005", "Egan", "et", "al.", "2009", "-rsb-", "mean", "sample", "draw", "one", "pixel", "tell", "we", "something", "about", "integrand", "other", "pixel", "well", "which", "enable", "reconstruction", "better", "image", "from", "same", "sample", "recent", "trend", "anisotropic", "reconstruction", "can", "trace", "back", "multidimensional", "adaptive", "sampling", "reconstruction", "-lrb-", "mda", "-rrb-", "algorithm", "Hachisuka", "et", "al.", "-lsb-", "2008", "-rsb-", "while", "numerous", "earlier", "technique", "have", "address", "generation", "sample", "accord", "expect", "variance", "integrand", "-lrb-", "importance", "sampling", "adaptive", "sampling", "method", "-rrb-", "Hachisuka", "et", "al.", "concentrate", "example", "4d", "integrand", "over", "image", "lens", "correspond", "diffuse", "fronto-parallel", "planar", "object", "out", "focus", "actually", "only", "two-dimensional", "author", "measure", "local", "anisotropy", "from", "sample", "use", "structure", "tensor", "use", "locally", "warp", "distance", "metric", "use", "reconstruct", "integrand", "use", "highdimensional", "k-nearest-neighbor", "search", "-lrb-", "k-nn", "-rrb-", "can", "yield", "significant", "benefit", "unfortunately", "noise", "texture", "may", "confuse", "anisotropy", "estimator", "Egan", "et", "al.", "-lsb-", "2009", "-rsb-", "analyze", "anisotropy", "spacetime", "integrand", "motion", "blur", "from", "first", "principle", "through", "fourier", "analysis", "use", "prediction", "local", "spectrum", "drive", "adaptive", "sampling", "sheared", "filter", "allow", "sharing", "sample", "between", "pixel", "adapt", "predict", "spectrum", "similar", "analysis", "algorithm", "be", "present", "soft", "shadow", "directional", "occlusion", "same", "author", "-lsb-", "Egan", "et", "al.", "2011b", "Egan", "et", "al.", "2011a", "-rsb-", "while", "analysis", "successful", "drive", "adaptive", "sampling", "sheared", "reconstruction", "filter", "less", "efficient", "when", "direction", "anisotropy", "change", "either", "continuously", "discontinuously", "across", "image", "issue", "evident", "e.g.", "occlusion", "discontinuity", "force", "fallback", "QMC", "sampling", "issue", "address", "Lehtinen", "et", "al.", "-lsb-", "2011", "-rsb-", "whose", "algorithm", "account", "both", "individual", "anisotropy", "information", "each", "sample", "discontinuity", "due", "visibility", "produce", "high-quality", "result", "simultaneous", "motion", "blur", "depth", "field", "area", "lighting", "goal", "present", "work", "extend", "approach", "more", "challenging", "unstructured", "case", "indirect", "illumination", "sample-based", "algorithm", "several", "technique", "e.g.", "anisotropic", "diffusion", "-lsb-", "McCool", "1999", "-rsb-", "-lrb-", "cascade", "-rrb-", "cross-bilateral", "filter", "-lrb-", "-rrb-", "-lsb-", "Dammertz", "et", "al.", "2010", "-rsb-", "post-process", "sample", "pixel", "generate", "initial", "render", "pass", "reduce", "noise", "Sen", "Darabi", "-lsb-", "2012", "-rsb-", "improve", "quality", "cross-bilateral", "filter", "compute", "weight", "adaptively", "each", "pixel", "base", "apparent", "dependency", "between", "domain", "variable", "scene", "feature", "sample", "color", "while", "method", "can", "example", "detect", "sample", "some", "pixel", "heavily", "affect", "time", "do", "know", "magnitude", "direction", "motion", "Overbeck", "et", "al.", "-lsb-", "2009", "-rsb-", "Rousselle", "et", "al.", "-lsb-", "2011", "-rsb-", "describe", "method", "maintain", "basis-representation", "image", "adaptively", "request", "more", "sample", "from", "renderer", "area", "exhibit", "high", "variance", "shade", "reuse", "several", "algorithm", "reuse", "shade", "result", "radiance", "irradiance", "while", "determine", "visibility", "actual", "scene", "-lsb-", "Ward", "et", "al.", "1988", "Bala", "et", "al.", "1999", "Bekaert", "et", "al.", "2002", "Gassenbauer", "et", "al.", "2009", "-rsb-", "sampling", "often", "adaptive", "drive", "error", "heuristic", "contrast", "we", "do", "adapt", "sampling", "instead", "post-process", "sample", "give", "we", "separate", "renderer", "run", "lightor", "scene-dependent", "preprocessing", "stage", "do", "require", "scene", "resident", "reconstruction", "phase", "make", "we", "algorithm", "much", "less", "intrusive", "original", "renderer", "geometric", "resampling", "we", "method", "reasoning", "about", "occlusion", "base", "sample", "from", "initial", "sampling", "have", "also", "connection", "ray", "trace", "point-based", "geometry", "-lsb-", "schaufler", "Wann", "Jensen", "2000", "Christensen", "2008", "Ritschel", "et", "al.", "2009", "-rsb-", "algorithm", "convert", "scene", "point-based", "representation", "preprocess", "compute", "solution", "use", "representation", "contrast", "we", "obtain", "sparse", "set", "path", "segment", "from", "renderer", "-lrb-", "which", "use", "whatever", "representation", "internally", "-rrb-", "treat", "segment", "sample", "indirect", "light", "field", "upsample", "solution", "accounting", "angular", "effect", "gloss", "photon", "vpl", "some", "method", "generate", "cloud", "point", "light", "-lrb-", "call", "photon", "virtual", "point", "light", "vpl", "-rrb-", "represent", "entire", "indirect", "light", "field", "-lsb-", "Wann", "Jensen", "1996", "Keller", "1997", "Walter", "et", "al.", "2005", "-rsb-", "can", "see", "resampling", "tegrand", "form", "more", "amenable", "final", "gathering", "original", "scene", "still", "necessary", "visibility", "determination", "when", "generate", "digital", "image", "simulate", "light", "transport", "each", "pixel", "compute", "integral", "multidimensional", "radiance", "function", "whose", "dimension", "range", "over", "image", "xy", "lens", "-lrb-", "depth", "field", "-rrb-", "time", "-lrb-", "motion", "blur", "-rrb-", "incident", "angle", "-lrb-", "indirect", "illumination", "glossy", "reflection", "-rrb-", "light", "source", "-lrb-", "area", "lighting", "-rrb-", "integral", "typically", "compute", "use", "Monte", "Carlo", "sampling", "evaluate", "indirect", "illumination", "entail", "sampling", "radiance", "over", "hemisphere", "each", "visible", "point", "notoriously", "prone", "noise", "due", "high", "variance", "integrand", "cause", "visibility", "texture", "illumination", "we", "seek", "generate", "high-quality", "image", "feature", "glossy", "reflection", "indirect", "illumination", "intelligently", "reuse", "sample", "draw", "stochastic", "renderer", "only", "few", "sample", "per", "pixel", "we", "cast", "problem", "reconstruct", "indirect", "light", "field", "shade", "point", "base", "sample", "record", "during", "initial", "coarse", "sampling", "step", "reconstructed", "light", "field", "use", "standard", "brdf", "sampling", "step", "generate", "final", "image", "we", "assume", "proper", "importance", "sampling", "apply", "require", "control", "over", "how", "sample", "generate", "throughout", "paper", "we", "assume", "direct", "illumination", "compute", "use", "other", "technique", "we", "build", "recent", "work", "have", "provide", "solid", "understanding", "local", "anisotropy", "exhibit", "temporal", "light", "field", "-lsb-", "Durand", "et", "al.", "2005", "Ramamoorthi", "et", "al.", "2007", "Egan", "et", "al.", "2009", "Soler", "et", "al.", "2009", "Egan", "et", "al.", "2011b", "-rsb-", "while", "previous", "sampling", "scheme", "anisotropic", "reconstruction", "filter", "can", "capture", "essence", "Input", "-lrb-", "spp", "-rrb-", "we", "reconstruction", "path", "trace", "-lrb-", "512", "spp", "-rrb-", "integrand", "better", "than", "naive", "point", "sampling", "situation", "motion", "blur", "depth", "field", "area", "lighting", "case", "general", "indirect", "illumination", "remain", "unsolved", "due", "complex", "visibility", "highly", "irregular", "input", "sampling", "glossy", "material", "we", "contribute", "method", "adapt", "sampling", "rate", "nonuniform", "sparse", "uncontrolled", "input", "both", "space", "angle", "spatio-angular", "anisotropic", "reconstruction", "method", "filter", "radiance", "from", "sparse", "sample", "robust", "method", "reasoning", "about", "occlusion", "base", "only", "input", "ray", "contrast", "large", "body", "previous", "work", "aim", "reduce", "noise", "global", "illumination", "we", "do", "utilize", "scene", "geometry", "reconstruction", "pass", "give", "we", "crucial", "advantage", "performance", "only", "weakly", "dependent", "scene", "sense", "we", "algorithm", "can", "see", "advanced", "image", "filter", "we", "algorithm", "produce", "megapixel", "image", "glossy", "reflection", "global", "illumination", "ambient", "occlusion", "few", "minute", "result", "dramatically", "higher", "quality", "than", "possible", "use", "earlier", "reconstruction", "method", "comparable", "use", "hundred", "path", "per", "pixel", "-lrb-", "figure", "-rrb-", "both", "visually", "term", "psnr", "anisotropic", "reconstruction", "image", "synthesis", "involve", "integrate", "high-dimensional", "function", "across", "image", "auxiliary", "dimension", "-lrb-", "time", "lens", "incident", "angle", "etc.", "-rrb-", "body", "recent", "work", "concentrate", "anisotropy", "i.e.", "fact", "signal", "often", "vary", "slowly", "along", "certain", "non-axis-aligned", "subspace", "fourier", "analysis", "elegantly", "reveal", "signal", "locally", "effectively", "lower", "dimension", "case", "-lsb-", "Durand", "et", "al.", "2005", "Egan", "et", "al.", "2009", "-rsb-", "mean", "sample", "draw", "one", "pixel", "tell", "we", "something", "about", "integrand", "other", "pixel", "well", "which", "enable", "reconstruction", "better", "image", "from", "same", "sample", "recent", "trend", "anisotropic", "reconstruction", "can", "trace", "back", "multidimensional", "adaptive", "sampling", "reconstruction", "-lrb-", "mda", "-rrb-", "algorithm", "Hachisuka", "et", "al.", "-lsb-", "2008", "-rsb-", "while", "numerous", "earlier", "technique", "have", "address", "generation", "sample", "accord", "expect", "variance", "integrand", "-lrb-", "importance", "sampling", "adaptive", "sampling", "method", "-rrb-", "Hachisuka", "et", "al.", "concentrate", "example", "4d", "integrand", "over", "image", "lens", "correspond", "diffuse", "fronto-parallel", "planar", "object", "out", "focus", "actually", "only", "two-dimensional", "author", "measure", "local", "anisotropy", "from", "sample", "use", "structure", "tensor", "use", "locally", "warp", "distance", "metric", "use", "reconstruct", "integrand", "use", "highdimensional", "k-nearest-neighbor", "search", "-lrb-", "k-nn", "-rrb-", "can", "yield", "significant", "benefit", "unfortunately", "noise", "texture", "may", "confuse", "anisotropy", "estimator", "Egan", "et", "al.", "-lsb-", "2009", "-rsb-", "analyze", "anisotropy", "spacetime", "integrand", "motion", "blur", "from", "first", "principle", "through", "fourier", "analysis", "use", "prediction", "local", "spectrum", "drive", "adaptive", "sampling", "sheared", "filter", "allow", "sharing", "sample", "between", "pixel", "adapt", "predict", "spectrum", "similar", "analysis", "algorithm", "be", "present", "soft", "shadow", "directional", "occlusion", "same", "author", "-lsb-", "Egan", "et", "al.", "2011b", "Egan", "et", "al.", "2011a", "-rsb-", "while", "analysis", "successful", "drive", "adaptive", "sampling", "sheared", "reconstruction", "filter", "less", "efficient", "when", "direction", "anisotropy", "change", "either", "continuously", "discontinuously", "across", "image", "issue", "evident", "e.g.", "occlusion", "discontinuity", "force", "fallback", "QMC", "sampling", "issue", "address", "Lehtinen", "et", "al.", "-lsb-", "2011", "-rsb-", "whose", "algorithm", "account", "both", "individual", "anisotropy", "information", "each", "sample", "discontinuity", "due", "visibility", "produce", "high-quality", "result", "simultaneous", "motion", "blur", "depth", "field", "area", "lighting", "goal", "present", "work", "extend", "approach", "more", "challenging", "unstructured", "case", "indirect", "illumination", "sample-based", "algorithm", "several", "technique", "e.g.", "anisotropic", "diffusion", "-lsb-", "McCool", "1999", "-rsb-", "-lrb-", "cascade", "-rrb-", "cross-bilateral", "filter", "-lrb-", "-rrb-", "-lsb-", "Dammertz", "et", "al.", "2010", "-rsb-", "post-process", "sample", "pixel", "generate", "initial", "render", "pass", "reduce", "noise", "Sen", "Darabi", "-lsb-", "2012", "-rsb-", "improve", "quality", "cross-bilateral", "filter", "compute", "weight", "adaptively", "each", "pixel", "base", "apparent", "dependency", "between", "domain", "variable", "scene", "feature", "sample", "color", "while", "method", "can", "example", "detect", "sample", "some", "pixel", "heavily", "affect", "time", "do", "know", "magnitude", "direction", "motion", "Overbeck", "et", "al.", "-lsb-", "2009", "-rsb-", "Rousselle", "et", "al.", "-lsb-", "2011", "-rsb-", "describe", "method", "maintain", "basis-representation", "image", "adaptively", "request", "more", "sample", "from", "renderer", "area", "exhibit", "high", "variance", "shade", "reuse", "several", "algorithm", "reuse", "shade", "result", "radiance", "irradiance", "while", "determine", "visibility", "actual", "scene", "-lsb-", "Ward", "et", "al.", "1988", "Bala", "et", "al.", "1999", "Bekaert", "et", "al.", "2002", "Gassenbauer", "et", "al.", "2009", "-rsb-", "sampling", "often", "adaptive", "drive", "error", "heuristic", "contrast", "we", "do", "adapt", "sampling", "instead", "post-process", "sample", "give", "we", "separate", "renderer", "run", "lightor", "scene-dependent", "preprocessing", "stage", "do", "require", "scene", "resident", "reconstruction", "phase", "make", "we", "algorithm", "much", "less", "intrusive", "original", "renderer", "geometric", "resampling", "we", "method", "reasoning", "about", "occlusion", "base", "sample", "from", "initial", "sampling", "have", "also", "connection", "ray", "trace", "point-based", "geometry", "-lsb-", "schaufler", "Wann", "Jensen", "2000", "Christensen", "2008", "Ritschel", "et", "al.", "2009", "-rsb-", "algorithm", "convert", "scene", "point-based", "representation", "preprocess", "compute", "solution", "use", "representation", "contrast", "we", "obtain", "sparse", "set", "path", "segment", "from", "renderer", "-lrb-", "which", "use", "whatever", "representation", "internally", "-rrb-", "treat", "segment", "sample", "indirect", "light", "field", "upsample", "solution", "accounting", "angular", "effect", "gloss", "photon", "vpl", "some", "method", "generate", "cloud", "point", "light", "-lrb-", "call", "photon", "virtual", "point", "light", "vpl", "-rrb-", "represent", "entire", "indirect", "light", "field", "-lsb-", "Wann", "Jensen", "1996", "Keller", "1997", "Walter", "et", "al.", "2005", "-rsb-", "can", "see", "resampling", "tegrand", "form", "more", "amenable", "final", "gathering", "original", "scene", "still", "necessary", "visibility", "determination" ],
  "content" : "When generating digital images by simulating light transport, each pixel is computed as an integral of a multidimensional radiance function whose dimensions range over image xy, the lens (depth of field), time (motion blur), incident angle (indirect illumination and glossy reflection), and light source (area lighting). Such integrals are typically computed using Monte Carlo sampling. Evaluating the indirect illumination entails sampling the radiance over the hemisphere at each visible point, and is notoriously prone to noise due to the high variance of the integrand caused by visibility, texture, and illumination. We seek to generate high-quality images featuring glossy reflections and indirect illumination by intelligently reusing the samples drawn by a stochastic renderer at only a few samples per pixel. We cast the problem as that of reconstructing an indirect light field at shading points, based on samples recorded during an initial coarse sampling step. The reconstructed light field is then used in a standard BRDF sampling step to generate the final image. We assume proper importance sampling is applied, but require no control over how the samples are generated. Throughout the paper, we assume direct illumination is computed using other techniques. We build on recent work that has provided a solid understanding of the local anisotropies exhibited by the temporal light field [Durand et al. 2005; Ramamoorthi et al. 2007; Egan et al. 2009; Soler et al. 2009; Egan et al. 2011b]. But while previous sampling schemes and anisotropic reconstruction filters can capture the essence of the Input (8 spp) Our reconstruction Path tracing (512 spp) integrand better than naive point sampling in situations such as motion blur, depth of field, and area lighting, the case for general indirect illumination remains unsolved due to complex visibility, highly irregular input sampling, and glossy materials. We contribute a method for adapting to the sampling rate of nonuniform, sparse, uncontrolled input in both space and angle, a spatio-angular anisotropic reconstruction method for filtering radiance from the sparse samples, and a robust method for reasoning about occlusion based on only the input rays. In contrast to a large body of previous work aiming to reduce noise in global illumination, we do not utilize the scene geometry in the reconstruction pass. This gives us the crucial advantage that performance is only weakly dependent on the scene. In this sense, our algorithm can be seen as an advanced image filter. Our algorithm produces megapixel images with glossy reflection, global illumination, and ambient occlusion in a few minutes. The results are of dramatically higher quality than was possible using earlier reconstruction methods, and comparable to using hundreds of paths per pixel ( Figure 1 ) both visually and in terms of PSNR. Anisotropic reconstruction Image synthesis involves integrating a high-dimensional function across the image and auxiliary dimensions (time, lens, incident angle, etc.). A body of recent work concentrates on anisotropy, i.e., the fact that the signal often varies slowly along certain non-axis-aligned subspaces. Fourier analysis elegantly reveals that the signal is locally effectively of a lower dimension 1 in these cases [Durand et al. 2005; Egan et al. 2009]. This means that samples drawn at one pixel tell us something about the integrand at other pixels as well, which enables reconstruction of better images from the same samples. The recent trend in anisotropic reconstruction can be traced back to the Multidimensional Adaptive Sampling and Reconstruction (MDAS) algorithm of Hachisuka et al. [2008]. While numerous earlier techniques had addressed the generation of samples according to the expected variance in the integrand (importance sampling and adaptive sampling methods), Hachisuka et al. concentrated on\n      1 For example, the 4D integrand over the image and the lens that corresponds to a diffuse fronto-parallel planar object that is out of focus is actually only two-dimensional. The authors measure the local anisotropy from the samples using a structure tensor, and use it for locally warping the distance metric used for reconstructing the integrand using a highdimensional k-nearest-neighbor search (k-NN). This can yield significant benefits, but unfortunately, noise and texture may confuse the anisotropy estimator. Egan et al. [2009] analyzed the anisotropy in the spacetime integrand of motion blur from first principles through Fourier analysis, and used predictions of the local spectra for driving adaptive sampling. A sheared filter that allows sharing of samples between pixels was then adapted to the predicted spectra. Similar analysis and algorithms were presented for soft shadows and directional occlusion by the same authors [Egan et al. 2011b; Egan et al. 2011a]. While the analysis is successful in driving adaptive sampling, the sheared reconstruction filter is less efficient when the direction of anisotropy changes, either continuously or discontinuously, across the image. This issue is evident, e.g., in occlusion discontinuities, and force a fallback to QMC sampling. These issues are addressed by Lehtinen et al. [2011], whose algorithm accounts for both the individual anisotropy information of each sample and discontinuities due to visibility, producing high-quality results for simultaneous motion blur, depth of field, and area lighting. The goal of the present work is to extend this approach to the more challenging and unstructured case of indirect illumination. Sample-based algorithms Several techniques, e.g., anisotropic diffusion [McCool 1999] or a (cascade of) cross-bilateral filter(s) [Dammertz et al. 2010], post-process the samples or pixels generated in an initial rendering pass to reduce noise. Sen and Darabi [2012] improve the quality of cross-bilateral filters by computing the weights adaptively for each pixel based on apparent dependencies between domain variables, scene features, and sample colors. While their method can, for example, detect that samples in some pixels are heavily affected by time, it does not know the magnitude or direction of the motion. Overbeck et al. [2009] and Rousselle et al. [2011] describe methods that maintain a basis-representation of the image and adaptively request more samples from the renderer to the areas that exhibit high variance. Shading reuse Several algorithms reuse shading results, radiance or irradiance, while determining visibility with the actual scene [Ward et al. 1988; Bala et al. 1999; Bekaert et al. 2002; Gassenbauer et al. 2009]. Sampling is often adaptive and driven by an error heuristic. In contrast, we do not adapt the sampling but instead post-process the samples given to us by a separate renderer, run no lightor scene-dependent preprocessing stages, and do not require the scene to be resident in the reconstruction phase. This makes our algorithm much less intrusive for the original renderer. Geometric resampling Our method for reasoning about occlusion based on samples from the initial sampling has also connections to ray tracing point-based geometry [Schaufler and Wann Jensen 2000; Christensen 2008; Ritschel et al. 2009]. These algorithms convert the scene to a point-based representation as a preprocess, and then compute the solution using that representation. In contrast, we obtain a sparse set of path segments from a renderer (which uses whatever representation internally), treat the segments as samples of the indirect light field, and upsample the solution accounting for angular effects of gloss. Photons and VPLs Some methods generate a cloud of point lights (called photons or virtual point lights or VPLs) that represents the entire indirect light field [Wann Jensen 1996; Keller 1997; Walter et al. 2005]. They can be seen as a resampling of the in- tegrand to a form more amenable to final gathering. The original scene is still necessary for visibility determination. When generating digital images by simulating light transport, each pixel is computed as an integral of a multidimensional radiance function whose dimensions range over image xy, the lens (depth of field), time (motion blur), incident angle (indirect illumination and glossy reflection), and light source (area lighting). Such integrals are typically computed using Monte Carlo sampling. Evaluating the indirect illumination entails sampling the radiance over the hemisphere at each visible point, and is notoriously prone to noise due to the high variance of the integrand caused by visibility, texture, and illumination. We seek to generate high-quality images featuring glossy reflections and indirect illumination by intelligently reusing the samples drawn by a stochastic renderer at only a few samples per pixel. We cast the problem as that of reconstructing an indirect light field at shading points, based on samples recorded during an initial coarse sampling step. The reconstructed light field is then used in a standard BRDF sampling step to generate the final image. We assume proper importance sampling is applied, but require no control over how the samples are generated. Throughout the paper, we assume direct illumination is computed using other techniques. We build on recent work that has provided a solid understanding of the local anisotropies exhibited by the temporal light field [Durand et al. 2005; Ramamoorthi et al. 2007; Egan et al. 2009; Soler et al. 2009; Egan et al. 2011b]. But while previous sampling schemes and anisotropic reconstruction filters can capture the essence of the Input (8 spp) Our reconstruction Path tracing (512 spp) integrand better than naive point sampling in situations such as motion blur, depth of field, and area lighting, the case for general indirect illumination remains unsolved due to complex visibility, highly irregular input sampling, and glossy materials. We contribute a method for adapting to the sampling rate of nonuniform, sparse, uncontrolled input in both space and angle, a spatio-angular anisotropic reconstruction method for filtering radiance from the sparse samples, and a robust method for reasoning about occlusion based on only the input rays. In contrast to a large body of previous work aiming to reduce noise in global illumination, we do not utilize the scene geometry in the reconstruction pass. This gives us the crucial advantage that performance is only weakly dependent on the scene. In this sense, our algorithm can be seen as an advanced image filter. Our algorithm produces megapixel images with glossy reflection, global illumination, and ambient occlusion in a few minutes. The results are of dramatically higher quality than was possible using earlier reconstruction methods, and comparable to using hundreds of paths per pixel ( Figure 1 ) both visually and in terms of PSNR. Anisotropic reconstruction Image synthesis involves integrating a high-dimensional function across the image and auxiliary dimensions (time, lens, incident angle, etc.). A body of recent work concentrates on anisotropy, i.e., the fact that the signal often varies slowly along certain non-axis-aligned subspaces. Fourier analysis elegantly reveals that the signal is locally effectively of a lower dimension 1 in these cases [Durand et al. 2005; Egan et al. 2009]. This means that samples drawn at one pixel tell us something about the integrand at other pixels as well, which enables reconstruction of better images from the same samples. The recent trend in anisotropic reconstruction can be traced back to the Multidimensional Adaptive Sampling and Reconstruction (MDAS) algorithm of Hachisuka et al. [2008]. While numerous earlier techniques had addressed the generation of samples according to the expected variance in the integrand (importance sampling and adaptive sampling methods), Hachisuka et al. concentrated on\n      1 For example, the 4D integrand over the image and the lens that corresponds to a diffuse fronto-parallel planar object that is out of focus is actually only two-dimensional. The authors measure the local anisotropy from the samples using a structure tensor, and use it for locally warping the distance metric used for reconstructing the integrand using a highdimensional k-nearest-neighbor search (k-NN). This can yield significant benefits, but unfortunately, noise and texture may confuse the anisotropy estimator. Egan et al. [2009] analyzed the anisotropy in the spacetime integrand of motion blur from first principles through Fourier analysis, and used predictions of the local spectra for driving adaptive sampling. A sheared filter that allows sharing of samples between pixels was then adapted to the predicted spectra. Similar analysis and algorithms were presented for soft shadows and directional occlusion by the same authors [Egan et al. 2011b; Egan et al. 2011a]. While the analysis is successful in driving adaptive sampling, the sheared reconstruction filter is less efficient when the direction of anisotropy changes, either continuously or discontinuously, across the image. This issue is evident, e.g., in occlusion discontinuities, and force a fallback to QMC sampling. These issues are addressed by Lehtinen et al. [2011], whose algorithm accounts for both the individual anisotropy information of each sample and discontinuities due to visibility, producing high-quality results for simultaneous motion blur, depth of field, and area lighting. The goal of the present work is to extend this approach to the more challenging and unstructured case of indirect illumination. Sample-based algorithms Several techniques, e.g., anisotropic diffusion [McCool 1999] or a (cascade of) cross-bilateral filter(s) [Dammertz et al. 2010], post-process the samples or pixels generated in an initial rendering pass to reduce noise. Sen and Darabi [2012] improve the quality of cross-bilateral filters by computing the weights adaptively for each pixel based on apparent dependencies between domain variables, scene features, and sample colors. While their method can, for example, detect that samples in some pixels are heavily affected by time, it does not know the magnitude or direction of the motion. Overbeck et al. [2009] and Rousselle et al. [2011] describe methods that maintain a basis-representation of the image and adaptively request more samples from the renderer to the areas that exhibit high variance. Shading reuse Several algorithms reuse shading results, radiance or irradiance, while determining visibility with the actual scene [Ward et al. 1988; Bala et al. 1999; Bekaert et al. 2002; Gassenbauer et al. 2009]. Sampling is often adaptive and driven by an error heuristic. In contrast, we do not adapt the sampling but instead post-process the samples given to us by a separate renderer, run no lightor scene-dependent preprocessing stages, and do not require the scene to be resident in the reconstruction phase. This makes our algorithm much less intrusive for the original renderer. Geometric resampling Our method for reasoning about occlusion based on samples from the initial sampling has also connections to ray tracing point-based geometry [Schaufler and Wann Jensen 2000; Christensen 2008; Ritschel et al. 2009]. These algorithms convert the scene to a point-based representation as a preprocess, and then compute the solution using that representation. In contrast, we obtain a sparse set of path segments from a renderer (which uses whatever representation internally), treat the segments as samples of the indirect light field, and upsample the solution accounting for angular effects of gloss. Photons and VPLs Some methods generate a cloud of point lights (called photons or virtual point lights or VPLs) that represents the entire indirect light field [Wann Jensen 1996; Keller 1997; Walter et al. 2005]. They can be seen as a resampling of the in- tegrand to a form more amenable to final gathering. The original scene is still necessary for visibility determination.",
  "resources" : [ ]
}