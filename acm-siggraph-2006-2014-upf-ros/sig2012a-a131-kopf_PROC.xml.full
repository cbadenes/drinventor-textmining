{
  "uri" : "sig2012a-a131-kopf_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012a/a131-kopf_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Quality Prediction for Image Completion",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Johannes-Kopf",
      "name" : "Johannes",
      "surname" : "Kopf"
    }, {
      "uri" : "http://drinventor/Wolf-Kienzle",
      "name" : "Wolf",
      "surname" : "Kienzle"
    }, {
      "uri" : "http://drinventor/Steven M.-Drucker",
      "name" : "Steven M.",
      "surname" : "Drucker"
    }, {
      "uri" : "http://drinventor/Sing Bing-Kang",
      "name" : "Sing Bing",
      "surname" : "Kang"
    } ]
  },
  "bagOfWords" : [ "d75da5d025b3456e121f92f2c0359645c05975c76fd34e46f42cb4bde2957fb4", "p2m", "10.1145", "2366145.2366150", "name", "identification", "possible", "Quality", "Prediction", "image", "Completion", "Johannes", "Kopf", "Wolf", "Kienzle", "Microsoft", "Research", "Microsoft", "Research", "Input", "panorama", "quality", "prediction", "-lrb-", "brighter", "higher", "quality", "-rrb-", "best", "crop", "use", "only", "know", "pixel", "-lrb-", "conservative", "crop", "-rrb-", "Figure", "we", "data-driven", "technique", "capable", "predict", "image", "completion", "quality", "-lrb-", "top", "left", "-rrb-", "before", "completion", "actually", "compute", "-lrb-", "top", "right", "-rrb-", "base", "we", "prediction", "we", "compute", "optimal", "crop", "rectangle", "try", "include", "many", "known", "pixel", "possible", "while", "avoid", "low-quality", "region", "-lrb-", "bottom", "right", "-rrb-", "compare", "previous", "crop", "approach", "do", "fill", "-lrb-", "bottom", "left", "-rrb-", "we", "can", "usually", "include", "larger", "amount", "input", "image", "we", "crop", "we", "algorithm", "only", "complete", "crop", "region", "thus", "save", "significant", "amount", "computation", "compare", "full", "completion", "we", "present", "data-driven", "method", "predict", "quality", "image", "completion", "method", "we", "method", "base", "state-of-the-art", "non-parametric", "framework", "Wexler", "et", "al.", "-lsb-", "2007", "-rsb-", "use", "automatically", "derive", "search", "space", "constraint", "patch", "source", "region", "which", "lead", "improve", "texture", "synthesis", "semantically", "more", "plausible", "result", "constraint", "also", "facilitate", "performance", "prediction", "allow", "we", "correlate", "output", "quality", "against", "feature", "possible", "region", "use", "synthesis", "we", "use", "we", "algorithm", "first", "crop", "complete", "stitch", "panorama", "we", "predictive", "ability", "use", "find", "optimal", "crop", "shape", "before", "completion", "compute", "potentially", "save", "significant", "amount", "computation", "we", "optimize", "crop", "include", "much", "original", "panorama", "possible", "while", "avoid", "region", "can", "less", "successfully", "fill", "we", "predictor", "can", "also", "apply", "hole", "filling", "interior", "image", "addition", "extensive", "comparative", "result", "we", "run", "several", "user", "study", "validate", "we", "predictive", "feature", "good", "relative", "quality", "we", "result", "against", "those", "other", "state-of-the-art", "algorithm", "we", "automatic", "crop", "algorithm", "cr", "category", "i.", "4.9", "-lsb-", "image", "processing", "computer", "Vision", "-rsb-", "application", "keyword", "image", "completion", "quality", "prediction", "crop", "Links", "dl", "pdf", "EB", "ACM", "Reference", "Format", "Kopf", "J.", "Kienzle", "W.", "Drucker", "S.", "Kang", "S.", "2012", "Quality", "Prediction", "image", "completion", "ACM", "Trans", "graph", "31", "Article", "131", "-lrb-", "November", "2012", "-rrb-", "page", "dous", "10.1145", "2366145.2366150", "http://doi.acm.org/10.1145/2366145.2366150", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2012", "ACM", "0730-0301/2012", "11-art131", "15.00", "DOI", "10.1145", "2366145.2366150", "http://doi.acm.org/10.1145/2366145.2366150", "Steven", "Drucker", "Sing", "Bing", "Kang", "Microsoft", "Research", "Microsoft", "Research", "full", "completion", "we", "optimize", "crop", "base", "quality", "prediction", "introduction", "image", "completion", "inpainting", "popular", "image", "editing", "tool", "object", "removal", "replacement", "digital", "photograph", "restoration", "variety", "completion", "algorithm", "have", "be", "develop", "scientific", "community", "over", "past", "decade", "image", "completion", "now", "feature", "commercial", "photo", "editing", "software", "Adobe", "Photoshop", "most", "previous", "work", "image", "completion", "use", "fill", "hole", "after", "unwanted", "object", "remove", "same", "algorithm", "however", "can", "also", "use", "extend", "image", "beyond", "its", "original", "boundary", "useful", "fill", "beyond", "irregular", "boundary", "stitch", "panorama?this", "application", "focus", "paper", "casually", "shoot", "panorama", "often", "have", "irregular", "boundary", "-lrb-", "e.g.", "Figure", "-rrb-", "most", "user", "however", "prefer", "output", "image", "rectangular", "boundary", "trivial", "solution", "implement", "most", "stitching", "software", "crop", "largest", "box", "fully", "contain", "within", "panorama", "simple", "method", "often", "remove", "large", "part", "input", "alternative", "apply", "any", "exist", "completion", "algorithm", "fill", "miss", "region", "panorama", "bound", "box", "unfortunately", "all", "exist", "image", "completion", "algorithm", "fail", "occasion", "failure", "typically", "show", "up", "either", "inability", "synthesize", "some", "texture", "well", "result", "semantically", "implausible", "-lrb-", "see", "Figure", "-rrb-", "addition", "difficult", "anticipate", "when", "where", "algorithm", "fail", "give", "arbitrary", "input", "image", "paper", "we", "use", "machine", "learn", "predict", "quality", "image", "completion", "support", "prediction", "we", "design", "we", "image", "completion", "algorithm", "produce", "high", "quality", "result", "allow", "association", "between", "complete", "pixel", "know", "pixel", "create", "we", "build", "exist", "non-parametric", "optimization", "framework", "Wexler", "et", "al.", "-lsb-", "2007", "-rsb-", "-lrb-", "which", "also", "implement", "Content", "Aware", "fill", "feature", "Adobe", "Photoshop", "-rrb-", "previous", "work", "show", "algorithm", "perform", "best", "source", "location", "patch", "constrain", "certain", "area", "e.g.", "see", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "we", "use", "heuristic", "automatically", "derive", "search", "space", "see", "http://www.adobe.com/technology/projects/content-aware-fill.html", "constraint", "base", "overlap", "texture", "segmentation", "constraint", "allow", "we", "design", "simple", "method", "predict", "algorithm", "performance", "base", "prediction", "we", "compute", "crop", "shape", "before", "completion", "actually", "carry", "out", "avoid", "unnecessarily", "complete", "crop", "pixel", "validate", "train", "we", "prediction", "function", "we", "run", "mechanical", "Turk", "user", "study", "obtain", "about", "9,500", "good", "bad", "label", "crop", "from", "complete", "image", "from", "large", "number", "subject", "label", "use", "estimate", "we", "prediction", "function", "via", "crossvalidation", "we", "test", "we", "algorithm", "extensive", "collection", "input", "image", "another", "user", "study", "compare", "we", "result", "those", "various", "state-of-the-art", "completion", "algorithm", "yet", "another", "user", "study", "we", "evaluate", "performance", "we", "automatic", "crop", "optimization", "addition", "example", "include", "paper", "all", "we", "result", "comparison", "include", "supplementary", "material", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "131:2", "J.", "Kopf", "et", "al.", "previous", "work", "while", "substantial", "amount", "previous", "work", "image", "completion", "-lrb-", "also", "refer", "inpainting", "image", "filling", "-rrb-", "best", "we", "knowledge", "we", "aware", "any", "method", "can", "predict", "quality", "before", "completion", "be", "able", "predict", "quality", "would", "allow", "system", "determine", "input", "image", "can", "properly", "restore", "edit", "we", "case", "estimate", "desire", "crop", "panorama", "result", "system", "complete", "only", "what", "need", "note", "exist", "crop", "technique", "-lsb-", "Zhang", "et", "al.", "2005", "-rsb-", "use", "only", "available", "image", "datum", "crop", "texture", "quality", "assessment", "technique", "closest", "approach", "ours", "objective", "function", "Kwatra", "et", "al.", "-lsb-", "2005", "-rsb-", "measure", "similarity", "local", "patch", "target", "texture", "measure", "poor", "predictor", "quality", "-lrb-", "show", "section", "-rrb-", "Swarmy", "et", "al.", "-lsb-", "2011", "-rsb-", "show", "linear", "combination", "image", "parameter", "-lrb-", "intensity", "mean", "variance", "entropy", "band", "information", "-rrb-", "can", "use", "assess", "texture", "quality", "both", "case", "however", "texture", "have", "generate", "first", "we", "now", "briefly", "survey", "representative", "method", "image", "completion", "which", "we", "classify", "primarily", "example-based", "diffusionbased", "Komodakis", "Tziritas", "-lsb-", "2007", "-rsb-", "provide", "excellent", "review", "inpaint", "technique", "diffusion-based", "technique", "often", "refer", "inpainting", "typically", "work", "well", "small", "narrow", "hole", "e.g.", "remove", "scratch", "from", "scan", "old", "photograph", "less", "appropriate", "complete", "stitch", "panorama", "large", "open-ended", "miss", "region", "due", "inability", "synthesize", "texture", "one", "good", "representative", "diffusion-based", "technique", "Bertalmio", "et", "al.", "-lsb-", "2000", "-rsb-", "prolong", "isophote", "line", "miss", "area", "example-based", "technique", "tend", "more", "effective", "fill", "larger", "hole", "than", "diffusion-based", "technique", "Efros", "Leung", "-lsb-", "1999", "-rsb-", "popularize", "use", "non-parametric", "sampling", "texture", "synthesis", "-lrb-", "extension", "image", "completion", "-rrb-", "many", "example-based", "technique", "base", "core", "concept", "Representative", "technique", "include", "block-based", "matching", "structurebased", "priority", "-lsb-", "Criminisi", "et", "al.", "2003", "-rsb-", "use", "hierarchical", "filter", "initialization", "adaptive", "region", "instead", "patch", "-lsb-", "Drori", "et", "al.", "2003", "-rsb-", "optimization", "texture", "energy", "function", "-lsb-", "Wexler", "et", "al.", "2007", "Kwatra", "et", "al.", "2005", "Darabi", "et", "al.", "2012", "-rsb-", "application", "user", "specification", "structure", "-lsb-", "Sun", "et", "al.", "2005", "-rsb-", "mrf", "exemplar", "sample", "label", "-lsb-", "Komodakis", "Tziritas", "2007", "-rsb-", "search", "globally-transformed", "patch", "-lsb-", "Mansfield", "et", "al.", "2011", "-rsb-", "Kawai", "et", "al.", "-lsb-", "2008", "-rsb-", "adapt", "work", "Wexler", "et", "al.", "-lsb-", "2007", "-rsb-", "use", "ssd-based", "objective", "function", "handle", "regular", "-lrb-", "fine-grained", "-rrb-", "texture", "addition", "compensate", "local", "brightness", "change", "linearly", "fitting", "intensity", "match", "patch", "Pritch", "et", "al.", "-lsb-", "2009", "-rsb-", "cast", "problem", "image", "synthesis", "find", "optimal", "shift-map", "pixel", "base", "global", "factor", "-lrb-", "image", "size", "object", "arrangement", "-rrb-", "local", "feature", "-lrb-", "saliency", "map", "-rrb-", "well", "spatial", "regularization", "have", "demonstrate", "technique", "image", "retargeting", "rearrangement", "completion", "many", "previous", "method", "rely", "quickly", "find", "similar", "image", "patch", "recent", "PatchMatch", "algorithm", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "greatly", "improve", "speed", "replace", "previously", "employ", "tree-based", "search", "technique", "much", "faster", "randomize", "algorithm", "practically", "all", "automatic", "technique", "rely", "exemplar", "-lrb-", "exception", "-lsb-", "Matsushita", "et", "al.", "2006", "-rsb-", "-rrb-", "do", "restrict", "search", "result", "technique", "prone", "same", "problem", "those", "-lsb-", "Wexler", "et", "al.", "2007", "-rsb-", "where", "perceptually", "implausible", "patch", "could", "use", "image", "completion", "addition", "be", "design", "performance", "prediction", "mind", "difficult", "anticipate", "degree", "success", "failure", "within", "miss", "region", "Overview", "we", "algorithm", "build", "non-parametric", "optimization", "algorithm", "introduce", "Wexler", "et", "al.", "-lsb-", "2007", "-rsb-", "we", "implement", "optimization", "describe", "paper", "use", "weighted", "average", "update", "rule", "-lrb-", "we", "do", "find", "necessary", "use", "much", "slower", "mean-shift", "base", "update", "rule", "-rrb-", "we", "describe", "more", "implementation", "detail", "algorithm", "supplementary", "document", "we", "bias", "we", "algorithm", "toward", "continue", "image", "content", "near", "boundary", "missing", "region", "each", "miss", "pixel", "may", "only", "fill", "from", "tightly", "constrain", "part", "known", "region", "know", "advance", "where", "every", "miss", "pixel", "may", "come", "from", "enable", "we", "predict", "perceive", "quality", "complete", "result", "use", "training", "datum", "we", "learn", "function", "map", "low-level", "feature", "closest", "known", "image", "region", "perceive", "quality", "complete", "result", "low-level", "feature", "include", "color", "edge", "density", "edge", "orientation", "contour", "length", "region", "size", "we", "prediction", "function", "learn", "validate", "from", "datum", "we", "collect", "mechanical", "Turk", "user", "study", "where", "subject", "be", "ask", "categorize", "random", "patch", "from", "complete", "region", "good", "bad", "next", "section", "we", "review", "optimization", "framework", "we", "algorithm", "base", "describe", "we", "extension", "improve", "result", "quality", "make", "algorithm", "more", "predictable", "image", "Completion", "Algorithm", "we", "algorithm", "minimize", "texture", "energy", "term", "which", "measure", "extent", "which", "synthesize", "region", "deviate", "from", "known", "region", "over", "set", "overlap", "local", "patch", "basic", "form", "energy", "minimization", "problem", "min", "-lcb-", "-rcb-", "??", "where", "set", "center", "pixel", "all", "patch", "completely", "contain", "within", "image", "domain", "overlap", "least", "one", "miss", "pixel", "image", "domain", "we", "mean", "minimum", "bound", "box", "contain", "panorama", "denote", "-lrb-", "target", "-rrb-", "patch", "center", "pixel", "-lrb-", "source", "-rrb-", "patch", "known", "region", "close", "appearance", "energy", "minimize", "iterative", "fashion", "alternate", "between", "minimize", "respect", "set", "while", "other", "set", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "Quality", "prediction", "image", "completion", "131:3", "input", "prediction", "-lrb-", "brighter", "higher", "quality", "-rrb-", "Adobe", "Photoshop", "CS5", "-lrb-", "Content", "Aware", "fill", "-rrb-", "figure", "unguided", "image", "completion", "often", "produce", "semantically", "implausible", "result", "please", "zoom", "pdf", "see", "detail", "note", "artifact", "top", "left", "corner", "near", "bottom", "both", "ours", "photoshop?s", "implementation", "-lsb-", "Wexler", "et", "al.", "2007", "-rsb-", "synthesize", "wrong", "rock", "texture", "snow", "patch", "sky", "previous", "work", "issue", "have", "be", "address", "use", "manual", "search", "space", "constraint", "we", "propose", "fully", "automatic", "algorithm", "derive", "constraint", "we", "result", "while", "artifact", "free", "appear", "more", "plausible", "many", "more", "example", "can", "find", "supplementary", "material", "fix", "minimize", "assignment", "require", "find", "nearest", "neighbor", "each", "synthesize", "patch", "task", "most", "compute-intensive", "require", "use", "approximative", "technique", "keep", "run", "time", "reasonable", "-lrb-", "we", "use", "PatchMatch", "algorithm", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "-rrb-", "minimize", "amount", "simple", "averaging", "overlap", "assignment", "we", "result", "compute", "coarse-to-fine", "fashion", "use", "image", "pyramid", "detail", "please", "refer", "-lsb-", "Wexler", "et", "al.", "2007", "-rsb-", "pseudo-code", "supplementary", "material", "4.1", "Automatic", "search", "space", "constraint", "completion", "algorithm", "prone", "get", "stick", "bad", "local", "minimum", "sensitive", "its", "initial", "state", "most", "successful", "application", "initialization", "already", "close", "final", "result", "image", "retargeting", "-lsb-", "Simakov", "et", "al.", "2008", "-rsb-", "use", "scale", "copy", "input", "while", "image", "reshuffling", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "from", "copy", "known", "region", "both", "case", "amount", "repair", "small", "plain", "image", "completion", "algorithm", "typically", "initialize", "randomly", "which", "make", "quality", "result", "hard", "predict", "without", "random", "initialization", "algorithm", "tend", "exhibit", "quality", "issue", "blurry/incorrect", "texture", "semantically", "implausible", "result", "show", "figure", "certain", "algorithm", "manually", "guide", "toward", "better", "solution", "either", "constrain", "location", "some", "pixel", "-lrb-", "e.g.", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "-rrb-", "specify", "structure", "-lrb-", "e.g.", "-lsb-", "Sun", "et", "al.", "2005", "-rsb-", "-rrb-", "other", "work", "search", "space", "constrain", "use", "simple", "heuristic", "P?rez", "et", "al.", "-lsb-", "2004", "-rsb-", "restrict", "source", "region", "band", "constant", "radius", "around", "missing", "region", "however", "simple", "solution", "we", "algorithm", "unconstrained", "we", "algorithm", "automatic", "constraint", "problematic", "radius", "too", "small", "enough", "datum", "can", "sample", "effective", "completion", "which", "may", "result", "repetition", "radius", "too", "large", "result", "suffer", "from", "same", "problem", "observe", "unconstrained", "result", "figure", "Bornard", "et", "al.", "-lsb-", "2002", "-rsb-", "describe", "more", "localized", "heuristic", "every", "miss", "pixel", "restrict", "search", "space", "smallest", "centered", "window", "contain", "least", "certain", "number", "known", "pixel", "while", "largely", "avoid", "problem", "select", "incorrect", "texture", "window", "still", "well", "adapt", "image", "content", "we", "use", "similar", "heuristic", "which", "use", "texture", "segmentation", "select", "large", "restriction", "region", "relatively", "homogeneous", "content", "order", "facilitate", "predictability", "achieve", "continuation", "semantic", "region", "we", "consider", "use", "image", "segmentation", "technique", "-lsb-", "Jia", "Tang", "2003", "-rsb-", "however", "difficult", "control", "granularity", "result", "segment", "addition", "segment", "non-overlapping", "which", "might", "lead", "artifact", "form", "artificial", "hard", "edge", "result", "instead", "we", "oversegment", "superpixel", "associate", "each", "superpixel", "cluster", "similar", "superpixel", "surround", "cluster", "homogeneous", "overlap", "desire", "we", "experiment", "several", "superpixel", "algorithm", "include", "normalize", "cut", "-lsb-", "yu", "Shi", "2003", "-rsb-", "graph-based", "base", "segmentation", "-lsb-", "felzenszwalb", "Huttenlocher", "2004", "-rsb-", "turn", "out", "add", "complexity", "algorithm", "necessary", "achieve", "good", "result", "instead", "we", "partition", "entire", "image", "nonoverlapping", "square", "tile", "each", "be", "16", "16", "pixel", "three", "-lrb-", "non-disjunct", "-rrb-", "category", "tile", "boundary", "tile", "overlap", "touch", "known", "image", "boundary", "know", "tile", "contain", "least", "one", "known", "pixel", "miss", "tile", "contain", "least", "one", "miss", "pixel", "-lrb-", "figure", "3a", "-rrb-", "note", "each", "boundary", "tile", "also", "either", "know", "miss", "both", "every", "boundary", "tile", "we", "compute", "segment", "comprise", "surround", "known", "tile", "sufficiently", "similar", "texture", "-lrb-", "decribe", "next", "paragraph", "-rrb-", "next", "we", "associate", "each", "miss", "tile", "restriction", "region", "form", "union", "several", "overlap", "segment", "union", "contain", "segment", "all", "boundary", "tile", "within", "1.5", "time", "distance", "closest", "boundary", "tile", "we", "associate", "every", "pixel", "within", "miss", "tile", "restriction", "region", "tile", "algorithm", "have", "desire", "effect", "produce", "small", "restriction", "region", "near", "image", "boundary", "force", "completion", "continue", "semantic", "region", "whereas", "further", "away", "from", "boundary", "result", "larger", "restriction", "region", "give", "completion", "algorithm", "more", "freedom", "work", "both", "hole", "outside", "image", "-lrb-", "panorama", "-rrb-", "interior", "image", "algorithm", "illustrate", "Figure", "right", "compute", "homogeneous", "segment", "around", "boundary", "tile", "we", "consider", "tile", "four-connected", "grid", "graph", "each", "edge", "weight", "affinity", "attach", "tile", "compute", "Earth", "mover?s", "distance", "-lrb-", "emd", "-rrb-", "color", "histogram", "-lrb-", "one", "16-bin", "histogram", "per", "channel", "-rrb-", "we", "also", "try", "euclidean", "distance", "achieve", "slightly", "better", "result", "use", "EMD", "each", "boundary", "tile", "we", "compute", "shortest", "distance", "path", "every", "other", "tile", "use", "dijkstra?s", "algorithm", "we", "define", "segment", "union", "all", "tile", "whose", "distance", "smaller", "than", "threshold", "-lrb-", "distance", "between", "two", "bin", "EMD", "set", "-rrb-", "segment", "have", "fewer", "than", "32", "tile", "threshold", "automatically", "adjust", "select", "least", "32", "tile", "we", "conduct", "user", "study", "13", "participant", "-lrb-", "female", "10", "male", "-rrb-", "we", "test", "we", "algorithm", "against", "three", "competitor", "we", "algorithm", "without", "automatic", "search", "space", "constraint", "Adobe", "Photoshop", "CS5?s", "Content", "Aware", "fill", "resynthesizer", "popular", "plugin", "GIMP", "image", "editor", "mechanical", "Turk", "use", "because", "high", "resolution", "display", "requirement", "each", "participant", "present", "60", "pair", "image", "two", "monitor", "-lrb-", "each", "image", "show", "full", "24", "diagonal", "monitor", "-rrb-", "30", "image", "be", "panorama", "hole", "outside", "other", "30", "have", "hole", "interior", "image", "order", "pseudo-randomly", "varied", "each", "participant", "compare", "we", "algorithm", "against", "20", "example", "from", "each", "other", "technique", "result", "show", "Figure", "analysis", "between", "each", "condition", "indicate", "we", "algorithm", "significantly", "prefer", "over", "each", "other", "technique", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "131:4", "J.", "Kopf", "et", "al.", "Figure", "computation", "search", "space", "constraint", "image", "partition", "square", "tile", "white", "area", "part", "missing", "region", "leave", "boundary", "tile", "-lrb-", "dark", "-rrb-", "know", "miss", "tile", "-lrb-", "light", "-rrb-", "right", "dark", "miss", "tile", "closest", "boundary", "tile", "show", "blue", "red", "green", "boundary", "tile", "within", "1.5", "distance", "closest", "boundary", "tile", "respective", "segment", "outline", "red", "green", "blue", "union", "all", "three", "segment", "yield", "restricted", "search", "space", "-lrb-", "outline", "dash", "black", "line", "-rrb-", "4.2", "evaluation", "http://www.logarithmic.net/pfh/resynthesizer", "60.00", "we", "Algorithm", "50.00", "preference", "other", "algorithm", "40.00", "30.00", "20.00", "10.00", "0.00", "we", "vs.", "unconstrained", "we", "vs.", "Photoshop", "we", "vs.", "Resynthesizer", "we", "vs.", "unconstrained", "we", "vs.", "Photoshop", "we", "vs.", "Resynthesizer", "-lrb-", "260", "-rrb-", "46.92", "-lrb-", "260", "-rrb-", "63.70", "-lrb-", "260", "-rrb-", "74.68", "0.0001", "0.0001", "0.0001", "show", "statistically", "significant", "result", "Figure", "result", "user", "study", "compare", "we", "algorithm", "against", "several", "other", "state-of-the-art", "algorithm", "Quality", "Prediction", "automatic", "search", "space", "constraint", "design", "part", "facilitate", "prediction", "quality", "section", "we", "describe", "how", "we", "generate", "prediction", "function", "map", "give", "miss", "pixel", "measure", "perceptual", "quality", "datum", "use", "learn", "prediction", "function", "obtain", "use", "mechanical", "Turk", "from", "set", "125", "complete", "panorama", "we", "randomly", "select", "64", "64", "square", "least", "half", "miss", "pixel", "mark", "they", "red", "bound", "box", "image", "we", "crop", "380", "380", "region", "around", "square", "provide", "visual", "context", "ask", "subject", "rate", "completion", "square", "either", "good", "bad", "we", "generate", "1,500", "batch", "twelve", "question", "each", "each", "batch", "contain", "ten", "real", "question", "two", "control", "question", "obvious", "answer", "control", "question", "enable", "we", "prune", "subject", "who", "be", "perform", "task", "properly", "common", "problem", "mechanical", "Turk", "subject", "consider", "invalid", "fewer", "than", "80", "control", "question", "be", "incorrectly", "answer", "after", "pruning", "we", "have", "66", "valid", "subject", "-lrb-", "down", "from", "117", "subject", "-rrb-", "generate", "total", "8,738", "good", "802", "bad", "label", "example", "complete", "region", "last", "two", "number", "suggest", "average", "user", "find", "about", "92", "we", "filled-in", "region", "good", "i.e.", "plausible-looking", "we", "use", "label", "datum", "learn", "function", "predict", "perceive", "quality", "complete", "miss", "region", "recall", "each", "miss", "pixel", "constrain", "come", "from", "certain", "restriction", "region", "compose", "union", "several", "homogeneous", "segment", "we", "prediction", "function", "learn", "correlation", "between", "perceive", "quality", "complete", "pixel", "some", "low-level", "feature", "segment", "comprise", "restriction", "region", "feature", "vector", "segment", "have", "follow", "component", "color", "histogram", "-lrb-", "separate", "each", "channel", "-rrb-", "edge", "density", "-lrb-", "percentage", "pixel", "edge", "pixel", "-rrb-", "edge", "orientation", "histogram", "histogram", "contour", "straight", "line", "length", "each", "histogram", "characterize", "its", "entropy", "mean", "standard", "deviation", "we", "provide", "additional", "implementation", "detail", "supplementary", "document", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "Quality", "prediction", "image", "completion", "131:5", "feature", "vector", "miss", "pixel", "define", "where", "image", "coordinate", "miss", "pixel", "-lrb-", "normalize", "so", "each", "range", "between", "-rrb-", "its", "distance", "boundary", "nearest", "known", "region", "size", "segment", "area", "-lrb-", "typically", "smaller", "than", "can", "deduce", "from", "Figure", "-rrb-", "let", "label", "pixel", "-lrb-", "good", "bad", "-rrb-", "we", "goal", "construct", "function", "-lrb-", "-rrb-", "predict", "unknown", "label", "from", "feature", "vector", "we", "use", "gentle", "adaboost", "-lsb-", "Friedman", "et", "al.", "2000", "-rsb-", "standard", "machine", "learn", "algorithm", "binary", "classification", "combine", "we", "feature", "vector", "scalar", "quality", "prediction", "prediction", "function", "have", "form", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "which", "sum", "regression", "stump", "-lrb-", "-rrb-", "otherwise", "where", "denote", "th", "element", "feature", "vector", "gentle", "AdaBoost", "determine", "model", "parameter", "solve", "problem", "min", "exp", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "i?t", "which", "penalize", "positive", "value", "bad", "example", "negative", "value", "good", "example", "here", "denote", "set", "label", "training", "example", "number", "regression", "stump", "design", "parameter", "learning", "algorithm", "set", "empirically", "cross-validation", "we", "randomly", "split", "training", "datum", "five", "validation", "fold", "each", "fold", "learn", "prediction", "function", "use", "datum", "from", "remain", "four", "training", "fold", "average", "prediction", "performance", "five", "validation", "fold", "provide", "estimate", "generalization", "performance", "model", "we", "repeat", "process", "...", "128", "find", "performance", "increase", "until", "about", "32", "level", "off", "Figure", "illustrate", "effectiveness", "learn", "approach", "discriminative", "power", "we", "learn", "predictor", "higher", "than", "any", "feature", "take", "itself", "-lrb-", "individual", "element", "-rrb-", "aside", "we", "also", "test", "pixel-wise", "energy", "eq", "feature", "find", "poor", "quality", "predictor", "-lrb-", "auc", "0.53", "standard", "error", "0.006", "-rrb-", "compare", "we", "dedicated", "quality", "feature", "-lrb-", "eq", "auc", "0.77", "-rrb-", "please", "note", "evaluation", "all", "other", "result", "present", "here", "be", "compute", "image", "be", "part", "training", "set", "we", "show", "example", "we", "quality", "prediction", "throughout", "paper", "supplementary", "material", "quality", "prediction", "-lrb-", "-rrb-", "arbitrary", "unit", "make", "quality", "parameter", "we", "method", "more", "intuitive", "we", "transform", "quality", "prediction", "probability", "use", "platt?s", "method", "-lsb-", "1999", "-rsb-", "method", "construct", "sigmoid", "mapping", "from", "raw", "quality", "prediction", "-lrb-", "-rrb-", "estimate", "good", "bad", "label", "probability", "-lsb-", "-rsb-", "note", "since", "sum", "regression", "stump", "both", "real-valued", "piecewise", "constant", "very", "fast", "compute", "since", "require", "only", "32", "compare-add", "per", "pixel", "Automatic", "Cropping", "Algorithm", "we", "use", "we", "prediction", "function", "compute", "optimal", "crop", "shape", "include", "many", "known", "pixel", "possible", "while", "avoid", "ing", "pixel", "predict", "low", "quality", "objective", "achieve", "solve", "optimization", "problem", "Figure", "roc", "-lrb-", "receiver", "operating", "characteristic", "-rrb-", "curve", "we", "learn", "prediction", "function", "vs.", "five", "most", "predictive", "individual", "feature", "-lrb-", "out", "21", "-rrb-", "area", "under", "roc", "curve", "-lrb-", "auc", "-rrb-", "measure", "discriminability", "each", "predictor", "auc", "0.5", "denote", "chance", "level", "auc", "1.0", "indicate", "perfect", "discrimination", "min", "subject", "-lrb-", "-rrb-", "i?c", "where", "denote", "region", "outside", "crop", "shape", "-lrb-", "-rrb-", "average", "predict", "quality", "inside", "crop", "shape", "-lrb-", "known", "pixel", "-rrb-", "solution", "minimize", "number", "exclude", "known", "pixel", "while", "ensure", "minimum", "average", "probability", "inside", "crop", "shape", "parameter", "balance", "between", "two", "high-level", "objective", "higher", "value", "lead", "more", "aggressive", "crop", "since", "less", "potentially", "low-quality", "area", "allow", "crop", "practice", "we", "find", "-lsb-", "0.99", "-rsb-", "reasonable", "range", "value", "value", "upper", "range", "because", "subject", "user", "study", "from", "section", "rate", "about", "9/10", "all", "sample", "good", "result", "relatively", "high", "everywhere", "Figure", "illustrate", "effect", "vary", "all", "other", "result", "paper", "supplementary", "material", "we", "set", "0.9925", "setting", "score", "best", "evaluation", "study", "describe", "below", "generally", "seem", "work", "well", "across", "wide", "range", "panorama", "-lrb-", "please", "refer", "result", "supplementary", "material", "-rrb-", "Figure", "illustrate", "how", "we", "crop", "optimization", "behave", "panorama", "predict", "mostly", "high", "mostly", "low", "quality", "while", "most", "commonly", "use", "shape", "rectangle", "formulation", "above", "support", "arbitrary", "parametric", "crop", "shape", "we", "have", "experiment", "various", "shape", "include", "trapezoid", "convex", "ngon", "ellipsis", "t-shape", "l-shape", "Figure", "show", "various", "result", "use", "general", "shape", "solve", "constrain", "optimization", "problem", "eq", "we", "first", "replace", "sequence", "unconstrained", "subproblem", "use", "logarithmic", "barrier", "method", "-lsb-", "nocedal", "Wright", "2000", "-rsb-", "instead", "solve", "eq", "we", "solve", "unconstrained", "combined", "objective/barrier", "problem", "min", "log", "-lrb-", "-rrb-", "minimizer", "eq", "approach", "solution", "eq", "we", "use", "Simplex", "algorithm", "-lsb-", "nelder", "mead", "1965", "-rsb-", "implement", "GNU", "scientific", "library", "-lsb-", "Galassi", "et", "al.", "2009", "-rsb-", "solve", "problem", "since", "objective", "function", "may", "contain", "local", "minimum", "we", "approximate", "global", "minimizer", "start", "optimization", "from", "100", "random", "initial", "state", "use", "best", "result", "6.1", "evaluation", "Rectangles", "we", "run", "user", "study", "involve", "13", "subject", "-lrb-", "10", "male", "female", "-rrb-", "evaluate", "we", "automatic", "crop", "technique", "each", "subject", "show", "20", "full", "panorama", "randomly", "select", "from", "subset", "50", "panorama", "high-resolution", "24", "monitor", "each", "panorama", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "131:6", "J.", "Kopf", "et", "al.", "input", "prediction", "-lrb-", "brighter", "better", "-rrb-", "more", "aggressive", "crop", "Figure", "parameter", "control", "balance", "between", "we", "high", "level", "goal", "when", "crop", "high", "setting", "-lrb-", "middle", "-rrb-", "avoid", "low-quality", "area", "might", "crop", "more", "lower", "setting", "-lrb-", "right", "we", "default", "setting", "-rrb-", "include", "more", "known", "pixel", "expense", "potentially", "lower", "completion", "quality", "input", "prediction", "crop", "follow", "-lrb-", "brighter", "higher", "quality", "-rrb-", "image", "completion", "figure", "easy", "hard", "input", "image", "left", "panorama", "we", "prediction", "function", "indicate", "most", "miss", "region", "can", "complete", "well", "we", "automatic", "crop", "large", "right", "input", "prediction", "function", "indicate", "most", "miss", "region", "can", "complete", "well", "we", "automatic", "crop", "more", "conservative", "avoid", "most", "region", "Figure", "leave", "result", "user", "study", "compare", "non-cropped", "image", "-lrb-", "full", "-rrb-", "versus", "intelligently", "crop", "-lrb-", "smart", "-rrb-", "versus", "nocropping", "-lrb-", "conservative", "-rrb-", "over", "crop", "-lrb-", "extracon", "-rrb-", "user", "significantly", "prefer", "smart", "crop", "-lrb-", "260", "-rrb-", "165.94", "0.0001", "middle", "average", "hausdorff", "distance", "crop", "choose", "subject", "we", "user", "study", "we", "automatic", "crop", "average", "closer", "user", "preference", "than", "either", "full", "conservative", "crop", "right", "average", "hausdorff", "distance", "linearly", "interpolate", "crop", "box", "between", "full", "conservative", "case", "where", "predict", "quality", "each", "miss", "pixel", "set", "constant", "we", "generate", "version", "full", "uncropped", "completion", "three", "autocrop", "0.9925", "0.9950", "0.9975", "conservative", "crop", "-lrb-", "i.e.", "maximum", "know", "region", "crop", "-rrb-", "we", "also", "add", "overcrop", "version", "eliminate", "known", "pixel", "help", "determine", "when", "subject", "be", "crop", "base", "frame", "judgement", "oppose", "noticeable", "artifact", "each", "set", "panorama", "original", "stitch", "photograph", "be", "first", "show", "subject", "subject", "could", "use", "left/right", "arrow", "key", "cycle", "through", "differently", "crop", "panorama", "subject", "be", "ask", "choose", "panorama", "would", "want", "share", "friend", "mechanical", "Turk", "use", "because", "high-resolution", "requirement", "subject", "finish", "average", "7.5", "min", "-lrb-", "standard", "deviation", "2.67", "min", "-rrb-", "0.9975", "less", "aggressive", "crop", "0.9925", "input", "prediction", "crop", "follow", "-lrb-", "brighter", "higher", "quality", "-rrb-", "image", "completion", "result", "study", "show", "participant", "prefer", "some", "form", "guide", "crop", "52", "time", "use", "uncropped", "version", "35", "conservative", "crop", "only", "11.5", "time", "-lrb-", "see", "Figure", "leave", "-rrb-", "analysis", "show", "only", "part", "picture", "depend", "prediction", "distribution", "different", "crop", "choice", "may", "appear", "very", "similar", "example", "show", "figure", "where", "we", "smart", "crop", "almost", "same", "full", "image", "conservative", "crop", "respectively", "case", "arbitrary", "decision", "may", "make", "skewing", "result", "account", "issue", "we", "perform", "another", "analysis", "take", "account", "relative", "shape", "distance", "between", "different", "crop", "each", "crop", "version", "select", "user", "we", "compute", "symmetric", "hausdorff", "distance", "all", "available", "crop", "choice", "-lrb-", "normalize", "longer", "dimension", "image", "-rrb-", "Hausdorff", "distance", "commonly", "use", "shape", "similarity", "metric", "e.g.", "template", "matching", "computer", "vision", "application", "result", "analysis", "-lrb-", "blue", "bar", "Figure", "middle", "-rrb-", "show", "we", "guide", "crop", "have", "smallest", "average", "Hausdorff", "distance", "crop", "select", "subject", "show", "clear", "preference", "we", "guide", "crop", "over", "full", "image", "conservatively", "crop", "image", "we", "also", "compare", "various", "na?ve", "crop", "user", "choice", "-lrb-", "figure", "right", "-rrb-", "result", "red", "show", "average", "distance", "crop", "achieve", "interpolate", "crop", "shape", "between", "full", "conservative", "version", "result", "green", "be", "achieve", "use", "same", "optimization", "we", "result", "-lrb-", "use", "0.9925", "-rrb-", "use", "same", "constant", "quality", "value", "every", "miss", "pixel", "both", "case", "distance", "significantly", "higher", "-lrb-", "hence", "less", "desirable", "-rrb-", "than", "we", "best", "setting", "additional", "result", "we", "test", "we", "algorithm", "several", "hundred", "panorama", "image", "hole", "interior", "supplementary", "material", "we", "show", "extensive", "comparison", "various", "state-of-the-art", "algorithm", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "Quality", "prediction", "image", "completion", "131:7", "input", "prediction", "-lrb-", "brighter", "higher", "quality", "-rrb-", "Adobe", "Photoshop", "CS5", "-lrb-", "Content", "Aware", "fill", "-rrb-", "Figure", "10", "comparison", "other", "image", "completion", "algorithm", "please", "zoom", "pdf", "see", "detail", "note", "blur", "problem", "Photoshop", "Komodakis", "Tziritas", "result", "bottom", "corner", "result", "both", "previous", "method", "also", "contain", "incorrectly", "synthesize", "texture", "bottom", "semantic", "error", "i.e.", "snow", "patch", "sky", "many", "more", "example", "can", "find", "supplementary", "material", "input", "prediction", "-lrb-", "brighter", "higher", "quality", "-rrb-", "best", "-lrb-", "axis-aligned", "-rrb-", "rectangular", "crop", "best", "rotate", "rectangle", "best", "isosceles", "trapezoid", "figure", "result", "use", "alternative", "crop", "shape", "both", "case", "significantly", "higher", "portion", "known", "pixel", "include", "crop", "-lrb-", "Adobe", "Photoshop", "CS5", "Content", "Aware", "fill", "GIMP", "Resynthesizer", "-lsb-", "Pritch", "et", "al.", "2009", "-rsb-", "-lsb-", "Komodakis", "Tziritas", "2007", "-rsb-", "-lsb-", "Criminisi", "et", "al.", "2003", "-rsb-", "-rrb-", "25", "representative", "sample", "from", "each", "class", "example", "show", "Figure", "10", "addition", "we", "show", "automatic", "crop", "result", "25", "panorama", "representative", "result", "show", "throughout", "paper", "we", "result", "-lrb-", "automatic", "crop", "show", "here", "-rrb-", "Komodakis", "Tziritas", "-lsb-", "2007", "-rsb-", "dual", "Intel", "Xeon", "E5640", "PC", "we", "observe", "follow", "median", "timing", "25", "panorama", "include", "supplementary", "material", "we", "believe", "number", "can", "reduce", "code", "optimization", "full", "completion", "auto-cropping", "restriction", "region", "0.32", "0.22", "feature", "extraction", "applicable", "0.93", "crop", "optimization", "applicable", "1.78", "Completion", "13.29", "6.52", "total", "13.70", "9.17", "panorama", "we", "automatic", "crop", "contain", "average", "slightly", "less", "than", "50", "miss", "pixel", "since", "completion", "algorithm", "runtime", "roughly", "linear", "number", "miss", "pixel", "lead", "significant", "speed-up", "compare", "first", "complete", "full", "panorama", "before", "crop", "Limitations", "Future", "Work", "image", "completion", "remain", "very", "challenging", "problem", "like", "other", "recent", "approach", "we", "algorithm", "lack", "higher-level", "-lrb-", "object-level", "-rrb-", "understanding", "input", "image", "thus", "occasion", "generate", "semantically", "implausible", "result", "although", "we", "source", "location", "restriction", "significantly", "reduce", "problem", "we", "crop", "optimization", "currently", "ignore", "scene", "context", "may", "crop", "out", "important", "object", "scene", "see", "Figure", "12", "fail", "realize", "importance", "two", "subject", "possible", "solution", "use", "face", "and/or", "saliency", "detector", "we", "prediction", "function", "fit", "perfect", "most", "likely", "due", "occasional", "mismatch", "subject", "rating", "training", "database", "result", "we", "prediction", "function", "would", "occasion", "mislabel", "miss", "region", "-lrb-", "e.g.", "Figure", "11", "where", "mislabeling", "result", "smaller", "crop", "-rrb-", "future", "course", "action", "would", "either", "analyze", "function", "per-person", "basis", "-lrb-", "i.e.", "personalize", "automatic", "crop", "function", "-rrb-", "partition", "datum", "cluster", "similar", "preference", "each", "have", "different", "crop", "function", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012", "131:8", "J.", "Kopf", "et", "al.", "input", "prediction", "entire", "completion", "-lrb-", "brighter", "higher", "quality", "-rrb-", "figure", "11", "we", "prediction", "function", "sometimes", "tag", "good", "pixel", "bad", "example", "practically", "entire", "missing", "region", "can", "complete", "well", "result", "overly-conservative", "prediction", "suggest", "crop", "smaller", "than", "necessary", "input", "prediction", "crop", "completion", "-lrb-", "brighter", "higher", "quality", "-rrb-", "figure", "12", "example", "where", "we", "crop", "fail", "take", "consideration", "object-level", "importance", "reference", "arne", "C.", "hechtman", "E.", "inkelstein", "a.", "old", "man", "D.", "B.", "2009", "Patchmatch", "randomize", "correspondence", "algorithm", "structural", "image", "editing", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "28", "24:1", "24:11", "ertalmio", "m.", "apiro", "G.", "ASELLES", "V.", "ALLESTER", "C.", "2000", "image", "inpainting", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "19", "ornard", "R.", "ECAN", "E.", "ABORELLI", "L.", "HENOT", "J.H.", "2002", "miss", "datum", "correction", "still", "image", "image", "sequence", "Proceedings", "tenth", "acm", "international", "conference", "Multimedia", "355", "361", "riminisus", "a.", "erez", "P.", "oyama", "K.", "2003", "object", "removal", "exemplar-based", "inpainting", "cvpr", "417", "424", "arabus", "S.", "hechtman", "E.", "ARNES", "C.", "OLDMAN", "D.", "B.", "EN", "P.", "2012", "image", "melding", "combine", "inconsistent", "image", "use", "patch-based", "synthesis", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "31", "rorus", "i.", "ohen", "D.", "eshurun", "H.", "2003", "fragment-based", "image", "completion", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "22", "303", "312", "fro", "a.", "eung", "t.", "1999", "texture", "synthesis", "nonparametric", "sampling", "cvpr", "1033", "1038", "elzenszwalb", "P.", "uttenlocher", "D.", "2004", "efficient", "graph-based", "image", "segmentation", "ijcv", "59", "167", "181", "riedman", "J.", "ASTIE", "T.", "ibshiranus", "R.", "2000", "additive", "logistic", "regression", "statistical", "view", "boost", "annals", "Statistics", "28", "337", "407", "alassus", "M.", "AVIES", "J.", "heiler", "J.", "OUGH", "B.", "UNG", "MAN", "G.", "2009", "GNU", "Scientific", "Library", "Reference", "Manual", "Third", "Edition", "Network", "Theory", "Ltd.", "ia", "J.", "ang", "c.-k", "2003", "image", "repair", "robust", "image", "synthesis", "adaptive", "nd", "tensor", "voting", "Proc", "CVPR", "2003", "643", "650", "AWAI", "N.", "ato", "T.", "okoya", "N.", "2008", "image", "inpainting", "consider", "brightness", "change", "spatial", "locality", "texture", "its", "evaluation", "PSIVT", "09", "271", "282", "OMODAKIS", "N.", "zirita", "G.", "2007", "image", "completion", "use", "efficient", "belief", "propagation", "via", "priority", "scheduling", "dynamic", "pruning", "IEEE", "Trans", "image", "processing", "16", "2649", "2661", "WATRA", "V.", "SSA", "I.", "OBICK", "a.", "WATRA", "N.", "2005", "texture", "optimization", "example-based", "synthesis", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "24", "795", "802", "ansfield", "a.", "rasad", "M.", "OTHER", "C.", "harp", "T.", "OHLI", "P.", "ool", "L.", "2011", "transform", "image", "completion", "british", "machine", "Vision", "Conf", "-lrb-", "bmvc", "-rrb-", "atsushita", "Y.", "FEK", "E.", "W.", "ang", "X.", "HUM", "H.Y.", "2006", "full-frame", "video", "stabilization", "motion", "inpainting", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "28", "-lrb-", "July", "-rrb-", "1150", "1163", "elder", "J.", "ead", "R.", "1965", "simplex", "method", "function", "minimization", "Computer", "Journal", "308", "313", "ocedal", "J.", "RIGHT", "S.", "J.", "2000", "numerical", "optimization", "Springer", "REZ", "P.", "ANGNET", "M.", "lake", "a.", "2004", "Patchworks", "example-based", "region", "tile", "image", "editing", "Tech", "Rep.", "MSRTR-2004-04", "Microsoft", "Research", "latt", "J.", "C.", "1999", "Probabilistic", "output", "support", "vector", "machine", "comparison", "regularize", "likelihood", "method", "advance", "large", "margin", "classifier", "MIT", "Press", "61", "74", "ritch", "Y.", "AV", "ENAKI", "E.", "eleg", "S.", "2009", "shift-map", "image", "editing", "iccv", "09", "151", "158", "imakov", "D.", "ASPI", "Y.", "hechtman", "E.", "rani", "M.", "2008", "summarize", "visual", "datum", "use", "bidirectional", "similarity", "cvpr", "UN", "J.", "uan", "L.", "IA", "J.", "hum", "h.-y", "2005", "image", "completion", "structure", "propagation", "ACM", "Trans", "graphic", "-lrb-", "Proceedings", "Siggraph", "-rrb-", "24", "861", "868", "wamy", "D.", "HANDLER", "D.", "UTLER", "K.", "EMAMI", "S.", "2011", "Parametric", "quality", "assessment", "synthesize", "texture", "Proc", "human", "Vision", "Electronic", "Imaging", "2011", "EXLER", "Y.", "hechtman", "E.", "rani", "M.", "2007", "Spacetime", "video", "completion", "tpamus", "S.", "HI", "J.", "2003", "multiclass", "spectral", "clustering", "313", "319", "vol", ".1", "hang", "m.", "hang", "L.", "UN", "Y.", "ENG", "L.", "W.", "2005", "auto", "crop", "digital", "photograph", "icme", "ACM", "transaction", "Graphics", "Vol", "31", "no.", "Article", "131", "publication", "date", "November", "2012" ],
  "content" : "\n  \n    d75da5d025b3456e121f92f2c0359645c05975c76fd34e46f42cb4bde2957fb4\n    p2m\n    10.1145/2366145.2366150\n    Name identification was not possible. \n  \n  \n    \n      \n        Quality Prediction for Image Completion\n      \n      Johannes Kopf Wolf Kienzle Microsoft Research Microsoft Research\n      \n        \n      \n      Input panorama and quality prediction (brighter is higher quality)\n      \n        \n      \n      Best crop using only ?known? pixels (?conservative crop?)\n      \n        Figure 1: Our data-driven technique is capable of predicting image completion quality (top left) before the completion is actually computed (top right). Based on our prediction, we compute an optimal crop rectangle that tries to include as many known pixels as possible while avoiding low-quality regions (bottom right). Compared to previous cropping approaches that do not fill in (bottom left) we can usually include a larger amount of the input image in our crop. Our algorithm only completes the cropped region, thus saving a significant amount of computation compared to full completion.\n      \n      We present a data-driven method to predict the quality of an image completion method. Our method is based on the state-of-the-art non-parametric framework of Wexler et al. [2007]. It uses automatically derived search space constraints for patch source regions, which lead to improved texture synthesis and semantically more plausible results. These constraints also facilitate performance prediction by allowing us to correlate output quality against features of possible regions used for synthesis. We use our algorithm to first crop and then complete stitched panoramas. Our predictive ability is used to find an optimal crop shape before the completion is computed, potentially saving significant amounts of computation. Our optimized crop includes as much of the original panorama as possible while avoiding regions that can be less successfully filled in. Our predictor can also be applied for hole filling in the interior of images. In addition to extensive comparative results, we ran several user studies validating our predictive feature, good relative quality of our results against those of other state-of-the-art algorithms, and our automatic cropping algorithm. CR Categories: I.4.9 [Image Processing and Computer Vision]: Applications; Keywords: image completion, quality prediction, cropping Links: DL PDF W EB\n      \n        \n        \n        \n      \n      ACM Reference Format Kopf, J., Kienzle, W., Drucker, S., Kang, S. 2012. Quality Prediction for Image Completion. ACM Trans. Graph. 31 6, Article 131 (November 2012), 8 pages. DOI = 10.1145/2366145.2366150 http://doi.acm.org/10.1145/2366145.2366150. Copyright Notice Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2012 ACM 0730-0301/2012/11-ART131 $15.00 DOI 10.1145/2366145.2366150 http://doi.acm.org/10.1145/2366145.2366150\n      Steven Drucker Sing Bing Kang Microsoft Research Microsoft Research\n      Full completion\n      Our optimized crop based on quality prediction\n    \n    \n      \n        1 Introduction\n      \n      Image completion or inpainting is a popular image editing tool for object removal and replacement or digital photograph restoration. A variety of completion algorithms have been developed in the scientific community over the past decade, and image completion is now a feature in commercial photo editing software such as Adobe Photoshop. In most previous work, image completion is used to fill holes after unwanted objects are removed. The same algorithms, however, can also be used to extend an image beyond its original boundaries. This is useful for filling beyond the irregular boundaries of a stitched panorama?this application is the focus of this paper. Casually shot panoramas often have irregular boundaries (e.g., Figure 1). Most users, however, prefer output images with rectangular boundaries. The trivial solution implemented by most stitching software is to crop to the largest box that is fully contained within the panorama. This simple method often removes large parts of the input. The alternative is to apply any existing completion algorithm to fill the missing regions of the panorama bounding box. Unfortunately, all existing image completion algorithms fail on occasion; the failure typically shows up as either inability to synthesize some textures well or results that are semantically implausible (see Figure 2 ). In addition, it is difficult to anticipate when and where such algorithms will fail given an arbitrary input image. In this paper, we use machine learning to predict the quality of image completion. To support this prediction, we design our image completion algorithm to produce high quality results and to allow associations between completed pixels and known pixels to be created. We build on the existing non-parametric optimization framework of Wexler et al. [2007] (which is also implemented in the Content Aware Fill feature of Adobe Photoshop 1 ). Previous work showed that this algorithm performs best if the source locations for patches are constrained to certain areas, e.g., see [Barnes et al. 2009]. We use a heuristic that automatically derives search space 1 see http://www.adobe.com/technology/projects/content-aware-fill.html constraints based on overlapping texture segmentation. These constraints allow us to design a simple method to predict algorithm performance. Based on the prediction we compute a crop shape before the completion is actually carried out, avoiding unnecessarily completing cropped pixels. To validate and train our prediction function, we ran a Mechanical Turk user study to obtain about 9,500 ?good? / ?bad? labels on crops from completed images from a large number of subjects. These labels are used to estimate our prediction function via crossvalidation. We tested our algorithm on an extensive collection of input images, and in another user study, compared our results with those of various state-of-the-art completion algorithms. In yet another user study, we evaluated the performance of our automatic crop optimization. In addition to the examples included in this paper, all of our results and comparisons are included in the supplementary material.\n      ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n      131:2\n      ?\n      J. Kopf et al.\n      \n        2 Previous Work\n        While there is a substantial amount of previous work on image completion (also referred to as inpainting or image filling), to the best of our knowledge, we are not aware of any methods that can predict quality before completion. Being able to predict quality would allow the system to determine if the input image can be properly restored or edited, or in our case, estimate the desired crop of a panorama. As a result, the system completes only what is needed. Note that existing cropping techniques such as that of [Zhang et al. 2005] use only available image data for cropping. Texture quality assessment techniques are the closest approaches to ours. The objective function of Kwatra et al. [2005] measures similarity of local patches to the target texture, but this measure is a poor predictor of quality (as shown in Section 5). Swarmy et al. [2011] show that a linear combination of image parameters (such as intensity mean, variance, entropy, and band information) can be used to assess texture quality. In both cases, however, the texture has to be generated first. We now briefly survey representative methods for image completion, which we classify as primarily example-based or diffusionbased. Komodakis and Tziritas [2007] provided an excellent review of inpainting techniques. Diffusion-based techniques , often referred to as ?inpainting,? typically work well for small or narrow holes, e.g., for removing scratches from a scanned old photograph. They are less appropriate for completing stitched panoramas with large open-ended missing regions due to their inability to synthesize textures. One good representative of diffusion-based techniques is that of Bertalmio et al. [2000]; it prolongs isophote lines in the missing areas. Example-based techniques tend to be more effective for filling larger holes than diffusion-based techniques. Efros and Leung [1999] popularized the use of non-parametric sampling for texture synthesis (and by extension, image completion). Many example-based techniques are based on this core concept. Representative techniques include block-based matching with structurebased priority [Criminisi et al. 2003], use of hierarchical filtering as initialization and adaptive regions instead of patches [Drori et al. 2003], optimization of a texture energy function [Wexler et al. 2007; Kwatra et al. 2005; Darabi et al. 2012], application of user specification of structure [Sun et al. 2005], MRF with exemplars as sample labels [Komodakis and Tziritas 2007], and search for globally-transformed patches [Mansfield et al. 2011]. Kawai et al. [2008] adapted the work of Wexler et al. [2007] using an SSD-based objective function to handle regular (fine-grained)  textures. In addition, they compensate for local brightness changes by linearly fitting the intensity of the matched patch. Pritch et al. [2009] cast the problem of image synthesis as finding an optimal shift-map of pixels based on global factors (such as image size and object arrangement) and local features (such as saliency map) as well as spatial regularization. They have demonstrated their technique on image retargeting, rearrangement, and completion. Many of the previous methods rely on quickly finding similar image patches. The recent PatchMatch algorithm [Barnes et al. 2009] greatly improves their speed by replacing the previously employed tree-based search techniques with a much faster randomized algorithm. Practically all the automatic techniques that rely on exemplars (with the exception of [Matsushita et al. 2006]) do not restrict their search. As a result, such techniques are prone to the same problems as those of [Wexler et al. 2007], where perceptually implausible patches could be used for image completion. In addition, they were not designed with performance prediction in mind. It is difficult to anticipate the degree of success or failure within the missing regions.\n      \n      \n        3 Overview\n        Our algorithm builds on the non-parametric optimization algorithm introduced by Wexler et al. [2007]. We implemented the optimization as described in that paper using the weighted average updating rule (we did not find it necessary to use the much slower mean-shift based updating rule). We describe more implementation details of the algorithm in a supplementary document. We bias our algorithm toward continuing image content near the boundary into the missing region. Each missing pixel may only be filled from a tightly constrained part of the known region. Knowing in advance where every missing pixel may come from enables us to predict the perceived quality of the completed result. Using training data, we learn a function that maps low-level features of the closest known image regions to the perceived quality of the completed result. The low-level features include color, edge density, edge orientation, contour length, and region size. Our prediction function is learnt and validated from data that we collected in a Mechanical Turk user study, where subjects were asked to categorize random patches from the completed regions as ?good? or ?bad?. In the next section, we review the optimization framework that our algorithm is based on and then describe our extension that improves the result quality and makes the algorithm more predictable.\n      \n      \n        4 Image Completion Algorithm\n        Our algorithm minimizes a ?texture energy? term which measures the extent to which the synthesized region deviates from the known region over a set of overlapping local patches. The basic form of the energy minimization problem is\n        \n          1\n          min t i ? s i ,\n        \n        \n          1\n          {t i ,s i } i??\n        \n        where ? is the set of center pixels of all 7?7 patches that are completely contained within the image domain and overlap at least one missing pixel. By ?image domain,? we mean the minimum bounding box containing the panorama. t i denotes a (target) patch centered at pixel i, and s i is a (source) patch in the known region K that is close in appearance to t i . The energy is minimized in an iterative fashion, alternating between minimizing with respect to set of t i or s i while the other set is\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n        Quality Prediction for Image Completion\n        ?\n        131:3\n        \n          \n        \n        Input and prediction (brighter is higher quality)\n        \n          \n        \n        Adobe Photoshop CS5 (Content Aware Fill)\n        \n          Figure 2: Unguided image completion often produces semantically implausible results. Please zoom into the PDF to see details. Note the artifacts in the top left corner and near the bottom. Both ours and Photoshop?s implementation of [Wexler et al. 2007] synthesized the wrong rock texture and snow patches into the sky. In previous work these issues have been addressed by using manual search space constraints. We propose a fully automatic algorithm to derive such constraints. Our result, while not artifact free, appears more plausible. Many more examples can be found in the supplementary material.\n        \n        fixed. Minimizing the s i assignments requires finding the nearest neighbor for each synthesized patch t i . This task is the most compute-intensive, and requires using approximative techniques to keep the run time reasonable (we use the PatchMatch algorithm [Barnes et al. 2009]). Minimizing t i amounts to a simple averaging of the overlapping s i assignments. Our results are computed in a coarse-to-fine fashion using an image pyramid. For details, please refer to [Wexler et al. 2007] and the pseudo-code in the supplementary material.\n        \n          4.1 Automatic search space constraints\n          This completion algorithm is prone to getting stuck in bad local minima, and is sensitive to its initial state. The most successful applications start with initializations that are already close to the final result: image retargeting [Simakov et al. 2008] uses a scaled copy of the input, while image reshuffling [Barnes et al. 2009] starts from a copied known region. In both cases, the amount of repair is small. For plain image completion the algorithm is typically initialized randomly, which makes the quality of the results hard to predict. Without a random initialization the algorithm tends to exhibit quality issues such as blurry/incorrect textures and semantically implausible results, as shown in Figure 2 . Certain algorithms are manually guided toward better solutions by either constraining the locations of some pixels (e.g., [Barnes et al. 2009]) or specifying structure (e.g., [Sun et al. 2005]). In other works, the search space is constrained using simple heuristics. P?rez et al. [2004] restrict the source region to a band of constant radius around the missing region. However, this simple solution\n          Our algorithm\n        \n      \n      \n        unconstrained\n        Our Algorithm with automatic constraints\n        is problematic: if the radius is too small, not enough data can be sampled for effective completion, which may result in repetitions; if the radius is too large, the results will suffer from the same problems as observed in the unconstrained result in Figure 2 . Bornard et al. [2002] describe a more localized heuristic. For every missing pixel they restrict the search space to the smallest centered window that contains at least a certain number of known pixels. While this largely avoids the problem of selecting incorrect textures the windows are still not well adapted to the image content. We use a similar heuristic, which uses texture segmentation to select large restriction regions with relatively homogeneous content, in order to facilitate predictability and to achieve continuation of semantic regions. We considered using image segmentation techniques [Jia and Tang 2003], however, it is difficult to control the granularity of the resulting segments. In addition, these segments are non-overlapping, which might lead to artifacts in the form of artificial hard edges in the result. Instead, we oversegment into superpixels and then associate each superpixel with a cluster of similar superpixels surrounding it. These clusters are homogeneous and overlapping as desired. We experimented with several superpixel algorithms including normalized cuts [Yu and Shi 2003] and graph-based based segmentation [Felzenszwalb and Huttenlocher 2004]. It turned out that the added complexity of these algorithms was not necessary for achieving good results; instead, we partition the entire image into nonoverlapping square tiles, each being 16?16 pixels. There are three (non-disjunct) categories of tiles: boundary tiles overlap or touch the known image boundary, known tiles contain at least one known pixel, and missing tiles contain at least one missing pixel ( Figure 3a ). Note that each boundary tile is also either ?known?, ?missing?, or both. For every boundary tile, we compute a ?segment? comprising of surrounding known tiles of sufficiently similar texture (decribed in the next paragraph). Next, we associate with each missing tile a restriction region that is formed as the union of several overlapping segments. This union contains the segments of all boundary tiles within 1.5 times the distance of the closest boundary tile. We associate every pixel within a missing tile with the restriction region of that tile. This algorithm has the desired effect of producing small restriction regions near to the image boundary, forcing the completion to continue semantic regions, whereas further away from the boundary it results in larger restriction regions giving the completion algorithm more freedom. It works both for holes on the outside of the image (panoramas) and in the interior of the image. The algorithm is illustrated in Figure 3 , right. For computing the homogeneous segments around boundary tiles, we consider the tiles as a four-connected grid graph. Each edge is weighted by the affinity of the attached tiles, computed as the Earth Mover?s distance (EMD) of their color histograms (one 16-bin histogram per channel). We also tried ? 2 and Euclidean distance but achieved slightly better results using EMD. For each boundary tile, we compute the shortest distance path to every other tile using Dijkstra?s algorithm. We define the segment as the union of all tiles whose distance is smaller than the threshold ? = 5 (the distance between two bins in the EMD is set to 1). If the segment has fewer than 32 tiles, the threshold is automatically adjusted to select at least 32 tiles. We conducted a user study with 13 participants (3 female and 10 male). We tested our algorithm against three competitors: our algorithm without automatic search space constraints, Adobe Photoshop CS5?s Content Aware Fill, and Resynthesizer 2 , a popular plugin for the GIMP image editor. Mechanical Turk was not used because of the high resolution display requirement. Each participant was presented with 60 pairs of images on two monitors (each image was shown on a full 24? diagonal monitor). 30 of these images were panoramas with holes on the outside, and the other 30 had holes in the interior of the image. The ordering was pseudo-randomly varied such that each participant compared our algorithm against 20 examples from each of the other techniques. Results are shown in Figure 4 . A ? 2 analysis between each condition indicates that our algorithm was significantly preferred over each of the other techniques.\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n        131:4\n        ?\n        J. Kopf et al.\n        \n          \n          \n          Figure 3: Computation of search space constraints. The image is partitioned into square tiles, the white area is part of the missing region. Left: boundary tiles (dark), known and missing tiles (light). Right: for the dark missing tile the closest boundary tile is shown in blue. The red and green boundary tiles are within 1.5? distance of the closest boundary tile. Their respective segments are outlined in red, green, and blue. The union of all three segments yields the restricted search space (outlined with dashed black line).\n        \n        4.2 Evaluation\n        2 http://www.logarithmic.net/pfh/resynthesizer\n        60.00% Our Algorithm 50.00% No Preference Other Algorithm 40.00% 30.00% 20.00% 10.00% 0.00% Our vs. Unconstrained Our vs. Photoshop Our vs. Resynthesizer Our vs. unconstrained Our vs. Photoshop Our vs. Resynthesizer ? 2 (2, 260) = 46.92 ? 2 (2, 260) = 63.70 ? 2 (2, 260) = 74.68 p < 0.0001 ? p < 0.0001 ? p < 0.0001 ? ? shows statistically significant result.\n        \n          Figure 4: Results of the user study comparing our algorithm against several other state-of-the-art algorithms.\n        \n      \n      \n        5 Quality Prediction\n        The automatic search space constraints are designed in part to facilitate prediction of quality. In this section, we describe how we generate the prediction function that maps a given missing pixel to a measure of perceptual quality. The data used to learn the prediction function was obtained using Mechanical Turk. From a set of 125 completed panoramas, we randomly selected 64?64 squares with at least half missing pixels and marked them with red bounding boxes in the image. We cropped 380?380 regions around the squares to provide visual context and asked subjects to rate the completion in the square as either ?good? or ?bad?. We generated 1,500 batches with twelve questions each. Each batch contained ten real questions and two control questions with obvious answers. These control questions enabled us to prune subjects who were not performing the task properly, a common problem on Mechanical Turk. A subject is considered invalid if fewer than 80% of the control questions were incorrectly answered. After pruning, we have 66 valid subjects (down from 117 subjects), generating a total of 8,738 ?good? and 802 ?bad? labeled examples of completed regions. The last two numbers suggest that, on average, users will find about 92% of our filled-in regions ?good?, i.e., plausible-looking. We use the labeled data to learn a function that predicts the perceived quality of a completed missing region. Recall that each missing pixel i is constrained to come from a certain restriction region R i = S j , composed as the union of several homogeneous segments S j . Our prediction function learns the correlation between the perceived quality of a completed pixel and some low-level features of the segments comprising the restriction region. The feature vector u j of a segment S j has the following components: color histograms (separate for each channel), edge density (percentage of pixels that are edge pixels), edge orientation histogram, and histograms of contour and straight line lengths. Each histogram is characterized by its entropy, mean, and standard deviation. We provide additional implementation details in a supplementary document.\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n        Quality Prediction for Image Completion\n        ?\n        131:5\n        The feature vector v i of a missing pixel i is defined as: ? j a j u j / j a j ?\n        \n          2\n          ? x i ? v i = ? y i ? , ? ? ? d i ? b i\n        \n        where x i , y i are the image coordinates of the missing pixel (normalized so that each ranges between 0 and 1), d i is its distance to the boundary of the nearest known region, a j is the size of segment j, and b i is the area of R i (typically smaller than j a j , as can be deduced from Figure 3 ). Let t i be the label of pixel i (?good? = 1 or ?bad? = ?1). Our goal is to construct a function f (v i ) that predicts the unknown label from a feature vector v i . We use Gentle AdaBoost [Friedman et al. 2000], a standard machine learning algorithm for binary classification, to combine our feature vector into a scalar quality prediction. The prediction function has the form m\n        \n          3\n          f (v i ) = h k (v i ), k=1\n        \n        which is a sum of m regression stumps e\n        \n          4\n          h k (v i ) = l r k k if otherwise v i k > t k ,\n        \n        where v i e k denotes the e k th element of the feature vector v i . Gentle AdaBoost determines the model parameters e k , t k , l k , and r k by solving the problem\n        \n          5\n          min exp(?t i f (v i )),\n        \n        \n          5\n          f i?T\n        \n        which penalizes positive values of f on ?bad? examples and negative values on ?good? that examples. Here, T denotes the set of labeled training examples. The number of regression stumps m is a design parameter of the learning algorithm. It is set empirically by cross-validation: we randomly split the training data into five validation folds, and then for each fold learn a prediction function using the data from the remaining four training folds. The average prediction performance on the five validation folds provides an estimate of the generalization performance of the model. We repeated this process for m = 1, . . . , 128 and found that the performance increased with m until about m = 32, and then leveled off. Figure 5 illustrates the effectiveness of the learning approach. The discriminative power of our learned predictor f is higher than any feature taken by itself (the individual elements of v i ). As an aside, we also tested the pixel-wise energy Eq. 1 as a feature, but found it to be a poor quality predictor (AUC = 0.53, standard error: 0.006) compared to our dedicated quality features (Eq. 2, AUC = 0.77). Please note that this evaluation and all other results presented here were computed on images that were not part of the training set. We show examples of our quality prediction throughout the paper and in the supplementary material. The quality predictions f (v i ) are in arbitrary units. To make the quality parameter in our method more intuitive, we transform the quality predictions into probabilities using Platt?s method [1999]. This method constructs a sigmoid mapping from the raw quality predictions f (v i ) to estimates of ?good? or ?bad? label probabilities, p i ? [0; 1]. Note that since f is a sum of regression stumps both f and p i are real-valued and piecewise constant. p i is very fast to compute, since it requires only m = 32 compare-adds per pixel.\n      \n      \n        6 Automatic Cropping Algorithm\n        We use our prediction function to compute an optimal crop shape C that includes as many of the known pixels as possible while avoid ing pixels that are predicted as low quality. These objectives are achieved by solving the optimization problem\n        \n          Figure 5: ROC (receiver operating characteristic) curves for our learned prediction function vs. the five most predictive individual features (out of 21). The area under the ROC curve (AUC) measures discriminability of each predictor. AUC = 0.5 denotes chance level, AUC = 1.0 indicates perfect discrimination.\n        \n        \n          6\n          1 min C ? K subject to ? p (C) = p i ? ? p ,\n        \n        \n          6\n          C |C| i?C\n        \n        where C denotes the region outside the crop shape, and ? p (C) is the average predicted quality inside the crop shape (p i = 1 for known pixels). The solution minimizes the number of excluded known pixels while ensuring a minimum average probability ? p inside the crop shape. The parameter ? p balances between the two high-level objectives: higher values lead to more aggressive crops, since less potentially low-quality areas are allowed in the crop. In practice, we found ? p ? [0.99, 1] to be a reasonable range of values. The values for ? p are in the upper range because the subjects of the user study from Section 5 rated about 9/10 of all samples ?good?, resulting in relatively high p i ?s everywhere. Figure 6 illustrates the effect of varying ? p . For all other results in this paper and in the supplementary materials we set ? p = 0.9925, the setting that scored best in the evaluation study described below and generally seems to work well across a wide range of panoramas (please refer to the results in the supplementary material). Figure 7 illustrates how our cropping optimization behaves on panoramas that are predicted mostly high or mostly low quality. While the most commonly used shape is a rectangle, the formulation above supports arbitrary parametric crop shapes. We have experimented with various shapes including trapezoids, convex ngons, ellipses, T-shapes, and L-shapes. Figure 9 shows various results using general shapes. To solve the constrained optimization problem in Eq. 6, we first replace it with a sequence of unconstrained subproblems using the logarithmic barrier method [Nocedal and Wright 2000]. Instead of solving Eq. 6, we solve the unconstrained combined objective/barrier problem\n        \n          7\n          min C ? K ? ? log ? p (C). C\n        \n        The minimizer of Eq. 7 approaches the solution of Eq. 6 as ? ? 0. We use the Simplex algorithm [Nelder and Mead 1965] as implemented in the GNU scientific library [Galassi et al. 2009] to solve this problem. Since the objective function may contain local minima, we approximate the global minimizer by starting the optimization from 100 random initial states and use the best result.\n        \n          6.1 Evaluation for Rectangles\n          We ran a user study involving 13 subjects (10 males and 3 females) to evaluate our automatic cropping technique. Each subject was shown 20 full panoramas randomly selected from a subset of 50 panoramas on a high-resolution 24? monitor. For each panorama,\n          ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n          131:6\n          ?\n          J. Kopf et al.\n          \n            \n          \n          Input and prediction (brighter is better) More aggressive crop,\n          \n            Figure 6: The ? p parameter controls the balance between our high level goals when cropping: a high setting (middle) avoids low-quality areas but might crop more; a lower setting (right, our default setting) includes more of the known pixels at the expense of a potentially lower completion quality.\n            \n          \n          Input and prediction Cropped followed by (brighter is higher quality) image completion\n          \n            Figure 7: An ?easy? and a ?hard? input image. For the left panorama our prediction function indicates most of the missing regions can be completed well; our automatic crop is large. For the right input the prediction function indicates most of the missing regions can not be completed well. Our automatic crop is more conservative and avoids most of these regions.\n          \n          \n            Figure 8: Left: Results of the user study comparing non-cropped images (full), versus intelligently cropped (smart), versus nocropping (conservative) or over cropping (extracons). Users significantly preferred smart cropping: ? 2 (3, 260) = 165.94, p < 0.0001%. Middle: Average Hausdorff distances to crops chosen by the subjects of our user study. Our automatic crops are on average closer to the user preferences than either full or conservative cropping. Right: Average Hausdorff distances for linearly interpolated crop boxes between ?full? and ?conservative,? and for cases where the predicted quality of each missing pixel is set to a constant.\n          \n          we generated 5 versions: the full uncropped completion, three autocrops with ? p = 0.9925, 0.9950, 0.9975, and a conservative crop (i.e., maximum known region crop). We also added an overcropped version that eliminated known pixels to help determine when subjects were cropping based on framing judgements as opposed to noticeable artifacts. For each set of panoramas, the original stitched photographs were first shown to the subject and subjects could use the left/right arrow keys to cycle through the differently cropped panoramas. Subjects were asked to choose a panorama that they would want to share with friends. Mechanical Turk was not used because of the high-resolution requirement. Subjects finished on average in 7.5 mins (standard deviation of 2.67 mins).\n          ? p = 0.9975 Less aggressive crop, ? p = 0.9925\n          Input and prediction Cropped followed by (brighter is higher quality) image completion\n          The results of the study showed that participants preferred some form of guided cropping 52% of the time. They used the uncropped version 35% and the conservative cropping only 11.5% of the time (see Figure 8 , left). This analysis shows only part of the picture; depending on the prediction distribution, different crop choices may appear to be very similar. Such examples are shown in Figure 7 , where our smart cropping is almost the same as the full image and conservative crop, respectively. In such cases, arbitrary decisions may be made, skewing the results. To account for this issue, we performed another analysis that takes into account the relative shape distances between different crops. For each crop version selected by a user, we compute the symmetric Hausdorff distance to all available crop choices (normalized by the longer dimension of the image). The Hausdorff distance is a commonly used shape similarity metric, e.g., for template matching in computer vision applications. The results of this analysis (blue bars in Figure 8 , middle) show that our guided crops have the smallest average Hausdorff distances to the crops selected by the subjects. This shows a clear preference for our guided crops over the full image or conservatively cropped images. We also compare various na?ve crops to the user choices ( Figure 8 , right). The results in red show the average distances of crops achieved by interpolating the crop shapes between the full and conservative versions. The results in green were achieved using the same optimization as our results (using ? p = 0.9925), but using the same constant quality value for every missing pixel. For both cases, the distances are significantly higher (and hence less desirable) than for our best setting.\n        \n      \n      \n        7 Additional Results\n        We tested our algorithm on several hundred panoramas and images with holes in the interior. In the supplementary material, we show an extensive comparison to various state-of-the-art algorithms\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n        Quality Prediction for Image Completion\n        ?\n        131:7\n        \n          \n        \n        Input and prediction (brighter is higher quality)\n        \n          \n        \n        Adobe Photoshop CS5 (Content Aware Fill)\n        \n          Figure 10: Comparison with other image completion algorithms. Please zoom into the PDF to see details. Note the blurring problem in the Photoshop and Komodakis and Tziritas? results at the bottom corners. The results of both previous methods also contain incorrectly synthesized texture at the bottom and semantic errors, i.e. snow patches in the sky. Many more examples can be found in the supplementary material.\n          \n        \n        Input and prediction (brighter is higher quality)\n        \n          \n        \n        Best (axis-aligned) rectangular crops\n        \n          \n        \n        Best rotated rectangle Best isosceles trapezoid\n        \n          Figure 9: Results of using alternative crop shapes. In both cases a significantly higher portion of the known pixels is included in the crop.\n        \n        (Adobe Photoshop CS5 Content Aware Fill, GIMP Resynthesizer, [Pritch et al. 2009], [Komodakis and Tziritas 2007], [Criminisi et al. 2003]) on 25 representative samples from each class. An example is shown in Figure 10 . In addition, we show automatic cropping results for the 25 panoramas; representative results are shown throughout the paper.\n        Our result (automatic crop not shown here)\n        Komodakis and Tziritas [2007]\n        On a dual Intel Xeon E5640 PC, we observe the following median timings for the 25 panoramas included in the supplementary material. We believe these numbers can be reduced with code optimization. Full completion With auto-cropping Restriction regions 0.32s 0.22s Feature extraction not applicable 0.93s Crop optimization not applicable 1.78s Completion 13.29s 6.52s Total 13.70s 9.17s  For these panoramas, our automatic crops contain on average slightly less than 50% of the missing pixels. Since the completion algorithm runtime is roughly linear in the number of missing pixels, this leads to a significant speed-up compared to first completing the full panoramas before cropping.\n      \n      \n        8 Limitations and Future Work\n        Image completion remains a very challenging problem. Like other recent approaches, our algorithm lacks higher-level (object-level) understanding of the input image. Thus, it will on occasion generate semantically implausible results, although our source location restriction significantly reduces these problems. Our cropping optimization currently ignores scene context and may crop out important objects in the scene. As seen in Figure 12 , it fails to realize the importance of the two subjects. A possible solution is to use face and/or saliency detectors. Our prediction function fit is not perfect, most likely due to occasional mismatches in subject ratings in the training database. As a result, our prediction function would, on occasion, mislabel the missing regions (e.g., Figure 11 , where mislabeling resulted in a smaller crop). A future course of action would be to either analyze the function on a per-person basis (i.e., personalize the automatic cropping function), or partition the data into clusters of similar preferences, with each having a different cropping function.\n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n        131:8\n        ?\n        J. Kopf et al.\n        \n          \n        \n        Input and prediction Entire completion (brighter is higher quality)\n        \n          Figure 11: Our prediction function sometimes tags ?good? pixels as ?bad.? In this example, practically the entire missing region can be completed well. As a result of the overly-conservative prediction, the suggested crop is smaller than necessary.\n          \n        \n        Input and prediction Cropped completion (brighter is higher quality)\n        \n          Figure 12: Example where our cropping fails to take into consideration object-level importance.\n        \n      \n      \n        References\n        \n          B ARNES , C., S HECHTMAN , E., F INKELSTEIN , A., AND G OLD MAN , D. B. 2009. Patchmatch: a randomized correspondence algorithm for structural image editing. ACM Trans. on Graphics (Proceedings of Siggraph) 28, 24:1?24:11.\n          B ERTALMIO , M., S APIRO , G., C ASELLES , V., AND B ALLESTER , C. 2000. Image inpainting. ACM Trans. on Graphics (Proceedings of Siggraph) 19.\n          B ORNARD , R., L ECAN , E., L ABORELLI , L., AND C HENOT , J.H. 2002. Missing data correction in still images and image sequences. Proceedings of the tenth ACM international conference on Multimedia, 355?361.\n          C RIMINISI , A., P EREZ , P., AND T OYAMA , K. 2003. Object removal by exemplar-based inpainting. In CVPR, 417?424.\n          D ARABI , S., S HECHTMAN , E., B ARNES , C., G OLDMAN , D. B., AND S EN , P. 2012. Image melding: Combining inconsistent images using patch-based synthesis. ACM Trans. on Graphics (Proceedings of Siggraph) 31, 4.\n          D RORI , I., C OHEN -O R , D., AND Y ESHURUN , H. 2003. Fragment-based image completion. ACM Trans. on Graphics (Proceedings of Siggraph) 22, 303?312.\n          E FROS , A., AND L EUNG , T. 1999. Texture synthesis by nonparametric sampling. In CVPR, 1033?1038.\n          F ELZENSZWALB , P., AND H UTTENLOCHER , D. 2004. Efficient graph-based image segmentation. IJCV 59, 2, 167?181.\n          F RIEDMAN , J., H ASTIE , T., AND T IBSHIRANI , R. 2000. Additive logistic regression: a statistical view of boosting. Annals of Statistics 28, 2, 337?407.\n          G ALASSI , M., D AVIES , J., T HEILER , J., G OUGH , B., AND J UNG MAN , G. 2009. GNU Scientific Library ? Reference Manual, Third Edition. Network Theory Ltd.\n          J IA , J., AND T ANG , C.-K. 2003. Image repairing: robust image synthesis by adaptive nd tensor voting. Proc. CVPR 2003, 643? 650.\n          K AWAI , N., S ATO , T., AND Y OKOYA , N. 2008. Image inpainting considering brightness change and spatial locality of textures and its evaluation. In PSIVT ?09, 271?282.\n          K OMODAKIS , N., AND T ZIRITAS , G. 2007. Image completion using efficient belief propagation via priority scheduling and dynamic pruning. IEEE Trans. Image Processing 16, 2649?2661.\n          K WATRA , V., E SSA , I., B OBICK , A., AND K WATRA , N. 2005. Texture optimization for example-based synthesis. ACM Trans. on Graphics (Proceedings of Siggraph) 24, 795?802.\n          M ANSFIELD , A., P RASAD , M., R OTHER , C., S HARP , T., K OHLI , P., AND V AN G OOL , L. 2011. Transforming image completion. In British Machine Vision Conf. (BMVC).\n          M ATSUSHITA , Y., O FEK , E., G E , W., T ANG , X., AND S HUM , H.Y. 2006. Full-frame video stabilization with motion inpainting. IEEE Trans. Pattern Anal. Mach. Intell. 28, 7 (July), 1150?1163.\n          N ELDER , J., AND M EAD , R. 1965. A simplex method for function minimization. Computer Journal 7, 308?313.\n          N OCEDAL , J., AND W RIGHT , S. J. 2000. Numerical Optimization. Springer.\n          P ? REZ , P., G ANGNET , M., AND B LAKE , A. 2004. Patchworks: example-based region tiling for image editing. Tech. Rep. MSRTR-2004-04, Microsoft Research.\n          P LATT , J. C. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in large margin classifiers, MIT Press, 61?74.\n          P RITCH , Y., K AV -V ENAKI , E., AND P ELEG , S. 2009. Shift-map image editing. In ICCV?09, 151?158.\n          S IMAKOV , D., C ASPI , Y., S HECHTMAN , E., AND I RANI , M. 2008. Summarizing visual data using bidirectional similarity. In CVPR.\n          S UN , J., Y UAN , L., J IA , J., AND S HUM , H.-Y. 2005. Image completion with structure propagation. ACM Trans. on Graphics (Proceedings of Siggraph) 24, 861?868.\n          S WAMY , D., C HANDLER , D., B UTLER , K., AND H EMAMI , S. 2011. Parametric quality assessment of synthesized textures. Proc. Human Vision and Electronic Imaging 2011.\n          W EXLER , Y., S HECHTMAN , E., AND I RANI , M. 2007. Spacetime video completion. TPAMI.\n          Y U , S., AND S HI , J. 2003. Multiclass spectral clustering. 313?319 vol.1.\n          Z HANG , M., Z HANG , L., S UN , Y., F ENG , L., AND M A , W. 2005. Auto cropping for digital photographs. In ICME.\n        \n        ACM Transactions on Graphics, Vol. 31, No. 6, Article 131, Publication Date: November 2012\n      \n    \n  ",
  "resources" : [ ]
}