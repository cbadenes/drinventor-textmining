{
  "uri" : "sig2010-a30-joshi_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2010/a30-joshi_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Image Deblurring using Inertial Measurement Sensors",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Ondrej-Sindelar",
      "name" : "Ondrej",
      "surname" : "Sindelar"
    }, {
      "uri" : "http://drinventor/Filip-Sroubek",
      "name" : "Filip",
      "surname" : "Sroubek"
    } ]
  },
  "bagOfWords" : [ "we", "approach", "use", "combination", "inexpensive", "gyroscope", "accelerometer", "energy", "optimization", "framework", "estimate", "blur", "function", "from", "camera?s", "acceleration", "angular", "velocity", "during", "exposure", "we", "solve", "camera", "motion", "high", "sampling", "rate", "during", "exposure", "infer", "latent", "image", "use", "joint", "optimization", "we", "method", "completely", "automatic", "handle", "per-pixel", "spatially-varying", "blur", "out-perform", "current", "lead", "image-based", "method", "we", "experiment", "show", "handle", "large", "kernel", "up", "least", "100", "pixel", "typical", "size", "30", "pixel", "we", "also", "present", "method", "perform", "ground-truth", "measurement", "camera", "motion", "blur", "we", "use", "method", "validate", "we", "hardware", "deconvolution", "approach", "we", "have", "present", "novel", "hardware", "attachment", "can", "affix", "any", "consumer", "camera", "we", "derive", "model", "handle", "spatially-varying", "blur", "due", "full", "6-dof", "camera", "motion", "spatially-varying", "scene", "depth", "however", "we", "system", "assume", "spatially", "invariant", "depth", "we", "method", "completely", "automatic", "handle", "per-pixel", "spatially-varying", "blur", "out-performs", "current", "lead", "image-based", "method", "we", "experiment", "show", "handle", "large", "kernel", "up", "100", "pixel", "typical", "size", "30", "pixel", "second", "contribution", "we", "expand", "previous", "work", "develop", "validation", "method", "recover", "ground-truth", "per-pixel", "spatiallyvarying", "motion", "blur", "due", "camera-shake", "we", "use", "method", "validate", "we", "hardware", "blur", "estimation", "approach", "also", "use", "study", "property", "motion", "blur", "due", "camera", "shake", "image", "deblurring", "combination", "two", "tightly", "couple", "sub-problem", "psf", "estimation", "non-blind", "image", "deconvolution", "Blind", "deconvolution", "inherently", "ill-posed", "problem", "due", "loss", "information", "during", "blur", "alternative", "approach", "those", "use", "additional", "hardware", "augment", "camera", "aid", "blur", "process", "-lsb-", "Ben-Ezra", "Nayar", "2004", "Tai", "et", "al.", "2008", "Park", "et", "al.", "2008", "-rsb-", "contrast", "we", "method", "record", "actual", "camera", "motion", "remove", "blur", "from", "image", "further", "difference", "method", "can", "only", "dampen", "2d", "motion", "e.g.", "method", "handle", "camera", "roll", "while", "we", "method", "can", "handle", "six", "degree", "motion", "say", "we", "method", "could", "use", "conjunction", "image", "stabilization", "one", "be", "able", "obtain", "reading", "mechanical", "offset", "perform", "system", "recent", "research", "hardware-based", "approach", "image", "deblurr", "modify", "image", "capture", "process", "aid", "deblurring", "area", "we", "work", "most", "similar", "approach", "use", "hybrid", "camera", "-lsb-", "Ben-Ezra", "Nayar", "2004", "Tai", "et", "al.", "2008", "-rsb-", "which", "track", "camera", "motion", "use", "datum", "from", "video", "camera", "attach", "still", "camera", "work", "compute", "global", "frame-to-frame", "motion", "calculate", "2d", "camera", "motion", "during", "image-exposure", "window", "allow", "we", "measure", "more", "degree", "camera", "motion", "higher-rate", "lower", "cost", "another", "difficulty", "hybrid", "camera", "approach", "we", "avoid", "can", "difficult", "get", "high-quality", "properly", "expose", "image", "out", "video", "camera", "low", "light", "condition", "where", "image", "blur", "prevalent", "we", "do", "however", "use", "modified", "form", "Ben-Ezra", "Nayar?s", "work", "controlled", "situation", "help", "validate", "we", "estimate", "camera", "motion", "also", "similar", "we", "work", "Park", "et", "al.", "-lsb-", "2008", "-rsb-", "who", "use", "3-axis", "accelerometer", "measure", "motion", "blur", "we", "work", "also", "complementary", "Raskar", "et", "al.", "-lsb-", "2006", "-rsb-", "who", "develop", "fluttered", "camera", "shutter", "create", "image", "blur", "more", "easily", "invert", "section", "we", "describe", "design", "challenge", "decision", "build", "we", "sense", "platform", "image", "deblurring", "next", "we", "give", "overview", "camera", "dynamics", "inertial", "sensor", "follow", "we", "deblurr", "approach", "specifically", "blur", "formation", "commonly", "model", "where", "blur", "kernel", "-lrb-", "-rrb-", "noise", "few", "exception", "most", "image", "deblurr", "work", "assume", "spatially", "invariant", "kernel", "however", "often", "do", "hold", "practice", "-lsb-", "Joshi", "et", "al.", "2008", "Levin", "et", "al.", "2009", "-rsb-", "first", "let", "we", "consider", "image", "camera", "capture", "during", "its", "exposure", "window", "intensity", "light", "from", "scene", "point", "-lrb-", "-rrb-", "instantaneous", "time", "capture", "image", "plane", "location", "-lrb-", "-rrb-", "which", "function", "camera", "projection", "matrix", "homogenous", "coordinate", "can", "write", "camera", "motion", "vary", "time", "function", "camera", "rotation", "translation", "cause", "fix", "point", "scene", "project", "different", "location", "each", "time", "camera", "projection", "matrix", "can", "where", "intrinsic", "matrix", "canonical", "perspective", "projection", "matrix", "time", "dependent", "extrinsic", "matrix", "compose", "camera", "rotation", "translation", "case", "image", "blur", "necessary", "consider", "absolute", "motion", "camera", "only", "relative", "motion", "its", "effect", "image", "we", "model", "consider", "planar", "homography", "map", "initial", "projection", "point", "any", "other", "time", "i.e.", "reference", "coordinate", "frame", "coincident", "frame", "time", "-lrb-", "-rrb-", "particular", "depth", "where", "unit", "vector", "orthogonal", "image", "plane", "thus", "give", "image", "time", "pixel", "value", "any", "subsequent", "image", "image", "warp", "can", "re-written", "matrix", "form", "where", "column-vectorized", "image", "-lrb-", "-rrb-", "sparse", "re-sampling", "matrix", "implement", "image", "warping", "resampling", "due", "homography", "spatially", "invariant", "kernel", "equation", "now", "replace", "spatially-variant", "blur", "represent", "sparse-matrix", "we", "spatially-varying", "blur", "model", "give", "thus", "camera-induced", "spatially-varying", "blur", "estimation", "process", "reduce", "estimate", "rotation", "translation", "time", "-lsb-", "...", "-rsb-", "scene", "depths", "camera", "intrinsic", "K.", "represent", "camera-shake", "blur", "six", "degree", "motion", "camera", "instead", "purely", "image", "plane", "number", "unknown", "reduce", "significantly", "six", "unknown", "per", "each", "time-step", "unknown", "depth", "per-pixel", "-lrb-", "unknown", "-rrb-", "camera", "intrinsic", "which", "focal", "length", "most", "important", "factor", "result", "6m", "wh", "unknown", "oppose", "image-based", "approach", "must", "recover", "kernel", "each", "pixel", "result", "wh", "unknown", "practice", "since", "we", "assume", "single", "depth", "scene", "unknown", "we", "system", "reduce", "6m", "we", "modify", "formulation", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "use", "we", "spatially-varying", "blur", "model", "we", "formulate", "image", "deconvolution", "use", "bayesian", "framework", "find", "most", "likely", "estimate", "sharp", "image", "give", "observe", "blur", "image", "blur", "matrix", "noise", "level", "use", "maximum", "posteriorus", "-lrb-", "map", "-rrb-", "technique", "we", "express", "maximization", "over", "probability", "distribution", "posterior", "use", "Bayes", "rule", "result", "minimization", "sum", "negative", "log", "likelihood", "give", "blur", "formation", "model", "-lrb-", "equation", "-rrb-", "datum", "negative", "log", "likelihood", "contribution", "we", "deconvolution", "approach", "new", "datum", "term", "use", "spatially-varying", "model", "derive", "previous", "section", "we", "image", "negative", "log", "likelihood", "same", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "sparse", "gradient", "penalty", "which", "enforce", "hyper-laplacian", "distribution", "-lrb-", "-rrb-", "0.8", "minimization", "perform", "use", "iteratively", "re-weighted", "least-square", "-lsb-", "Stewart", "1999", "-rsb-", "discuss", "previous", "section", "camera", "motion", "blur", "dependent", "rotation", "translation", "time", "-lsb-", "...", "Accelerometers", "measure", "total", "acceleration", "give", "point", "along", "axis", "while", "gyroscope", "measure", "angular", "velocity", "give", "point", "around", "axis", "note", "move", "rigid", "body", "pure", "rotation", "all", "point", "same", "while", "translation", "all", "point", "same", "when", "body", "rotate", "before", "derive", "how", "compute", "camera", "motion", "from", "inertial", "measurement", "we", "first", "present", "we", "notation", "summarize", "Table", "symbol", "description", "initial", "current", "frame", "current", "angular", "po", "accel", "initial", "frame", "current", "angular", "vel", "current", "frame", "current", "po", "accel", "initial", "frame", "Accel", "measure", "acceleration", "sum", "acceleration", "due", "translation", "camera", "centripetal", "acceleration", "due", "rotation", "tangential", "component", "angular", "acceleration", "gravity", "all", "rotate", "current", "frame", "camera", "recover", "relative", "camera", "rotation", "necessary", "recover", "angular", "velocity", "each", "time-step", "coordinate", "system", "initial", "frame", "which", "can", "integrate", "get", "angular", "position", "recover", "relative", "camera", "translation", "we", "need", "first", "compute", "accelerometer", "position", "each", "time-step", "relative", "initial", "frame", "from", "we", "can", "recover", "camera", "translation", "camera", "rotation", "can", "recover", "sequentially", "integrate", "rotate", "measure", "angular", "velocity", "initial", "camera", "frame", "where", "angleaxist", "om", "convert", "angular", "position", "vector", "rotation", "matrix", "since", "we", "only", "concern", "relative", "rotation", "initial", "rotation", "zero", "once", "rotation", "compute", "each", "time-step", "we", "can", "compute", "acceleration", "initial", "frame?s", "coordinate", "system", "integrate", "acceleration", "minus", "constant", "acceleration", "gravity", "get", "accelerometer?s", "relative", "position", "each", "timestep", "we", "concern", "relative", "position", "we", "set", "initial", "position", "zero", "we", "also", "assume", "initial", "velocity", "zero", "give", "we", "can", "compute", "camera", "position", "time", "equation", "20", "necessary", "subtract", "value", "gravity", "initial", "frame", "camera", "we", "note", "however", "initial", "rotation", "camera", "relative", "world", "unknown", "gyroscope", "only", "measure", "velocity", "accelerometer", "can", "use", "estimate", "initial", "orientation", "camera", "initially", "have", "external", "force", "other", "than", "gravity", "we", "have", "find", "assumption", "unreliable", "so", "we", "instead", "make", "assumption", "measure", "acceleration", "normally", "distribute", "about", "constant", "force", "gravity", "we", "have", "find", "reliable", "when", "camera", "motion", "due", "high-frequency", "camera-shake", "thus", "we", "set", "direction", "mean", "acceleration", "vector", "direction", "gravity", "summarize", "camera", "rotation", "translation", "recover", "integrate", "measure", "acceleration", "angular", "velocity", "rotate", "camera?s", "initial", "coordinate", "frame", "furthermore", "even", "camera", "motion", "know", "perfectly", "one", "still", "need", "know", "scene", "depth", "discuss", "section", "well", "know", "compute", "motion", "integrate", "differential", "sensor", "can", "lead", "drift", "computed", "result", "drift", "due", "noise", "present", "sensor", "reading", "integration", "noisy", "signal", "lead", "temporally", "increase", "deviation", "computed", "motion", "from", "true", "motion", "we", "have", "measure", "standard", "deviation", "we", "gyroscope?s", "noise", "0.5", "deg/s", "accelerometer", "noise", "0.006", "m/s", "use", "sample", "from", "when", "gyroscope", "accelerometer", "hold", "stationary", "-lrb-", "zero", "angular", "velocity", "constant", "acceleration", "respectively", "-rrb-", "we", "experiment", "significantly", "less", "drift", "rotation", "due", "need", "perform", "only", "single", "integration", "step", "gyroscope", "datum", "necessity", "integrate", "twice", "get", "positional", "datum", "from", "accelerometer", "cause", "more", "drift", "we", "propose", "novel", "aid", "blind", "deconvolution", "algorithm", "compute", "camera-motion", "in-turn", "image", "blur", "function", "best", "match", "measure", "acceleration", "angular", "velocity", "while", "maximize", "likelihood", "deblurred", "latent", "image", "accord", "natural", "image", "prior", "we", "deconvolution", "algorithm", "compensate", "positional", "drift", "assume", "linear", "time", "which", "can", "estimate", "one", "know", "final", "end", "position", "camera", "we", "assume", "rotational", "drift", "minimal", "thus", "contrast", "traditional", "blind-deconvolution", "algorithm", "solve", "each", "value", "kernel", "psf", "we", "algorithm", "only", "have", "solve", "few", "unknown", "we", "solve", "use", "energy", "minimization", "framework", "perform", "search", "small", "local", "neighborhood", "around", "initially", "compute", "end", "point", "we", "experiment", "we", "have", "find", "camera", "travel", "order", "couple", "millimeter", "during", "long", "exposure", "-lrb-", "longest", "we", "have", "try", "1/2", "second", "-rrb-", "we", "note", "few", "millimeter", "translation", "depth", "-lrb-", "-rrb-", "have", "little", "effect", "image", "lens", "common", "focal", "length", "thus", "drift", "only", "significant", "source", "error", "we", "set", "we", "optimization", "parameter", "search", "optimal", "end", "point", "within", "1mm", "radius", "initially", "compute", "end", "point", "subject", "constraint", "acceleration", "along", "recover", "path", "match", "measure", "acceleration", "best", "least-square", "sense", "optimal", "end", "point", "one", "result", "deconvolved", "image", "highest", "log-likelihood", "measure", "hyper-laplacian", "image", "prior", "-lrb-", "discuss", "section", "3.1", "-rrb-", "specifically", "let", "we", "define", "function", "give", "potential", "end", "point", "-lrb-", "-rrb-", "compute", "camera?s", "translational", "path", "which", "best", "match", "least", "square", "sense", "observe", "acceleration", "terminate", "-lrb-", "-rrb-", "notational", "convenience", "let", "define", "function", "form", "blur", "sampling", "matrix", "from", "camera", "intrinsic", "extrinsic", "scene", "depth", "use", "rigid-body", "dynamics", "temporal", "integration", "process", "discuss", "section", "3.1", "3.3", "drift-compensated", "blur", "matrix", "deconvolution", "equation", "we", "search", "over", "space", "-lrb-", "-rrb-", "find", "-lrb-", "-rrb-", "result", "image", "have", "highest", "likelihood", "give", "observation", "image", "prior", "we", "perform", "energy", "minimization", "use", "Nelder-Mead", "simplex", "method", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.2", "use", "error", "function", "inner", "loop", "optimization", "we", "perform", "optimization", "1/10", "down-sampled", "version", "-lrb-", "we", "21", "mp", "image", "-rrb-", "result", "from", "we", "search", "process", "show", "plot", "camera", "motion", "Figure", "visually", "figure", "when", "use", "deblur", "image", "run", "time", "search", "method", "about", "minute", "0.75", "mp", "image", "search", "only", "need", "run", "once", "either", "entire", "image", "could", "run", "subsection", "image", "preferable", "once", "drift", "correct", "PSF", "more", "accurate", "entire", "image", "Computing", "Scene", "Depth", "note", "spatially", "invariant", "scene", "depth", "implicitly", "compute", "during", "drift", "compensation", "process", "scale", "end", "point", "equally", "dimension", "equivalent", "scale", "depth", "value", "specifically", "optimization", "over", "u/d", "v/d", "thus", "solve", "single", "depth", "value", "entire", "scene", "previous", "section", "we", "discuss", "how", "remove", "camera", "motion", "blur", "recover", "camera", "rotation", "translation", "from", "accelerometer", "gyroscope", "section", "we", "describe", "we", "hardware", "record", "accelerometer", "gyroscope", "datum", "implementation", "related", "concern", "challenge", "since", "six", "unknown", "per", "time-step", "minimal", "configuration", "sensor", "six", "possible", "recover", "rotation", "translation", "use", "six", "accelerometer", "alone", "sense", "each", "axis", "pair", "three", "different", "point", "rigid-body", "however", "after", "experiment", "method", "we", "find", "accelerometer", "alone", "too", "noisy", "reliable", "computation", "rotation", "all", "part", "commodity", "off-theshelf", "component", "purchase", "online", "additionally", "slr?s", "hotshoe", "i.e.", "flash", "trigger", "signal", "wire", "Arduino", "board", "trigger", "signal", "from", "slr", "remain", "low", "entire", "length", "exposure", "high", "otherwise", "Arduino", "board", "interrupt", "drive", "when", "trigger", "signal", "from", "SLR", "fire", "accelerometer", "gyroscope", "poll", "200hz", "during", "exposure", "window", "each", "time", "sensor", "read", "value", "send", "over", "Bluetooth", "serial", "port", "interface", "additionally", "internal", "high-resolution", "counter", "read", "actual", "elapsed", "time", "between", "each", "reading", "sensor", "report", "sensor", "Arduino", "board", "mount", "laser-cut", "acrylic", "base", "secure", "board", "sensor", "battery", "pack", "acrylic", "mount", "tightly", "screw", "camera", "tripod", "mount", "only", "other", "connection", "camera", "flash", "trigger", "cable", "we", "hardware", "attachment", "have", "optional", "feature", "use", "calibration", "validation", "experiment", "mount", "hole", "point", "Grey", "high-speed", "camera", "when", "Arduino", "board", "sampling", "inertial", "sensor", "can", "also", "send", "100", "hz", "trigger", "high-speed", "camera", "we", "use", "high-speed", "datum", "help", "calibrate", "we", "sensor", "acquire", "datum", "get", "ground", "truth", "measurement", "motion", "blur", "demonstrate", "importance", "accounting", "spatially", "variance", "bottom", "row", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "non-corresponding", "area", "accurately", "compute", "camera", "motion", "from", "inertial", "sensor", "necessary", "calibrate", "several", "aspect", "we", "system", "we", "need", "accurately", "calibrate", "sensor", "response", "we", "have", "find", "they", "deviate", "from", "publish", "response", "range", "also", "necessary", "know", "position", "accelerometer", "relative", "camera?s", "optical", "center", "lastly", "we", "need", "calibrate", "camera", "intrinsic", "we", "calibrate", "value", "two", "stage", "first", "stage", "calibrate", "sensor", "response", "we", "do", "rotate", "gyroscope", "known", "constant", "angular", "velocity", "recover", "mapping", "from", "10-bit", "a/d", "output", "degrees/s", "we", "perform", "measurement", "several", "known", "angular", "velocity", "confirm", "gyroscope", "have", "linear", "response", "calibrate", "accelerometer", "we", "hold", "they", "stationary", "six", "orientation", "respect", "gravity", "-lrb-", "-rrb-", "which", "allow", "we", "map", "a/d", "output", "unit", "m/s", "calibrate", "we", "setup", "measure", "ground-truth", "measurement", "camera-shake", "we", "develop", "method", "accurately", "recover", "camera?s", "position", "during", "exposure", "use", "method", "inspire", "Ben-Ezra", "Nayar", "-lsb-", "2004", "-rsb-", "however", "instead", "track", "2d", "motion", "we", "track", "6d", "motion", "we", "attach", "high-speed", "camera", "-lrb-", "200", "fp", "pointgrey", "DragonFly", "Express", "-rrb-", "we", "sensor", "platform", "we", "Arduino", "micro-controller", "code", "set", "trigger", "high-speed", "camera", "100", "fp", "during", "slr?s", "exposure", "window", "lab", "setting", "we", "create", "scene", "significant", "amount", "texture", "take", "about", "10", "image", "exposure", "range", "from", "1/10", "1/2", "second", "each", "image", "accelerometer", "gyro", "datum", "record", "addition", "high-speed", "frame", "we", "take", "high-speed", "frame", "from", "shot", "acquire", "additional", "widebaseline", "shot", "slr", "high-speed", "camera", "use", "all", "datum", "we", "create", "3d", "reconstruction", "scene", "use", "bundle", "adjustment", "-lrb-", "we", "process", "use", "ransac", "feature", "matching", "compute", "sparse", "3d", "structure", "from", "motion", "compute", "camera", "focal", "length", "-rrb-", "Figure", "show", "we", "high-speed", "camera", "attachment", "few", "frame", "from", "high", "speed", "camera", "-lrb-", "we", "take", "about", "hundred", "total", "both", "camera", "process", "-rrb-", "reconstruction", "process", "give", "we", "collection", "sparse", "3d", "point", "camera", "rotation", "camera", "translation", "unknown", "global", "transformation", "scale", "ambiguity", "between", "camera", "depth", "scene", "depth", "we", "resolve", "scale", "ambiguity", "use", "calibration", "grid", "known", "size", "we", "now", "describe", "result", "we", "ground-truth", "camera-shake", "measurement", "compare", "result", "use", "we", "deblurring", "method", "ground-truth", "measurement", "we", "also", "compare", "we", "result", "those", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "use", "implementation", "author", "have", "available", "online", "we", "also", "show", "result", "we", "method", "run", "natural", "image", "acquire", "outside", "lab", "setup", "compare", "result", "use", "previous", "work", "well", "Figure", "we", "show", "visualization", "ground-truth", "spatiallyvarying", "psf", "image", "from", "we", "lab", "setup", "image", "show", "some", "interesting", "property", "significant", "variation", "across", "image", "plane", "also", "kernel", "display", "fix", "depth", "thus", "all", "spatial", "variance", "due", "rotation", "demonstrate", "importance", "accounting", "spatial", "variance", "bottom", "row", "Figure", "we", "show", "result", "where", "we", "have", "deconvolve", "use", "psf", "correct", "part", "image", "psf", "different", "non-corresponding", "area", "result", "quite", "interesting", "show", "some", "common", "assumption", "make", "image", "deconvolution", "do", "always", "hold", "most", "deconvolution", "work", "assume", "spatially", "invariant", "kernel", "which", "really", "only", "apply", "camera", "motion", "under", "orthographic", "model", "however", "typical", "imaging", "setup", "-lrb-", "we", "use", "40mm", "lens", "-rrb-", "perspective", "effect", "strong", "enough", "induce", "spatially-varying", "blur", "we", "also", "note", "often", "roll", "component", "blur", "something", "also", "model", "spatially", "invariant", "kernel", "lastly", "we", "observe", "translation", "thus", "depth", "dependent", "effect", "can", "significant", "which", "interesting", "often", "think", "most", "camera-shake", "blur", "due", "rotation", "please", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "example", "Figure", "use", "same", "scene", "above", "we", "show", "comparison", "deconvolution", "result", "use", "we", "method", "ground-truth", "two", "other", "image", "show", "lab", "calibration", "scene", "be", "take", "exposure", "from", "1/3", "1/10", "second", "each", "image", "we", "show", "input", "result", "deconvolve", "psf", "from", "initial", "motion", "estimate", "after", "perform", "drift", "correction", "compare", "deconvolution", "use", "psf", "from", "recover", "groundtruth", "motion", "psf", "recover", "use", "method", "Shan", "et", "al.", "-lsb-", "2008", "-rsb-", "Fergus", "et", "al.", "-lsb-", "2006", "-rsb-", "latter", "two", "comparison", "we", "make", "best", "effort", "adjust", "parameter", "recover", "best", "blur", "kernel", "possible", "make", "comparison", "fair", "all", "result", "be", "deblurr", "use", "exactly", "same", "deconvolution", "method", "Levin", "et", "al.", "-lsb-", "2007", "-rsb-", "result", "figure", "show", "wide", "variety", "blur", "yet", "we", "method", "recover", "accurate", "kernel", "provide", "deconvolution", "result", "very", "close", "ground-truth", "all", "case", "we", "result", "better", "than", "those", "use", "Shan", "et", "al.", "Fergus", "et", "al.", "method", "after", "calibrate", "we", "hardware", "system", "use", "method", "discuss", "section", "4.2", "we", "take", "camera", "outside", "lab", "use", "laptop", "Bluetooth", "adapter", "capture", "inertial", "sensor", "datum", "Figure", "we", "show", "several", "result", "where", "we", "have", "deblurr", "image", "use", "we", "method", "Shan", "et", "al.", "method", "Fergus", "et", "al.", "method", "Shan", "et", "al.", "Fergus", "et", "al.", "result", "be", "deconvolve", "use", "Levin", "et", "al.", "method", "-lsb-", "2007", "-rsb-", "we", "result", "deconvolve", "use", "we", "spatially-varying", "deconvolution", "method", "discuss", "section", "3.1", "all", "image", "we", "result", "show", "clear", "improvement", "over", "input", "blurry", "image", "still", "some", "residually", "ring", "unavoidable", "due", "frequency", "loss", "during", "blur", "Shan", "et", "al.", "result", "vary", "quality", "many", "show", "large", "ring", "artifact", "due", "kernel", "misestimation", "over-sharpening", "Fergus", "et", "al.", "result", "generally", "blurrier", "than", "ours", "all", "image", "show", "here", "be", "shoot", "1/2", "1/10", "second", "exposure", "40mm", "lens", "Canon", "1Ds", "Mark", "III", "additional", "result", "visit", "http://research.microsoft.com/en", "us/um/redmond", "groups/ivm/imudeblurring", "work", "we", "present", "aid", "blind", "deconvolution", "algorithm", "use", "hardware", "attachment", "conjunction", "corresponding", "blurry", "input", "image", "natural", "image", "prior", "compute", "perpixel", "spatially-varying", "blur", "deconvolve", "image", "produce", "sharp", "result", "several", "benefit", "we", "method", "over", "previous", "approach", "-lrb-", "-rrb-", "we", "method", "automatic", "have", "user-tuned", "parameter", "-lrb-", "-rrb-", "use", "inexpensive", "commodity", "hardware", "could", "easily", "build", "camera", "produce", "mass-market", "attachment", "more", "compact", "than", "we", "prototype", "we", "have", "show", "advantage", "over", "purely", "image-based", "method", "which", "some", "sense", "surprising?blind", "deconvolution", "inherently", "ill-posed", "problem", "thus", "extra", "information", "inertial", "measurement", "should", "helpful", "biggest", "limitation", "we", "method", "sensor", "accuracy", "noise", "we", "method?s", "performance", "degrade", "under", "few", "case", "-lrb-", "-rrb-", "drift", "large", "enough", "search", "space", "we", "optimization", "process", "too", "large", "i.e.", "greater", "than", "couple", "mm", "-lrb-", "-rrb-", "we", "estimation", "initial", "camera", "rotation", "relative", "gravity", "incorrect", "similarly", "camera", "move", "way", "normally", "distribute", "about", "gravity", "vector", "-lrb-", "-rrb-", "significant", "depth", "variation", "scene", "camera", "undergo", "significant", "translation", "-lrb-", "-rrb-", "camera", "translate", "some", "initial", "constant", "velocity", "-lrb-", "-rrb-", "large", "image", "frequency", "information", "loss", "blur", "sensor", "inexpensive", "one", "could", "easily", "add", "sensor", "redundancy", "perform", "denoising", "average", "either", "analog", "digital", "domain", "-lrb-", "-rrb-", "one", "could", "recover", "depth", "scene", "perform", "depth", "from", "motion", "blur", "algorithm", "similar", "depth", "from", "defocus", "we", "pursue", "problem", "however", "important", "note", "vary", "scene", "depth", "do", "always", "significantly", "affect", "blur", "typical", "situation", "we", "have", "find", "depth", "only", "need", "accurate", "deblurring", "object", "within", "meter", "from", "camera", "most", "often", "people", "take", "image", "where", "scene", "farther", "than", "distance", "-lrb-", "-rrb-", "we", "assume", "initial", "translation", "velocity", "zero", "we", "accelerometer", "give", "we", "measure", "while", "we", "currently", "only", "consider", "accelerometer", "datum", "during", "exposure", "we", "actually", "record", "all", "sensor", "datum", "before", "after", "each", "exposure", "well", "-lrb-", "-rrb-", "issue", "all", "deblurr", "method", "frequency", "loss", "cause", "unavoidable", "artifact", "during", "deconvolution", "could", "appear", "ringing", "banding", "over-smoothing", "depend", "deconvolution", "method", "we", "also", "thank", "Mike", "Sinclair", "Turner", "Whitted", "help", "hardware", "lab" ],
  "content" : "Our approach uses a combination of inexpensive gyroscopes and accelerometers in an energy optimization framework to estimate a blur function from the camera?s acceleration and angular velocity during an exposure. We solve for the camera motion at a high sampling rate during an exposure and infer the latent image using a joint optimization. Our method is completely automatic, handles per-pixel, spatially-varying blur, and out-performs the current leading image-based methods. Our experiments show that it handles large kernels ? up to at least 100 pixels, with a typical size of 30 pixels. We also present a method to perform ?ground-truth? measurements of camera motion blur. We use this method to validate our hardware and deconvolution approach. We have present a novel hardware attachment that can be affixed to any consumer camera. We derive a model that handles spatially-varying blur due to full 6-DOF camera motion and spatially-varying scene depth; however, our system assumes spatially invariant depth. Our method is completely automatic, handles per-pixel, spatially-varying blur, out-performs current leading image-based methods, and our experiments show it handles large kernels ? up to 100 pixels, with a typical size of 30 pixels. As a second contribution, we expand on previous work and develop a validation method to recover ?ground-truth?, per-pixel spatiallyvarying motion blurs due to camera-shake. We use this method to validate our hardware and blur estimation approach, and also use it to study the properties of motion blur due to camera shake. Image deblurring is the combination of two tightly coupled sub-problems: PSF estimation and non-blind image deconvolution. Blind deconvolution is an inherently ill-posed problem due to the loss of information during blurring. Alternative approaches are those that use additional hardware to augment a camera to aid in the blurring process [Ben-Ezra and Nayar 2004; Tai et al. 2008; Park et al. 2008]. In contrast, our method records the actual camera motion and removes the blur from the image. A further difference is that IS methods can only dampen 2D motion, e.g., these methods will not handle camera roll, while our method can handle six degrees of motion. That said, our method could be used in conjunction with image stabilization, if one were able to obtain readings of the mechanical offsetting performed by the IS system. Recent research in hardware-based approaches to image deblurring modify the image capture process to aid in deblurring. In this area, our work is most similar to approaches that uses hybrid cameras [Ben-Ezra and Nayar 2004; Tai et al. 2008], which track camera motion using data from a video camera attached to a still camera. This work compute a global frame-to-frame motion to calculate the 2D camera motion during the image-exposure window. This allows us to measure more degrees of camera motion at a higher-rate and lower cost. Another difficulty of the hybrid camera approach that we avoid is that it can be difficult to get high-quality, properly exposed images out of the video camera in the low light conditions where image blur is prevalent. We do, however, use a modified form of Ben-Ezra and Nayar?s work in a controlled situation to help validate our estimated camera motions. Also similar to our work is that of Park et al. [2008], who use a 3-axis accelerometer to measure motion blur. Our work is also complementary to that of Raskar et al. [2006], who developed a fluttered camera shutter to create images with blur that was more easily inverted. In this section, we describe the design challenges and decisions for building our sensing platform for image deblurring. Next we give an overview of camera dynamics and inertial sensors followed by our deblurring approach. Specifically, blur formation is commonly modeled as: where K is the blur kernel, N ? N (0, ? 2 ) is the noise. With a few exceptions, most image deblurring work assumes a spatially invariant kernel; however, this often does not hold in practice [Joshi et al. 2008; Levin et al. 2009]. First, let us consider the image a camera captures during its exposure window. The intensity of light from a scene point (X, Y, Z) at an instantaneous time t is captured on the image plane at a location (u t , v t ), which is a function of the camera projection matrix P t . In homogenous coordinates, this can be written as: If there is camera motion, P t varies with time as a function of camera rotation and translation causing fixed points in the scene to project to different locations at each time. The camera projection matrix can be where K is the intrinsics matrix, ? is the canonical perspective projection matrix, and E t is the time dependent extrinsics matrix that is composed of the camera rotation R t and translation T t . In the case of image blur, it is not necessary to consider the absolute motion of the camera, only the relative motion and its effect on the image. We model this by considering the planar homography that maps the initial projection of points at t = 0 to any other time t, i.e., the reference coordinate frame is coincident with the frame at time t = 0:\n          (4) for a particular depth d, where N is the unit vector that is orthogonal to the image plane. Thus given an image I at time t = 0, the pixel value of any subsequent image is: This image warp can be re-written in matrix form as: where I t and I are column-vectorized images and A t (d) is a sparse re-sampling matrix that implements the image warping and resampling due to the homography. The spatially invariant kernel in Equation 1 is now replaced by a spatially-variant blur represented by a sparse-matrix: Z s our spatially-varying blur model is given by: 2 Thus, the camera-induced, spatially-varying blur estimation process is reduced to estimating the rotations R and translations T for times [0... t], the scene depths d, and the camera intrinsics K. By representing the camera-shake blur in the six degrees of motion of the camera, instead of purely in the image plane, the number of unknowns is reduced significantly ? there are six unknowns per each of M time-steps, an unknown depth per-pixel (w ? h unknowns), and the camera intrinsics, of which the focal length is the most important factor. This results in 6M + wh + 1 unknowns as opposed to an image-based approach that must recover an k ? k kernel for each pixel, resulting in k 2 ? wh unknowns. In practice, since we assume a single depth for the scene, the unknowns in our system reduce to 6M + 2. We modify the formulation of Levin et al. [2007] to use our spatially-varying blur model. We formulate image deconvolution using a Bayesian framework and find the most likely estimate of the sharp image I, given the observed blurred image B, the blur matrix A, and noise level ? 2 using a maximum a posteriori (MAP) technique. We express this as a maximization over the probability distribution of the posterior using Bayes? rule. The result is a minimization of a sum of negative log likelihoods: Given the blur formation model (Equation 1), the ?data? negative log likelihood is: The contribution of our deconvolution approach is this new data term that uses the spatially-varying model derived in the previous section. Our ?image? negative log likelihood is the same as Levin et al.?s [2007] sparse gradient penalty, which enforces a hyper-Laplacian distribution: L(I) = ?|| I|| 0.8 . The minimization is performed using iteratively re-weighted least-squares [Stewart 1999]. As discussed in the previous section, camera motion blur is dependent on rotations R and translations T for times [0... Accelerometers measure the total acceleration at a given point along an axis, while gyroscopes measure the angular velocity at a given point around an axis. Note that for a moving rigid body, the pure rotation at all points is the same, while the translations for all points is not the same when the body is rotating. Before deriving how to compute camera motion from inertial measurements, we first present our notation, as summarized in Table 1 . Symbol Description t R i Initial to current frame ? t i , ? t i , ? t i Current angular pos. , and accel. in initial frame ? t t Current angular vel. in the current frame x i t ,v t i ,a t i Current pos. and accel. in the initial frame a p t Accel. The measured acceleration is the sum of the acceleration due to translation of the camera, centripetal acceleration due to rotation, the tangential component of angular acceleration, and gravity, all rotated into the current frame of the camera. To recover the relative camera rotation, it is necessary to recover the angular velocity for each time-step t in the coordinate system of the initial frame ? t i , which can be integrated to get the angular position. To recover relative camera translation, we need to first compute the accelerometer position for each time-step relative to the initial frame. From this, we can recover the camera translation. The camera rotation can be recovered by sequentially integrating and rotating the measured angular velocity into the initial camera frame. where ?angleAxisT oM at? converts the angular position vector to a rotation matrix. Since we are only concerned with relative rotation, the initial rotation is zero: Once the rotations are computed for each time-step, we can compute the acceleration in the initial frame?s coordinate system: and integrate the acceleration, minus the constant acceleration of gravity, to get the accelerometer?s relative position at each timestep: As we are concerned with relative position, we set the initial position to zero, and we also assume that the initial velocity is zero: Given this, we can compute the camera position at time t: In Equation 20, it is necessary to subtract the value of gravity in the initial frame of the camera. We note, however, that the initial rotation of the camera relative to the world is unknown, as the gyroscopes only measure velocity. The accelerometers can be used to estimate the initial orientation if the camera initially has no external forces on it other than gravity. We have found this assumption unreliable, so we instead make the assumption that the measured acceleration is normally distributed about the constant force of gravity. We have found this reliable when the camera motion is due to high-frequency camera-shake. Thus we set the direction of mean acceleration vector as the direction of gravity: To summarize, the camera rotation and translation are recovered by integrating the measured acceleration and angular velocities that  are rotated into the camera?s initial coordinate frame. Furthermore, even if the camera motion is known perfectly, one still needs to know the scene depth, as discussed in Section 3. It is well known that computing motion by integrating differential sensors can lead to drift in the computed result. This drift is due to the noise present in the sensor readings. The integration of a noisy signal leads to a temporally increasing deviation of the computed motion from the true motion. We have measured the standard deviation of our gyroscope?s noise to be 0.5deg/s and the accelerometer noise is 0.006m/s 2 , using samples from when the gyroscopes and accelerometers are held stationary (at zero angular velocity and constant acceleration, respectively) . In our experiments, there is significantly less drift in rotation, due to the need to perform only a single integration step on the gyroscope data. The necessity to integrate twice to get positional data from the accelerometers causes more drift. We propose a novel aided blind deconvolution algorithm that computes the camera-motion, and in-turn the image blur function, that best matches the measured acceleration and angular velocity while maximizing the likelihood of the deblurred latent image according to a natural image prior. Our deconvolution algorithms compensate for positional drift by assuming it is linear in time, which can be estimated if one knows the final end position of the camera. We assume the rotational drift is minimal. Thus in contrast to a traditional blind-deconvolution algorithm that solves for each value of a kernel or PSF, our algorithm only has to solve for a few unknowns. We solve for these using an energy minimization framework that performs a search in a small local neighborhood around the initially computed end point. In our experiments, we have found that the camera travels on the order of a couple millimeters during a long exposure (the longest we have tried is a 1/2 second). We note that a few millimeter translation in depth (z) has little effect on the image for lenses of common focal lengths, thus the drift in x and y is the only significant source of error. We set our optimization parameters to search for the optimal end point within a 1mm radius of the initially computed end point, subject to the constraints that the acceleration along that recovered path matches the measured accelerations best in the least-squares sense. The optimal end point is the one that results in a deconvolved image with the highest log-likelihood as measured by the hyper-Laplacian image prior (discussed in Section 3.1). Specifically, let us define a function ? that given a potential end point (u, v) computes the camera?s translational path as that which best matches, in the least squares sense, the observed acceleration and terminates at (u, v): For notational convenience, let ? define a function that forms the blur sampling matrix from the camera intrinsics, extrinsics, and scene depth as using the rigid-body dynamics and temporal integration processes discussed in Section 3.1 and 3.3: The drift-compensated blur matrix and deconvolution equations are: We then search over the space of (u, v) to find the (u, v) that results in the image I that has the highest likelihood given the observation and image prior. We perform this energy minimization using the Nelder-Mead simplex method, and the spatially-varying deconvolution method discussed in Section 3.2 is used as the error function in the inner loop of the optimization. We perform the optimization on 1/10 down-sampled versions (of our 21 MP images). The results from our search process are shown as plots of the camera motion in Figure 5 and visually in Figure 7 , when used to deblur images. The running time for this search method is about 5 minutes on a 0.75 MP image. The search only needs to be run once either for the entire image, or could be run on a subsection of the image if that is preferable. Once the drift is corrected for, the PSF is more accurate for the entire image. Computing Scene Depth: Note that a spatially invariant scene depth is implicitly computed during the drift compensation process, as scaling the end point equally in the x and y dimensions is equivalent to scaling the depth value. Specifically, the optimization is over u = u/d and v = v/d and thus solves for a single depth value for the entire scene. In the previous section, we discussed how to remove camera motion blur by recovering the camera rotation and translation from accelerometers and gyroscopes. In this section, we describe our hardware for recording the accelerometer and gyroscope data and implementation related concerns and challenges. Since there are six unknowns per time-step, the minimal configuration of sensors is six. It is possible to recover rotation and translation using six accelerometers alone, by sensing each axis in pairs at three different points on a rigid-body; however, after experimenting with this method we found accelerometers alone to be too noisy for reliable computation of rotation. All parts are commodity, off-theshelf components purchased online. Additionally, the SLR?s hotshoe, i.e., flash trigger signal, is wired to the Arduino board. The trigger signal from the SLR remains low for the entire length of the exposure and is high otherwise. The Arduino board is interrupt driven such that when the trigger signal from the SLR fires, the accelerometers and gyroscopes are polled at 200Hz during the exposure window. Each time the sensors are read, the values are sent over the Bluetooth serial port interface. Additionally, an internal high-resolution counter is read and the actual elapsed time between each reading of the sensors is reported. The sensors and Arduino board are mounted to a laser-cut acrylic base that secures the board, the sensors, and a battery pack. The acrylic mount is tightly screwed into the camera tripod mount. The only other connection to the camera is the flash trigger cable. Our hardware attachment has an optional feature used for calibration and validation experiments: mounting holes for a Point Grey high-speed camera. When the Arduino board is sampling the inertial sensors, it can also send a 100 Hz trigger to the high-speed camera. We use the high-speed data to help calibrate our sensors and to acquire data to get ground truth measurements for motion blur. To demonstrate the importance of accounting for spatially variance, in the bottom row we show a result where we have deconvolved using the PSF for the correct part of the image and a non-corresponding area. To accurately compute camera motion from inertial sensors, it is necessary to calibrate several aspects of our system. We need to accurately calibrate the sensor responses, as we have found them to deviate from the published response ranges. It is also necessary to know the position of the accelerometer relative to the camera?s optical center. Lastly, we need to calibrate the camera intrinsics. We calibrate these values in two stages. The first stage is to calibrate the sensors? responses. We do this by rotating the gyroscopes at a known constant angular velocity to recover the mapping from the 10-bit A/D output to degrees/s. We performed this measurement at several known angular velocities to confirm that the gyroscopes have a linear response. To calibrate the accelerometers, we held them stationary in six orientations with respect to gravity, (?x, ?y, ?z), which allows us to map the A/D output to units of m/s 2 . To calibrate our setup and to measure ground-truth measurements for camera-shake, we developed a method to accurately recover a camera?s position during an exposure using a method inspired by Ben-Ezra and Nayar [2004]. However, instead of tracking 2D motion, we track 6D motion. We attached a high-speed camera (200 FPS PointGrey DragonFly Express) to our sensor platform, and our Arduino micro-controller code is set to trigger the high-speed camera at 100 FPS during the SLR?s exposure window. In a lab setting, we created a scene with a significant amount of texture and took about 10 images with exposures ranging from 1/10 to 1/2 of a second. For each of these images, accelerometer and gyro data was recorded in addition to high-speed frames. We took the high-speed frames from these shots and acquired additional widebaseline shots with the SLR and high-speed camera. Using all of this data, we created a 3D reconstruction of the scene using bundle adjustment (our process uses RANSAC for feature matching, computes sparse 3D structure from motion, and computes the camera focal length). Figure 4 shows our high-speed camera attachment and a few frames from the high speed cameras (we took about a hundred total with both cameras for the process). This reconstruction process gives us a collection of sparse 3D points, camera rotations, and camera translations, with an unknown global transformation and a scale ambiguity between camera depth and scene depth. We resolve the scale ambiguity using a calibration grid of known size. We will now describe the results of our ground-truth camera-shake measurements and compare results using our deblurring method to the ground-truth measurements. We also compare our results to those of Shan et al. [2008] and Fergus et al. [2006], using the implementations the authors have available online. We also show results of our methods running on natural images acquired outside of a lab setup and compare these to results using previous work as well. In Figure 6 , we show visualizations of the ground-truth spatiallyvarying PSFs for an image from our lab setup. This image shows some interesting properties. There is a significant variation across the image plane. Also, the kernels displayed are for a fixed depth, thus all the spatial variance is due to rotation. To demonstrate the importance of accounting for spatial variance, on the bottom row of Figure 6 , we show a result where we have deconvolved using the PSF for the correct part of the image and the PSF for a different, non-corresponding area. These results are quite interesting as they show that some of the common assumptions made in image deconvolution do not always hold. Most deconvolution work assumes spatially invariant kernels, which really only applies for camera motion under an orthographic model; however, with a typical imaging setup (we use a 40mm lens), the perspective effects are strong enough to induce a spatially-varying blur. We also note that there is often a roll component to the blur, something that is also not modeled by spatially invariant kernels. Lastly, we observe that translation, and thus depth dependent effects can be significant, which is interesting as it is often thought that most camera-shake blur is due to rotation. Please visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/ for examples. In Figure 7 , using the same scene as above, we show comparisons of deconvolution results using our method, ground-truth, and two others. The images shown of the lab calibration scene were taken at exposures from 1/3 to 1/10 of a second. For each image we show the input, the result of deconvolving with PSFs from the initial motion estimate and after performing drift correction, and compare these to a deconvolution using PSFs from the recovered groundtruth motions, and PSFs recovered using the methods of Shan et al. [2008] and Fergus et al. [2006]. For these latter two comparisons, we made a best effort to adjust the parameters to recover the best blur kernel possible. To make the comparison fair, all results were deblurred using exactly the same deconvolution method, that of Levin et al. [2007]. The results in Figure 7 , show a wide variety of blurs, yet our method recovers an accurate kernel and provides deconvolution results that are very close to that of the ground-truth. In all cases, our results are better than those using Shan et al.?s and Fergus et al.?s methods. After calibrating our hardware system using the method discussed in Section 4.2, we took the camera outside of the lab, using a laptop with a Bluetooth adapter to capture the inertial sensor data. In Figure 8 , we show several results where we have deblurred images using our method, Shan et al.?s method, and Fergus et al.?s method. The Shan et al. and Fergus et al. results were deconvolved using Levin et al.?s method [2007], and our results are deconvolved using our spatially-varying deconvolution method discussed in Section 3.1. For all the images, our results show a clear improvement over the input blurry image. There is still some residually ringing that is unavoidable due to frequency loss during blurring. The Shan et al. results are of varying quality, and many show large ringing artifacts that are due to kernel misestimation and over-sharpening; the Fergus et al. results are generally blurrier than ours. All the images shown here were shot with 1/2 to 1/10 second exposures with a 40mm lens on a Canon 1Ds Mark III. For additional results, visit http://research.microsoft.com/en- us/um/redmond/groups/ivm/imudeblurring/. In this work, we presented an aided blind deconvolution algorithm that uses a hardware attachment in conjunction with a corresponding blurry input image and a natural image prior to compute perpixel, spatially-varying blur and that deconvolves an image to produce a sharp result. There are several benefits to our method over previous approaches: (1) our method is automatic and has no user-tuned parameters and (2) it uses inexpensive commodity hardware that could easily be built into a camera or produced as a mass-market attachment that is more compact than our prototype. We have shown advantages over purely image-based methods, which in some sense is not surprising?blind deconvolution is an inherently ill-posed problem,  thus the extra information of inertial measurements should be helpful. The biggest limitation of our method is sensor accuracy and noise. Our method?s performance will degrade under a few cases: (1) if the drift is large enough that the search space for our optimization process is too large, i.e., greater than a couple mm, (2) if our estimation of the initial camera rotation relative to gravity is incorrect or similarly, if the camera moves in a way that is not normally distributed about the gravity vector, (3) if there is significant depth variation in the scene and the camera undergoes significant translation, (4) if the camera is translating at some initial, constant velocity, and (5) if there is large image frequency information loss to blurring. The sensors are inexpensive and one could easily add sensors for redundancy and perform denoising by averaging either in the analog or digital domain. For (3) one could recover the depth in the scene by performing a ?depth from motion blur? algorithm, similar to depth from defocus. We are pursuing this problem; however, it is important to note that varying scene depth does not always significantly affect the blur. For typical situations, we have found that depth is only needed for accurate deblurring of objects within a meter from the camera. Most often people take images where the scene is farther than this distance. For (4) we assume the initial translation velocity is zero and our accelerometer gives us no measure of this. While we currently only consider the accelerometer data during an exposure, we actually record all the sensors? data before and after each exposure as well. (5) is an issue with all deblurring methods. Frequency loss will cause unavoidable artifacts during deconvolution that could appear as ringing, banding, or over-smoothing depending on the deconvolution method. We also thank Mike Sinclair and Turner Whitted for their help in the hardware lab.",
  "resources" : [ ]
}