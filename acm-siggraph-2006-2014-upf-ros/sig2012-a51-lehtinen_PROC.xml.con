{
  "uri" : "sig2012-a51-lehtinen_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a51-lehtinen_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Reconstructing the Indirect Light Field for Global Illumination",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Jaakko-Lehtinen",
      "name" : "Jaakko",
      "surname" : "Lehtinen"
    }, {
      "uri" : "http://drinventor/Timo-Aila",
      "name" : "Timo",
      "surname" : "Aila"
    }, {
      "uri" : "http://drinventor/Samuli-Laine",
      "name" : "Samuli",
      "surname" : "Laine"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "we", "have", "implement", "we", "algorithm", "standalone", "library", "take", "buffer", "sample", "input", "modify", "pbrt", "-lsb-", "pharr", "Humphreys", "2010", "-rsb-", "generate", "input", "we", "have", "produce", "both", "multicore", "CPU", "GPU", "implementation", "reconstruction", "algorithm", "we", "test", "platform", "3.2", "GHz", "quad", "core", "Intel", "Core", "i7", "NVIDIA", "GTX480", "all", "result", "image", "be", "render", "1280", "720", "-lrb-", "720p", "-rrb-", "section", "we", "first", "test", "reconstruction", "indirect", "illumination", "diffuse", "scene", "follow", "glossy", "reflection", "we", "perform", "more", "target", "test", "demonstrate", "we", "occlusion", "heuristic", "allow", "faithful", "reproduction", "visibility", "we", "use", "ambient", "occlusion", "test", "case", "because", "its", "quality", "directly", "determine", "accuracy", "visibility", "finally", "we", "show", "result", "defocus", "motion", "blur", "we", "focus", "quality", "reconstructed", "image", "postpone", "discussion", "about", "scalability", "memory", "consumption", "extension", "Section", "we", "compare", "we", "result", "method", "similarly", "we", "reconstruct", "image", "base", "small", "number", "fat", "sample", "-lsb-", "Lehtinen", "et", "al.", "2011", "Dammertz", "et", "al.", "2010", "Sen", "Darabi", "2012", "-rsb-", "auxiliary", "material", "contain", "all", "uncompressed", "image", "show", "refer", "section", "we", "reconstruct", "multi-bounce", "diffuse", "global", "illumination", "iguel", "light", "area", "light", "source", "-lrb-", "figure", "-rrb-", "input", "contain", "sample", "per", "pixel", "-lrb-", "leftmost", "inset", "-rrb-", "we", "reconstruct", "incident", "light", "field", "over", "hemisphere", "visible", "point", "integrate", "produce", "result", "-lrb-", "third", "inset", "from", "left", "-rrb-", "per", "equation", "we", "pay", "special", "attention", "fact", "radiance", "input", "sample", "contain", "noise", "due", "multiple", "bounce", "render", "path", "tracer", "we", "reconstruction", "more", "true", "ground", "truth", "-lrb-", "second", "left", "-rrb-", "than", "comparison", "method", "a-trous", "wavelet", "-lsb-", "Dammertz", "et", "al.", "2010", "-rsb-", "-lrb-", "fourth", "inset", "from", "leave", "-rrb-", "random", "parameter", "filter", "-lrb-", "rpf", "last", "one", "right", "-rrb-", "-lsb-", "Sen", "Darabi", "2012", "-rsb-", "we", "use", "both", "comparison", "method", "reconstruct", "incident", "irradiance", "i.e.", "before", "multiplication", "texture", "render", "spp", "input", "pbrt", "take", "36.6", "we", "reconstruction", "256", "reconstruction", "ray", "per", "pixel", "GPU", "take", "189.5", "s.", "preprocessing", "include", "build", "bvh", "size", "sample", "use", "k-nn", "shrink", "sample", "use", "input", "ray", "take", "9.5", "rest", "spend", "reconstruction", "ray", "considerably", "more", "efficient", "than", "render", "similar-quality", "512", "spp", "image", "pbrt", "which", "take", "3910.5", "18x", "longer", "than", "combine", "input", "reconstruction", "we", "256", "spp", "result", "exhibit", "significantly", "less", "noise", "than", "pbrt?s", "256", "spp", "rendering", "thanks", "we", "spatial", "filter", "sample", "Notice", "we", "ability", "perform", "image", "antialiasing", "limit", "number", "primary", "hit", "input", "sample", "supplemental", "video", "contain", "short", "animation", "render", "from", "input", "sample", "per", "pixel", "use", "512", "reconstruction", "ray", "per", "pixel", "each", "frame", "process", "isolation", "temporal", "filter", "any", "kind", "employ", "result", "demonstrate", "temporal", "stability", "we", "reconstruction", "even", "from", "severely", "noisy", "input", "comparison", "method", "A-Trous", "iterative", "approximation", "cross-bilateral", "filter", "twist", "when", "spatial", "support", "widen", "between", "iteration", "range", "filter", "tighten", "allow", "application", "very", "large", "spatial", "filter", "without", "disproportionate", "blur", "original", "implementation", "use", "three", "range", "filter", "color", "normal", "world-space", "position", "since", "we", "input", "very", "noisy", "we", "do", "wish", "clamp", "input", "sample", "energy", "we", "choose", "use", "color", "range", "filter", "-lrb-", "use", "give", "worse", "result", "-rrb-", "while", "quality", "leave", "some", "room", "improvement", "a-trous", "filter", "efficient", "we", "CPU", "implementation", "take", "58", "scene", "since", "reference", "implementation", "random", "parameter", "filter", "-lrb-", "rpf", "-rrb-", "available", "we", "reimplement", "follow", "author", "detailed", "guideline", "-lsb-", "Sen", "Darabi", "2011", "-rsb-", "rpf", "sophisticated", "cross-bilateral", "filter", "automatically", "tune", "weight", "each", "feature", "each", "pixel", "base", "mutual", "information", "between", "domain", "variable", "scene", "feature", "-lrb-", "position", "normal", "texture", "etc.", "-rrb-", "radiance", "input", "sample", "almost", "identical", "ours", "unfortunately", "original", "formulation", "rpf", "make", "heavy", "use", "color", "channel", "range", "filter", "require", "special", "treatment", "bright", "sample", "-lrb-", "hdr", "clamp", "-rrb-", "which", "essentially", "discard", "high-energy", "spike", "lose", "significant", "amount", "energy", "up", "50", "we", "test", "we", "modify", "algorithm", "reinsert", "lose", "energy", "evenly", "all", "sample", "within", "pixel", "we", "reasonably", "tune", "CPU", "implementation", "filter", "image", "2895", "roughly", "line", "result", "we", "estimate", "10x", "possible", "speedup", "from", "GPU", "port", "we", "use", "parameter", "recommend", "author", "guideline", "Figure", "show", "onkey", "ead", "scene", "three", "statuette", "decrease", "gloss", "glossy", "ground", "plane", "scene", "light", "single-bounce", "indirect", "illumination", "from", "distant", "point", "light", "source", "-lrb-", "direct", "illumination", "show", "-rrb-", "consequently", "input", "sample", "noise-free", "input", "contain", "sample", "per", "pixel", "we", "result", "compute", "use", "512", "reconstruction", "ray", "per", "pixel", "a-trous", "rpf", "result", "show", "below", "we", "notice", "result", "a-trous", "roughness", "0.01", "0.05", "0.25", "statuette", "0.01", "ground", "plane", "could", "significantly", "improve", "introduce", "additional", "range", "filter", "over", "normal", "secondary", "hit", "which", "conveniently", "present", "we", "datum", "we", "present", "extension", "we", "comparison", "result", "addition", "original", "algorithm", "we", "result", "significantly", "closer", "grind", "truth", "than", "either", "comparison", "method", "both", "visually", "term", "psnr", "-lrb-", "38.6", "db", "vs.", "31", "33db", "-rrb-", "full-size", "image", "provide", "auxiliary", "material", "closer", "inspection", "particular", "we", "reconstruct", "position", "sharpness", "reflection", "faithfully", "thanks", "4d", "treatment", "input", "sample", "account", "both", "space", "angle", "Figure", "10", "demonstrate", "importance", "accounting", "angular", "effect", "while", "comparison", "method", "produce", "smooth", "result", "reflection", "have", "shift", "noticeably", "sharp", "should", "note", "unlike", "comparison", "method", "we", "use", "render", "system", "-lrb-", "pbrt", "-rrb-", "evaluate", "brdf", "final", "bounce", "towards", "eye", "per", "equation", "while", "closely", "match", "ground", "truth", "we", "algorithm", "currently", "2.6", "slower", "scene", "than", "reference", "method", "mainly", "due", "extreme", "geometric", "simplicity", "scene", "which", "play", "we", "disadvantage", "because", "we", "algorithm", "scale", "weakly", "scene", "complexity", "-lrb-", "section", "-rrb-", "we", "spatial", "hierarchy", "do", "currently", "include", "angular", "subdivision", "-lsb-", "Arvo", "Kirk", "1987", "-rsb-", "we", "may", "therefore", "process", "redundant", "splat", "glossy", "surface", "where", "spatial", "support", "larger", "-lrb-", "section", "2.2", "-rrb-", "another", "possible", "improvement", "discuss", "section", "regardless", "result", "demonstrate", "information", "require", "reconstruct", "high-quality", "reflection", "present", "sparse", "input", "we", "algorithm", "correctly", "extract", "produce", "high-quality", "result", "we", "algorithm", "can", "easily", "use", "render", "ambient", "occlusion", "bypass", "radiance", "filter", "threshold", "distance", "closest", "apparent", "surface", "find", "Figure", "show", "we", "ambient", "occlusion", "reconstruction", "iguel", "although", "scene", "contain", "fine", "geometric", "feature", "groove", "vegetation", "we", "reconstruction", "from", "only", "input", "sample", "per", "pixel", "very", "close", "agreement", "ground", "truth", "compute", "use", "2048", "sample", "per", "pixel", "case", "image", "antialiasing", "particularly", "limit", "since", "input", "contain", "only", "primary", "hit", "per", "pixel", "GPU", "execution", "time", "145", "seconds", "256", "reconstruction", "ray", "per", "pixel", "720p", "contrast", "ambient", "occlusion", "technique", "Egan", "et", "al.", "-lsb-", "2011a", "-rsb-", "we", "do", "resort", "brute", "force", "Monte", "Carlo", "sampling", "original", "scene", "all", "finally", "we", "demonstrate", "generality", "we", "reconstruction", "apply", "motion", "blur", "defocus", "-lrb-", "figure", "11", "-rrb-", "specifically", "instead", "secondary", "ray", "we", "now", "store", "primary", "ray", "cast", "from", "lens", "reconstruct", "incident", "temporal", "light", "field", "different", "point", "lens", "instead", "indirect", "lighting", "scene", "point", "we", "use", "utterfly", "dataset", "from", "Lehtinen", "et", "al.", "-lsb-", "2011", "-rsb-", "compare", "against", "publicly", "available", "implementation", "we", "result", "slightly", "more", "accurate", "approx", "db", "term", "pnsr", "image", "available", "supplemental", "material", "we", "better", "result", "explain", "we", "better", "adaptation", "input", "sampling", "rate", "use", "worst-case", "radius", "derive", "from", "dispersion", "sampling", "pattern", "which", "tend", "blur", "in-focus", "geometry", "slightly", "may", "still", "result", "lack", "support", "region", "temporal", "light", "field", "contain", "feature", "visible", "from", "only", "small", "fraction", "lens", "shutter", "interval", "contrast", "we", "k-nn", "approach", "adapt", "local", "sampling", "density", "lead", "sharp", "result", "well", "input", "-lrb-", "spp", "-rrb-", "we", "reconstruction", "we", "reconstruction", "a-trous", "rpf", "ground", "truth", "-lrb-", "2048", "spp", "-rrb-", "sample", "area", "better", "support", "lower", "sampling", "density", "area", "demonstrate", "Figure", "12", "we", "reconstruction", "take", "4x", "longer", "than", "algorithm", "Lehtinen", "et", "al.", "primarily", "because", "use", "specialize", "2d", "hierarchy", "post-perspective", "space", "essentially", "tailor", "datum", "structure", "known", "distribution", "reconstruction", "ray", "-lrb-", "all", "originate", "from", "lens", "-rrb-", "we", "focus", "reconstruct", "incident", "light", "field", "arbitrary", "point", "scene", "we", "use", "true", "3d", "hierarchy", "we", "result", "angular", "bandwidth" ],
  "content" : "We have implemented our algorithm as a standalone library that takes a buffer of samples as input, and modified PBRT [Pharr and Humphreys 2010] to generate the input. We have produced both a multicore CPU and a GPU implementation of the reconstruction algorithm, and our test platform is a 3.2GHz quad core Intel Core i7 and NVIDIA GTX480. All result images were rendered at 1280?720 (720p). In this section, we first test reconstruction of indirect illumination in diffuse scenes, followed by glossy reflections. We then perform more targeted tests and demonstrate that our occlusion heuristic allows faithful reproduction of visibility. We use ambient occlusion as a test case because its quality is directly determined by the accuracy of visibility. Finally, we show results for defocus and motion blur. We focus on the quality of the reconstructed images, and postpone discussion about scalability, memory consumption, and extensions to Section 4. We compare our results to methods that, similarly to us, reconstruct the image based on a small number of ?fat? samples [Lehtinen et al. 2011; Dammertz et al. 2010; Sen and Darabi 2012]. The auxiliary material contains all of the uncompressed images that are shown or referred to in this section. We reconstruct multi-bounce diffuse global illumination in S AN M IGUEL lit with an area light source ( Figure 7 ). The input contains 8 samples per pixel (leftmost inset). We reconstruct the incident light field over the hemisphere at visible points, and integrate to produce the result (third inset from the left), as per Equation 3. We pay no special attention to the fact that the radiances L of the input samples contain noise due to the multiple bounces rendered by the path tracer. Our reconstruction is more true to ground truth (second left) than the comparison methods, A-Trous wavelets [Dammertz et al. 2010] (fourth inset from left) and random parameter filtering (RPF, last one on right) [Sen and Darabi 2012]. We use both comparison methods for reconstructing incident irradiance, i.e., before multiplication by texture. Rendering the 8 spp input with PBRT takes 36.6s. Our reconstruction with 256 reconstruction rays per pixel on the GPU takes 189.5s. Of this, preprocessing, including building the BVH, sizing the samples using k-NN, and shrinking the samples using the input rays takes 9.5s. The rest is spent on reconstruction rays. This is considerably more efficient than rendering the similar-quality 512 spp image with PBRT, which takes 3910.5s, or 18x longer than combined input+reconstruction. Our 256 spp result exhibits significantly less noise than PBRT?s 256 spp rendering, thanks to our spatial filtering of the samples. Notice that our ability to perform image antialiasing is limited by the number of primary hits in the input samples. The supplemental video contains a short animation rendered from 8 input samples per pixel using 512 reconstruction rays per pixel. Each frame is processed in isolation; no temporal filtering of any kind is employed. The result demonstrates the temporal stability of our reconstruction even from severely noisy input. Comparison methods A-Trous is an iterative approximation to a cross-bilateral filter, with the twist that when spatial support widens between iterations, the range filters tighten. This allows the application of a very large spatial filter without disproportionate blurring. The original implementation uses three range filters: color, normal, and world-space position. Since our input is very noisy, and we do not wish to clamp the input samples? energies, we chose not to use color as a range filter (using it gave worse results). While the quality leaves some room for improvement, the A-Trous filter is efficient: our CPU implementation takes 58s in this scene. Since no reference implementation of random parameter filtering (RPF) is available, we reimplemented it by following the authors? detailed guidelines [Sen and Darabi 2011]. RPF is a sophisticated cross-bilateral filter that automatically tunes the weights for each feature, at each pixel, based on mutual information between domain variables, scene features (position, normal, texture, etc.), and radiance. Their input samples are almost identical to ours. Unfortunately, the original formulation of RPF makes heavy use of the color channel as a range filter, requiring special treatment for bright samples (?HDR clamp?), which essentially discards high-energy spikes. This loses a significant amount of energy, up to 50% in our tests. We modified their algorithms to reinsert the lost energy evenly to all samples within the pixel. Our reasonably tuned CPU implementation filters the image in 2895s, roughly in line with their results. We estimate a 10x possible speedup from a GPU port. We use the parameters recommended in the authors? guidelines. Figure 9 shows M ONKEY H EADS , a scene with three statuettes of decreasing gloss 2 on a glossy ground plane. The scene is lit by single-bounce indirect illumination from a distant point light source (direct illumination not shown), and consequently the input samples are noise-free. The input contains 8 samples per pixel. Our result is computed using 512 reconstruction rays per pixel. A-Trous and RPF results are shown below. We noticed the results for A-Trous\n        2 The roughnesses are 0.01, 0.05 and 0.25 for the statuettes and 0.01 for the ground plane. could be significantly improved by introducing an additional range filter over the normal of the secondary hit, which is conveniently present in our data. We present this extension in our comparison results in addition to the original algorithm. Our results are significantly closer to ground truth than either comparison method both visually and in terms of PSNR (38.6dB vs. 31?33dB). The full-size images are provided in the auxiliary material for closer inspection. In particular, we reconstruct the position and sharpness of the reflections faithfully, thanks to the 4D treatment of the input samples that accounts for both space and angle. Figure 10 demonstrates the importance of accounting for angular effects. While the comparison methods produce smooth results, the reflections have shifted noticeably and are not as sharp as they should. Note that unlike the comparison methods, we use the rendering system (PBRT) to evaluate the BRDFs of the final bounce towards the eye, as per Equation 3. While closely matching ground truth, our algorithm is currently 2.6x slower in this scene than the reference method. This is mainly due to the extreme geometric simplicity of the scene, which plays to our disadvantage because our algorithm scales weakly with scene complexity (Section 4). Our spatial hierarchy does not currently include angular subdivisions [Arvo and Kirk 1987], and we may therefore process redundant splats on glossy surfaces, where the spatial supports are larger (Section 2.2). Another possible improvement is discussed in Section 4. Regardless, the result demonstrates that the information required for reconstructing high-quality reflections is present in the sparse input, and that our algorithm correctly extracts it to produce a high-quality result. Our algorithm can be easily used for rendering ambient occlusion by bypassing the radiance filter and thresholding the distance to the closest apparent surface found. Figure 8 shows our ambient occlusion reconstruction in S AN M IGUEL . Although the scene contains fine geometric features such as grooves and vegetation, our reconstruction from only 4 input samples per pixel is in very close agreement with the ground truth computed using 2048 samples per pixel. In this case, image antialiasing is particularly limited since the input contains only 4 primary hits per pixel. The GPU execution time was 145 seconds for 256 reconstruction rays per pixel at 720p. In contrast to the ambient occlusion technique of Egan et al. [2011a], we do not resort to brute force Monte Carlo sampling of the original scene at all. Finally, we demonstrate the generality of our reconstruction by applying it to motion blur and defocus ( Figure 11 ). Specifically, instead of secondary rays, we now store the primary rays cast from a lens, and reconstruct the incident temporal light field at different points on the lens instead of indirect lighting at scene points. We use the B UTTERFLIES dataset from Lehtinen et al. [2011] and compare against their publicly available implementation. Our result is slightly more accurate, approx. 1 dB in terms of PNSR. The images are available in the supplemental material. Our better result is explained by our better adaptation to the input sampling rate. They use a worst-case radius derived from the dispersion of the sampling pattern, which tends to blur in-focus geometry slightly, but may still result in lack of support in regions of the temporal light field that contain features that are visible from only a small fraction of the lens and shutter interval. In contrast, our k-NN approach adapts to the local sampling density, leading to sharp results in well- Input (4 spp)\n        Our reconstruction Our reconstruction A-Trous RPF\n        Ground truth (2048 spp) sampled areas, and better support for the lower sampling density areas, as demonstrated in Figure 12 . Our reconstruction takes 3?4x longer than the algorithm of Lehtinen et al., primarily because they use a specialized 2D hierarchy in post-perspective space, essentially tailoring the data structure to the known distribution of reconstruction rays (they all originate from the lens). As our focus is reconstructing the incident light field at arbitrary points in the scene, we use a true 3D hierarchy. Our result No angular bandwidth",
  "resources" : [ ]
}