{
  "uri" : "sig2014-a35-xu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a35-xu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Organizing Heterogeneous Scene Collections through Contextual Focal Points",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Kai Xu-null",
      "name" : "Kai Xu",
      "surname" : null
    }, {
      "uri" : "http://drinventor/Rui-Ma",
      "name" : "Rui",
      "surname" : "Ma"
    }, {
      "uri" : "http://drinventor/Hao Zhang-null",
      "name" : "Hao Zhang",
      "surname" : null
    }, {
      "uri" : "http://drinventor/Chenyang-Zhu",
      "name" : "Chenyang",
      "surname" : "Zhu"
    }, {
      "uri" : "http://drinventor/Ariel-Shamir",
      "name" : "Ariel",
      "surname" : "Shamir"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Hui Huang-null",
      "name" : "Hui Huang",
      "surname" : null
    } ]
  },
  "bagOfWords" : [ "we", "present", "result", "obtain", "we", "algorithm", "focal", "point", "drive", "analysis", "indoor", "scene", "collection", "scene", "retrieval", "we", "compare", "we", "result", "those", "obtain", "from", "state-of-the-art", "method", "both", "through", "precision-recall", "curve", "preliminary", "user", "study", "target", "hybrid", "scene", "more", "extensive", "result", "accompany", "video", "can", "find", "supplementary", "material", "dataset", "we", "experiment", "be", "provide", "Stanford", "repository", "-lsb-", "Fisher", "et", "al.", "2012", "-rsb-", "Tsinghua", "repository", "-lsb-", "xu", "et", "al.", "2013", "-rsb-", "both", "dataset", "contain", "semantic", "tag", "object", "originally", "collect", "from", "Google", "-lrb-", "now", "Trimble", "-rrb-", "3D", "Warehouse", "since", "tag", "from", "two", "dataset", "inconsistent", "we", "run", "we", "test", "each", "dataset", "separately", "each", "scene", "we", "remove", "wall", "focus", "only", "interior", "scene", "object", "Stanford", "collection", "consist", "132", "scene", "461", "object", "encompass", "78", "object", "category", "five", "labeled", "scene", "category", "Tsinghua", "dataset", "consist", "792", "scene", "13", "365", "object", "encompass", "119", "object", "category", "six", "labeled", "scene", "category", "Tsinghua", "dataset", "contain", "102", "hybrid", "scene", "which", "compose", "many", "subscene", "each", "represent", "room", "parameter", "statistics", "key", "parameter", "we", "algorithm", "include", "minimum", "support", "min", "use", "frequent", "substructure", "mining", "first", "iteration", "rooted", "path", "combination", "weight", "use", "compute", "graph", "kernel", "all", "result", "report", "paper", "be", "obtain", "same", "parameter", "setting", "min", "40", "Tsinghua", "dataset", "min", "20", "Stanford", "dataset", "parameter", "graph", "kernel", "use", "optimal", "one", "available", "from", "publish", "work", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "value", "all", "other", "parameter", "fix", "throughout", "describe", "section", "statistics", "timing", "Table", "show", "some", "statistics", "from", "focal", "point", "extraction", "scene", "clustering", "timing", "wise", "take", "10.5", "minute", "process", "whole", "Tsinghua", "dataset", "-lrb-", "792", "scene", "-rrb-", "3.2", "minute", "Stanford", "scene", "collection", "-lrb-", "132", "scene", "-rrb-", "over", "iteration", "compactness", "evaluation", "-lrb-", "include", "FCGK", "computation", "-rrb-", "take", "60", "time", "spectral", "clustering", "30", "inexact", "frequent", "pattern", "mining", "note", "first", "two", "part", "be", "both", "implement", "Matlab", "could", "see", "significant", "speed-up", "code", "c/c", "timing", "measure", "quad-core", "2.80", "GHz", "Intel", "Core", "CPU", "12gb", "RAM", "focal", "point", "extraction", "Figure", "show", "several", "cluster", "representative", "focal", "point", "extract", "from", "Tsinghua", "collection", "complete", "set", "result", "focal", "extraction", "can", "find", "supplementary", "material", "we", "can", "observe", "hybrid", "scene", "contain", "multiple", "focal", "point", "which", "fairly", "typical", "result", "cluster", "overlap", "also", "worth", "note", "extraction", "non-local", "focal", "which", "compose", "relatively", "distant", "object", "group", "e.g.", "-lcb-", "tv", "tv-stand", "table", "sofa", "-rcb-", "etc.", "Table", "give", "number", "non-local", "focal", "extract", "both", "dataset", "see", "also", "last", "two", "row", "Figure", "effect", "focal", "join", "iterative", "clustering", "Figure", "10", "plot", "how", "normalize", "compactness", "cluster", "change", "iterative", "clustering", "algorithm", "progress", "while", "change", "strictly", "monotone", "evident", "iteration", "generally", "improve", "cluster", "quality", "over", "time", "final", "cluster", "count", "two", "set", "respectively", "precision-recall", "scene", "retrieval", "Figure", "11", "compare", "we", "method", "two", "other", "method", "scene", "retrieval", "GK", "graph", "kernel", "Fisher", "et", "al.", "-lsb-", "2011", "-rsb-", "measure", "similarity", "between", "whole", "scene", "since", "we", "be", "unable", "obtain", "author", "code", "we", "code", "up", "we", "own", "implementation", "two", "major", "difference", "original", "work", "first", "we", "use", "we", "structural", "graph", "which", "only", "encode", "two", "type", "relationship", "-lrb-", "support", "proximity", "-rrb-", "do", "consider", "hierarchical", "scene", "graph", "second", "computation", "node", "edge", "kernel", "slightly", "different", "see", "section", "4.2", "both", "GK", "FCGK", "scheme", "node", "edge", "kernel", "estimation", "graph", "kernel", "normalization", "well", "all", "parameter", "same", "original", "work", "bow", "baseline", "method", "where", "we", "use", "bag-of-words", "feature", "focal", "point", "only", "scene-to-scene", "similarity", "fcgk", "-lrb-", "sg", "-rrb-", "Tsinghua", "dataset", "we", "also", "apply", "we", "fcgk", "similarity", "scene", "where", "focal", "we", "use", "212", "structural", "group", "detect", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "when", "apply", "we", "method", "which", "use", "fcgk", "scene", "similarity", "we", "show", "result", "three", "setting", "-rrb-", "use", "initial", "set", "focal", "after", "only", "one", "step", "frequent", "pattern", "mining", "-rrb-", "use", "intermediate", "set", "focal", "-rrb-", "use", "final", "set", "focal", "extract", "Tsinghua", "dataset", "ground", "truth", "evaluate", "scene", "retrieval", "give", "scene", "labels/categories", "which", "come", "dataset", "since", "dataset", "contain", "many", "hybrid", "scene", "we", "separate", "subset", "simple", "scene", "remain", "hybrid", "-lrb-", "complex", "-rrb-", "scene", "report", "result", "each", "combination", "since", "Stanford", "collection", "do", "come", "scene", "label", "we", "provide", "we", "own", "label", "obtain", "manually", "which", "admittedly", "could", "introduce", "evaluation", "bias", "potentially", "more", "reliable", "method", "vote", "from", "multiple", "user", "could", "employ", "from", "precision-recall", "curve", "we", "see", "we", "focal-centric", "similarity", "base", "final", "set", "focal", "best", "all", "four", "case", "moreover", "performance", "gain", "more", "prominent", "hybrid", "scene", "result", "demonstrate", "only", "merit", "utilize", "focal", "scene", "comparison", "also", "merit", "we", "focal", "extraction", "scheme", "seem", "evident", "retrieval", "performance", "improve", "we", "iterative", "algorithm", "progress", "comparison", "GK", "Figure", "12", "show", "explicit", "comparison", "between", "GK", "FCGK", "scene", "similarity", "attest", "effectiveness", "utilize", "focal", "we", "experiment", "we", "also", "observe", "matching", "performance", "GK", "tend", "negatively", "affect", "presence", "many", "small/trivial", "object", "example", "when", "scene", "contain", "shelf", "support", "many", "small", "object", "GK", "count", "rooted", "walk", "from", "all", "object", "which", "would", "influence", "similarity", "between", "more", "prominent", "object", "fcgk", "more", "discriminative", "trivial", "object", "less", "likely", "have", "be", "choose", "focal", "user", "evaluation", "retrieval", "hybrid", "scene", "may", "difficult", "assign", "unambiguous", "category", "label", "ground", "truth", "use", "retrieval", "scene", "may", "unreliable", "thus", "instead", "rely", "scene", "category", "ground", "truth", "we", "let", "human", "user", "judge", "scene", "similarity", "base", "prior", "knowledge", "second", "comparative", "study", "scene", "retrieval", "we", "focus", "exclusively", "retrieval", "where", "query", "hybrid", "scene", "we", "present", "user", "10", "query", "each", "query", "top", "return", "from", "three", "compare", "method", "-lrb-", "gk", "bow", "fcgk", "-rrb-", "present", "user", "user", "ask", "choose", "which", "three", "most", "similar", "query", "we", "repeat", "total", "102", "query", "hybrid", "scene", "Tsinghua", "dataset", "against", "GK", "we", "obtain", "win", "percentage", "70.2", "against", "BOW", "we", "obtain", "73.9", "result", "statistically", "significant", "-lrb-", "0.01", "-rrb-", "study", "each", "scene", "have", "be", "render", "three", "random", "bird?s", "eye", "view", "image", "be", "present", "randomly", "among", "43", "participant", "80", "computer", "science", "researcher", "age", "20", "50", "rest", "frequent", "computer", "user", "vary", "background", "core", "datum", "organization", "problem", "mechanism", "compare", "datum", "traditional", "approach", "rely", "holistic", "datum", "view", "unique", "distance", "define", "between", "datum", "item", "group", "clustering", "however", "when", "datum", "become", "complex", "multifaceted", "fix", "global", "view", "datum", "similarity", "can", "hardly", "express", "rich", "characteristic", "datum", "we", "advocate", "use", "focal", "point", "compare", "organize", "complex", "heterogeneous", "datum", "use", "3d", "indoor", "scene", "prototype", "demonstrate", "its", "feasibility", "performance", "gain", "e.g.", "retrieval", "new", "approach", "seem", "particularly", "apt", "deal", "complex", "hybrid", "scene", "perhaps", "its", "most", "compelling", "feature", "ability", "process", "large", "heterogeneous", "collection", "scene", "organize", "they", "interlink", "well-connected", "cluster", "formation", "which", "facilitate", "scene", "exploration", "FCGK", "vs.", "GK", "while", "we", "retrieval", "experiment", "show", "superior", "performance", "fcgk", "over", "GK", "one", "should", "realize", "direct", "comparison", "between", "two", "exactly", "fair", "GK", "standalone", "graph", "similarity", "measure", "where", "only", "two", "graph", "compare", "need", "fcgk-based", "comparison", "come", "higher", "cost", "require", "set", "graph", "co-analysis", "focal", "extraction", "say", "scene", "collection", "available", "we", "would", "still", "suggest", "use", "fcgk", "its", "better", "performance", "modest", "processing", "cost", "comparison", "structural", "group", "we", "work", "focal", "point", "consist", "group", "scene", "object", "derive", "via", "structural", "scene", "analysis", "name", "alone", "suggest", "similarity", "structural", "group", "compute", "Xu", "et", "al.", "-lsb-", "2013", "-rsb-", "however", "major", "difference", "first", "structural", "group", "category", "group", "while", "we", "focal", "object", "group", "more", "importantly", "group", "extraction", "involve", "only", "frequent", "pattern", "mining", "through", "local", "proximity", "base", "search", "latter", "imply", "method", "unlikely", "return", "non-local", "structural", "group", "part", "evidence", "much", "higher", "number", "group", "-lrb-", "212", "-rrb-", "obtain", "vs.", "34", "focal", "we", "obtain", "same", "scene", "collection", "-lrb-", "tsinghua", "792", "scene", "-rrb-", "retrieval", "result", "Figure", "11", "seem", "suggest", "nonlocal", "focal", "extract", "via", "mining", "clustering", "provide", "better", "perspective", "meaningful", "scene", "comparison", "non-unique", "distance", "retrieval", "experiment", "use", "fcgk", "seem", "suggest", "we", "method", "assign", "unique", "distance", "between", "any", "two", "scene", "true", "once", "set", "focal", "fix", "fcgk", "compute", "base", "those", "focal", "clustering", "result", "however", "non-uniqueness", "focal-centric", "distance", "well", "utilize", "other", "setting", "include", "comprehensive", "retrieval", "multi-query", "retrieval", "roi-driven", "scene", "exploration", "where", "relevant", "focal", "query", "scene", "all", "determine", "on-demand", "Limitations", "we", "current", "algorithm", "depend", "semantic", "labeling", "scene", "object", "remain", "see", "whether", "work", "effectively", "noisy", "incomplete", "label", "base", "pure", "geometry", "analysis", "example", "interesting", "test", "we", "method", "input", "various", "level", "label", "noise", "however", "would", "hard", "quantitatively", "evaluate", "robustness", "against", "noisy", "label", "since", "may", "difficult", "reproduce", "realistic", "labeling", "noise", "introduce", "human", "nevertheless", "two", "dataset", "we", "use", "do", "contain", "some", "incorrect", "label", "which", "do", "seem", "affect", "overall", "performance", "perhaps", "more", "than", "desirable", "number", "parameter", "algorithm", "whose", "value", "be", "determine", "experimentally", "from", "technical", "stand", "point", "improvement", "possible", "various", "component", "algorithm", "example", "we", "layout", "similarity", "operate", "obb", "only", "which", "may", "unsuitable", "object", "complex", "geometry", "spatial", "arrangement", "structural", "graph", "model", "scene", "only", "flat", "arrangement", "object", "hierarchical", "organization", "may", "potentially", "advantageous", "future", "work", "one", "obvious", "pursuit", "apply", "we", "focal-driven", "approach", "other", "dataset", "e.g.", "large", "heterogeneous", "collection", "annotated", "image", "interesting", "technical", "question", "whether", "we", "scene", "organization", "can", "update", "additional", "set", "scene", "without", "recompute", "everything", "also", "rather", "than", "replace", "one", "object", "time", "scene", "synthesis", "like", "previous", "work", "we", "scene", "organization", "focal-based", "partial", "scene", "retrieval", "may", "allow", "substitute", "sub-scene", "synthesis", "task", "we", "conclude", "paper", "question", "what", "best", "way", "compare", "complex", "scene", "work", "along", "other", "before", "assume", "compare", "attribute", "graph", "define", "semantic", "tag", "object", "arrangement", "best", "way", "however", "we", "observe", "visually", "many", "retrieval", "result", "do", "look", "so", "compelling", "even", "best", "method", "date", "one", "take", "away", "coloring", "Figure", "14", "contrast", "between", "GK", "FCGK", "would", "salient", "hence", "focal-centric", "view", "we", "advocate", "offer", "perspective", "worth", "consider", "general", "question", "also", "one", "attribute", "complex", "datum", "beyond", "those", "indoor", "scene", "should", "perhaps", "answer", "user", "application", "intent", "mind" ],
  "content" : "We present results obtained by our algorithm for focal point driven analysis of indoor scene collections. For scene retrieval, we compare our results to those obtained from state-of-the-art methods both through precision-recall curves and a preliminary user study, targeted for hybrid scenes. More extensive results and an accompanying video can be found in the supplementary material. The datasets we experiment on were provided by the Stanford repository [Fisher et al. 2012] and the Tsinghua repository [Xu et al. 2013]. Both datasets contain semantic tags with the objects originally collected from Google (now Trimble) 3D Warehouse. Since the tags from the two datasets are inconsistent, we run our test on each dataset separately. For each scene, we remove the walls and focus only on the interior scene objects. The Stanford collection consists of 132 scenes and 3, 461 objects, encompassing 78 object categories and five labeled scene categories. The Tsinghua dataset consists of 792 scenes and 13, 365 objects, encompassing 119 object categories and six labeled scene categories. The Tsinghua dataset contains 102 hybrid scenes which is composed of many subscenes, each representing a room. Parameters and statistics. The key parameters of our algorithm include: the minimum support s min used for frequent substructure mining in the first iteration, and the rooted paths combination weights used in computing graph kernel. All the results reported in the paper were obtained with the same parameter setting: s min = 40 for Tsinghua dataset and s min = 20 for the Stanford dataset. The parameters for graph kernel use the optimal ones available from the published work of Fisher et al. [2011]. Values for all other parameters are fixed throughout and described in Section 4. Statistics and timing. Table 1 shows some statistics from focal point extraction and scene clustering. Timing wise, it took 10.5 minutes to process the whole Tsinghua dataset (792 scenes) and 3.2 minutes for the Stanford scene collection (132 scenes). Over an iteration, compactness evaluation (including FCGK computation) takes ~60% of the time, with spectral clustering ~30%, and inexact frequent pattern mining ~5%. Note that the first two parts were both implemented in Matlab and could see significant speed-up if coded in C/C++. Timing is measured on a 4 quad-core 2.80GHz Intel Core CPU with 12GB RAM. Focal point extraction. Figure 9 shows several clusters and their representative focal points extracted from the Tsinghua collection; the complete set of results for focal extraction can be found in the supplementary material. We can observe hybrid scenes containing multiple focal points, which is fairly typical and results in cluster overlap. Also worth noting is the extraction of non-local focals, which are composed of relatively distant object groups, e.g., {TV, TV-stand, table, sofa}, etc. Table 1 gives the number of non-local focals extracted for both datasets. See also the last two rows in Figure 9 for the effect of focal joining. Iterative clustering. Figure 10 plots how the normalized compactness of the clusters change as the iterative clustering algorithm progresses. While the change is not strictly monotone, it is evident that the iteration generally improves cluster quality over time. The final cluster counts for the two sets are 5 and 9, respectively. Precision-recall on scene retrieval. Figure 11 compares our method to two other methods for scene retrieval: GK: Graph kernels of Fisher et al. [2011] to measure similarity between whole scenes. Since we were unable to obtain the authors? code, we coded up our own implementation with two major differences to the original work. First, we use our structural graphs which only encode two types of relationships (support and proximity) and do not consider hierarchical scene graphs. Second, the computation of node and edge kernels are slightly different; see Section 4.2. For both GK and FCGK, the schemes for node and edge kernel estimation and graph kernel normalization, as well as all the parameters, are the same as the original work. BOW: A baseline method where we use bag-of-words features on the focal points only as a scene-to-scene similarity. FCGK (SG): On the Tsinghua dataset, we also apply our FCGK similarity on the scenes where as focals, we use the 212 structural groups detected by Xu et al. [2013]. When applying our method, which uses FCGK for scene similarity, we show results in three settings: 1) using the initial set of focals after only one step of frequent pattern mining; 2) using an intermediate set of focals; 3) using the final set of focals extracted. For the Tsinghua dataset, the ground truth for evaluating scene retrieval is given by the scene labels/categories which come with the dataset. Since this dataset contains many hybrid scenes, we separate it into a subset of simple scenes and the remaining hybrid (complex) scenes and report results on each and their combination. Since the Stanford collection does not come with scene labels, we provide our own labels obtained manually, which, admittedly, could introduce an evaluation bias. A potentially more reliable method, such as voting from multiple users, could be employed. From the precision-recall curves, we see that our focal-centric similarity based on the final set of focals is the best in all four cases. Moreover, the performance gain is more prominent for hybrid scenes. These results demonstrate not only the merit of utilizing focals for scene comparison but also the merit of our focal extraction scheme, as it seems evident that retrieval performance improves as our iterative algorithm progresses. Comparison to GK. Figure 12 shows an explicit comparison between GK and FCGK on scene similarity, attesting to the effectiveness of utilizing focals. In our experiment, we also observed that the matching performance of GK tends to be negatively affected by the presence of many small/trivial objects. For example, when a scene contains a shelf supporting many small objects, GK counts rooted walks from all these objects, which would influence the similarity between more prominent objects. FCGK is more discriminative and trivial objects are less likely to have been chosen as focals. User evaluation on retrieval. For a hybrid scene, it may be difficult to assign an unambiguous category label. The ground truth used for retrieval on such scenes may be unreliable. Thus instead of relying on scene categories as ground truth, we let human users judge scene similarity based on their prior knowledge. In this second comparative study on scene retrieval, we focus exclusively on retrieval where the query is a hybrid scene. We present a user with 10 queries. For each query, the top return from the three compared methods (GK, BOW and FCGK) are presented to the user and the user is asked to choose which of the three is most similar to the query. We repeat this for a total of 102 queries for the hybrid scenes in the Tsinghua dataset. Against GK, we obtain a winning percentage of 70.2% and against BOW, we obtain 73.9%. The results are statistically significant (with p = 0.01). In the studies, each scene has been rendered in three random bird?s eye views and the images were presented randomly. Among the 43 participants, 80% are computer science researchers, with ages 20 to 50. The rest are frequent computer users with varying backgrounds. At the core of the data organization problem is the mechanism for comparing data. Traditional approaches rely on holistic data views and unique distances defined between data items for grouping or clustering. However, when the data become complex and multifaceted, a fixed and global view on data similarity can hardly express the rich characteristics in the data. We advocate the use of focal points for comparing and organizing complex and heterogeneous data and use 3D indoor scenes as a prototype to demonstrate its feasibility and performance gains, e.g., in retrieval. The new approach seems particularly apt at dealing with complex and hybrid scenes. Perhaps its most compelling feature is the ability to process large and heterogeneous collections of scenes and to organize them into an interlinked and well-connected cluster formation, which facilitates scene exploration. FCGK vs. GK. While our retrieval experiment showed superior performance of FCGK over GK, one should realize that a direct comparison between the two is not exactly fair. GK is a standalone graph similarity measure, where only two graphs to compare are needed. FCGK-based comparison comes with a higher cost as it requires a set of graphs and a co-analysis for focal extraction. That said, if a scene collection is available, we would still suggest using FCGK for its better performance and modest processing costs. Comparison to structural groups. In our work, a focal point consists of a group of scene objects and it is derived via structural scene analysis. By name alone, this suggests similarity to the structural groups computed by Xu et al. [2013]. There are however major differences. First, their structural groups are category groups, while our focals are object groups. More importantly, their group extraction involves only frequent pattern mining through local proximity based search. The latter implies that their method is unlikely to return non-local structural groups. This is in part evidenced by the much higher number of groups (212) they obtain vs. the 34 focals we obtain, on the same scene collection (Tsinghua, 792 scenes). The retrieval results in Figure 11 seem to suggest that nonlocal focals extracted via mining and clustering provide the better perspectives for meaningful scene comparison. Non-unique distance. The retrieval experiment using FCGK seems to suggest that our method assigns a unique distance between any two scenes. This is true once the set of focals is fixed and FCGK is to be computed based on those focals and the clustering result. However, the non-uniqueness of focal-centric distances is well utilized in other settings including comprehensive retrieval, multi-query retrieval, and ROI-driven scene exploration, where the relevant focals in the query scenes are all determined on-demand. Limitations. Our current algorithm depends on semantic labeling of scene objects. It remains to be seen whether it works effectively with noisy or incomplete labels, based on pure geometry analysis. For example, it is interesting to test our method on inputs with various levels of label noise. However, it would be hard to quantitatively evaluate the robustness against noisy labels since it may be difficult to reproduce realistic labeling noise introduced by humans. Nevertheless, the two datasets we used do contain some incorrect labels, which did not seem to affect the overall performance. There are perhaps more than a desirable number of parameters in the algorithm, whose values were determined experimentally. From a technical stand point, improvements are possible in various components of the algorithm. For example, our layout similarity operates on OBBs only, which may be unsuitable for objects with complex geometry and spatial arrangements. The structural graphs model the scenes only as flat arrangements of objects. Hierarchical organization may be potentially advantageous. Future work. One obvious pursuit is to apply our focal-driven approach to other datasets, e.g., large and heterogeneous collections of annotated images. An interesting technical question is whether our scene organization can be updated with an additional set of scenes without recomputing everything. Also, rather than replacing one object at a time for scene synthesis like in previous works, our scene organization and focal-based partial scene retrieval, may allow for substituting sub-scenes for the synthesis task. We conclude the paper with a question: ?what is the best way to compare complex scenes? ? This work, along with others before it, assume that comparing attributed graphs defined by semantic tags and object arrangements is the best way. However, we observe that visually, many retrieval results do not look so compelling even with the best method to date. If one takes away the colorings in Figure 14, then the contrast between GK and FCGK would not be as salient. Hence, the focal-centric view we advocate offers a perspective worth considering. The general question, also one that is attributed to complex data beyond those of indoor scenes, should perhaps be answered with user and application intent in mind.",
  "resources" : [ ]
}