{
  "uri" : "sig2008a-a116-kopf_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2008a/a116-kopf_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Deep Photo: Model-Based Photograph Enhancement and Viewing",
    "published" : "2008",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Johannes-Kopf",
      "name" : "Johannes",
      "surname" : "Kopf"
    }, {
      "uri" : "http://drinventor/Boris-Neubert",
      "name" : "Boris",
      "surname" : "Neubert"
    }, {
      "uri" : "http://drinventor/Billy-Chen",
      "name" : "Billy",
      "surname" : "Chen"
    }, {
      "uri" : "http://drinventor/Michael F.-Cohen",
      "name" : "Michael F.",
      "surname" : "Cohen"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Oliver-Deussen",
      "name" : "Oliver",
      "surname" : "Deussen"
    }, {
      "uri" : "http://drinventor/Matthew-Uyttendaele",
      "name" : "Matthew",
      "surname" : "Uyttendaele"
    }, {
      "uri" : "http://drinventor/Dani-Lischinski",
      "name" : "Dani",
      "surname" : "Lischinski"
    } ]
  },
  "bagOfWords" : [ "we", "result", "show", "augment", "photograph", "already", "available", "3d", "model", "world", "support", "wide", "variety", "new", "way", "we", "experience", "interact", "we", "everyday", "snapshot", "first", "trend", "geo-tagged", "photo", "also", "number", "manufacturer", "offer", "small", "gp", "unit", "allow", "photo", "easily", "geo-tag", "software", "synchronize", "gp", "log", "photo", "addition", "location", "tag", "can", "enhance", "digital", "compass", "able", "measure", "orientation", "-lrb-", "tilt", "head", "-rrb-", "camera", "we", "demonstrate", "we", "approach", "combine", "number", "photograph", "-lrb-", "obtain", "from", "flickr", "tm", "-rrb-", "model", "example", "we", "show", "outdoor", "scene", "say", "we", "expect", "kind", "application", "we", "demonstrate", "scale", "include", "any", "improvement", "automatic", "computer", "vision", "algorithm", "depth", "acquisition", "technology", "we", "system", "touch", "upon", "quite", "few", "distinct", "topic", "computer", "vision", "computer", "graphic", "thus", "comprehensive", "review", "all", "related", "work", "feasible", "due", "space", "constraint", "recent", "research", "have", "show", "various", "challenging", "task", "image", "completion", "insertion", "object", "photograph", "-lsb-", "Hays", "Efros", "2007", "Lalonde", "et", "al.", "2007", "-rsb-", "can", "greatly", "benefit", "from", "availability", "enormous", "amount", "photograph", "have", "already", "be", "capture", "unlike", "image", "database", "which", "consist", "mostly", "unrelated", "item", "geometric", "model", "we", "use", "all", "anchor", "world", "surround", "we", "remove", "effect", "haze", "dehazing", "challenging", "problem", "because", "degree", "effect", "each", "pixel", "depend", "depth", "corresponding", "scene", "point", "some", "haze", "removal", "technique", "make", "use", "multiple", "image", "e.g.", "image", "take", "under", "different", "weather", "condition", "-lsb-", "Narasimhan", "Nayar", "2003a", "-rsb-", "different", "polarizer", "orientation", "-lsb-", "Schechner", "et", "al.", "2003", "-rsb-", "Narasimhan", "Nayar", "-lsb-", "2003b", "-rsb-", "dehaze", "single", "image", "base", "rough", "depth", "approximation", "provide", "user", "derive", "from", "satellite", "orthophoto", "we", "work", "differ", "from", "previous", "single", "image", "dehaze", "method", "leverage", "availability", "more", "accurate", "3d", "model", "use", "novel", "data-driven", "dehaze", "procedure", "result", "we", "method", "capable", "effective", "stable", "high-quality", "contrast", "restoration", "even", "extremely", "distant", "region", "classic", "tour", "Picture", "system", "-lsb-", "Horry", "et", "al.", "1997", "-rsb-", "demonstrate", "fitting", "simple", "mesh", "scene", "sometimes", "enough", "enable", "compelling", "3d", "navigation", "experience", "system", "despite", "simplicity", "model", "3d", "experience", "can", "quite", "compelling", "typically", "system", "make", "use", "highly", "accurate", "geometric", "model", "and/or", "collection", "photograph", "often", "take", "under", "different", "lighting", "condition", "give", "input", "often", "able", "predict", "appearance", "scene", "under", "novel", "lighting", "condition", "very", "high", "degree", "accuracy", "realism", "we", "case", "we", "assume", "availability", "geometric", "model", "have", "just", "one", "photograph", "work", "furthermore", "although", "model", "might", "detail", "typically", "quite", "far", "from", "perfect", "match", "photograph", "example", "tree", "cast", "shadow", "nearby", "building", "typically", "absent", "from", "we", "model", "browse", "experience", "we", "provide", "very", "different", "moreover", "contrast", "Photo", "Tourism", "we", "system", "require", "only", "single", "geo-tagged", "photograph", "make", "applicable", "even", "location", "without", "many", "available", "photo", "Photo", "Tourism", "system", "also", "demonstrate", "transfer", "annotation", "from", "one", "register", "photograph", "another", "once", "photo", "register", "geo-referenced", "datum", "map", "3d", "model", "plethora", "information", "become", "available", "furthermore", "discuss", "earlier", "also", "enable", "variety", "other", "application", "addition", "enhance", "photo", "location", "also", "useful", "organize", "visualize", "photo", "collection", "system", "develop", "Toyama", "et", "al.", "-lsb-", "2003", "-rsb-", "enable", "user", "browse", "large", "collection", "geo-referenced", "photo", "2d", "map", "map", "serve", "both", "visualization", "device", "well", "way", "specify", "spatial", "query", "i.e.", "all", "photo", "within", "region", "we", "assume", "photograph", "have", "be", "capture", "simple", "pinhole", "camera", "whose", "parameter", "consist", "position", "pose", "focal", "length", "-lrb-", "seven", "parameter", "total", "-rrb-", "assume", "rough", "position", "from", "which", "photograph", "take", "available", "-lrb-", "either", "from", "geotag", "provide", "user", "-rrb-", "we", "able", "render", "model", "from", "roughly", "correct", "position", "let", "user", "specify", "sufficiently", "many", "correspondence", "recover", "parameter", "solve", "nonlinear", "system", "equation", "-lsb-", "nister", "Stewenius", "2007", "-rsb-", "image", "depict", "foreground", "object", "contain", "model", "we", "ask", "user", "matte", "out", "foreground", "we", "create", "matte", "Soft", "Scissors", "system", "-lsb-", "Wang", "et", "al.", "2007", "-rsb-", "process", "take", "about", "1-2", "minute", "per", "photo", "every", "result", "produce", "use", "matte", "we", "show", "matte", "next", "input", "photograph", "have", "sufficiently", "accurate", "match", "between", "photograph", "geometric", "model", "offer", "new", "possibility", "enhance", "photograph", "we", "able", "easily", "remove", "haze", "unwanted", "color", "shift", "experiment", "alternative", "lighting", "condition", "atmospheric", "phenomenon", "haze", "fog", "can", "reduce", "visibility", "distant", "region", "image", "outdoor", "scene", "furthermore", "light", "mix", "airlight", "-lrb-", "scatter", "ambient", "light", "between", "object", "camera", "-rrb-", "here", "observe", "hazy", "intensity", "pixel", "original", "intensity", "reflect", "towards", "camera", "from", "corresponding", "scene", "point", "airlight", "-lrb-", "-rrb-", "exp", "-lrb-", "??", "-rrb-", "attenuation", "intensity", "function", "distance", "due", "outscattering", "thus", "after", "Input", "Model", "texture", "final", "dehazed", "result", "Input", "Dehazed", "estimate", "parameter", "original", "intensity", "may", "recover", "invert", "model", "point", "out", "Narasimhan", "Nayar", "-lsb-", "2003a", "-rsb-", "model", "assume", "single-scattering", "homogeneous", "athmosphere", "furthermore", "since", "exponential", "attenuation", "go", "quickly", "down", "zero", "noise", "might", "severely", "amplify", "distant", "area", "both", "artifact", "may", "observe", "inversion", "result", "Figure", "-lrb-", "-rrb-", "would", "map", "average", "color", "photograph", "corresponding", "average", "-lrb-", "color-corrected", "-rrb-", "model", "texture", "color", "note", "although", "we", "-lrb-", "-rrb-", "have", "same", "physical", "interprertation", "previous", "approach", "due", "we", "estimation", "process", "subject", "constraint", "physicially-based", "model", "since", "we", "estimate", "single", "curve", "represent", "possibly", "spatially", "0.8", "0.6", "intensity", "0.4", "0.2", "2000 4000 6000 8000", "Depth", "estimate", "haze", "curve", "-lrb-", "-rrb-", "Input", "Dehazed", "vary", "haze", "can", "also", "contain", "non-monotonicity", "all", "parameter", "estimate", "completely", "automatically", "robustness", "we", "operate", "average", "color", "over", "depth", "range", "each", "value", "we", "compute", "average", "model", "texture", "color", "-lrb-", "-rrb-", "all", "pixel", "whose", "depth", "-lsb-", "-rsb-", "well", "average", "hazy", "image", "color", "-lrb-", "-rrb-", "same", "pixel", "we", "implementation", "depth", "interval", "parameter", "set", "500", "meter", "all", "image", "we", "experiment", "averaging", "make", "we", "approach", "less", "sensitive", "model", "texture", "artifact", "registration", "stitching", "error", "bad", "pixel", "contain", "shadow", "cloud", "example", "Landsat", "use", "seven", "sensor", "whose", "spectral", "response", "differ", "from", "typical", "RGB", "camera", "sensor", "thus", "color", "result", "texture", "only", "approximation", "one", "would", "have", "be", "capture", "camera", "-lrb-", "see", "Figure", "-rrb-", "we", "correct", "color", "bias", "measure", "ratio", "between", "photo", "texture", "color", "foreground", "-lrb-", "each", "channel", "-rrb-", "use", "ratio", "correct", "color", "entire", "texture", "more", "precisely", "we", "compute", "global", "multiplicative", "correction", "vector", "Input", "Fattal?s", "result", "where", "average", "-lrb-", "-rrb-", "similarly", "compute", "average", "model", "texture", "lum", "-lrb-", "-rrb-", "denote", "luminance", "color", "we", "set", "1600", "meter", "all", "we", "image", "ignore", "moment", "physical", "interpretation", "-lrb-", "-rrb-", "note", "eq", "-lrb-", "-rrb-", "simply", "stretch", "intensity", "image", "around", "use", "scale", "coefficient", "-lrb-", "-rrb-", "substitute", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "eq", "-lrb-", "-rrb-", "we", "get", "different", "choice", "result", "different", "scale", "curve", "-lrb-", "-rrb-", "we", "set", "since", "guarantee", "-lrb-", "-rrb-", "use", "would", "result", "larger", "value", "-lrb-", "-rrb-", "hence", "less", "contrast", "dehazed", "image", "use", "might", "prone", "instability", "figure", "show", "-lrb-", "-rrb-", "curve", "estimate", "describe", "above", "recover", "haze", "curve", "-lrb-", "-rrb-", "allow", "effectively", "restore", "contrast", "photo", "however", "color", "background", "might", "undergo", "color", "shift", "we", "compensate", "adjust", "while", "keep", "-lrb-", "-rrb-", "fix", "after", "change", "dehaze", "preserve", "color", "photo", "background", "adjust", "we", "first", "compute", "average", "background", "color", "photo", "average", "-lrb-", "-rrb-", "similarly", "compute", "average", "model", "texture", "we", "set", "5000m", "all", "we", "image", "color", "background", "preserve", "ratio", "thus", "we", "rewrite", "eq", "-lrb-", "-rrb-", "obtain", "set", "max", "-lrb-", "red", "red", "green", "green", "blue", "blue", "-rrb-", "particular", "choice", "result", "maximum", "guarantee", "finally", "we", "use", "eq", "-lrb-", "-rrb-", "recovered", "-lrb-", "-rrb-", "adjust", "dehaze", "photograph", "Inversion", "result", "we", "result", "figure", "show", "various", "image", "dehaze", "we", "method", "Figure", "compare", "we", "method", "other", "approach", "comparison", "we", "focus", "method", "applicable", "we", "context", "work", "single", "image", "only", "fattal?s", "method", "-lsb-", "2008", "-rsb-", "dehaze", "image", "nicely", "up", "certain", "distance", "-lrb-", "particularly", "consider", "method", "do", "require", "any", "input", "addition", "image", "itself", "-rrb-", "unable", "effectively", "dehaze", "more", "distant", "part", "closer", "horizon", "Inversion", "result", "obtain", "via", "eq", "-lrb-", "-rrb-", "exponential", "haze", "curve", "here", "we", "use", "we", "accurate", "depth", "map", "instead", "use", "multiple", "image", "user-provided", "depth", "approximation", "airlight", "color", "set", "sky", "color", "near", "horizon", "optical", "depth", "adjust", "manually", "result", "suffer", "from", "amplify", "noise", "distance", "break", "down", "next", "horizon", "contrast", "we", "result", "manage", "remove", "more", "haze", "than", "two", "other", "approach", "while", "preserve", "natural", "color", "input", "photo", "note", "practice", "one", "might", "want", "remove", "haze", "completely", "we", "have", "do", "because", "haze", "sometimes", "provide", "perceptually", "significant", "depth", "cue", "also", "dehaze", "typically", "amplify", "some", "noise", "region", "where", "little", "visible", "detail", "remain", "original", "image", "still", "almost", "every", "image", "benefit", "from", "some", "degree", "dehazing", "do", "simply", "invert", "eq", "demonstrate", "companion", "video", "particular", "landscape", "photography", "vast", "majority", "breathtaking", "photograph", "take", "during", "golden", "hour", "after", "sunrise", "before", "sunset", "-lsb-", "Reichmann", "2001", "-rsb-", "unfortunately", "most", "we", "outdoor", "snapshot", "take", "under", "rather", "boring", "lighting", "Deep", "Photo", "Input", "Relighted", "Relighted", "Input", "Relighted", "Original", "Relighted", "Lit", "Model", "setup", "do", "allow", "we", "correctly", "recover", "reflectance", "each", "pixel", "thus", "we", "use", "following", "simple", "workflow", "which", "only", "approximate", "appearance", "lighting", "change", "scene", "we", "begin", "dehaze", "image", "describe", "previous", "section", "modulate", "color", "use", "lightmap", "compute", "novel", "lighting", "original", "sky", "replace", "new", "one", "simulate", "desire", "time", "day", "-lrb-", "we", "use", "Vue", "infinite", "-lsb-", "e-on", "Software", "2008", "-rsb-", "synthesize", "new", "sky", "-rrb-", "finally", "we", "add", "haze", "back", "use", "eq", "-lrb-", "-rrb-", "after", "multiply", "haze", "curve", "-lrb-", "-rrb-", "global", "color", "mood", "transfer", "coefficient", "global", "color", "mood", "transfer", "coefficient", "compute", "each", "color", "channel", "let", "ref", "new", "average", "color", "two", "sky", "dome", "color", "mood", "transfer", "coefficient", "give", "new", "ref", "we", "current", "implementation", "offer", "user", "set", "control", "various", "aspect", "lighting", "include", "atmosphere", "parameter", "diffuse", "ambient", "color", "etc.", "we", "compute", "lightmap", "simple", "local", "shade", "model", "scale", "color", "mood", "coefficient", "where", "-lsb-", "shadow", "-rsb-", "shadow", "coefficient", "indicate", "amount", "light", "attenuation", "due", "shadow", "ambient", "coefficient", "diffuse", "coefficient", "point", "normal", "direction", "sun", "final", "result", "obtain", "simply", "multiply", "image", "L.", "note", "we", "do", "attempt", "remove", "exist", "illumination", "before", "apply", "new", "one", "however", "we", "find", "even", "basic", "procedure", "yield", "convincing", "change", "lighting", "-lrb-", "see", "Figure", "dynamic", "relight", "sequence", "video", "-rrb-", "Figure", "demonstrate", "relight", "geo-registered", "photo", "generate", "completely", "different", "-lrb-", "more", "realistic", "-rrb-", "effect", "than", "simply", "render", "underlie", "geometric", "model", "under", "desire", "lighting", "one", "compelling", "feature", "Deep", "Photo", "ability", "modify", "viewpoint", "from", "which", "original", "photograph", "take", "bring", "static", "photo", "life", "manner", "significantly", "enhance", "photo", "browse", "experience", "show", "companion", "video", "assume", "photograph", "have", "be", "register", "sufficiently", "accurate", "geometric", "model", "scene", "challenge", "change", "viewpoint", "reduce", "complete", "miss", "texture", "area", "either", "occluded", "simply", "outside", "original", "view", "frustum", "we", "use", "image", "completion", "-lsb-", "efro", "Leung", "1999", "Drori", "et", "al.", "2003", "-rsb-", "fill", "miss", "area", "texture", "from", "other", "part", "photograph", "we", "image", "completion", "process", "similar", "texture-by-numbers", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "where", "instead", "hand-painted", "label", "map", "we", "use", "guidance", "map", "derive", "from", "texture", "3d", "model", "texture", "synthesize", "over", "cylindrical", "layered", "depth", "image", "-lrb-", "LDI", "-rrb-", "-lsb-", "shade", "et", "al.", "1998", "-rsb-", "center", "around", "original", "camera", "position", "LDI", "image", "store", "each", "pixel", "depths", "normal", "scene", "point", "intersect", "corresponding", "ray", "from", "viewpoint", "we", "use", "datum", "structure", "since", "able", "represent", "both", "visible", "occluded", "part", "scene", "-lrb-", "we", "example", "we", "use", "LDI", "four", "depth", "layer", "per", "pixel", "-rrb-", "color", "frontmost", "layer", "each", "pixel", "take", "from", "original", "photograph", "provide", "inside", "original", "view", "frustum", "while", "remain", "color", "synthesize", "we", "guide", "texture", "transfer", "we", "begin", "texture", "transfer", "process", "compute", "guide", "value", "all", "layer", "each", "pixel", "we", "experiment", "we", "try", "various", "other", "feature", "include", "terrain", "normal", "slope", "height", "combination", "thereof", "we", "achieve", "best", "result", "however", "realtively", "simple", "feature", "vector", "above", "include", "distance", "feature", "vector", "bias", "synthesis", "towards", "generate", "texture", "correct", "scale", "normalize", "so", "distance", "from", "5000", "meter", "map", "-lsb-", "-rsb-", "we", "only", "include", "chrominance", "information", "feature", "vector", "-lrb-", "luminance", "-rrb-", "alleviate", "problem", "associate", "exist", "transient", "feature", "shade", "shadow", "model", "texture", "texture", "synthesis", "carry", "out", "multi-resolution", "manner", "first", "-lrb-", "coarsest", "-rrb-", "level", "synthesize", "grow", "texture", "outwards", "from", "known", "region", "each", "unknown", "pixel", "we", "examine", "square", "neighborhood", "around", "exhaustively", "search", "best", "matching", "neighborhood", "from", "known", "region", "-lrb-", "use", "norm", "-rrb-", "since", "we", "neighborhood", "contain", "miss", "pixel", "we", "can", "apply", "pca", "compression", "other", "speed-up", "structure", "straight", "forward", "way", "however", "first", "level", "sufficiently", "coarse", "its", "synthesis", "rather", "fast", "synthesize", "each", "next", "level", "we", "upsample", "result", "previous", "level", "perform", "small", "number", "k-coherence", "synthesis", "pass", "-lsb-", "ashikhmin", "2001", "-rsb-", "refine", "result", "here", "we", "use", "look-ahead", "region", "total", "synthesis", "time", "about", "minute", "per", "image", "total", "texture", "size", "typically", "order", "4800", "1600", "pixel", "time", "four", "layer", "should", "note", "when", "work", "ldi", "concept", "pixel?s", "neighborhood", "must", "adjust", "account", "existence", "multiple", "depth", "layer", "each", "pixel", "we", "define", "neighborhood", "following", "way", "each", "depth", "layer", "pixel", "have", "up", "pixel", "surround", "neighbor", "pixel", "have", "multiple", "depth", "layer", "pixel", "layer", "closest", "depth", "value", "assign", "immediate", "neighbor", "render", "image", "from", "novel", "viewpoint", "we", "use", "shader", "project", "LDI", "image", "onto", "geometric", "model", "compute", "distance", "model", "camera", "use", "pixel", "color", "from", "depth", "layer", "closest", "distance", "significant", "change", "viewpoint", "eventually", "cause", "texture", "distortion", "one", "keep", "use", "texture", "from", "photograph", "alleviate", "problem", "we", "blend", "photograph?s", "texture", "model?s", "texture", "new", "virtual", "camera", "get", "farther", "away", "from", "original", "viewpoint", "we", "find", "significantly", "improve", "3d", "view", "experience", "even", "drastic", "view", "change", "go", "bird?s", "eye", "view", "thus", "texture", "color", "each", "terrain", "point", "give", "where", "blending", "factor", "-lrb-", "-rrb-", "determine", "respect", "current", "view", "accord", "follow", "principle", "-lrb-", "-rrb-", "pixel", "original", "photograph", "which", "correspond", "surface", "face", "camera", "consider", "more", "reliable", "than", "those", "oblique", "surface", "-lrb-", "ii", "-rrb-", "pixel", "original", "photograph", "also", "prefer", "whenever", "corresponding", "scene", "point", "view", "from", "same", "direction", "current", "view", "original", "one", "specifically", "let", "-lrb-", "-rrb-", "denote", "surface", "normal", "original", "camera", "position", "from", "which", "photograph", "take", "new", "current", "camera", "position", "next", "let", "-lrb-", "-rrb-", "denote", "normalize", "vector", "from", "scene", "point", "original", "camera", "position", "similarly", "new", "-lrb-", "new", "-rrb-", "new", "finally", "we", "also", "apply", "re-hazing", "on-the-fly", "first", "we", "remove", "haze", "from", "texture", "completely", "describe", "section", "4.1", "we", "add", "haze", "back", "time", "use", "distance", "from", "current", "camera", "position", "result", "may", "see", "figure", "video", "have", "register", "photograph", "model", "have", "GIS", "datum", "associate", "allow", "display", "various", "information", "about", "scene", "while", "browse", "photograph", "we", "have", "implement", "simple", "application", "demonstrate", "several", "type", "information", "visualization", "application", "photograph", "show", "sideby-side", "top", "view", "model", "refer", "map", "view", "view", "frustum", "correspond", "photograph", "display", "map", "view", "update", "dynamically", "whenever", "view", "change", "-lrb-", "describe", "section", "-rrb-", "map", "view", "user", "able", "switch", "between", "street", "map", "orthographic", "photo", "combination", "thereof", "etc.", "addition", "text", "label", "also", "possible", "superimpose", "graphical", "map", "element", "road", "directly", "onto", "photo", "view", "ability", "demonstrate", "figure", "companion", "video", "various", "database", "geo-tagged", "media", "available", "web", "we", "able", "highlight", "location", "both", "view", "-lrb-", "photo", "map", "-rrb-", "particular", "interest", "geo-tagged", "Wikipedia", "article", "about", "various", "landmark", "we", "display", "small", "Wikipedia", "icon", "location", "which", "open", "browser", "window", "corresponding", "article", "when", "click", "also", "demonstrate", "companion", "video", "another", "nice", "visualization", "feature", "we", "system", "ability", "highlight", "object", "under", "mouse", "photo", "view", "can", "useful", "example", "when", "view", "night", "time", "photograph", "urban", "scene", "shot", "night", "building", "under", "cursor", "may", "show", "use", "daylight", "texture", "from", "underlie", "model", "we", "present", "Deep", "Photo", "novel", "system", "editing", "browse", "outdoor", "photograph", "leverage", "high", "quality", "3d", "model", "earth", "now", "become", "widely", "available", "we", "have", "demonstrate", "once", "simple", "geo-registration", "photo", "perform", "model", "can", "use", "many", "interesting", "photo", "manipulation", "range", "from", "deand", "rehazing", "relighting", "integrate", "gi", "information", "application", "we", "show", "vary", "haze", "removal", "challenging", "problem", "due", "fact", "haze", "function", "depth", "we", "have", "show", "now", "depth", "available", "geo-registered", "photograph", "excellent", "haze", "editing", "can", "achieve", "similarly", "have", "underlie", "geometric", "model", "make", "possible", "generate", "convincing", "relighted", "photograph", "dynamically", "change", "view", "finally", "we", "demonstrate", "enormous", "wealth", "information", "available", "online", "can", "now", "use", "annotate", "help", "browse", "photograph", "within", "we", "framework", "we", "use", "model", "obtain", "from", "virtual", "Earth", "manual", "registration", "do", "within", "minute", "mat", "out", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "foreground", "also", "easy", "task", "use", "state-of-the-art", "technique", "Soft", "Scissors", "-lsb-", "Wang", "et", "al.", "2007", "-rsb-", "all", "other", "operation", "dehazing", "relighting", "run", "interactive", "speed", "however", "compute", "very", "detailed", "shadow", "map", "relighting", "can", "time", "consuming", "can", "expect", "always", "some", "difference", "misalignment", "between", "photograph", "model", "may", "arise", "due", "insufficiently", "accurate", "model", "also", "due", "fact", "photograph", "be", "capture", "ideal", "pinhole", "camera", "although", "can", "lead", "some", "artifact", "-lrb-", "see", "Figure", "-rrb-", "we", "find", "many", "case", "difference", "less", "problematic", "than", "one", "might", "fear", "we", "believe", "application", "present", "here", "represent", "just", "small", "fraction", "possible", "geo-photo", "editing", "operation", "many", "exist", "digital", "photography", "product", "could", "greatly", "enhance", "use", "geo", "information", "Operations", "could", "encompass", "noise-reduction", "image", "sharpen", "3d", "model", "prior", "postcapture", "refocussing", "object", "recovery", "under", "over-exposed", "area", "well", "illumination", "transfer", "between", "photograph", "gi", "database", "contain", "wealth", "information", "which", "we", "have", "just", "use", "small", "amount", "water", "grass", "pavement", "build", "material", "etc", "can", "all", "potentially", "automatically", "label", "use", "improve", "photo", "tone", "adjustment", "label", "can", "transfer", "automatically", "from", "one", "image", "other", "again", "have", "single", "consistent", "3d", "model", "we", "photograph", "provide", "much", "more", "than", "just", "depth", "value", "per", "pixel", "paper", "we", "mostly", "deal", "single", "image", "most", "application", "we", "demonstrate", "become", "even", "stronger", "when", "combine", "multiple", "input", "photo", "particularly", "interesting", "direction", "might", "combine", "Deep", "Photo", "Photo", "Tourism", "system", "once", "Photo", "Tour", "geo-registered", "coarse", "3d", "information", "generate", "Photo", "Tourism", "could", "use", "enhance", "online", "3d", "datum", "vice-versa", "information", "visualization", "novel", "view", "synthesis", "application", "we", "demonstrate", "here", "could", "combine", "Photo", "Tourism", "viewer", "idea", "fuse", "multiple", "image", "could", "even", "extend", "video", "could", "register", "model", "research", "support", "part", "grant", "from", "follow", "funding", "agency", "Lion", "foundation", "gif", "foundation", "Israel", "Science", "Foundation", "DFG", "graduiertenkolleg/1042", "explorative", "analysis", "visualization", "large", "information", "space", "University", "Konstanz", "Germany" ],
  "content" : "Our results show that augmenting photographs with already available 3D models of the world supports a wide variety of new ways for us to experience and interact with our everyday snapshots. The first trend is that of geo-tagged photos. Also, a number of manufacturers offer small GPS units that allow photos to be easily geo-tagged by software that synchronizes the GPS log with the photos. In addition, location tags can be enhanced by digital compasses that are able to measure the orientation (tilt and heading) of the camera. We demonstrate our approach by combining a number of photographs (obtained from flickr TM ) with these models. The examples we show are of outdoor scenes. That said, we expect the kinds of applications we demonstrate will scale to include any improvements in automatic computer vision algorithms and depth acquisition technologies. Our system touches upon quite a few distinct topics in computer vision and computer graphics; thus, a comprehensive review of all related work is not feasible due to space constraints. Recent research has shown that various challenging tasks, such as image completion and insertion of objects into photographs [Hays and Efros 2007; Lalonde et al. 2007] can greatly benefit from the availability of the enormous amounts of photographs that had already been captured. But unlike image databases, which consist mostly of unrelated items, the geometric models we use are all anchored to the world that surrounds us. Removing the effect of haze, or dehazing, is a challenging problem, because the degree of this effect at each pixel depends on the depth of the corresponding scene point. Some haze removal techniques make use of multiple images; e.g., images taken under different weather conditions [Narasimhan and Nayar 2003a], or with different polarizer orientations [Schechner et al. 2003]. Narasimhan and Nayar [2003b] dehaze single images based on a rough depth approximation provided by the user, or derived from satellite orthophotos. Our work differs from these previous single image dehazing methods in that it leverages the availability of more accurate 3D models, and uses a novel data-driven dehazing procedure. As a result, our method is capable of effective, stable high-quality contrast restoration even of extremely distant regions. The classic ?Tour Into the Picture? system [Horry et al. 1997] demonstrates that fitting a simple mesh to the scene is sometimes enough to enable a compelling 3D navigation experience. In these systems, despite the simplicity of the models, the 3D experience can be quite compelling. Typically, such systems make use of a highly accurate geometric model, and/or a collection of photographs, often taken under different lighting conditions. Given this input they are often able to predict the appearance of a scene under novel lighting conditions with a very high degree of accuracy and realism. In our case, we assume the availability of a geometric model, but have just one photograph to work with. Furthermore, although the model might be detailed, it is typically quite far from a perfect match to the photograph. For example, a tree casting a shadow on a nearby building will typically be absent from our model. But, the browsing experience that we provide is very different. Moreover, in contrast to ?Photo Tourism?, our system requires only a single geo-tagged photograph, making it applicable even to locations without many available photos. The ?Photo Tourism? system also demonstrates the transfer of annotations from one registered photograph to another. Once a photo is registered to geo-referenced data such as maps and 3D models, a plethora of information becomes available. Furthermore, as discussed earlier, it also enables a variety of other applications. In addition to enhancing photos, location is also useful in organizing and visualizing photo collections. The system developed by Toyama et al. [2003] enables a user to browse large collections of geo-referenced photos on a 2D map. The map serves as both a visualization device, as well as a way to specify spatial queries, i.e., all photos within a region. We assume that the photograph has been captured by a simple pinhole camera, whose parameters consist of position, pose, and focal length (seven parameters in total). Assuming that the rough position from which the photograph was taken is available (either from a geotag, or provided by the user), we are able to render the model from roughly the correct position, let the user specify sufficiently many correspondences, and recover the parameters by solving a nonlinear system of equations [Nister and Stewenius 2007]. For images that depict foreground objects not contained in the model, we ask the user matte out the foreground. We created mattes with the Soft Scissors system [Wang et al. 2007]. The process took about 1-2 minutes per photo. For every result produced using a matte we show the matte next to the input photograph. Having a sufficiently accurate match between a photograph and a geometric model offers new possibilities for enhancing such photographs. We are able to easily remove haze and unwanted color shifts and to experiment with alternative lighting conditions. Atmospheric phenomena, such as haze and fog can reduce the visibility of distant regions in images of outdoor scenes. Furthermore, this light is mixed with airlight (scattered ambient light between the object and camera). Here I h is the observed hazy intensity at a pixel, I o is the original intensity reflected towards the camera from the corresponding scene point, A is the airlight, and f (z) = exp(?? z) is the attenuation in intensity as a function of distance due to outscattering. Thus, after Input Model textures Final dehazed result Input\n        Dehazed estimating the parameters A and ? the original intensity may be recovered by inverting the model: As pointed out by Narasimhan and Nayar [2003a], this model assumes single-scattering and a homogeneous athmosphere. Furthermore, since the exponential attenuation goes quickly down to zero, noise might be severely amplified in the distant areas. Both of these artifacts may be observed in the ?inversion result? of Figure 4 . (2) would map averages of colors in the photograph to the corresponding averages of (color-corrected) model texture colors. Note that although our f (z) has the same physical interprertation as in the previous approaches, due to our estimation process it is not subject to the constraints of a physicially-based model. Since we estimate a single curve to represent the possibly spatially\n        1 0.8 0.6 Intensity 0.4 0.2 0 2000 4000 6000 8000 Depth Estimated haze curves f (z)\n        Input\n        Dehazed\n        varying haze it can also contain non-monotonicities. All of the parameters are estimated completely automatically. For robustness, we operate on averages of colors over depth ranges. For each value of z, we compute the average model texture color I ? m (z) for all pixels whose depth is in [z ? ? , z + ? ], as well as the average hazy image color I ? h (z) for the same pixels. In our implementation, the depth interval parameter ? is set to 500 meters, for all images we experimented with. The averaging makes our approach less sensitive to model texture artifacts, such as registration and stitching errors, bad pixels, or contained shadows and clouds. For example, Landsat uses seven sensors whose spectral responses differ from the typical RGB camera sensors. Thus, the colors in the resulting textures are only an approximation to ones that would have been captured by a camera (see Figure 2 ). We correct this color bias by measuring the ratio between the photo and the texture colors in the foreground (in each channel), and using these ratios to correct the colors of the entire texture. More precisely, we compute a global multiplicative correction vector C as Input Fattal?s Result where F h is the average of I ? h (z) with z < z F , and F m is a similarly computed average of the model texture. lum(c) denotes the luminance of a color c. We set z F to 1600 meters for all our images. Ignoring for the moment the physical interpretation of A and f (z), note that eq. (2) simply stretches the intensities of the image around A, using the scale coefficient f (z) ?1 . Substituting I ? h (z) for I h , and C I ? m (z) for I o , in eq. (2) we get Different choices of A will result in different scaling curves f (z). We set A = 1 since this guarantees f (z) ? 0. Using A > 1 would result in larger values of f (z), and hence less contrast in the dehazed image, and using A < 1 might be prone to instabilities. Figure 2 shows the f (z) curve estimated as described above. The recovered haze curve f (z) allows to effectively restore the contrasts in the photo. However, the colors in the background might undergo a color shift. We compensate for this by adjusting A, while keeping f (z) fixed, such that after the change the dehazing preserves the colors of the photo in the background. To adjust A, we first compute the average background color B h of the photo as the average of I ? h (z) with z > z B , and a similarly computed average of the model texture B m . We set z B to 5000m for all our images. The color of the background is preserved, if the ratio Thus, we rewrite eq. (5) to obtain A as and set R = max(B m,red /B h,red , B m,green /B h,green , B m,blue /B h,blue ). This particular choice of R results in the maximum A that guarantees A ? 1. Finally, we use eq. (2) with the recovered f (z) and the adjusted A to dehaze the photograph. Inversion Result Our Result\n        Figures 2 and 3 show various images dehazed with our method. Figure 4 compares our method with other approaches. In this comparison we focused on methods that are applicable in our context of working with a single image only. Fattal?s method [2008] dehazes the image nicely up to a certain distance (particularly considering that this method does not require any input in addition to the image itself), but it is unable to effectively dehaze the more distant parts, closer to the horizon. The ?Inversion Result? was obtained via eq. (2) with an exponential haze curve. Here, we use our accurate depth map instead of using multiple images or user-provided depth approximations. The airlight color was set to the sky color near the horizon, and the optical depth ? was adjusted manually. The result suffers from amplified noise in the distance, and breaks down next to the horizon. In contrast, our result manages to remove more haze than the two other approaches, while preserving the natural colors of the input photo. Note that in practice one might not want to remove the haze completely as we have done, because haze sometimes provides perceptually significant depth cues. Also, dehazing typically amplifies some noise in regions where little or no visible detail remain in the original image. Still, almost every image benefits from some degree of dehazing. This is done simply by inverting eq. This is demonstrated in the companion video. In particular, in landscape photography, the vast majority of breathtaking photographs are taken during the ?golden hour?, after sunrise, or before sunset [Reichmann 2001]. Unfortunately most of our outdoor snapshots are taken under rather boring lighting. With Deep Photo Input Relighted Relighted Input Relighted Original Relighted Lit Model This setup does not allow us to correctly recover the reflectance at each pixel. Thus, we use the following simple workflow, which only approximates the appearance of lighting changes in the scene. We begin by dehazing the image, as described in the previous section, and modulate the colors using a lightmap computed for the novel lighting. The original sky is replaced by a new one simulating the desired time of day (we use Vue 6 Infinite [E-on Software 2008] to synthesize the new sky). Finally, we add haze back in using Eq. (7), after multiplying the haze curves f (z) by a global color mood transfer coefficient. The global color mood transfer coefficient L G is computed for each color channel. Let I ref and I new be the average colors of the two sky domes. The color mood transfer coefficients are then given by L G = I new /I ref . Our current implementation offers the user a set of controls for various aspects of the lighting, including atmosphere parameters, diffuse and ambient colors, etc. We then compute the lightmap with a simple local shading model and scale it by the color mood coefficient: where L S ? [I shadow , 1] is the shadow coefficient that indicates the amount of light attenuation due to shadows, L A is the ambient coefficient, L D is the diffuse coefficient, n the point normal, and l the direction to the sun. The final result is obtained simply by multiplying the image by L. Note that we do not attempt to remove the existing illumination before applying the new one. However, we found even this basic procedure yields convincing changes in the lighting (see Figure 5, and the dynamic relighting sequences in the video). Figure 6 demonstrates that relighting a geo-registered photo generates a completely different (and more realistic) effect than simply rendering the underlying geometric model under the desired lighting. One of the compelling features of Deep Photo is the ability to modify the viewpoint from which the original photograph was taken. Bringing the static photo to life in this manner significantly enhances the photo browsing experience, as shown in the companion video. Assuming that the photograph has been registered with a sufficiently accurate geometric model of the scene, the challenge in changing the viewpoint is reduced to completing the missing texture in areas that are either occluded, or are simply outside the original view frustum. We use image completion [Efros and Leung 1999; Drori et al. 2003] to fill the missing areas with texture from  other parts of the photograph. Our image completion process is similar to texture-by-numbers [Hertzmann et al. 2001], where instead of a hand-painted label map we use a guidance map derived from the textures of the 3D model. The texture is synthesized over a cylindrical layered depth image (LDI) [Shade et al. 1998], centered around the original camera position. The LDI image stores, for each pixel, the depths and normals of scene points intersected by the corresponding ray from the viewpoint. We use this data structure, since it is able to represent both the visible and the occluded parts of the scene (in our examples we used a LDI with four depth layers per pixel). The colors of the frontmost layer in each pixel are taken from the original photograph provided that they are inside the original view frustum, while the remaining colors are synthesized by our guided texture transfer. We begin the texture transfer process by computing the guiding value for all of the layers at each pixel. In our experiments, we tried various other features, including terrain normal, slope, height, and combinations thereof. We achieved the best results, however, with the realtively simple feature vector above. Including the distance D in the feature vector biases the synthesis towards generating textures at the correct scale. D is normalized so that distances from 0 to 5000 meters map to [0, 1]. We only include chrominance information in the feature vector (and not luminance) to alleviate problems associated with existing transient features such as shading and shadows in the model textures. Texture synthesis is carried out in a multi-resolution manner. The first (coarsest) level is synthesized by growing the texture outwards from the known regions. For each unknown pixel we examine a square neighborhood around it, and exhaustively search for the best matching neighborhood from the known region (using the L 2 norm). Since our neighborhoods contain missing pixels we cannot apply PCA compression and other speed-up structures in a straight forward way. However, the first level is sufficiently coarse and its synthesis is rather fast. To synthesize each next level we upsample the result of the previous level and perform a small number of k-coherence synthesis passes [Ashikhmin 2001] to refine the result. Here we use a 5 ? 5 look-ahead region and k = 4. The total synthesis time is about 5 minutes per image. The total texture size is typically on the order of 4800 ? 1600 pixels, times four layers. It should be noted that when working with LDIs the concept of a pixel?s neighborhood must be adjusted to account for the existence of multiple depth layers at each pixel. We define the neighborhood in the following way: On each depth layer, a pixel has up to 8 pixels surrounding it. If the neighboring pixel has multiple depth layers, the pixel on the layer with the closest depth value is assigned as the immediate neighbor. To render images from novel viewpoints, we use a shader to project the LDI image onto the geometric model by computing the distance of the model to the camera and using the pixel color from the depth layer closest to this distance. Significant changes in the viewpoint eventually cause texture distortions if one keeps using the texture from the photograph. To alleviate this problem, we blend the photograph?s texture into the model?s texture as the new virtual camera gets farther away from the original viewpoint. We found this to significantly improve the 3D viewing experience, even for drastic view changes, such as going to bird?s eye view. Thus, the texture color T at each terrain point x is given by where the blending factor g(x) is determined with respect to the current view, according to the following principles: (i) pixels in the original photograph which correspond to surfaces facing camera are considered more reliable than those on oblique surfaces; and, (ii) pixels in the original photograph are also preferred whenever the corresponding scene point is viewed from the same direction in the current view, as it was in the original one. Specifically, let n(x) denote the surface normal, C 0 the original camera position from which the photograph was taken, and C new the current camera position. Next, let v 0 = (C 0 ? x)/ C 0 ? x denote the normalized vector from the scene point to the original camera position, and similarly v new = (C new ? x)/ C new ? x . Finally, we also apply re-hazing on-the-fly. First, we remove haze from the texture completely as described in Section 4.1. Then, we add haze back in, this time using the distances from the current camera position. The results may be seen in Figure 7 and in the video. Having registered a photograph with a model that has GIS data associated with it allows displaying various information about the scene, while browsing the photograph. We have implemented a simple application that demonstrates several types of information visualization. In this application, the photograph is shown sideby-side with a top view of the model, referred to as the map view. The view frustum corresponding to the photograph is displayed in the map view, and is updated dynamically whenever the view is changed (as described in Section 5). In the map view, the user is able to switch between a street map, an orthographic photo, a combination thereof, etc. In addition to text labels it is also possible to superimpose graphical map elements, such as roads, directly onto the photo view. These abilities are demonstrated in Figures 1 and 8 and in the companion video. There are various databases with geo-tagged media available on the web. We are able to highlight these locations in both views (photo and map). Of particular interest are geo-tagged Wikipedia articles about various landmarks. We display a small Wikipedia icon at such locations, which opens a browser window with the corresponding article, when clicked. This is also demonstrated in the companion video. Another nice visualization feature of our system is the ability to highlight the object under the mouse in the photo view. This can be useful, for example, when viewing night time photographs: in an urban scene shot at night, the building under the cursor may be shown using daylight textures from the underlying model. We presented Deep Photo, a novel system for editing and browsing outdoor photographs. It leverages the high quality 3D models of the earth that are now becoming widely available. We have demonstrated that once a simple geo-registration of a photo is performed, the models can be used for many interesting photo manipulations that range from deand rehazing and relighting to integrating GIS information. The applications we show are varied. Haze removal is a challenging problem due to the fact that haze is a function of depth. We have shown that now that depth is available in a geo-registered photograph, excellent ?haze editing? can be achieved. Similarly, having an underlying geometric model makes it possible to generate convincing relighted photographs, and dynamically change the view. Finally, we demonstrate that the enormous wealth of information available online can now be used to annotate and help browse photographs. Within our framework we used models obtained from Virtual Earth. The manual registration is done within a minute, matting out the\n        (c) (d) (e) foreground is also an easy task using state-of-the-art techniques such as Soft Scissors [Wang et al. 2007]. All other operations such as dehazing and relighting run at interactive speeds; however, computing very detailed shadow maps for the relighting can be time consuming. As can be expected, there are always some differences and misalignments between the photograph and the model. The may arise due to insufficiently accurate models, and also due to the fact that the photographs were not captured with an ideal pinhole camera. Although they can lead to some artifacts (see Figure 9 ), we found that in many cases these differences are less problematic than one might fear. We believe that the applications presented here represent just a small fraction of possible geo-photo editing operations. Many of the existing digital photography products could be greatly enhanced with the use of geo information. Operations could encompass noise-reduction and image sharpening with 3D model priors, postcapture refocussing, object recovery in under or over-exposed areas as well as illumination transfer between photographs. GIS databases contain a wealth of information, of which we have just used a small amount. Water, grass, pavement, building materials, etc, can all potentially be automatically labeled and used to improve photo tone adjustment. Labels can be transferred automatically from one image to others. Again, having a single consistent 3D model for our photographs provides much more than just a depth value per pixel. In this paper we mostly dealt with single images. Most of the applications that we demonstrated become even stronger when combining multiple input photos. A particularly interesting direction might be to combine Deep Photo with the Photo Tourism system. Once a Photo Tour is geo-registered, the coarse 3D information generated by Photo Tourism could be used to enhance online 3D data and vice-versa. The information visualization and novel view synthesis applications we demonstrate here could be combined with the Photo Tourism viewer. This idea of fusing multiple images could even be extended to video that could be registered to the models. This research was supported in parts by grants from the the following funding agencies: the Lion foundation, the GIF foundation, the Israel Science Foundation, and by DFG Graduiertenkolleg/1042 ?Explorative Analysis and Visualization of Large Information Spaces? at University of Konstanz, Germany.",
  "resources" : [ ]
}