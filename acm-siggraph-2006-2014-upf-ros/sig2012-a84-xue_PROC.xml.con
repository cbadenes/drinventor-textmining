{
  "uri" : "sig2012-a84-xue_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012/a84-xue_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Understanding and Improving the Realism of Image Composites",
    "published" : null,
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ ]
  },
  "bagOfWords" : [ "we", "show", "number", "composit", "result", "adjust", "use", "we", "automatic", "method", "compare", "they", "other", "technique", "Table", "show", "one", "way", "evaluate", "success", "we", "method", "image", "we", "already", "know", "real", "however", "more", "useful", "evaluation", "involve", "adjustment", "composite", "region", "take", "from", "separate", "image", "source", "we", "therefore", "create", "test", "set", "48", "composite", "make", "effort", "ensure", "each", "composite", "semantically", "reasonable", "we", "create", "adjust", "result", "use", "five", "technique", "simple", "cut-and-paste", "manually", "adjust", "composite", "Photoshop", "Match", "Color", "method", "Lalonde", "Efros", "-lsb-", "2007", "-rsb-", "-lrb-", "which", "we", "label", "ColorComp", "-rrb-", "we", "method", "six", "example", "show", "Figure", "11", "manually-adjusted", "composite", "create", "Photoshop", "one", "author", "who", "have", "extensive", "Photoshop", "experience", "typically", "take", "3-4", "minute", "create", "we", "compute", "result", "ColorComp", "use", "code", "datum", "provide", "its", "author", "which", "typically", "take", "3-5", "minute", "execute", "per", "example", "Photoshop", "Match", "Color", "use", "standard", "color", "transfer", "technique", "similar", "Reinhard", "et", "al.", "-lsb-", "2004", "-rsb-", "color", "transfer", "useful", "number", "creative", "task", "compositing", "necessarily", "its", "main", "application", "we", "method", "take", "5-15", "seconds", "execute", "use", "unoptimized", "Matlab", "code", "bottleneck", "contrast", "adjustment", "step", "result", "all", "technique", "all", "48", "composite", "give", "supplemental", "material", "evaluate", "relative", "realism", "five", "technique", "we", "perform", "experiment", "use", "mechanical", "Turk", "simplify", "task", "we", "use", "forced", "choice", "test", "between", "two", "alternate", "version", "same", "composite", "where", "subject", "ask", "choose", "most", "realistic", "alternative", "-lrb-", "alternate", "methodology", "we", "consider", "collect", "individual", "realism", "rating", "each", "composite", "however", "those", "rating", "would", "more", "sensitive", "factor", "influence", "realism", "we", "study", "semantic", "likelihood", "depict", "scene", "-rrb-", "methodology", "we", "experiment", "describe", "appendix", "we", "collect", "average", "12.3", "human", "choice", "each", "480", "possible", "comparison", "first", "we", "use", "one-tailed", "t-test", "compare", "each", "method", "against", "each", "other", "method", "we", "show", "which", "method", "better", "than", "other", "significance", "level", "0.05", "Figure", "10", "we", "method", "outperform", "all", "other", "automatic", "method", "its", "performance", "significantly", "different", "than", "manual", "adjustment", "second", "we", "convert", "series", "paired", "comparison", "scale", "result", "place", "performance", "each", "method", "single", "scale", "scaling", "can", "perform", "individually", "each", "composite", "-lrb-", "figure", "11", "inset", "each", "row", "well", "supplemental", "material", "-rrb-", "average", "over", "all", "composite", "-lrb-", "figure", "12", "-rrb-", "we", "use", "Thurstone?s", "Law", "comparative", "judgment", "Case", "-lsb-", "David", "1988", "-rsb-", "which", "assume", "each", "method", "have", "single", "quality", "score", "observer", "estimate", "score", "normally", "distribute", "resultant", "scale", "value", "linear", "multiple", "score", "so", "difference", "between", "scale", "value", "unit", "standard", "deviation", "preference", "higher", "value", "better", "Figure", "12", "show", "we", "method", "perform", "slightly", "worse", "than", "manual", "adjustment", "much", "better", "than", "all", "other", "automatic", "method", "ColorComp", "significantly", "outperform", "color", "transfer", "however", "surprisingly", "do", "outperform", "cut-and-paste", "may", "because", "many", "we", "scene", "have", "natural", "lighting", "thus", "do", "require", "large", "adjustment", "appear", "natural", "scale", "individual", "composite", "-lrb-", "inset", "Figure", "11", "supplemental", "material", "-rrb-", "can", "diverge", "quite", "significantly", "from", "average", "example", "fifth", "row", "Figure", "11", "ColorComp", "perform", "best", "however", "fourth", "row", "show", "typical", "failure", "color", "transfer", "-lrb-", "which", "also", "component", "ColorComp", "-rrb-", "where", "green", "color", "forest", "unnaturally", "transfer", "person", "last", "row", "we", "method", "significantly", "outperform", "other", "automatic", "technique", "however", "also", "show", "limitation", "we", "method", "we", "can", "correct", "harsh", "lighting", "foreground", "still", "make", "composite", "unrealistic", "along", "example", "Figure", "11", "supplemental", "material", "show", "additional", "failure", "case", "several", "source", "error", "first", "we", "classifier", "sometimes", "choose", "wrong", "offset", "match", "second", "even", "minimal", "offset", "between", "foreground", "background", "real", "image", "exactly", "zero", "so", "perfect", "classification", "still", "yield", "perfect", "adjustment", "even", "most", "peaked", "distribution", "figure", "have", "tail", "so", "any", "algorithm", "use", "meanmatching", "have", "error", "Third", "we", "do", "use", "spatial", "proximity", "cue", "even", "though", "area", "background", "close", "foreground", "probably", "more", "relevant", "than", "area", "farther", "away", "example", "lighting", "can", "change", "respect", "depth", "scene", "proximity", "light", "source", "fourth", "we", "use", "hard", "threshold", "decide", "zone", "appropriate", "match", "during", "training", "phase", "near-threshold", "value", "can", "cause", "incorrect", "decision", "finally", "natural", "question", "whether", "we", "method", "could", "use", "main", "problem", "address", "Lalonde", "Efros", "-lsb-", "2007", "-rsb-", "predict", "whether", "choose", "foreground", "background", "appear", "realistic", "together", "unfortunately", "realism", "rating", "we", "collect", "section", "measure", "human", "response", "variation", "along", "single", "axis", "-lrb-", "e.g.", "luminance", "-rrb-", "unclear", "how", "combine", "simultaneous", "variation", "along", "multiple", "axis", "single", "realism", "prediction", "along", "example", "Figure", "11", "supplemental", "material", "show", "additional", "failure", "case", "several", "source", "error", "first", "we", "classifier", "sometimes", "choose", "wrong", "offset", "match", "second", "even", "minimal", "offset", "between", "foreground", "background", "real", "image", "exactly", "zero", "so", "perfect", "classification", "still", "yield", "perfect", "adjustment", "even", "most", "peaked", "distribution", "figure", "have", "tail", "so", "any", "algorithm", "use", "meanmatching", "have", "error", "Third", "we", "do", "use", "spatial", "proximity", "cue", "even", "though", "area", "background", "close", "foreground", "probably", "more", "relevant", "than", "area", "farther", "away", "example", "lighting", "can", "change", "respect", "depth", "scene", "proximity", "light", "source", "fourth", "we", "use", "hard", "threshold", "decide", "zone", "appropriate", "match", "during", "training", "phase", "near-threshold", "value", "can", "cause", "incorrect", "decision", "finally", "natural", "question", "whether", "we", "method", "could", "use", "main", "problem", "address", "Lalonde", "Efros", "-lsb-", "2007", "-rsb-", "predict", "whether", "choose", "foreground", "background", "appear", "realistic", "together", "unfortunately", "realism", "rating", "we", "collect", "section", "measure", "human", "response", "variation", "along", "single", "axis", "-lrb-", "e.g.", "luminance", "-rrb-", "unclear", "how", "combine", "simultaneous", "variation", "along", "multiple", "axis", "single", "realism", "prediction", "paper", "we", "study", "problem", "adjust", "composite", "appear", "realistic", "we", "automatic", "technique", "significantly", "outperform", "previous", "method", "biggest", "limitation", "we", "method", "we", "limit", "we", "scope", "standard", "2d", "image", "processing", "adjustment", "some", "composite", "need", "more", "specific", "complicated", "adjustment", "relighting", "truly", "appear", "realistic", "finally", "while", "we", "have", "identify", "image", "statistics", "correlate", "composite", "realism", "still", "much", "do", "truly", "understand", "factor", "influence", "human", "perception", "realism", "why", "statistics", "more", "correlate", "realism", "than", "other", "relationship", "causation", "correlation", "also", "we", "zone", "selection", "classifier", "black", "box", "how", "do", "determine", "which", "zone", "best", "match", "finally", "while", "we", "have", "evaluate", "we", "result", "human", "subject", "would", "expert", "compositor", "have", "different", "ranking", "method", "we", "explore", "question", "future", "work" ],
  "content" : "We show a number of compositing results adjusted using our automatic method, and compare them to other techniques. Table 3 shows one way to evaluate the success of our method on images that we already know are real; however, a more useful evaluation involves the adjustment of composites with regions taken from separate image sources. We therefore created a test set of 48 composites, and made an effort to ensure that each composite is semantically reasonable. We then created adjusted results using five techniques: simple cut-and-paste, a manually adjusted composite, Photoshop Match Color, the method of Lalonde and Efros [2007] (which we label ColorComp), and our method. Six examples are shown in Figure 11 . The manually-adjusted composite was created in Photoshop by one of the authors who has extensive Photoshop experience, and typically took 3-4 minutes to create. We computed the results of ColorComp using code and data provided by its authors, which typically took 3-5 minutes to execute per example. Photoshop Match Color uses a standard color transfer technique similar to Reinhard et al. [2004]; color transfer is useful for a number of creative tasks, and compositing is not necessarily its main application. Our method takes 5-15 seconds to execute using unoptimized Matlab code; the bottleneck is the contrast adjustment step. The results of all these techniques on all 48 composites are given in supplemental materials. To evaluate the relative realism of these five techniques, we performed an experiment using Mechanical Turk. To simplify the task, we used a forced choice test between two alternate versions of the same composite, where the subject was asked to choose the most realistic alternative. (An alternate methodology we considered was to collect individual realism ratings for each composite. However, those ratings would be more sensitive to factors that influence realism that we are not studying, such as the semantic likelihood of the depicted scene.) The methodology of our experiment is described in Appendix B; we collected, on average, 12.3 human choices for each of the 480 possible comparisons. First, we use one-tailed t-tests to compare each method against each other method. We show which methods are better than others with significance level p < 0.05 in Figure 10 . Our method outperforms all other automatic methods, and its performance is not significantly different than manual adjustment. Second, we convert the series of paired comparisons into scaling results that place the performance of each method on a single scale; this scaling can be performed individually for each composite ( Figure 11 , inset in each row, as well as supplemental materials), and averaged over all composites ( Figure 12 ). We use Thurstone?s Law of Comparative Judgments, Case V [David 1988], which assumes that each method has a single quality score, and observer estimates of this score are normally distributed. The resultant scale values are linear multiples of this score, so that differences between scale values are in the units of standard deviation of preference. Higher values are better. Figure 12 shows that our method performs slightly worse than manual adjustment, but much better than all other automatic methods. ColorComp significantly outperforms color transfer; however, surprisingly it does not outperform cut-and-paste. This may be because many of our scenes have natural lighting, and thus do not require large adjustments to appear natural. The scales for individual composites (the insets in Figure 11 , and supplemental materials) can diverge quite significantly from the average. For example, in the fifth row of Figure 11 , ColorComp performs best. However, the fourth row shows a typical failure of color transfer (which is also a component of ColorComp), where the green color of the forest is unnaturally transferred to the person. In the last row, our method significantly outperforms the other automatic techniques. However, it also shows a limitation of our method: we cannot correct the harsh lighting of the foreground that still makes this composite unrealistic. Along with the examples in Figure 11 , the supplemental materials show additional failure cases. There are several sources of error. First, our classifier sometimes chooses the wrong offset to match. Second, even the minimal offsets between foreground and background for real images are not exactly zero, so perfect classification will still not yield perfect adjustments. Even the most peaked distributions in Figure 2 have tails, so any algorithm that uses meanmatching will have errors. Third, we do not use spatial or proximity cues, even though areas of the background close the foreground are probably more relevant than areas farther away. For example, lighting can change with respect to depth in the scene or proximity to a light source. Fourth, we use hard thresholds to decide if a zone is appropriate for matching during the training phase; near-threshold values can cause incorrect decisions. Finally, a natural question is whether our method could be used for the main problem addressed by Lalonde and Efros [2007]: predicting whether a chosen foreground and background will appear realistic together. Unfortunately, the realism ratings we collect in Section 3 measure human response to variations along a single axis (e.g., luminance); it is unclear how to combine simultaneous variations along multiple axes into a single realism prediction. Along with the examples in Figure 11 , the supplemental materials show additional failure cases. There are several sources of error. First, our classifier sometimes chooses the wrong offset to match. Second, even the minimal offsets between foreground and background for real images are not exactly zero, so perfect classification will still not yield perfect adjustments. Even the most peaked distributions in Figure 2 have tails, so any algorithm that uses meanmatching will have errors. Third, we do not use spatial or proximity cues, even though areas of the background close the foreground are probably more relevant than areas farther away. For example, lighting can change with respect to depth in the scene or proximity to a light source. Fourth, we use hard thresholds to decide if a zone is appropriate for matching during the training phase; near-threshold values can cause incorrect decisions. Finally, a natural question is whether our method could be used for the main problem addressed by Lalonde and Efros [2007]: predicting whether a chosen foreground and background will appear realistic together. Unfortunately, the realism ratings we collect in Section 3 measure human response to variations along a single axis (e.g., luminance); it is unclear how to combine simultaneous variations along multiple axes into a single realism prediction. In this paper we studied the problem of adjusting a composite to appear realistic; our automatic technique significantly outperforms previous methods. The biggest limitation of our method is that we limit our scope to standard 2D image processing adjustments; some composites will need more specific or complicated adjustments such as relighting to truly appear realistic. Finally, while we have identified image statistics that are correlated with composite realism, there is still much to be done to truly understand the factors that influence human perception of realism. Why are these statistics more correlated with realism than others, and is the relationship causation or correlation? Also, our zone selection classifier is a black box; how does it determine which zone is best  for matching? Finally, while we have evaluated our results with human subjects, would expert compositors have different rankings of methods? We will explore these questions in future work.",
  "resources" : [ ]
}