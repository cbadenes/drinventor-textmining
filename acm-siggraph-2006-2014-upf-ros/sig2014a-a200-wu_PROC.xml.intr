{
  "uri" : "sig2014a-a200-wu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014a/a200-wu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Real-time Shading-based Refinement for Consumer Depth Cameras",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shahram-Izadi",
      "name" : "Shahram",
      "surname" : "Izadi"
    } ]
  },
  "bagOfWords" : [ "Consumer", "depth", "camera", "have", "open", "up", "many", "new", "real-time", "application", "field", "computer", "graphic", "vision", "robotic", "human-computer", "interaction", "include", "gestural", "interface", "live", "3d", "scanning", "augment", "reality", "robot", "navigation", "however", "noise", "resolution", "limitation", "even", "recent", "depth", "camera", "result", "only", "coarse", "geometry", "acquisition", "per", "frame", "ability", "capture", "higher", "fidelity", "geometry", "real-time", "could", "open", "up", "many", "new", "scenario", "track", "detailed", "feature", "user", "-lrb-", "e.g.", "facial", "expression", "clothing", "etc.", "-rrb-", "real-time", "performance", "capture", "other", "interactive", "scenario", "well", "ability", "scan", "higher", "quality", "3d", "model", "real-world", "object", "show", "previously", "input", "from", "stereo", "camera", "shape-fromshading", "-lrb-", "sf", "-rrb-", "can", "use", "capture", "detailed", "model", "result", "approach", "laser", "scan", "quality", "-lsb-", "Wu", "et", "al.", "2011", "Han", "et", "al.", "2013", "Yu", "et", "al.", "2013", "Beeler", "et", "al.", "2010", "-rsb-", "raise", "question", "can", "type", "shading-based", "refinement", "use", "improve", "depth", "camera", "datum", "only", "leverage", "additional", "RGB", "camera", "which", "most", "sensor", "typically", "provide", "unfortunately", "shading-based", "refinement", "technique", "require", "information", "about", "incident", "lighting", "surface", "material", "scene", "most", "case", "requirement", "fulfil", "make", "assumption", "about", "albedo", "work", "control", "lighting", "-lsb-", "hern?ndez", "et", "al.", "2008", "Fanello", "et", "al.", "2014", "-rsb-", "studio", "setup", "-lsb-", "Ghosh", "et", "al.", "2011", "Debevec", "2012", "Bermano", "et", "al.", "2014", "-rsb-", "when", "move", "general", "uncontrolled", "scene", "sf", "method", "thus", "need", "estimate", "albedo", "illumination", "along", "geometry", "solve", "complex", "inverse", "rendering", "problem", "-lsb-", "Wu", "et", "al.", "2011", "Wu", "et", "al.", "2013", "Han", "et", "al.", "2013", "Yu", "et", "al.", "2013", "-rsb-", "so", "far", "possible", "real", "time", "refinement", "technique", "have", "yet", "use", "interactively", "due", "performance", "bottleneck", "researcher", "have", "develop", "alternative", "heuristic", "fusion", "strategy", "enhance", "depth", "camera", "datum", "real", "time", "-lsb-", "Richardt", "et", "al.", "2012", "-rsb-", "many", "they", "use", "variant", "joint", "bilateral", "upsampling", "-lsb-", "Kopf", "et", "al.", "2007", "-rsb-", "lift", "depth", "datum", "pixel", "grid", "resolution", "concurrently", "acquire", "align", "RGB", "image", "while", "computation", "fast", "result", "base", "purely", "heuristic", "assumption", "about", "co-occurrence", "discontinuity", "RGB", "depth", "datum", "consequence", "reconstruction", "may", "look", "plausible", "estimated", "detail", "may", "metrically", "accurate", "further", "heuristic", "underpinning", "lead", "commonly", "know", "artifact", "texture", "copying", "where", "spatial", "albedo", "variation", "mistake", "geometric", "detail", "paper", "we", "propose", "new", "real-time", "method", "enhancement", "depth", "datum", "use", "sf", "general", "uncontrolled", "scene", "start", "from", "raw", "depth", "datum", "align", "RGB", "image", "algorithm", "estimate", "real", "time", "time-varying", "incident", "lighting", "distribution", "which", "use", "considerably", "enhance", "reconstruct", "geometric", "detail", "contrast", "previous", "fusion-based", "enhancement", "approach", "we", "reconstruction", "only", "plausible", "more", "metrically", "faithful", "avoid", "some", "texture-copy", "artifact", "see", "previously", "order", "refine", "depth", "map", "base", "shade", "real-time", "order", "magnitude", "faster", "than", "state-of-the-art", "offline", "system", "-lsb-", "Wu", "et", "al.", "2011", "-rsb-", "we", "must", "redesign", "shading-based", "energy", "function", "well", "its", "optimization", "method", "we", "rephrase", "shadingbased", "refinement", "problem", "fully", "exploit", "regular", "connectivity", "image", "grid", "instead", "use", "off-the-shelf", "conventional", "solver", "we", "introduce", "novel", "patch-based", "gauss-newton", "solver", "run", "GPU", "which", "specifically", "design", "we", "energy", "function", "careful", "design", "choice", "enable", "refinement", "depth", "map", "real-time", "make", "ideally", "suit", "modern", "commodity", "range", "sensor", "run", "30Hz", "specifically", "we", "algorithm", "provide", "follow", "contribution", "rephrase", "inverse", "render", "optimization", "problem", "use", "offline", "method", "-lsb-", "Wu", "et", "al.", "2011", "-rsb-", "highly", "parallelized", "manner", "enable", "real-time", "lighting", "estimation", "through", "spherical", "harmonic", "direct", "solve", "refine", "depth", "rather", "than", "displacement", "3d", "mesh", "space-time", "coherent", "estimation", "shape", "lighting", "use", "temporal", "correspondence", "derive", "from", "real-time", "alignment", "depth", "map", "adaptive", "shape", "refinement", "strategy", "reduce", "texture-copy", "artifact", "analyze", "approximate", "albedo", "image", "novel", "patch-based", "gauss-newton", "solver", "GPU", "compute", "metrically", "faithful", "geometry", "real-time", "frame-rate", "beyond", "technical", "contribution", "we", "show", "versatility", "we", "method", "reconstruct", "arbitrary", "scene", "even", "under", "motion", "demonstrate", "improve", "accuracy", "compare", "filter", "base", "refinement", "method", "we", "show", "integration", "real-time", "scanning", "framework", "akin", "kinectfusion", "-lsb-", "Newcombe", "et", "al.", "2011", "Izadi", "et", "al.", "2011", "Nie?ner", "et", "al.", "2013", "-rsb-", "show", "improved", "quality", "during", "realtime", "capture", "finally", "we", "demonstrate", "how", "we", "method", "enable", "improvement", "spatio-temporal", "reconstruction", "recent", "live", "non-rigid", "performance", "capture", "system", "-lsb-", "zollh?fer", "et", "al.", "2014a", "-rsb-", "Range", "image", "enhancement", "sensor", "fusion", "several", "method", "denoise", "enhance", "depth", "datum", "leverage", "higher", "pixel", "resolution", "one", "two", "concurrently", "capture", "rgb", "image", "most", "method", "rely", "heuristic", "assumption", "about", "correlation", "color", "depth", "e.g.", "edge", "both", "channel", "likely", "coincide", "Diebel", "Thrun", "-lsb-", "2006", "-rsb-", "compute", "upsampled", "depth", "use", "Markov-Random", "Field", "Park", "et", "al.", "-lsb-", "2011", "-rsb-", "formulate", "depth", "upsampling", "color", "image", "resolution", "optimization", "problem", "enforce", "discontinuity", "similarity", "mention", "earlier", "well", "additional", "regularization", "term", "implement", "above", "heuristic", "through", "filter", "also", "feasible", "-lsb-", "Lindner", "et", "al.", "2007", "-rsb-", "instance", "use", "joint", "bilateral", "upsampling", "-lsb-", "Kopf", "et", "al.", "2007", "-rsb-", "Yang", "et", "al", "-lsb-", "2007", "-rsb-", "create", "cost", "space", "from", "depth", "map", "filter", "joint-bilaterally", "use", "stereo", "image", "raise", "resolution", "similar", "idea", "have", "be", "explore", "joint", "reconstruction", "use", "stereo", "image", "depth", "datum", "where", "photometric", "constraint", "from", "stereo", "can", "exploit", "further", "datum", "refinement", "-lsb-", "Beder", "et", "al.", "2007", "Zhu", "et", "al.", "2008", "gudmundsson", "et", "al.", "2008", "-rsb-", "while", "above", "method", "run", "offline", "variant", "joint-bilateral", "multilateral", "filter", "depth", "upsampling", "can", "run", "real-time", "-lsb-", "Chan", "et", "al.", "2008", "Dolson", "et", "al.", "2010", "Richardt", "et", "al.", "2012", "-rsb-", "result", "however", "merely", "plausible", "metrically", "accurate", "texture-copy", "artifact", "frequently", "occur", "when", "texture", "variation", "mistake", "geometric", "detail", "multi-frame", "superresolution", "technique", "estimate", "higher", "resolution", "depth", "image", "from", "stack", "align", "low", "resolution", "image", "capture", "under", "slight", "lateral", "displacement", "-lsb-", "Cui", "et", "al.", "2013", "-rsb-", "real-time", "computation", "have", "be", "possible", "so", "far", "one", "final", "set", "method", "increase", "resolution", "single", "depth", "image", "offline", "use", "learn", "database", "local", "patch", "-lsb-", "Aodha", "et", "al.", "2012", "-rsb-", "shape-from-shade", "Photometric", "Stereo", "related", "topic", "acquire", "3d", "shape", "object", "use", "shape-from-shading", "-lrb-", "sf", "-rrb-", "where", "naturally", "occur", "intensity", "pattern", "across", "image", "use", "extract", "3d", "geometry", "from", "single", "image", "-lsb-", "Horn", "1975", "Zhang", "et", "al.", "1999", "-rsb-", "mathematics", "sf", "well-understood", "particularly", "when", "surface", "reflectance", "light", "source", "position", "know", "Prados", "Faugeras", "-lsb-", "2005", "-rsb-", "Fanello", "et", "al.", "-lsb-", "2014", "-rsb-", "reconstruct", "various", "object", "include", "face", "use", "controlled", "light", "source", "near", "camera", "center", "Ahmed", "Farag", "-lsb-", "2007", "-rsb-", "demonstrate", "geometry", "estimation", "non-lambertian", "surface", "vary", "illumination", "condition", "make", "strong", "scene", "assumption", "B?hme", "et", "al.", "-lsb-", "2008", "-rsb-", "use", "near", "infrared", "image", "available", "time-of-flight", "-lrb-", "tof", "-rrb-", "camera", "relate", "depth", "intensity", "filter", "however", "unlike", "we", "method", "approach", "limit", "only", "tof", "camera", "collocation", "light", "source", "camera", "run", "offline", "do", "increase", "x/y", "resolution", "image", "recent", "method", "have", "show", "sf", "can", "refine", "coarse", "image-based", "shape", "model", "-lsb-", "Beeler", "et", "al.", "2012", "-rsb-", "even", "be", "capture", "under", "general", "uncontrolled", "lighting", "several", "camera", "-lsb-", "Wu", "et", "al.", "2011", "Wu", "et", "al.", "2013", "-rsb-", "rgb-d", "camera", "-lsb-", "Han", "et", "al.", "2013", "Yu", "et", "al.", "2013", "-rsb-", "end", "illumination", "albedo", "distribution", "well", "refine", "geometry", "find", "via", "inverse", "render", "optimization", "while", "physics", "sf", "well", "know", "problem", "inherently", "ill-posed", "achieve", "compelling", "result", "require", "strong", "scene", "lighting", "assumption", "computationally", "complex", "algorithm", "particularly", "solve", "hard", "inverse", "rendering", "optimization", "real-time", "performance", "have", "rarely", "be", "demonstrate", "have", "lead", "work", "photometric", "stereo", "where", "multiple", "image", "scene", "capture", "under", "different", "controlled", "illumination", "compute", "geometry", "photometric", "stereo", "have", "demonstrate", "compelling", "reconstruction", "surface", "complex", "reflectance", "property", "-lsb-", "Mulligan", "Brolly", "2004", "Hern?ndez", "et", "al.", "2008", "Ghosh", "et", "al.", "2011", "Tunwattanapong", "et", "al.", "2013", "Debevec", "2012", "Bermano", "et", "al.", "2014", "Nehab", "et", "al.", "2005", "-rsb-", "however", "approach", "require", "complex", "control", "lighting", "setup", "which", "available", "many", "standard", "scenario", "more", "data-driven", "approach", "solve", "sf", "problem", "have", "also", "be", "propose", "Barron", "Malik", "-lsb-", "2013b", "-rsb-", "jointly", "solve", "reflectance", "shape", "illumination", "base", "prior", "derive", "statistically", "from", "image", "similar", "concept", "be", "also", "use", "offline", "intrinsic", "image", "decomposition", "rgb-d", "datum", "-lsb-", "Barron", "Malik", "2013a", "-rsb-", "zollh?fer", "et", "al.", "-lsb-", "2014b", "-rsb-", "use", "sf", "fit", "morphable", "face", "model", "rgb", "input", "stream", "we", "approach", "do", "impose", "strong", "prior", "shape", "recovery", "Khan", "et", "al.", "-lsb-", "2009", "-rsb-", "learn", "weighting", "parameter", "complex", "sf", "model", "aid", "facial", "reconstruction", "Wei", "Hirzinger", "-lsb-", "1996", "-rsb-", "use", "deep", "neural", "network", "learn", "aspect", "physical", "model", "sf", "demonstrate", "moderate", "result", "very", "constrain", "scene", "again", "none", "approach", "achieve", "real-time", "performance" ],
  "content" : "Consumer depth cameras have opened up many new real-time applications in the field of computer graphics and vision, robotics and human-computer interaction; including gestural interfaces, live 3D scanning, augmented reality, and robot navigation. However, the noise and resolution limitations of even recent depth cameras, result in only coarse geometry acquisition per frame. The ability to capture higher fidelity geometry in real-time could open up many new scenarios, such as tracking detailed features of the user (e.g., facial expressions, clothing etc.) for real-time performance capture or other interactive scenarios, as well as the ability to scan higher quality 3D models of real-world objects. As shown previously, input from a stereo camera and shape-fromshading (SfS) can be used to capture detailed models with results approaching laser scan quality [Wu et al. 2011; Han et al. 2013; Yu et al. 2013; Beeler et al. 2010]. This raises the question: can this type of shading-based refinement be used to improve depth camera data, only by leveraging an additional RGB camera, which most sensors typically provide. Unfortunately, shading-based refinement techniques require information about the incident lighting and surface material in the scene. In most cases this requirement is fulfilled by making assumptions about albedo, and by working with controlled lighting [Hern?ndez et al. 2008; Fanello et al. 2014], and studio setups [Ghosh et al. 2011; Debevec 2012; Bermano et al. 2014]. When moving to general uncontrolled scenes, SfS methods thus need to estimate albedo and illumination along with the geometry by solving a complex inverse rendering problem [Wu et al. 2011; Wu et al. 2013; Han et al. 2013; Yu et al. 2013]. So far, this was not possible in real time, and as such refinement techniques have yet to be used interactively. Due to this performance bottleneck, researchers have developed alternative heuristic fusion strategies to enhance depth camera data in real time [Richardt et al. 2012]. Many of them use variants of joint bilateral upsampling [Kopf et al. 2007] to lift the depth data to the pixel grid resolution of a concurrently acquired and aligned RGB image. While computation is fast, the results are based on a purely heuristic assumption about the co-occurrence of discontinuities in RGB and depth data. In consequence, reconstructions may look plausible but estimated detail may not be metrically accurate. Further, the heuristic underpinning leads to commonly known artifacts, such as texture copying, where spatial albedo variations are mistaken for geometric detail. In this paper, we propose a new real-time method for enhancement of depth data using SfS in general uncontrolled scenes. Starting from the raw depth data and an aligned RGB image, the algorithm estimates ? in real time ? the time-varying incident lighting distribution, which is then used to considerably enhance the reconstructed geometric detail. In contrast to previous fusion-based enhancement approaches, our reconstructions are not only plausible but more metrically faithful, and avoid some of the texture-copy artifacts seen previously. In order to refine a depth map based on the shading in real-time, orders of magnitude faster than state-of-the-art offline systems [Wu et al. 2011], we must redesign the shading-based energy function as well as its optimization method. As such, we rephrase the shadingbased refinement problem to fully exploit the regular connectivity of image grids. Instead of using an off-the-shelf conventional solver, we introduce a novel patch-based Gauss-Newton solver running on the GPU, which is specifically designed for our energy function. This careful design choice enables the refinement of depth maps in real-time, making it ideally suited to modern commodity range sensors that run at ? 30Hz. Specifically, our algorithm provides the following contributions: ? rephrasing the inverse rendering optimization problems used in offline methods [Wu et al. 2011] in a highly parallelized manner to enable real-time lighting estimation through spherical harmonics, and direct solving for refined depth rather than displacements on 3D meshes. ? space-time coherent estimation of shape and lighting using temporal correspondences derived from a real-time alignment of depth maps. ? an adaptive shape refinement strategy that reduces texture-copy artifacts by analyzing an approximate albedo image. ? a novel patch-based Gauss-Newton solver on the GPU to compute metrically faithful geometry at real-time frame-rates. Beyond these technical contributions, we show the versatility of our method for reconstructing arbitrary scenes, even under motion, and demonstrate improved accuracy compared to filtering based refinement methods. We show integration into a real-time scanning framework akin to KinectFusion [Newcombe et al. 2011; Izadi et al. 2011; Nie?ner et al. 2013], and show improved quality during realtime capture. Finally, we demonstrate how our method enables improvement of the spatio-temporal reconstructions of a recent live non-rigid performance capture system [Zollh?fer et al. 2014a]. Range Image Enhancement and Sensor Fusion Several methods to denoise and enhance depth data leverage the higher pixel resolution of one or two concurrently captured RGB images. Most of these methods rely on heuristic assumptions about the correlation of color and depth, e.g., that edges in both channels likely coincide. Diebel and Thrun [2006] compute the upsampled depth using a Markov-Random Field. Park et al. [2011] formulate depth upsampling to color image resolution as an optimization problem enforcing the discontinuity similarity mentioned earlier, as well as additional regularization terms. Implementing the above heuristics through filtering is also feasible [Lindner et al. 2007], for instance by using joint bilateral upsampling [Kopf et al. 2007]. Yang et al [2007] create  a cost space from the depth map, and filter it joint-bilaterally using a stereo image to raise resolution. Similar ideas have been explored for joint reconstruction using stereo images and depth data, where photometric constraints from stereo can be exploited for further data refinement [Beder et al. 2007; Zhu et al. 2008; Gudmundsson et al. 2008]. While the above methods run offline, variants of joint-bilateral or multilateral filtering for depth upsampling can run in real-time [Chan et al. 2008; Dolson et al. 2010; Richardt et al. 2012]. Their results, however, are merely plausible and not metrically accurate, and texture-copy artifacts frequently occur when texture variations are mistaken for geometric detail. Multi-frame superresolution techniques estimate higher resolution depth images from a stack of aligned low resolution images captured under slight lateral displacement [Cui et al. 2013], but real-time computation has not been possible so far. One final set of methods increases the resolution of a single depth image offline using a learned database of local patches [Aodha et al. 2012]. Shape-from-Shading and Photometric Stereo A related topic acquires the 3D shape of an object using shape-from-shading (SfS) where the naturally occurring intensity patterns across an image are used to extract the 3D geometry from a single image [Horn 1975; Zhang et al. 1999]. The mathematics of SfS is well-understood, particularly when surface reflectance and light source positions are known. Prados and Faugeras [2005] and Fanello et al. [2014] reconstruct various objects including faces, using controlled light sources near the camera center. Ahmed and Farag [2007] demonstrate geometry estimation for non-Lambertian surfaces and varying illumination conditions, but make strong scene assumptions. B?hme et al. [2008] use the near infrared image available on time-of-flight (ToF) cameras to relate depth to intensity for filtering. However, unlike our method, their approach is limited to only ToF cameras with collocation of light source and camera, runs offline, and does not increase the X/Y resolution of images. Recent methods have shown that SfS can refine coarse image-based shape models [Beeler et al. 2012], even if they were captured under general uncontrolled lighting with several cameras [Wu et al. 2011; Wu et al. 2013] or an RGB-D camera [Han et al. 2013; Yu et al. 2013]. To this end, illumination and albedo distributions, as well as refined geometry are found via inverse rendering optimization. While the physics of SfS is well known, the problem is inherently ill-posed, and achieving compelling results requires strong scene and lighting assumptions, and computationally complex algorithms, particularly to solve hard inverse rendering optimizations. As such, real-time performance has rarely been demonstrated. This has led to work on photometric stereo where multiple images of a scene are captured under different controlled illumination to compute geometry. Photometric stereo has demonstrated compelling reconstructions of surfaces with complex reflectance properties [Mulligan and Brolly 2004; Hern?ndez et al. 2008; Ghosh et al. 2011; Tunwattanapong et al. 2013; Debevec 2012; Bermano et al. 2014; Nehab et al. 2005]. However, these approaches require complex controlled lighting setups, which are not available in many standard scenarios. More data-driven approaches for solving the SfS problem have also been proposed. Barron and Malik [2013b] jointly solve for reflectance, shape and illumination, based on priors derived statistically from images. Similar concepts were also used for offline intrinsic image decomposition of RGB-D data [Barron and Malik 2013a]. Zollh?fer et al. [2014b] use SfS to fit a morphable face model to an RGB input stream. Our approach does not impose strong priors on shape recovery. Khan et al. [2009] learn weighting parameters for complex SfS models to aid facial reconstruction. Wei and Hirzinger [1996] use deep neural networks to learn aspects of the physical model for SfS, demonstrating moderate results for very constrained scenes. Again, none of these approaches achieves real-time performance.",
  "resources" : [ ]
}