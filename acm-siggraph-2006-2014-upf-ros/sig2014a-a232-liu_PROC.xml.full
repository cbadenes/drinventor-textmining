{
  "uri" : "sig2014a-a232-liu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014a/a232-liu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Fast Burst Images Denoising",
    "published" : null,
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ ]
  },
  "bagOfWords" : [ "8249c003658bcd50fea1e40787fba9e6d2ccc7f561f7f643077ded1796d84f13", "p7b", "10.1145", "2661229.2661277", "name", "identification", "possible", "fast", "Burst", "Images", "Denoising", "Ziwei", "Liu", "Lu", "Yuan", "Xiaoou", "Tang", "Matt", "Uyttendaele", "Jian", "Sun", "Chinese", "University", "Hong", "Kong", "Microsoft", "Research", "Microsoft", "Research", "Technologies", "Figure", "top-left", "burst", "noisy", "image", "-lrb-", "10", "frame", "image", "size", "3072", "1728", "-rrb-", "smartphone", "bottom-left", "run", "time", "-lrb-", "sec", "-rrb-", "different", "denoising", "method", "right", "comparison", "two", "close-up", "view", "-lrb-", "-rrb-", "input", "image", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lsb-", "Bennett", "McMillan", "2005", "-rsb-", "-lrb-", "-rrb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-lrb-", "-rrb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "median", "-lrb-", "-rrb-", "lucky", "imaging", "-lsb-", "Joshi", "Cohen", "2010", "-rsb-", "-lrb-", "-rrb-", "we", "method", "we", "method", "produce", "clean", "ghost-free", "image", "fine", "detail", "more", "importantly", "we", "method", "significantly", "faster", "than", "other", "method", "paper", "present", "fast", "denoising", "method", "produce", "clean", "image", "from", "burst", "noisy", "image", "we", "accelerate", "alignment", "image", "introduce", "lightweight", "camera", "motion", "representation", "call", "homography", "flow", "align", "image", "fuse", "create", "denoised", "output", "rapid", "per-pixel", "operation", "temporal", "spatial", "domain", "handle", "scene", "motion", "during", "capture", "mechanism", "select", "consistent", "pixel", "temporal", "fusion", "propose", "synthesize", "clean", "ghost-free", "image", "which", "can", "largely", "reduce", "computation", "track", "motion", "between", "frame", "combine", "efficient", "solution", "we", "method", "run", "several", "order", "magnitude", "faster", "than", "previous", "work", "while", "denoising", "quality", "comparable", "smartphone", "prototype", "demonstrate", "we", "method", "practical", "work", "well", "large", "variety", "real", "example", "cr", "category", "i.", "4.3", "-lsb-", "image", "processing", "computer", "Vision", "-rsb-", "enhancement?smoothing", "keyword", "denoising", "burst", "image", "homography", "flow", "ghost-free", "Links", "dl", "pdf", "work", "do", "when", "Ziwei", "intern", "msr", "Asia", "ACM", "Reference", "Format", "Liu", "Z.", "Yuan", "L.", "Tang", "X.", "Uyttendaele", "M.", "Sun", "J.", "2014", "fast", "Burst", "Images", "Denoising", "ACM", "Trans", "graph", "33", "Article", "232", "-lrb-", "November", "2014", "-rrb-", "page", "dous", "10.1145", "2661229.2661277", "http://doi.acm.org/10.1145/2661229.2661277", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "copyright", "ACM", "0730-0301/14", "11-art232", "15.00", "DOI", "http://doi.acm.org/10.1145/2661229.2661277", "introduction", "burst", "shooting", "mode", "most", "camera", "allow", "multiple", "shot", "capture", "quick", "succession", "either", "press", "hold", "shutter", "button", "design", "allow", "selection", "best", "shot", "record", "motion", "recently", "burst", "capture", "have", "become", "ubiquitous", "many", "hand-held", "imaging", "device", "-lrb-", "e.g.", "smartphone", "compact", "dslr", "camera", "-rrb-", "example", "iPhone", "5", "support", "burst", "up", "10", "shot", "per", "second", "burst", "mode", "have", "be", "successfully", "exploit", "computation", "photography", "reduce", "blur", "-lsb-", "Cai", "et", "al.", "2009", "-rsb-", "improve", "shadow/highlight", "detail", "-lsb-", "Reinhard", "et", "al.", "2010", "-rsb-", "increase", "resolution", "-lsb-", "Farsiu", "et", "al.", "2004", "-rsb-", "clarity", "-lsb-", "Joshi", "Cohen", "2010", "-rsb-", "depth", "field", "-lsb-", "Jacobs", "et", "al.", "2012", "-rsb-", "paper", "we", "present", "practical", "solution", "burst", "image", "denoise", "turn", "burst", "noisy", "image", "-lrb-", "typically", "capture", "low-light", "condition", "-rrb-", "single", "clean", "image", "show", "Figure", "problem", "new", "have", "be", "study", "context", "multiple", "images/video", "denoising", "-lsb-", "buade", "et", "al.", "2010", "Liu", "Freeman", "2010", "Zhang", "et", "al.", "2009", "-rsb-", "we", "focus", "practicality", "we", "goal", "design", "highly", "efficient", "method", "while", "produce", "high-quality", "result", "so", "algorithm", "can", "run", "mobile", "device", "limited", "computational", "resource", "practical", "approach", "need", "tackle", "two", "challenge", "First", "efficiency", "state-of-the-art", "method", "heavily", "rely", "optical", "flow", "patch", "matching", "establish", "temporal", "spatial", "correspondence", "which", "unacceptably", "slow", "Second", "quality", "fast", "method", "like", "average", "filter", "-lsb-", "Tomasi", "Manduchi", "1998", "-rsb-", "insufficient", "both", "noise", "reduction", "avoid", "ghost", "artifact", "cause", "either", "camera", "motion", "-lrb-", "hand", "shake", "-rrb-", "scene", "motion", "-lrb-", "dynamic", "object", "-rrb-", "moreover", "even", "some", "complicated", "method", "also", "fragile", "presence", "strong", "noise", "complex", "dynamic", "motion", "we", "propose", "fast", "noise", "reduction", "method", "produce", "clean", "image", "from", "burst", "image", "high", "speed", "we", "method", "enable", "introduce", "three", "accelerate", "step", "first", "step", "we", "use", "lightweight", "parametric", "motion", "representation", "homog", "raphy", "flow", "model", "motion", "cause", "camera", "movement", "representation", "inspire", "recent", "multiple", "homography", "model", "-lsb-", "Grundmann", "et", "al.", "2012", "Liu", "et", "al.", "2013", "-rsb-", "video", "stabilization", "since", "estimate", "homography", "flow", "only", "require", "spare", "feature", "matching", "step", "both", "efficient", "robust", "noise", "second", "step", "we", "handle", "scene", "motion", "identify", "consistent", "pixel", "-lrb-", "i.e.", "pixel", "similar", "color", "-rrb-", "along", "temporal", "axis", "from", "all", "align", "image", "-lrb-", "first", "step", "-rrb-", "per", "pixel", "location", "select", "consistent", "pixel", "use", "we", "temporal", "pixel", "fusion", "-lrb-", "third", "step", "-rrb-", "average", "thus", "we", "can", "generate", "ghostfree", "result", "while", "avoid", "complex", "motion", "tracking", "dynamic", "object", "which", "too", "slow", "too", "difficult", "idea", "successfully", "apply", "recent", "hdr", "deghosting", "-lsb-", "Granados", "et", "al.", "2013", "-rsb-", "we", "extend", "idea", "find", "many", "consistent", "pixel", "possible", "every", "pixel", "location", "purpose", "better", "denoising", "third", "step", "we", "apply", "temporal", "multiscale", "pixel", "fusion", "succession", "obtain", "denoised", "result", "temporal", "fusion", "base", "simple", "optimal", "linear", "estimator", "multiscale", "fusion", "complementary", "temporal", "fusion", "further", "enable", "significant", "denoising", "meanwhile", "whole", "step", "also", "very", "efficient", "design", "because", "only", "involve", "pixel-wise", "operation", "we", "have", "evaluate", "we", "algorithm", "variety", "real", "datum", "presence", "moderate", "strong", "noise", "we", "algorithm", "perform", "par", "state-of-the-art", "multi-image", "denoising", "method", "-lrb-", "e.g.", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-rrb-", "furthermore", "we", "algorithm", "two", "three", "order", "magnitude", "faster", "Figure", "show", "comparison", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "232:2", "Z.", "Liu", "et", "al.", "related", "work", "single", "image", "denoising", "have", "great", "progress", "recent", "decade", "Representative", "method", "include", "bilateral", "filter", "-lsb-", "Tomasi", "Manduchi", "1998", "-rsb-", "wavelet", "-lrb-", "gsm", "-rrb-", "-lsb-", "Portilla", "et", "al.", "2003", "-rsb-", "field-ofexpert", "-lsb-", "Roth", "Black", "2005", "-rsb-", "non-local", "means", "-lsb-", "buade", "et", "al.", "2005", "-rsb-", "bm3d", "-lsb-", "Dabov", "et", "al.", "2007b", "-rsb-", "so", "improve", "efficiency", "few", "fast", "variant", "have", "be", "propose", "fast", "bilateral", "filter", "-lsb-", "Paris", "Durand", "2009", "-rsb-", "gaussian", "kd-tree", "-lsb-", "Adams", "et", "al.", "2009", "-rsb-", "geodesic", "path", "-lsb-", "Chen", "et", "al.", "2013", "-rsb-", "most", "recently", "Levin", "et", "al.", "-lsb-", "2011", "-rsb-", "point", "out", "single", "image", "denoising", "may", "approach", "its", "performance", "limit", "noisebrush", "-lsb-", "Chen", "et", "al.", "2009", "-rsb-", "provide", "interactive", "way", "further", "quality", "improvement", "multiple", "image", "denoising", "superior", "single", "image", "denoising", "because", "its", "use", "more", "information", "some", "denoise", "technique", "have", "be", "successfully", "use", "burst", "image", "-lsb-", "Tico", "2008", "Buades", "et", "al.", "2009", "Joshi", "Cohen", "2010", "-rsb-", "video", "-lsb-", "Bennett", "McMillan", "2005", "Liu", "Freeman", "2010", "Dabov", "et", "al.", "2007a", "Chen", "Tang", "2007", "-rsb-", "multiple-view", "image", "-lsb-", "Zhang", "et", "al.", "2009", "-rsb-", "volumetric", "MRI", "datum", "-lsb-", "maggionus", "et", "al.", "2013", "-rsb-", "estimate", "camera", "motion", "Optical", "flow", "-lsb-", "Brox", "et", "al.", "2004", "-rsb-", "most", "general", "representation", "establish", "correspondence", "between", "frame", "recent", "work", "-lsb-", "Liu", "Freeman", "2010", "-rsb-", "show", "its", "importance", "video", "denoising", "optical", "flow", "itself", "have", "difficulty", "occlusion/large", "displacement", "fragile", "noise", "Patch", "matching", "more", "robust", "noise", "have", "be", "widely", "use", "multiple", "image", "processing", "-lsb-", "Tico", "2008", "Buades", "et", "al.", "2009", "Zhang", "et", "al.", "2009", "Maggioni", "et", "al.", "2013", "Sen", "et", "al.", "2012", "Kalantari", "et", "al.", "2013", "-rsb-", "however", "presence", "strong", "noise", "both", "nonparametric", "method", "degrade", "rapidly", "camera", "motion", "burst", "mode", "similar", "motion", "study", "video", "stabilization", "recent", "work", "-lsb-", "Grundmann", "et", "al.", "2012", "Liu", "et", "al.", "2013", "-rsb-", "demonstrate", "success", "use", "spatially-variant", "homography", "camera", "motion", "work", "we", "use", "similar", "more", "lightweight", "parametric", "motion", "representation", "homography", "flow", "handle", "scene", "motion", "since", "optical", "flow", "patch", "matching", "intrinsically", "hard", "problem", "recent", "work", "-lsb-", "Gallo", "et", "al.", "2009", "Granados", "et", "al.", "2013", "-rsb-", "hdr", "reconstruction", "bypass", "motion", "estimation", "find", "consistent", "subset", "color", "every", "pixel", "reconstruct", "ghost-free", "image", "Granados", "et", "al.", "-lsb-", "2013", "-rsb-", "consistency", "test", "rely", "accurate", "estimation", "noise", "distribution", "which", "may", "require", "complex", "calibration", "super-pixels", "computation", "we", "be", "inspire", "idea", "extend", "image", "denoising", "multiscale", "denoising", "effective", "way", "exploit", "cross-scale", "similarity", "noise", "reduction", "recently", "Zontak", "et", "al.", "-lsb-", "2013", "-rsb-", "propose", "directional", "pyramid", "technique", "find", "corresponding", "patch", "across", "scale", "which", "produce", "state-of-the-art", "result", "Zhang", "Gunturk", "-lsb-", "2008", "-rsb-", "extend", "bilateral", "filter", "multiscale", "framework", "we", "use", "pyramid-based", "pixel", "fusion", "method", "improve", "result", "quality", "figure", "Algorithm", "pipeline", "Figure", "-lrb-", "-rrb-", "pyramid", "homography", "graph", "-lrb-", "-rrb-", "homography", "-lrb-", "finest", "level", "-rrb-", "discretize", "obtain", "per-pixel", "homography", "flow", "field", "algorithm", "figure", "we", "algorithm", "pipeline", "we", "first", "build", "gaussian", "pyramid", "all", "noisy", "image", "set", "midmost", "frame", "reference", "frame", "default", "we", "estimate", "homography", "flow", "-lrb-", "section", "3.1", "-rrb-", "represent", "motion", "-lrb-", "camera", "-rrb-", "between", "reference", "frame", "any", "other", "frame", "across", "align", "image", "-lrb-", "homography", "flow", "-rrb-", "every", "pixel", "location", "we", "select", "set", "consistent", "pixel", "handle", "scene", "motion", "-lrb-", "section", "3.2", "-rrb-", "possible", "small", "misalignment", "cause", "homography", "flow", "finally", "we", "apply", "pixel", "fusion", "-lrb-", "section", "3.3", "-rrb-", "aggregate", "consistent", "pixel", "all", "scale", "produce", "final", "result", "3.1", "homography", "flow", "-lrb-", "Camera", "Motion", "-rrb-", "pyramid", "homography", "graph", "we", "represent", "motion", "between", "two", "frame", "through", "pyramid", "homography", "graph", "show", "Figure", "-lrb-", "-rrb-", "coarse", "level", "node", "provide", "robustness", "while", "fine", "level", "node", "help", "produce", "detail", "note", "independently", "estimate", "homography", "each", "node", "unreliable", "because", "some", "node", "may", "have", "insufficient", "match", "feature", "next", "we", "introduce", "coarse-to-fine", "optimization", "robustly", "obtain", "accurate", "result", "long", "side", "image", "coarsest", "scale", "more", "than", "400", "pixel", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "fast", "Burst", "Images", "Denoising", "232:3", "-lrb-", "-rrb-", "Optical", "flow", "global", "homography", "Optical", "flow", "Patch", "Match", "Global", "Homography", "Patch", "Match", "-lrb-", "-rrb-", "global", "homography", "homography", "flow", "db", "31", "29", "27", "25", "psnr", "23", "21", "19", "17", "20", "30", "40", "50", "60", "Noise", "Sigma", "100.00", "-lrb-", "sec", "-rrb-", "10.00", "Patch", "Optical", "Match", "flow", "-lrb-", "exhaustive", "-rrb-", "Time", "1.00", "0.2", "Patch", "Match", "-lrb-", "randomize", "-rrb-", "run", "0.10", "0.01", "homography", "global", "homography", "flow", "image", "size", "-lrb-", "mega-pixel", "-rrb-", "figure", "comparison", "registration", "error", "time", "cost", "various", "motion", "model", "-lrb-", "-rrb-", "sample", "image", "-lrb-", "-rrb-", "registration", "error", "measure", "psnr", "which", "compute", "register", "clean", "image", "motion", "estimate", "from", "noisy", "image", "-lrb-", "-rrb-", "run", "time", "-lrb-", "sec", "-rrb-", "different", "method", "we", "use", "randomize", "patch", "match", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "source", "code", "acceleration", "still", "order", "slower", "than", "homography", "flow", "optimization", "we", "perform", "level-by-level", "optimization", "start", "from", "global", "homography", "coarsest", "level", "-lrb-", "-rrb-", "let", "homography", "node", "level", "-lcb-", "-rcb-", "its", "neighbor", "homography", "same", "level", "we", "estimate", "-lcb-", "-rcb-", "minimize", "follow", "energy", "function", "-lcb-", "-rcb-", "arg", "min", "-lcb-", "-rcb-", "-lrb-", "-rrb-", "where", "-lrb-", "default", "0.1", "-rrb-", "control", "amount", "spatial", "regularization", "enforce", "second", "smoothness", "term", "first", "datum", "term", "best", "-lrb-", "-rrb-", "select", "from", "two", "candidate", "one", "its", "parent", "homography", "level", "other", "its", "own", "estimate", "homography", "-lrb-", "use", "all", "match", "feature", "within", "grid", "node", "-rrb-", "basic", "idea", "use", "backup", "when", "we", "can", "reliably", "compute", "we", "implementation", "we", "pick", "feature", "number", "grid", "insufficient", "-lrb-", "-rrb-", "rigidness", "-lsb-", "Hartley", "Zisserman", "2003", "-rsb-", "too", "weak", "otherwise", "we", "choose", "best", "model", "which", "have", "lower", "matching", "error", "all", "feature", "within", "grid", "because", "objective", "function", "-lrb-", "-rrb-", "quadratic", "we", "can", "obtain", "global", "optimum", "Jacobi", "solver", "-lsb-", "bronshtein", "Semendyayev", "1997", "-rsb-", "form", "we", "motion", "model", "similar", "mesh-based", "homography", "-lsb-", "Liu", "et", "al.", "2013", "-rsb-", "we", "coarse-to-fine", "optimization", "more", "efficient", "500", "feature", "we", "method", "take", "m", "per", "frame", "while", "mesh-based", "homography", "require", "50", "m", "per", "frame", "homography", "flow", "since", "pyramid", "homography", "graph", "parametric", "representation", "we", "require", "image", "warping", "coordinate", "transformation", "later", "denoising", "step", "operation", "all", "pixel", "-lrb-", "all", "frame", "all", "scale", "-rrb-", "very", "expensive", "address", "critical", "issue", "we", "application", "we", "discretize", "homography", "shear", "homography", "1.25", "modulus", "perspective", "0.1", "parameter", "empirically", "set", "fix", "all", "experiment", "graph", "-lrb-", "finest", "level", "only", "-rrb-", "obtain", "per-pixel", "translation", "vector", "homography", "flow", "show", "Figure", "-lrb-", "-rrb-", "we", "compute", "translation", "vector", "map", "each", "pixel", "from", "one", "frame", "another", "frame", "accord", "homography", "graph", "finally", "estimate", "homography", "flow", "scale", "accordingly", "round", "off", "use", "other", "scale", "algorithm", "validation", "we", "evaluate", "global", "homography", "optical", "flow", "patch", "match", "we", "homography", "flow", "set", "burst", "image", "we", "ask", "four", "different", "subject", "capture", "20", "set", "clean", "burst", "image", "-lrb-", "low", "iso", "under", "good", "lighting", "condition", "-rrb-", "we", "add", "gaussian", "noise", "different", "standard", "deviation", "-lrb-", "from", "20", "60", "-rrb-", "synthesize", "100", "set", "noisy", "burst", "image", "register", "noisy", "image", "we", "compare", "six", "algorithm", "global", "homography", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "global", "homography", "optical", "flow", "patch", "match", "-lrb-", "exhaustive", "search", "-rrb-", "global", "homography", "patch", "match", "we", "homography", "flow", "we", "compute", "psnr", "measure", "difference", "between", "registered", "clean", "image", "pair", "Figure", "-lrb-", "-rrb-", "show", "result", "from", "result", "we", "can", "observe", "two", "optical", "flow", "base", "method", "perform", "well", "when", "noise", "level", "small", "-lrb-", "20", "-rrb-", "when", "noise", "level", "increase", "degrade", "more", "quickly", "than", "other", "global", "homography", "can", "improve", "patch", "match", "optical", "flow", "we", "believe", "reason", "coarse-to-fine", "mechanism", "optical", "flow", "have", "already", "handle", "global", "motion", "we", "homography", "flow", "consistently", "best", "all", "noise", "level", "Figure", "-lrb-", "-rrb-", "further", "show", "run", "time", "method", "various", "image", "size", "since", "global", "homography", "we", "homography", "flow", "only", "rely", "sparse", "feature", "detection", "matching", "both", "outperform", "patch", "match", "-lrb-", "exhausive", "search", "even", "randomize", "search", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "-rrb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "speed", "only", "small", "margin", "between", "two", "run", "time", "curve", "-lrb-", "global", "homography", "we", "homography", "flow", "-rrb-", "which", "indicate", "we", "pyramid", "optimization", "very", "efficient", "efficient", "implementation", "bottleneck", "step", "sparse", "feature", "extraction", "matching", "we", "implementation", "we", "work", "coarse", "scale", "-lrb-", "-rrb-", "pyramid", "luminance", "channel", "efficiency", "robustness", "-lrb-", "noise", "-rrb-", "compare", "originalscale", "implementation", "time", "cost", "greatly", "reduce", "-lrb-", "factor", "average", "-rrb-", "psnr", "-lrb-", "measure", "registration", "error", "-rrb-", "improve", "0.005", "db", "0.045", "db", "various", "noise", "sigma", "-lrb-", "from", "20", "60", "-rrb-", "specially", "we", "use", "Harris", "corner", "detection", "-lsb-", "Harris", "Stephens", "1988", "-rsb-", "128-bit", "brief", "descriptor", "-lsb-", "calonder", "et", "al.", "2010", "-rsb-", "which", "can", "achieve", "real-time", "performance", "even", "mobile", "phone", "we", "reject", "incorrectly", "match", "feature", "use", "local", "ransac", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "addition", "we", "estimate", "homography", "flow", "each", "non-overlapped", "block", "-lrb-", "pixel", "-rrb-", "instead", "per", "pixel", "all", "pixel", "each", "block", "share", "same", "translation", "vector", "which", "compute", "map", "block", "center", "between", "two", "frame", "approximation", "only", "slightly", "scarify", "quality", "-lrb-", "nearly", "0.01", "db", "psnr", "-rrb-", "accelerate", "pixel", "map", "2.5", "time", "3.2", "consistent", "Pixels", "selection", "-lrb-", "scene", "motion", "-rrb-", "consistent", "pixel", "handle", "scene", "motion", "we", "borrow", "simple", "idea", "from", "hdr", "deghosting", "-lsb-", "Granados", "et", "al.", "2013", "-rsb-", "avoid", "complex", "motion", "tracking", "every", "pixel", "location", "-lrb-", "reference", "frame", "-rrb-", "we", "identify", "set", "consistent", "pixel", "1d", "profile", "-lrb-", "trace", "estimate", "homography", "flow", "-rrb-", "across", "all", "image", "temporal", "pixel", "fusion", "different", "from", "hdr", "deghosting", "purpose", "select", "consistent", "pixel", "only", "avoid", "ghost", "artifact", "-lrb-", "cause", "dynamic", "motion", "small", "frame", "misalignment", "-rrb-", "also", "find", "many", "consistent", "pixel", "possible", "denoise", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "232:4", "Z.", "Liu", "et", "al.", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "consistent", "pixel", "selection", "rapid", "motion", "-lrb-", "e.g.", "car", "top", "-rrb-", "small", "motion", "-lrb-", "e.g.", "head", "hand", "bottom", "-rrb-", "-lrb-", "-rrb-", "two", "input", "frame", "-lrb-", "-rrb-", "map", "record", "select", "frame", "index", "-lrb-", "blue", "cell", "-rrb-", "each", "pixel", "-lrb-", "-rrb-", "temporal", "fusion", "result", "only", "reference-based", "pixel", "selection", "-lrb-", "-rrb-", "temporal", "fusion", "result", "only", "median-based", "pixel", "selection", "-lrb-", "-rrb-", "temporal", "multiscale", "fusion", "result", "from", "combined", "strategy", "two", "method", "which", "separately", "satisfy", "one", "two", "goal", "one", "reference-based", "we", "bi-directionally", "trace", "profile", "from", "pixel", "reference", "frame", "collect", "consistent", "pixel", "until", "accumulate", "pixel", "difference", "exceed", "threshold", "other", "median-based", "we", "collect", "pixel", "consistent", "median", "-lrb-", "below", "same", "threshold", "-rrb-", "all", "pixel", "profile", "reference", "method", "can", "guarantee", "ghost-free", "result", "pixel", "dynamic", "object", "we", "may", "get", "insufficient", "sample", "denoising", "-lrb-", "show", "Figure", "-lrb-", "-rrb-", "-rrb-", "median", "method", "collect", "more", "consistent", "pixel", "might", "lead", "ghost", "because", "median", "may", "happen", "color", "move", "object", "-lrb-", "show", "Figure", "-lrb-", "-rrb-", "-rrb-", "combine", "strategy", "we", "propose", "simple", "combination", "strategy", "both", "method", "each", "pixel", "-lrb-", "reference", "frame", "-rrb-", "we", "compute", "two", "set", "consistent", "pixel", "-lcb-", "-rcb-", "separately", "median", "reference", "method", "-lrb-", "-rrb-", "record", "frame", "index", "consistent", "pixel", "every", "pixel", "reference", "frame", "belong", "we", "take", "union", "final", "result", "because", "both", "method", "agree", "otherwise", "we", "choose", "median", "result", "reliable", "-lrb-", "i.e.", "size", "more", "than", "half", "frame", "number", "-rrb-", "choose", "reference", "result", "unreliable", "further", "reduce", "chance", "ghost", "-lrb-", "enforce", "spatial", "coherence", "-rrb-", "we", "do", "measure", "reliability", "pixel", "pixel", "instead", "we", "find", "all", "connectedcomponent", "undecided", "pixel", "-lrb-", "where", "reference", "frame", "do", "belong", "-rrb-", "determine", "reliability", "each", "connect", "component", "whole", "majority", "voting", "after", "combination", "we", "obtain", "set", "consistent", "pixel", "all", "pixel", "further", "make", "combination", "seamless", "we", "run", "morphological", "-lrb-", "majority", "-rrb-", "filter", "-lsb-", "Gonzalez", "Woods", "2007", "-rsb-", "stack", "index", "consistent", "pixel", "frame-by-frame", "Figure", "-lrb-", "a-b", "-rrb-", "show", "two", "real", "example", "map", "which", "record", "number", "consistent", "pixel", "every", "pixel", "location", "efficient", "implementation", "consistent", "pixel", "also", "select", "coarse", "scale", "-lrb-", "i.e.", "-rrb-", "pyramid", "purpose", "enable", "fast", "computation", "detect", "motion", "relatively", "clean", "scale", "approximation", "could", "achieve", "98.7", "outlier", "detection", "rate", "operate", "original", "scale", "expense", "half", "time", "other", "scale", "just", "reuse", "index", "computed", "consistent", "pixel", "upsampling", "downsampling", "both", "median", "reference", "base", "method", "we", "use", "patch", "-lrb-", "-rrb-", "difference", "instead", "single", "pixel", "difference", "use", "threshold", "10", "we", "use", "integral", "image", "-lsb-", "Viola", "Jones", "2001", "-rsb-", "compute", "patch", "difference", "more", "efficiently", "give", "consistent", "pixel", "each", "pixel", "all", "scale", "we", "fuse", "all", "they", "temporal", "multi-scale", "fusion", "we", "keep", "we", "fusion", "simple", "possible", "while", "be", "able", "significantly", "denoise", "temporal", "fusion", "suppose", "-lcb-", "-rcb-", "consistent", "pixel", "pixel", "location", "-lrb-", "certain", "scale", "-rrb-", "where", "pixel", "color", "from", "tth", "frame", "we", "compute", "fusion", "result", "linear", "minimum", "mean", "square", "error", "-lrb-", "lmmse", "-rrb-", "estimator", "which", "widely", "use", "previous", "work", "-lrb-", "e.g.", "-lsb-", "Zhang", "Wu", "2005", "-rsb-", "-rrb-", "optimal", "unbiased", "denoising", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "mean", "all", "consistent", "pixel", "-lcb-", "-rcb-", "variance", "true", "pixel", "approximate", "max", "-lrb-", "-rrb-", "standard", "deviation", "-lcb-", "-rcb-", "noise", "LMMSE", "estimator", "can", "help", "we", "adaptively", "handle", "outlier", "some", "severely", "misalign", "pixel", "occur", "discontinuity", "depths", "-lrb-", "we", "homography", "flow", "more", "suitable", "spatially", "smooth", "depth", "variation", "-rrb-", "subtle", "residual", "move", "pixel", "fine", "scale", "-lrb-", "we", "scene", "motion", "detection", "perform", "coarse", "scale", "-rrb-", "wold", "make", "variance", "-lcb-", "-rcb-", "much", "larger", "than", "noise", "variance", "thus", "we", "fusion", "result", "would", "-lrb-", "denoising", "-rrb-", "otherwise", "result", "have", "value", "close", "mean", "-lcb-", "-rcb-", "temporal", "fusion", "run", "independently", "every", "scale", "next", "we", "describe", "how", "aggregate", "result", "all", "scale", "multi-scale", "fusion", "we", "aggregate", "result", "from", "top", "bottom", "point-wise", "manner", "let", "temporal", "fusion", "result", "two", "adjacent", "scale", "we", "update", "result", "3.3", "pixel", "fusion", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "bilinear", "upscale", "operator", "m/n", "adaptive", "fusion", "weight", "total", "frame", "number", "number", "inlier", "-lrb-", "inlier", "when", "where", "standard", "deviation", "-lcb-", "-rcb-", "-rrb-", "identify", "temporal", "fusion", "larger", "means", "we", "have", "more", "consistent", "pixel", "current", "scale", "we", "should", "trust", "current", "estimation", "more", "otherwise", "we", "should", "borrow", "more", "from", "parent", "scale", "furthermore", "multi-scale", "processing", "effective", "handle", "non-gaussian", "type", "noise", "-lrb-", "e.g.", "splotch", "-lsb-", "Chatterjee", "et", "al.", "2011", "-rsb-", "-rrb-", "real", "imaging", "pipeline", "above", "fusion", "do", "exploit", "spatial", "information", "which", "play", "central", "role", "single", "image", "denoising", "algorithm", "-lsb-", "Zontak", "et", "al.", "2013", "-rsb-", "here", "we", "replace", "temporal", "fusion", "result", "equation", "-lrb-", "-rrb-", "very", "fast", "filter", "spatial", "domain", "tex", "-lrb-", "-rrb-", "-lrb-", "tex", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "filter", "operator", "-lrb-", "-rrb-", "directional", "spatial", "pixel", "fusion", "we", "find", "all", "spatially", "consistent", "pixel", "along", "most", "probable", "since", "we", "detect", "motion", "coarser", "relatively", "clean", "scale", "we", "can", "use", "constant", "threshold", "instead", "adaptive", "threshold", "which", "may", "require", "complex", "noise", "modeling", "we", "find", "work", "well", "we", "experiment", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "fast", "Burst", "Images", "Denoising", "232:5", "edge", "direction", "within", "window", "lmmse", "estimator", "describe", "equation", "-lrb-", "-rrb-", "use", "pixel", "efficiency", "we", "only", "apply", "spatial", "fusion", "texture", "pixel", "-lrb-", "tex", "0.01", "-rrb-", "textureness", "probability", "tex", "compute", "sigmoid", "function", "-lrb-", "exp", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "-rrb-", "which", "maximum", "absolute", "difference", "between", "pixel", "its", "neighbor", "estimated", "standard", "deviation", "noise", "efficiency", "we", "estimate", "compute", "standard", "deviation", "pixel", "difference", "between", "median", "image", "reference", "image", "within", "flat", "-lrb-", "non-textured", "-rrb-", "area", "area", "median", "image", "-lrb-", "generate", "coarse", "scale", "-rrb-", "good", "approximation", "clean", "version", "reference", "image", "real", "noise", "we", "use", "approach", "similar", "-lsb-", "Liu", "et", "al.", "2008", "-rsb-", "divide", "illuminance", "10", "discrete", "bin", "estimate", "corresponding", "each", "bin", "Extension", "patch", "idea", "combine", "temporal", "multiscale", "fusion", "can", "also", "extend", "patch", "level", "better", "denoising", "quality", "different", "from", "point-wise", "fusion", "LMMSE", "estimator", "patch", "apply", "frequency", "domain", "which", "similar", "Wiener", "filter", "use", "transform", "domain", "-lsb-", "Dabov", "et", "al.", "2007b", "-rsb-", "addition", "patch-based", "lmmse", "estimator", "provide", "overlap", "estimate", "every", "pixel", "which", "need", "patch", "aggregation", "-lrb-", "along", "temporal", "axis", "spatial", "edge", "direction", "-rrb-", "get", "final", "fusion", "result", "algorithm", "validation", "we", "perform", "quantitative", "evaluation", "synthetic", "datum", "set", "ground-truth", "clean", "image", "come", "from", "68", "image", "bsd300", "-lsb-", "Martin", "et", "al.", "2001", "-rsb-", "simulate", "camera", "motion", "we", "reuse", "estimate", "global", "homography", "from", "real", "datum", "-lrb-", "figure", "-rrb-", "randomly", "apply", "they", "one", "clean", "image", "generate", "burst", "image", "-lrb-", "10", "frame", "-rrb-", "we", "add", "gaussian", "noise", "various", "noise", "level", "-lrb-", "20", "60", "-rrb-", "note", "we", "synthetic", "datum", "set", "ignore", "many", "key", "factor", "real", "datum", "parallax", "non-gaussian", "noise", "non-rigid", "object", "motion", "ignore", "key", "factor", "may", "lead", "incomplete", "conclusion", "however", "we", "still", "provide", "preliminary", "evaluation", "here", "reference", "help", "we", "gain", "better", "understanding", "Figure", "-lrb-", "-rrb-", "show", "average", "psnr", "average", "-lrb-", "baseline", "-rrb-", "optical", "flow", "median", "filter", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "we", "method", "-lrb-", "pixel", "fusion", "-rrb-", "we", "method", "-lrb-", "patch", "fusion", "-rrb-", "we", "apply", "same", "global", "homography", "all", "method", "better", "result", "overall", "we", "method", "-lrb-", "pixel", "fusion", "-rrb-", "perform", "comparably", "vbm3d", "bm4d", "-lrb-", "two", "state-of-the-art", "denoising", "method", "-rrb-", "better", "than", "average", "optical", "flow", "all", "noise", "level", "when", "noise", "level", "increase", "we", "method", "perform", "slightly", "worse", "than", "bm4d", "vbm3d", "drop", "more", "quickly", "than", "ours", "evaluation", "partially", "demonstrate", "true", "power", "we", "method", "presence", "real", "camera", "motion", "registration", "error", "keep", "mind", "we", "method", "2-3", "order", "magnitude", "faster", "than", "vbm3d", "optical", "flow", "bm4d", "Figure", "-lrb-", "-rrb-", "show", "run", "time", "method", "different", "image", "size", "addition", "we", "can", "achieve", "best", "result", "extend", "we", "fusion", "patch", "level", "compare", "two", "patch-based", "method", "-lrb-", "vbm3d", "bm4d", "-rrb-", "we", "patch-based", "fusion", "still", "efficient", "-lrb-", "1-2", "order", "magnitude", "faster", "-rrb-", "Figure", "also", "demonstrate", "patch", "perform", "better", "both", "temporal", "multi-scale", "fusion", "than", "pixel", "conclusion", "consistent", "previous", "work", "since", "patch", "can", "usually", "use", "more", "spatially", "correlate", "information", "than", "pixel", "more", "interestingly", "multiscale", "fusion", "complementary", "temporal", "pixel", "fusion", "play", "important", "role", "we", "pixel-based", "method", "help", "greatly", "reduce", "gap", "between", "we", "pixel-based", "method", "we", "patch-based", "method", "global", "align", "average", "Optical", "flow", "median", "vbm3d", "bm4d", "-lrb-", "-rrb-", "db", "32", "we", "Method", "-lrb-", "Pixel", "-rrb-", "we", "Method", "-lrb-", "Patch", "-rrb-", "30", "28", "psnr", "26", "24", "22", "20", "30", "40", "50", "60", "Noise", "Sigma", "-lrb-", "-rrb-", "10000", "1000", "-lrb-", "sec", "-rrb-", "100", "10", "Time", "run", "0.1", "0.2", "0.01", "image", "size", "-lrb-", "mega-pixel", "-rrb-", "figure", "psnr", "different", "method", "synthetic", "datum", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "we", "apply", "pre-warp", "use", "same", "estimate", "global", "homography", "better", "result", "temporal", "-lrb-", "pixel", "-rrb-", "temporal", "multiscale", "-lrb-", "pixel", "-rrb-", "temporal", "-lrb-", "Patch", "-rrb-", "temporal", "multiscale", "-lrb-", "Patch", "-rrb-", "31", "29", "-lrb-", "db", "-rrb-", "27", "psnr", "25", "23", "21", "20", "30", "40", "50", "60", "Noise", "Sigma", "Figure", "psnr", "comparison", "we", "different", "fusion", "experiment", "we", "acquire", "20", "set", "burst", "image", "various", "content", "camera", "include", "mobile", "phone", "dslr", "camera", "compact", "camera", "each", "burst", "contain", "10", "shot", "all", "we", "result", "generate", "set", "fix", "parameter", "pixel-based", "fusion", "all", "original", "sequence", "more", "result", "provide", "we", "webpage", "4.1", "comparison", "we", "compare", "we", "method", "three", "point-wise", "method", "-lrb-", "spatialtemporal", "filter", "-lsb-", "Bennett", "McMillan", "2005", "-rsb-", "lucky", "imaging", "-lsb-", "Joshi", "Cohen", "2010", "-rsb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "temporally", "median", "filter", "-rrb-", "two", "state-of-the-art", "patch-based", "method", "-lrb-", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-rrb-", "former", "two", "base", "we", "own", "implementation", "optical", "flow", "latter", "two", "from", "author", "all", "method", "we", "apply", "same", "global", "homography", "estimate", "we", "method", "help", "they", "obtain", "more", "reliable", "correspondence", "since", "some", "algorithm", "require", "known", "noise", "variance", "we", "try", "all", "possible", "noise", "level", "choose", "result", "best", "visual", "quality", "through", "balanced", "tradeoff", "between", "detail", "recovery", "noise", "reduction", "static", "scene", "example", "Figure", "capture", "HTC", "802d", "Android", "phone", "motion", "mainly", "cause", "camera", "movement", "challenge", "case", "how", "remove", "strong", "noise", "sky", "recover", "building", "structure", "we", "can", "see", "spatial-temporal", "filter", "vbm3d", "bm4d", "still", "leave", "certain", "noise", "flat", "region", "-lrb-", "e.g.", "sky", "area", "-rrb-", "building", "structure", "-lrb-", "Figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "well", "restore", "vbm3d", "bm4d", "because", "presence", "structure", "noise", "either", "spatial-temporal", "bilateral", "http://personal.ie.cuhk.edu.hk/", "lz013/projects/burstdenoising", "html", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "232:6", "Z.", "Liu", "et", "al.", "-lrb-", "-rrb-", "input", "reference", "image", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lrb-", "-rrb-", "lucky", "imaging", "-lrb-", "-rrb-", "bm4d", "-lrb-", "-rrb-", "we", "result", "figure", "static", "scene", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "portrait", "small", "motion", "-lrb-", "-rrb-", "input", "image", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lsb-", "Bennett", "McMillan", "2005", "-rsb-", "-lrb-", "-rrb-", "lucky", "imaging", "-lsb-", "Joshi", "Cohen", "2010", "-rsb-", "-lrb-", "-rrb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "median", "-lrb-", "-rrb-", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "-lrb-", "-rrb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-lrb-", "-rrb-", "we", "result", "filter", "patch", "matching", "have", "risk", "find", "mismatched", "correspondence", "which", "eventually", "lead", "undesired", "result", "overall", "result", "optical", "flow", "lucky", "imaging", "ours", "comparable", "we", "result", "be", "slightly", "cleaner", "portrait", "small", "motion", "typical", "scenario", "take", "portrait", "photo", "dark", "lighting", "example", "Figure", "record", "JVC", "gc-px10", "camera", "spatial-temporal", "filter", "lucky", "imaging", "optical", "flow", "method", "produce", "staircase", "artifact", "around", "eye", "-lrb-", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "because", "small", "non-rigid", "motion", "subject", "vbm3d", "bm4d", "do", "have", "issue", "blur", "fine", "detail", "scarf", "-lrb-", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "we", "result", "achieve", "best", "both", "kind", "region", "complex", "scene", "motion", "Figure", "10", "Figure", "11", "show", "two", "case", "complex", "dynamic", "scene", "first", "example", "capture", "Cannon", "EOS", "500d", "iso", "6400", "its", "noise", "level", "relatively", "low", "later", "example", "obtain", "Nokia", "lumia920", "we", "can", "see", "from", "both", "example", "lucky", "imaging", "optical", "flow", "-lrb-", "median", "filter", "-rrb-", "base", "result", "contain", "noticeable", "ghost", "while", "vbm3d", "result", "over-smoothed", "bm4d", "better", "than", "former", "two", "method", "still", "leave", "certain", "amount", "chrominance", "noise", "pattern", "background", "we", "solution", "we", "can", "automatically", "choose", "pixel", "color", "consistent", "reference", "frame", "dynamic", "region", "collect", "more", "consistent", "pixel", "static", "region", "-lrb-", "e.g.", "cloth", "door", "-rrb-", "result", "we", "result", "strike", "best", "balance", "among", "remove", "noise", "reconstruct", "fine", "detail", "avoid", "ghost", "-lrb-", "-rrb-", "optical", "flow", "median", "-lrb-", "-rrb-", "vbm3d", "close-up", "view", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "4.2", "more", "result", "handle", "motion", "blur", "during", "capture", "some", "individual", "frame", "-lrb-", "e.g.", "10", "30", "total", "frame", "-rrb-", "may", "blurry", "due", "sudden", "camera", "shake", "object", "motion", "Figure", "12", "show", "example", "capture", "iPhone", "5", "we", "examine", "what", "would", "happen", "blurry", "frame", "be", "select", "reference", "frame", "know", "we", "separately", "select", "frame", "-lrb-", "sharp", "-rrb-", "frame", "-lrb-", "blurry", "-rrb-", "reference", "frame", "generate", "two", "result", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "show", "we", "method", "insensitive", "selection", "because", "we", "method", "capable", "find", "consistent", "pixel", "from", "majority", "frame", "similar", "method", "propose", "video", "deblurring", "-lsb-", "Cho", "et", "al.", "2012", "-rsb-", "which", "find", "similar", "patch", "from", "sharp", "frame", "deblurr", "handle", "extreme", "low", "light", "Figure", "13", "sequence", "capture", "iPhone", "4", "under", "extreme", "low-light", "condition", "since", "input", "extremely", "dark", "we", "preprocess", "input", "boost", "brightness", "-lrb-", "apply", "shadow/highlight", "adjustment", "-rrb-", "while", "noise", "after", "boost", "very", "strong", "we", "method", "still", "manage", "produce", "clean", "image", "fine", "detail", "-lrb-", "thin", "wire", "air", "steel", "tower", "left", "-rrb-", "handle", "large", "occlusion", "Figure", "14", "show", "sequence", "large", "fast", "move", "foreground", "-lrb-", "person", "-rrb-", "consistent", "pixel", "map", "figure", "reveal", "how", "mechanism", "we", "algorithm", "can", "reliably", "deal", "-lrb-", "fast", "-rrb-", "large", "occlusion", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "fast", "Burst", "Images", "Denoising", "232:7", "-lrb-", "-rrb-", "input", "burst", "image", "-lrb-", "-rrb-", "reference", "image", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lrb-", "-rrb-", "lucky", "imaging", "-lrb-", "-rrb-", "optical", "flow", "median", "-lrb-", "-rrb-", "vbm3d", "-lrb-", "-rrb-", "bm4d", "-lrb-", "-rrb-", "we", "result", "figure", "10", "complex", "scene", "motion", "i.", "-lrb-", "-rrb-", "input", "burst", "image", "-lrb-", "-rrb-", "reference", "image", "-lrb-", "-rrb-", "optical", "flow", "median", "-lrb-", "-rrb-", "vbm3d", "figure", "11", "complex", "scene", "motion", "ii", "we", "run", "we", "method", "Intel", "i5", "3.2", "GHZ", "machine", "16g", "RAM", "we", "unoptimized", "C++", "implementation", "-lrb-", "single", "core", "sse", "simd", "acceleration", "-rrb-", "take", "920", "m", "average", "process", "10", "frame", "5mpixel", "image", "specifically", "we", "method", "take", "82m", "177m", "51m", "30m", "253m", "328m", "build", "pyramid", "extract", "match", "sparse", "feature", "estimate", "homography", "flow", "select", "consistent", "pixel", "run", "temporal", "fusion", "execute", "multi-scale", "fusion", "we", "prototype", "smartphone", "-lrb-", "Nokia", "Lumia", "920", "-rrb-", "cost", "about", "4.7", "seconds", "average", "without", "use", "multi-core", "neon", "instruction", "GPU", "acceleration", "we", "solution", "mainly", "base", "point-wise", "operation", "we", "expect", "can", "significantly", "accelerate", "Table", "further", "demonstrate", "processing", "time", "different", "method", "same", "machine", "figure", "10", "11", "conclude", "remark", "image", "burst", "we", "expect", "camera", "motion", "from", "hand", "shake", "small/moderate", "motion", "main", "subject", "-lrb-", "-rrb-", "we", "method", "design", "handle", "dramatic", "motion", "-lrb-", "e.g.", "sport", "-rrb-", "denoi", "close-up", "view", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lrb-", "-rrb-", "lucky", "imaging", "-lrb-", "-rrb-", "bm4d", "-lrb-", "-rrb-", "we", "result", "4.3", "Time", "complexity", "Figure", "Figure", "Figure", "10", "Figure", "11", "size", "1520", "2688", "1600", "1200", "2352", "1568", "1280", "720", "-lrb-", "-rrb-", "337.85", "139.88", "304.75", "81.54", "-lrb-", "-rrb-", "74.06", "40.27", "64.94", "28.46", "-lrb-", "-rrb-", "577.39", "298.73", "538.02", "126.63", "-lrb-", "-rrb-", "214.83", "123.42", "198.36", "53.92", "-lrb-", "-rrb-", "1867.27", "1019.31", "1576.49", "517.83", "-lrb-", "-rrb-", "0.80", "0.48", "0.77", "0.23", "4.3", "Time", "complexity", "Figure", "Figure", "Figure", "10", "Figure", "11", "size", "1520", "2688", "1600", "1200", "2352", "1568", "1280", "720", "-lrb-", "-rrb-", "337.85", "139.88", "304.75", "81.54", "-lrb-", "-rrb-", "74.06", "40.27", "64.94", "28.46", "-lrb-", "-rrb-", "577.39", "298.73", "538.02", "126.63", "-lrb-", "-rrb-", "214.83", "123.42", "198.36", "53.92", "-lrb-", "-rrb-", "1867.27", "1019.31", "1576.49", "517.83", "-lrb-", "-rrb-", "0.80", "0.48", "0.77", "0.23", "Table", "processing", "time", "-lrb-", "sec", "-rrb-", "different", "denoising", "method", "-lrb-", "-rrb-", "spatial-temporal", "filter", "-lrb-", "-rrb-", "lucky", "imaging", "-lrb-", "-rrb-", "optical", "flow", "median", "filter", "-lrb-", "-rrb-", "vbm3d", "-lrb-", "-rrb-", "bm4d", "-lrb-", "-rrb-", "we", "method", "ing", "general", "video", "when", "motion", "between", "two", "frame", "can", "well", "represent", "we", "homography", "flow", "scene", "transition", "fast", "camera", "panning", "even", "non-rigid", "deformation", "-lrb-", "e.g.", "water", "wave", "motion", "flag", "wave", "-rrb-", "we", "approach", "break", "besides", "two", "case", "which", "we", "consistent", "pixel", "selection", "may", "fail", "first", "case", "when", "motion", "blur", "cause", "dynamic", "object", "appear", "majority", "frame", "-lrb-", "more", "than", "half", "all", "frame", "-rrb-", "dynamic", "region", "we", "pixel", "selection", "would", "automatically", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "232:8", "Z.", "Liu", "et", "al.", "-lrb-", "-rrb-", "frame", "-lrb-", "reference", "-rrb-", "-lrb-", "-rrb-", "we", "result", "-lrb-", "use", "frame", "-rrb-", "close-up", "view", "Figure", "12", "sequence", "motion", "blur", "individual", "frame", "we", "respectively", "use", "4th", "frame", "-lrb-", "sharp", "-rrb-", "5th", "frame", "-lrb-", "blur", "-rrb-", "reference", "frame", "we", "solution", "produce", "similar", "result", "both", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "13", "sequence", "capture", "under", "extreme", "low", "light", "condition", "-lrb-", "-rrb-", "input", "image", "sequence", "-lrb-", "-rrb-", "reference", "frame", "after", "brightness", "amplification", "-lrb-", "-rrb-", "we", "result", "choose", "reference-based", "strategy", "reference", "frame", "contain", "blur", "we", "result", "would", "retain", "blur", "effect", "sharp", "frame", "choose", "reference", "issue", "can", "avoid", "Figure", "15", "-lrb-", "-rrb-", "show", "example", "therefore", "we", "need", "better", "strategy", "select", "reference", "frame", "besides", "fast", "move", "object", "would", "automatically", "remove", "median-based", "strategy", "aggressive", "denoising", "avoid", "issue", "we", "may", "provide", "another", "option", "allow", "user", "choose", "reference", "frame", "constrain", "reference", "region", "pixel", "selection", "second", "case", "when", "different", "move", "object", "background", "may", "have", "similar", "color", "same", "pixel", "location", "we", "pixel", "selection", "algorithm", "rely", "per-pixel", "color", "difference", "which", "too", "weak", "distinguish", "object", "Figure", "15", "-lrb-", "-rrb-", "show", "example", "different", "move", "person", "have", "very", "similar", "color", "region", "ambiguous", "region", "-lrb-", "indicate", "highlight", "box", "-rrb-", "occur", "majority", "frame", "-lrb-", "i.e.", "more", "than", "half", "all", "frame", "-rrb-", "finally", "lead", "ghost", "because", "we", "selection", "wrongly", "regard", "background", "issue", "also", "appear", "ghost-free", "hdr", "reconstruction", "-lsb-", "Granados", "et", "al.", "2013", "-rsb-", "which", "require", "interaction", "exclude", "ambiguous", "region", "despite", "above", "issue", "we", "believe", "we", "highly", "efficient", "solution", "practical", "enough", "deploy", "improve", "photo", "experience", "user", "broad", "range", "lighting", "condition", "acknowledgement", "we", "thank", "all", "reviewer", "helpful", "discussion", "Steve", "Lin", "Jiangyu", "Liu", "help", "proofread", "Figure", "14", "sequence", "large", "occlusion", "-lrb-", "move", "person", "-rrb-", "top", "input", "bottom", "left", "map", "show", "number", "select", "consistent", "pixel", "right", "image", "we", "result", "-lrb-", "-rrb-", "input", "image", "we", "result", "generate", "different", "reference", "-lrb-", "-rrb-", "figure", "15", "two", "failure", "case", "input", "burst", "image", "we", "result", "-lrb-", "-rrb-", "motion", "blur", "dynamic", "object", "-lrb-", "majority", "frame", "-rrb-", "-lrb-", "-rrb-", "ambiguous", "region", "-lrb-", "similar", "color", "-rrb-", "from", "different", "move", "object", "reference", "dam", "a.", "elfand", "N.", "OLSON", "J.", "EVOY", "M.", "2009", "gaussian", "kd-tree", "fast", "high-dimensional", "filter", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "28", "arne", "C.", "hechtman", "E.", "inkelstein", "a.", "old", "man", "D.", "B.", "2009", "Patchmatch", "randomize", "correspondence", "algorithm", "structural", "image", "editing", "SIGGRAPH", "28", "ennett", "E.", "P.", "ILLAN", "L.", "2005", "Video", "enhancement", "use", "per-pixel", "virtual", "exposure", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "24", "845", "852", "ronshtein", "i.", "N.", "EMENDYAYEV", "K.", "A.", "1997", "handbook", "mathematics", "Springer-Verlag", "New", "York", "NY", "USA", "rox", "T.", "RUHN", "a.", "apenberg", "N.", "EICKERT", "J.", "2004", "high", "accuracy", "optical", "flow", "estimation", "base", "theory", "warping", "Proc", "eccv", "uade", "a.", "oll", "B.", "orel", "j.-m", "2005", "non-local", "algorithm", "image", "denoising", "Proc", "cvpr", "uade", "a.", "ou", "Y.", "OREL", "J.-M.", "ang", "Z.", "2009", "note", "multi-image", "denoising", "Proceedings", "International", "Workshop", "local", "non-local", "approximation", "-lrb-", "lnlum", "-rrb-", "image", "processing", "uade", "a.", "ou", "Y.", "OREL", "J.-M.", "ang", "Z.", "2010", "multi", "image", "noise", "estimation", "denoising", "HAL", "aus", "J.", "F.", "H.", "IU", "C.", "hen", "Z.", "2009", "Blind", "motion", "deblurr", "use", "multiple", "image", "J.", "Comput", "physics", "228", "14", "5057", "5071", "alonder", "M.", "EPETIT", "V.", "trecha", "C.", "ua", "P.", "2010", "brief", "binary", "robust", "independent", "elementary", "feature", "Proc", "eccv", "hatterjee", "P.", "OSHI", "N.", "ang", "S.", "B.", "atsushita", "Y.", "2011", "noise", "suppression", "low-light", "image", "through", "joint", "denoising", "demosaicing", "Proc", "cvpr", "hen", "J.", "ang", "c.-k", "2007", "spatio-temporal", "markov", "random", "field", "video", "denoising", "Proc", "cvpr", "hen", "J.", "ang", "c.-k.", "ang", "J.", "2009", "noise", "brush", "interactive", "high", "quality", "image-noise", "separation", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "ASIA", "-rrb-", "28", "hen", "X.", "ang", "S.", "B.", "ang", "J.", "J.", "2013", "fast", "patchbased", "denoising", "use", "approximate", "patch", "geodesic", "path", "Proc", "cvpr", "ho", "S.", "ang", "J.", "ee", "S.", "2012", "Vdeo", "deblurr", "hand-held", "camera", "use", "patch-based", "synthesis", "Proc", "ACM", "SIGGRAPH", "31", "64:1", "64:9", "abov", "K.", "ous", "a.", "giazarian", "K.", "2007", "Video", "denoising", "sparse", "3d", "transform-domain", "collaborative", "filter", "Proc", "european", "signal", "process", "Conf.", "EUSIPCO", "abov", "K.", "ous", "a.", "giazarian", "K.", "GIAZARIAN", "K.", "2007", "image", "denoising", "sparse", "3-d", "transform-domain", "collaborative", "filter", "IEEE", "Trans", "image", "processing", "16", "2080", "2095", "arsiu", "S.", "OBINSON", "M.", "D.", "lad", "m.", "ilanfar", "P.", "2004", "fast", "robust", "multiframe", "super", "resolution", "IEEE", "Trans", "image", "processing", "13", "10", "1327", "1344", "allo", "O.", "ELFAND", "N.", "HEN", "W.", "ico", "M.", "ULLI", "K.", "2009", "artifact-free", "high", "dynamic", "range", "imaging", "onzalez", "R.", "C.", "OODS", "R.", "E.", "2007", "Digital", "image", "processing", "Prentice", "Hall", "3rd", "edition", "ranado", "M.", "IM", "K.", "I.", "ompkin", "J.", "heobalt", "C.", "2013", "Automatic", "noise", "modeling", "ghost-free", "hdr", "reconstruction", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "ASIA", "-rrb-", "32", "10", "rundmann", "M.", "WATRA", "V.", "ASTRO", "D.", "ssa", "i.", "2012", "calibration-free", "rolling", "shutter", "removal", "Proc", "iccp", "arris", "C.", "TEPHENS", "M.", "1988", "combined", "corner", "edge", "detector", "Proc", "fourth", "Alvey", "Vision", "Conference", "artley", "R.", "isserman", "a.", "2003", "multiple", "View", "Geometry", "Computer", "Vision", "ed", "Cambridge", "University", "Press", "New", "York", "NY", "USA", "ACOBS", "D.", "E.", "AEK", "J.", "EVOY", "M.", "2012", "focal", "stack", "composit", "depth", "field", "control", "Stanford", "Computer", "Graphics", "Laboratory", "Technical", "Report", "OSHI", "N.", "OHEN", "M.", "F.", "2010", "see", "mt.", "rainier", "lucky", "imaging", "multi-image", "denoising", "sharpen", "haze", "removal", "Proc", "iccp", "ALANTARI", "N.", "K.", "hechtman", "E.", "ARNES", "C.", "ARABI", "S.", "OLDMAN", "D.", "B.", "EN", "P.", "2013", "patch-based", "high", "dynamic", "range", "video", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "ASIA", "-rrb-", "32", "202:1", "202:8", "evin", "a.", "adler", "B.", "2011", "natural", "image", "denoising", "optimality", "inherent", "bound", "Proc", "CVPR", "2833", "2840", "iu", "C.", "reeman", "W.", "T.", "2010", "high-quality", "video", "denoise", "algorithm", "base", "reliable", "motion", "estimation", "Proc", "eccv", "706", "719", "iu", "C.", "zeliskus", "R.", "ang", "S.", "B.", "ITNICK", "C.", "L.", "reeman", "W.", "T.", "2008", "Automatic", "estimation", "removal", "noise", "from", "single", "image", "iu", "S.", "uan", "L.", "P.", "UN", "J.", "2013", "bundle", "camera", "path", "video", "stabilization", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "32", "78:1", "78:10", "iu", "C.", "2009", "Beyond", "Pixels", "explore", "New", "Representations", "application", "Motion", "analysis", "phd", "thesis", "Massachusetts", "Institute", "Technology", "aggionus", "M.", "ATKOVNIK", "V.", "GIAZARIAN", "K.", "ous", "a.", "2013", "nonlocal", "transform-domain", "filter", "volumetric", "datum", "denoising", "reconstruction", "IEEE", "Trans", "image", "processing", "119", "133", "artin", "D.", "owlke", "C.", "AL", "D.", "ALIK", "J.", "2001", "database", "human", "segmented", "natural", "image", "its", "application", "evaluate", "segmentation", "algorithm", "measure", "ecological", "statistics", "Proc", "iccv", "ari", "S.", "URAND", "F.", "2009", "fast", "approximation", "bilateral", "filter", "use", "signal", "processing", "approach", "International", "Journal", "Computer", "Vision", "81", "24", "52", "ortillum", "J.", "trelum", "V.", "AINWRIGHT", "M.", "J.", "IMON", "CELLI", "E.", "P.", "2003", "image", "denoising", "use", "scale", "mixture", "gaussian", "wavelet", "domain", "IEEE", "Trans", "image", "processing", "12", "11", "1338", "1351", "einhard", "E.", "ARD", "G.", "attanaik", "S.", "N.", "EBEVEC", "P.", "E.", "eidrich", "W.", "2010", "high", "Dynamic", "Range", "Imaging", "Acquisition", "display", "image-based", "Lighting", "-lrb-", "ed", "-rrb-", "Academic", "Press", "oth", "S.", "lack", "M.", "J.", "2005", "Fields", "expert", "framework", "learn", "image", "prior", "Proc", "cvpr", "en", "P.", "ALANTARI", "N.", "K.", "AESOUBI", "M.", "ARABI", "S.", "OLDMAN", "D.", "B.", "HECHTMAN", "E.", "2012", "robust", "patchbased", "hdr", "reconstruction", "dynamic", "scene", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "31", "203:1", "203:11", "ico", "M.", "2008", "multiframe", "image", "denoising", "stabilization", "EUSIPCO", "omasus", "C.", "anduchus", "R.", "1998", "bilateral", "filter", "gray", "color", "image", "Proc", "iccv", "839", "846", "iolum", "P.", "ONES", "M.", "2001", "robust", "real-time", "object", "detection", "International", "Journal", "Computer", "Vision", "hang", "m.", "unturk", "B.", "K.", "2008", "multiresolution", "bilateral", "filter", "image", "denoising", "IEEE", "Trans", "image", "processing", "17", "12", "2324", "2333", "hang", "L.", "X.", "2005", "Color", "demosaick", "via", "directional", "linear", "minimum", "mean", "square-error", "estimation", "tip", "14", "12", "2167", "2178", "hang", "L.", "ADDADI", "S.", "H.", "AYAR", "S.", "K.", "2009", "multiple", "view", "image", "denoising", "Proc", "CVPR", "1542", "1549", "ontak", "m.", "osserus", "i.", "rani", "M.", "2013", "separate", "signal", "from", "noise", "use", "patch", "recurrence", "across", "scale", "Proc", "cvpr", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014", "fast", "Burst", "Images", "Denoising", "232:9", "ACM", "transaction", "Graphics", "Vol", "33", "no.", "Article", "232", "publication", "date", "November", "2014" ],
  "content" : "\n  \n    8249c003658bcd50fea1e40787fba9e6d2ccc7f561f7f643077ded1796d84f13\n    p7b\n    10.1145/2661229.2661277\n    Name identification was not possible. \n  \n  \n    \n      \n        Fast Burst Images Denoising\n      \n      Ziwei Liu 1? Lu Yuan ? Xiaoou Tang ? Matt Uyttendaele ? Jian Sun ? ? The Chinese University of Hong Kong ? Microsoft Research ? Microsoft Research Technologies\n      \n        \n        Figure 1: Top-left: a burst of noisy images (10 frames with image size 3072 ? 1728) by a smartphone. Bottom-left: the running time (sec.) of different denoising methods. Right: comparison of two close-up views. (a) input images (b) spatial-temporal filtering [Bennett and McMillan 2005] (c) BM4D [Maggioni et al. 2013] (d) optical flow [Liu 2009] + median (e) lucky imaging [Joshi and Cohen 2010] (f) our method. Our method produces a clean, ghost-free image with fine details. More importantly, our method is significantly faster than other methods.\n      \n      This paper presents a fast denoising method that produces a clean image from a burst of noisy images. We accelerate alignment of the images by introducing a lightweight camera motion representation called homography flow. The aligned images are then fused to create a denoised output with rapid per-pixel operations in temporal and spatial domains. To handle scene motion during the capture, a mechanism of selecting consistent pixels for temporal fusion is proposed to ?synthesize? a clean, ghost-free image, which can largely reduce the computation of tracking motion between frames. Combined with these efficient solutions, our method runs several orders of magnitude faster than previous work, while the denoising quality is comparable. A smartphone prototype demonstrates that our method is practical and works well on a large variety of real examples.  CR Categories: I.4.3 [Image Processing and Computer Vision]: Enhancement?Smoothing Keywords: denoising, burst images, homography flow, ghost-free\n      Links:\n      \n        \n      \n      DL PDF\n      \n        \n      \n      1 This work was done when Ziwei was an intern at MSR Asia. ACM Reference Format Liu, Z., Yuan, L., Tang, X., Uyttendaele, M., Sun, J. 2014. Fast Burst Images Denoising. ACM Trans. Graph. 33, 6, Article 232 (November 2014), 9 pages. DOI = 10.1145/2661229.2661277 http://doi.acm.org/10.1145/2661229.2661277. Copyright Notice Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . Copyright ? ACM 0730-0301/14/11-ART232 $15.00. DOI: http://doi.acm.org/10.1145/2661229.2661277\n    \n    \n      \n        1 Introduction\n      \n      Burst, a shooting mode in most cameras, allows multiple shots to be captured in a quick succession by either pressing or holding the shutter button. It is designed to allow selection of the best shot or record the motion. Recently, burst capturing has become ubiquitous in many hand-held imaging devices (e.g., smartphone, compact and DSLR cameras). For example, iPhone 5s supports a burst of up to 10 shots per second. The burst mode has been successfully exploited in computation photography for reducing blur [Cai et al. 2009], or improving shadow/highlight details [Reinhard et al. 2010], or increasing resolution [Farsiu et al. 2004], or clarity [Joshi and Cohen 2010], or depth of the field [Jacobs et al. 2012]. In this paper, we present a practical solution for ?burst images denoising? turning a burst of noisy images (typically captured in a low-light condition) into a single clean image, as shown in Figure 1 . This problem is not new. It has been studied in the context of multiple images/video denoising [Buades et al. 2010; Liu and Freeman 2010; Zhang et al. 2009]. But we focus on practicality our goal is to design a highly efficient method while producing a high-quality result so that the algorithm can be run on a mobile device with limited computational resources. A practical approach needs to tackle two challenges. First is efficiency. The state-of-the-art methods heavily rely on optical flow or patch matching to establish temporal and spatial correspondence, which is unacceptably slow. Second is quality. Fast methods like averaging or filtering [Tomasi and Manduchi 1998] are insufficient on both noise reduction and avoiding ?ghost? artifacts caused by either camera motion (by hand shake) or scene motion (by dynamic objects). Moreover, even some complicated methods are also fragile in the presence of strong noise or complex dynamic motion. We propose a fast noise reduction method that produces a clean image from a burst of images. The high speed of our method is enabled by introducing three accelerating steps. In the first step, we use a lightweight, parametric motion representation homog- raphy flow to model the motion caused by camera movements. This representation is inspired by the recent multiple homographies model [Grundmann et al. 2012; Liu et al. 2013] for video stabilization. Since estimating homography flow only requires spare feature matching, this step is both efficient and robust to noise. In the second step, we handle the scene motion by identifying consistent pixels (i.e., pixels with similar colors) along the temporal axis from all aligned images (by the first step) per pixel location. These selected consistent pixels are used in our temporal pixel fusion (in the third step) by averaging. Thus, we can generate ghostfree results while avoiding complex motion tracking on dynamic objects, which is too slow or too difficult. The idea was successfully applied in recent HDR deghosting [Granados et al. 2013]. We extend this idea to find as many consistent pixels as possible at every pixel location for the purpose of better denoising. In the third step, we apply temporal and multiscale pixel fusions in succession to obtain the denoised result. The temporal fusion is based on a simple, optimal linear estimator. The multiscale fusion is complementary to temporal fusion and further enables significant denoising. Meanwhile, the whole step is also very efficient by design because it only involves pixel-wise operations. We have evaluated our algorithm on a variety of real data. In the presence of moderate or strong noise, our algorithm performs on par with state-of-the-art multi-image denoising methods (e.g., VBM3D [Dabov et al. 2007a], BM4D [Maggioni et al. 2013]). Furthermore, our algorithm is two or three orders of magnitude faster. Figure 1 shows a comparison.\n      ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n      232:2\n      ?\n      Z. Liu et al.\n      \n        2 Related Work\n        Single image denoising has great progresses in recent decades. Representative methods include bilateral filtering [Tomasi and Manduchi 1998], wavelet (GSM) [Portilla et al. 2003], Field-OfExpert [Roth and Black 2005], non-local means [Buades et al. 2005], BM3D [Dabov et al. 2007b] and so on. To improve the efficiency, a few fast variants have been proposed, such as fast bilateral filtering [Paris and Durand 2009], Gaussian kd-trees [Adams et al. 2009], and geodesic paths [Chen et al. 2013]. Most recently, Levin et al. [2011] pointed out that single image denoising may be approaching its performance limit. NoiseBrush [Chen et al. 2009] provided a interactive way for further quality improvement. Multiple image denoising is superior to single image denoising because of its use of more information. Some denoising techniques have been successfully used on burst images [Tico 2008; Buades et al. 2009; Joshi and Cohen 2010], videos [Bennett and McMillan 2005; Liu and Freeman 2010; Dabov et al. 2007a; Chen and Tang 2007], multiple-view images [Zhang et al. 2009], and volumetric MRI data [Maggioni et al. 2013]. Estimating camera motion. Optical flow [Brox et al. 2004] is the most general representation for establishing correspondences between frames. Recent work [Liu and Freeman 2010] showed its importance in video denoising. But optical flow itself has difficulties with occlusion/large displacement, and is fragile to noise. Patch matching is more robust to noise and has been widely used in multiple image processing [Tico 2008; Buades et al. 2009; Zhang et al. 2009; Maggioni et al. 2013; Sen et al. 2012; Kalantari et al. 2013]. However, in the presence of strong noise, both nonparametric methods degrade rapidly. Camera motion in burst mode is similar to the motion studied in video stabilization. Recent work [Grundmann et al. 2012; Liu et al. 2013] demonstrated the success of using a spatially-variant homography for the camera motion. In this work, we use a similar but more lightweight parametric motion representation homography flow.  Handling scene motion. Since optical flow or patch matching is an intrinsically hard problem, recent work [Gallo et al. 2009; Granados et al. 2013] in HDR reconstruction bypasses the motion estimation by finding a consistent subset of colors for every pixel to reconstruct a ghost-free image. Granados et al. [2013]?s consistency test relies on accurate estimation of the noise distribution, which may require complex calibration and super-pixels computation. We were inspired by this idea and extend it for image denoising. Multiscale denoising is an effective way to exploit cross-scale similarity for noise reduction. Recently, Zontak et al. [2013] proposed a directional pyramid technique to find corresponding patches across scales, which produces state-of-the-art results. Zhang and Gunturk [2008] extended the bilateral filter in a multiscale framework. We use a pyramid-based pixel fusion method to improve the result quality.\n        \n          \n          Figure 2: Algorithm pipeline.\n        \n        \n          \n          Figure 3: (a) Pyramid homography graph. (b) homographies (at the finest level) are discretized to obtain a per-pixel homography flow field.\n        \n      \n      \n        3 Algorithm\n         Figure 2 is our algorithm pipeline. We first build Gaussian pyramids 1 of all noisy images and set the midmost frame as the reference frame by default. Then, we estimate homography flow (in Section 3.1) to represent the motion (by camera) between the reference frame and any of the other frames. Across the aligned images (by homography flow), at every pixel location, we select a set of consistent pixels to handle scene motions (in Section 3.2) or possible small misalignment caused by the homography flow. Finally, we apply a pixel fusion (in Section 3.3) to aggregate consistent pixels at all scales for producing the final result.\n        \n          3.1 Homography Flow (for Camera Motion)\n          Pyramid homography graph. We represent the motion between two frames through a pyramid homography graph, as shown in Figure 3 (a). The coarse level node provides robustness while the fine level node helps produce details. Note that independently estimating the homography at each node is unreliable because some nodes may have insufficient matched features. Next, we introduce a coarse-to-fine optimization to robustly obtain accurate results.\n          1 The long side of image at the coarsest scale is no more than 400 pixels.\n          ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n          Fast Burst Images Denoising\n          ?\n          232:3\n          (a)\n          \n            \n          \n          Optical Flow Global Homography + Optical Flow Patch Match Global Homography + Patch Match (b) Global Homography Homography Flow\n          \n            dB\n            31 29 27 25 PSNR 23 21 19 17 20 30 40 50 60\n          \n          \n            c\n            Noise Sigma 100.00\n          \n          (sec.) 10.00 Patch Optical Match Flow (exhaustive) Time 1.00 0.2 1 5 8 Patch Match (randomized) Running 0.10 0.01 Homography Global Homography Flow Image Size (Mega-pixel)\n          \n            Figure 4: Comparisons of registration errors and time cost for various motion models. (a) sample images. (b) registration errors measured in PSNR, which is computed on registered clean images, but the motion is estimated from the noisy images. (c) running time (sec.) of different methods. We use the randomized patch match [Barnes et al. 2009] source code for acceleration, but it is still 1 ? 2 orders slower than homography flow.\n          \n          Optimization. We perform a level-by-level optimization starting from a global homography at the coarsest level (l = 0). Let H i l be the homography of node i at level l , and {H j l } be its 4 neighboring homographies at the same level. We estimate {H i l } by minimizing the following energy function:  2 2 { H ? i l } = arg min H i l ? R i l + ? H i l ? H j l , {H i l } i j (1) where ? (by default, ? = 0.1) controls the amount of spatial regularization enforced by the second smoothness term. In the first data term, R i l = best( H ? i l?1 , F i l ) is selected from two candidates: one is its parent homography H ? i l?1 at the level l ? 1, the other is its own estimated homography F i l (using all matched features within the grid of node i). The basic idea is to use H ? i l?1 as backup when we cannot reliably compute F i l . In our implementation, we pick H ? i l?1 if the feature number in the grid of i is insufficient (< 8) or the rigidness [Hartley and Zisserman 2003] of F i l is too weak 2 ; otherwise, we choose the best model which has lower matching errors of all features within the grid. Because the objective function (1) is quadratic, we can obtain the global optimum by a Jacobi solver [Bronshtein and Semendyayev 1997]. The form of our motion model is similar to a mesh-based homography [Liu et al. 2013]. But our coarse-to-fine optimization is more efficient. For 500 features, our method takes 5 ms per frame while the mesh-based homography requires 50 ms per frame. Homography flow. Since the pyramid homography graph is a parametric representation, we require an image warping or coordinate transformation in the later denoising step. But such operations for all pixels (in all frames, at all scales) are very expensive. To address this critical issue in our application, we discretized the homography\n          2 Shear of homography > 1.25 or modulus of perspective > 0.1. These parameters are empirically set and fixed in all experiments.\n          graph (the finest level only) to obtain per-pixel translation vector homography flow. As shown in Figure 3 (b), we compute the translation vector by mapping each pixel from one frame to another frame according to the homography graph. Finally, the estimated homography flows are scaled accordingly and rounded off for use at other scales. Algorithm validation. We evaluate global homography, optical flow, patch match, and our homography flow on a set of burst images. We asked four different subjects to capture 20 sets of clean burst images (with low ISO, under good lighting conditions). Then we added Gaussian noise with different standard deviations (from 20 to 60) to synthesize 100 sets of noisy burst images. To register these noisy images, we compare six algorithms: global homography, optical flow [Liu 2009], global homography + optical flow, patch match (exhaustive search), global homography + patch match, and our homography flow. We compute the PSNR to measure the difference between registered clean image pairs. Figure 4 (b) shows the results. From the results, we can observe that two optical flow based methods perform well when noise level is small (? = 20). But when the noise level increases, they degrade more quickly than others; global homography can improve patch match but not optical flow. We believe the reason is that the coarse-to-fine mechanism in the optical flow has already handled the global motion. Our homography flow is consistently the best at all noise levels. Figure 4 (c) further shows the running time of these methods on various image sizes. Since global homography and our homography flow only rely on sparse feature detection and matching, they both outperform patch match (exhausive search or even randomized search [Barnes et al. 2009]) and optical flow [Liu 2009] in speed. There is only a small margin between the two running time curves (of global homography and our homography flow), which indicates that our pyramid optimization is very efficient. Efficient implementation. The bottleneck in this step is sparse features extraction and matching. In our implementation, we work on the coarse scale (s = 1) in the pyramids of luminance channel for efficiency and robustness (to noise). Compared with originalscale implementation, the time cost is greatly reduced (by a factor of 6, on average) and the PSNR (for measuring registration error) is improved by 0.005dB to 0.045dB for various noise sigma (from 20 to 60). Specially, we use the Harris corner detection [Harris and Stephens 1988] and 128-bit BRIEF descriptor [Calonder et al. 2010], which can achieve real-time performance even on a mobile phone. We reject incorrectly matched features using local RANSAC [Grundmann et al. 2012]. In addition, we estimate homography flow in each non-overlapped block (8 ? 8 pixels) instead of per pixel. All pixels in each block share the same translation vector, which is computed by mapping the block center between two frames. The approximation only slightly scarifies the quality (by nearly 0.01dB in PSNR), but accelerates pixels mapping by 2.5 times.\n        \n        \n          3.2 Consistent Pixels Selection (for Scene Motions)\n          Consistent pixels. To handle scene motion, we borrow a simple idea from HDR deghosting [Granados et al. 2013] to avoid complex motion tracking. At every pixel location (on a reference frame), we identify a set of consistent pixels on a 1D profile (traced by the estimated homography flow) across all images for temporal pixel fusion. Different from HDR deghosting, the purpose of selecting consistent pixels is not only avoiding ghost artifacts (caused by dynamic motion and small frame misalignment), but also finding as many consistent pixels as possible for denoising.\n          ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n          232:4\n          ?\n          Z. Liu et al.\n          \n            \n          \n        \n      \n      \n        (a)\n      \n      \n        (b)\n        \n          \n        \n      \n      \n        (c) (d) (e)\n        \n          Figure 5: Consistent pixels selection for rapid motions (e.g., cars, on top) and small motions (e.g., head and hand, on bottom). (a) two of the input frames. (b) map recording the selected frame indices (blue cells) for each pixel. (c) temporal fusion result by only reference-based pixels selection. (d) temporal fusion result by only median-based pixels selection. (e) temporal + multiscale fusion result from a combined strategy.\n        \n        There are two methods which separately satisfy one of two goals. One is reference-based: we bi-directionally trace the profile from the pixel at the reference frame and collect consistent pixels until the accumulated pixel difference exceeds a threshold ? . The other is median-based: we collect pixels consistent to the median (below the same threshold ? ) of all pixels on the profile. The reference method can guarantee a ghost-free result. But for a pixel on a dynamic object, we may get insufficient samples for denoising (shown on Figure 5 (c)). The median method collects more consistent pixels but might lead to ghosting because the median may happen to be a color on the moving object (shown on Figure 5 (d)). Combined strategy. We propose a simple combination strategy of both methods: for each pixel (on the reference frame), we compute two sets of consistent pixels {M, R} separately by the median and the reference methods. M (or R) records the frame indices of consistent pixels for every pixel. If the reference frame belongs to M, we take the union of M and R as the final result because both methods agree. Otherwise, we choose the median result if it is reliable (i.e., the size of M is more than half of the frame number), and choose the reference result if it is unreliable. To further reduce the chance of ghosting (by enforcing spatial coherence), we do not measure reliability pixel by pixel. Instead, we find all connectedcomponents of undecided pixels (where the reference frame does not belong to M), and then determine the reliability of each connected component as a whole, by majority voting. After the combination, we obtain the sets of consistent pixels for all pixels. To further make the combination seamless, we run a 3 ? 3 morphological (majority) filter [Gonzalez and Woods 2007] on the stack of the indices of consistent pixels, frame-by-frame. Figure 5 (a-b) shows two real examples and the maps which record the number of consistent pixels at every pixel location. Efficient implementation. The consistent pixels are also selected at the coarse scale (i.e., s = 1) in the pyramids for the purpose of enabling fast computation and detecting motion at a relatively clean scale. The approximation could achieve 98.7% outlier detection rate as operating at the original scale at the expense of half the time. Other scales just reuse the indices of computed consistent pixels by upsampling or downsampling. In both median and reference based methods, we use patch (5 ? 5) difference instead of single pixel difference and use the threshold 3 ? = 10. We use integral image [Viola and Jones 2001] to compute patch differences more efficiently. Given consistent pixels for each pixel at all scales, we fuse all of them in a temporal and multi-scale fusion. We keep our fusion as simple as possible, while being able to significantly denoise. Temporal fusion. Suppose {x t } are consistent pixels at a pixel location (at a certain scale), where x t is pixel color from the tth frame. We compute the fusion result x by a linear minimum mean squared error (LMMSE) estimator, which is widely used in previous work (e.g., [Zhang and Wu 2005]) for optimal unbiased denoising: x = u + ? c 2 ? + c 2 ? 2 (x t ? u) , (2) where u is the mean of all consistent pixels {x t }. The variance ? c 2 of true pixels is approximated by max(0, ? t 2 ? ? 2 ). ? t and ? are the standard deviation of {x t } and noise. The LMMSE estimator can help us adaptively handle outliers. Some severely misaligned pixels occurring at discontinuities of depths (our homography flow is more suitable for spatially smooth depth variations) or subtle residual moving pixels at a fine scale (our scene motion detection is performed at a coarse scale) wold make the variance of {x t } much larger than the noise variance. Thus, our fusion result would be x ? x t (no denoising). Otherwise, the result x has a value close to the mean u of {x t }. The temporal fusion runs independently at every scale. Next, we describe how to aggregate results in all scales. Multi-scale fusion. We aggregate the results from top to bottom, in a point-wise manner. Let x s and x s?1 be temporal fusion results at two adjacent scales s and s ? 1. We update the result x s by:\n        3.3 Pixels Fusion\n        x s = ? ? x s + (1 ? ?) ? (x s?1 ) ?, (3)\n        where ? is a bilinear upscale operator. ? = m/N is an adaptive fusion weight. N is the total frame number and m is the number of inliers (x t is an inlier when |x t ? x| < 3? t , where ? t is the standard deviation of {x t }) identified by the temporal fusion. A larger ? means we have more consistent pixels at the current scale and we should trust current estimation more; otherwise, we should borrow more from the parent scale. Furthermore, the multi-scale processing is effective for handling non-gaussian types of noise (e.g., splotches [Chatterjee et al. 2011]) in the real imaging pipeline. The above fusion does not exploit spatial information which plays a central role in single image denoising algorithms [Zontak et al. 2013]. Here, we replace the temporal fusion result x s in Equation (3) with a very fast filtering in the spatial domain:\n        x s = p tex ? f (x s ) + (1 ? p tex ) ? (x s?1 ) ?, (4)\n        where the filtering operator f (x s ) is a directional spatial pixel fusion. We find all spatially consistent pixels along the most probable\n        3 Since we detect motion at a coarser and relatively clean scale, we can use a constant threshold instead of an adaptive threshold, which may require complex noise modeling. We find that it works well in our experiments.\n        ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n        Fast Burst Images Denoising\n        ?\n        232:5\n        edge direction within a 5 ? 5 window. Then a LMMSE estimator described in Equation (2) is used on these pixels. For efficiency, we only apply the spatial fusion on texture pixels (p tex > 0.01).  The textureness probability p tex is computed by a sigmoid function 1/(1 + exp (?5 ? (g/? ? 3))), in which g is the maximum absolute difference between the pixel and its 4 neighbors, and ? is the estimated standard deviation of noise. For efficiency, we estimate ? by computing the standard deviation of pixels differences between the median image and the reference image within the flat (non-textured) areas. On these areas, the median image (generated in the coarse scale) is a good approximation to a clean version of the reference image. For real noise, we use an approach similar to [Liu et al. 2008] to divide the illuminance into 10 discrete bins and estimate the corresponding ? for each bin. Extension to patch. The idea of combining temporal and multiscale fusion can also be extended to the patch level for better denoising quality. Different from point-wise fusion, the LMMSE estimator for patches is applied in the frequency domain, which is similar to the Wiener filter used in the transform domain [Dabov et al. 2007b]. In addition, the patch-based LMMSE estimator provides overlapping estimates for every pixel, which need patch aggregation (along the temporal axis or spatial edge direction) to get the final fusion result. Algorithm validation. We perform a quantitative evaluation on a synthetic data set. The ground-truth clean images come from 68 images of BSD300 [Martin et al. 2001]. To simulate the camera motion, we reuse the estimated global homographies from the real data ( Figure 4 ) and randomly apply them on one of the clean images to generate a burst of images (10 frames). Then, we add Gaussian noise with various noise levels (? = 20 ? 60). Note that our synthetic data set ignores many key factors in real data: parallax, non-Gaussian noise, and non-rigid object motion. Ignoring these key factors may lead to incomplete conclusions. However, we still provide a preliminary evaluation here for reference and to help us gain a better understanding.  Figure 6(a) shows the average PSNRs of: average (baseline), optical flow + median filtering, VBM3D [Dabov et al. 2007a], BM4D [Maggioni et al. 2013], our method (with pixel fusion), and our method (with patch fusion). We applied the same global homography to all methods for better results. Overall, our method (with pixel fusion) performs comparably to VBM3D and BM4D (two state-of-the-art denoising methods) and better than averaging and optical flow at all noise levels. When the noise level increases, our method performs slightly worse than BM4D, but VBM3D drops more quickly than ours. This evaluation partially demonstrates the true power of our method in the presence of real camera motion and registration error. Keep in mind that our method is 2-3 orders of magnitude faster than VBM3D, optical flow, and BM4D. Figure 6(b) shows the running time of these methods on different image sizes. In addition, we can achieve the best results by extending our fusion to the patch level. Compared with two patch-based methods (VBM3D and BM4D), our patch-based fusion is still efficient (1-2 orders of magnitude faster). Figure 7 also demonstrates that patches perform better in both temporal and multi-scale fusion than pixels. The conclusion is consistent to previous work since patches can usually use more spatially correlated information than pixels. More interestingly, multiscale fusion as complementary to temporal pixel fusion plays an important role in our pixel-based method. It helps greatly reduce the gap between our pixel-based method and our patch-based method.\n        Global Align + Average Optical Flow + Median VBM3D BM4D\n        (a)\n        \n          dB\n          32 Our Method (Pixel) Our Method (Patch) 30 28\n        \n        PSNR 26 24 22 20 30 40 50 60 Noise Sigma (b) 10000 1000 (sec.) 100 10 Time 1 Running 0.1 0.2 1 2 5 8 0.01 Image Size (Mega-pixel)\n        \n          Figure 6: PSNR of different methods on the synthetic data. For VBM3D [Dabov et al. 2007a] and BM4D [Maggioni et al. 2013], we apply a pre-warp using the same estimated global homography for better results.\n        \n        Temporal (Pixel) Temporal + Multiscale (Pixel) Temporal (Patch) Temporal + Multiscale (Patch) 31 29 (dB) 27 PSNR 25 23 21 20 30 40 50 60 Noise Sigma\n        \n          Figure 7: PSNR comparison of our different fusions.\n        \n      \n      \n        4 Experiments\n        We acquired 20 sets of burst images on various content with 5 cameras, including 3 mobile phones, 1 DSLR camera, and 1 compact camera. Each burst contains 10 shots. All our results are generated with a set of fixed parameters and by pixel-based fusion. All original sequences and more results are provided on our webpage 4 .\n        \n          4.1 Comparisons\n          We compare our method with three point-wise methods (spatialtemporal filtering [Bennett and McMillan 2005], lucky imaging [Joshi and Cohen 2010], and optical flow [Liu 2009] + temporally median filtering), and two state-of-the-art patch-based methods (VBM3D [Dabov et al. 2007a] and BM4D [Maggioni et al. 2013]). The former two are based our own implementations, and optical flow and the latter two are from the authors. For all methods, we apply the same global homography estimated by our method to help them to obtain more reliable correspondences. Since some algorithms require a known noise variance, we try all possible noise levels and choose the result with the best visual quality through a balanced tradeoff between detail recovery and noise reduction. Static scene. The example in Figure 8 was captured by a HTC 802d Android phone. The motion is mainly caused by camera movement. The challenges in this case are on how to remove strong noise in the sky and recover building structures. As we can see, spatial-temporal filtering, VBM3D, and BM4D still leave certain noises in flat regions (e.g., sky area). The building structures (in Figure 8(b) (f)) are not well restored by VBM3D and BM4D. This is because, in the presence of structure noise, either the spatial-temporal bilateral\n          4 http://personal.ie.cuhk.edu.hk/ ?lz013/projects/BurstDenoising.html\n          ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n          232:6\n          ?\n          Z. Liu et al.\n          \n            \n          \n          (a) input reference image (b) spatial-temporal filtering (c) lucky imaging\n          \n            \n          \n          (f) BM4D (g) our result\n          \n            Figure 8: Static scene.\n            \n          \n        \n      \n      \n        (a) (b) (c)\n        \n          \n        \n      \n      \n        (a) (b) (c) (d)\n        \n          Figure 9: Portrait with small motion. (a) input images. (b) spatial-temporal filtering [Bennett and McMillan 2005] (c) lucky imaging [Joshi and Cohen 2010] (d) optical flow [Liu 2009] + median, (e) VBM3D [Dabov et al. 2007a], (f) BM4D [Maggioni et al. 2013], (g) our result.\n        \n        filter or patch matching has the risk to find mismatched correspondences which will eventually lead to undesired results. Overall, the results by optical flow, lucky imaging, and ours are comparable, with our result being slightly cleaner. Portrait with small motion. This is a typical scenario for taking a portrait photo in dark lighting. The example in Figure 9 was recorded by a JVC GC-PX10 camera. Spatial-temporal filtering, lucky imaging, and the optical flow method produced ?staircase? artifacts around the eyes ( Figure 9 (b)(c)(d)). This is because of small non-rigid motion of the subject. VBM3D and BM4D do not have this issue but blurred fine details on scarves ( Figure 9 (e)(f)). Our result achieves the best on both kinds of regions. Complex scene motion. Figure 10 and Figure 11 show two cases of complex dynamic scenes. The first example was captured by a Cannon EOS 500D with ISO 6400. Its noise level is relatively low. The later example was obtained by a Nokia Lumia920. As we can see from both examples, lucky imaging and optical flow (+ median filtering) based results contain noticeable ghosting while the VBM3D results are over-smoothed. BM4D is better than the former two methods but still leaves a certain amount of chrominance noise patterns on the background. In our solution, we can automatically choose pixel colors consistent to the reference frame on the dynamic regions and collect more consistent pixels on the static regions (e.g., cloth and door). As a result, our result strikes the best balance among removing noise, reconstructing fine details, and avoiding ghosting.\n        (d) optical flow + median (e) VBM3D\n        close-up of views\n      \n      \n        (d) (e) (f) (g)\n      \n      \n        (e) (f) (g)\n        4.2 More Results\n        Handling motion blur. During the capture, some individual frames (e.g., 10% ? 30% of the total frames) may be blurry due to sudden camera shake or object motion. Figure 12 shows such an example captured with an iPhone 5S. We examined what would happen if a blurry frame were selected as the reference frame. To know this, we separately select frame 4 (sharp) and frame 5 (blurry) as the reference frame and generate two results. Figure 12 (b) and (d) show that our method is insensitive to the selection. This is because our method is capable of finding consistent pixels from the the majority of frames. A similar method was proposed in video deblurring [Cho et al. 2012], which found similar patches from sharp frames for deblurring. Handling extreme low light. Figure 13 is a sequence captured by an iPhone 4S under an extreme low-light condition. Since the inputs are extremely dark, we preprocess the inputs by boosting the brightness (applying a shadow/highlight adjustment). While the noise after the boosting is very strong, our method still managed to produce a clean image with fine details (thin wires in the air and steel tower on the left). Handling large occlusions. Figure 14 shows a sequence with a large, fast moving foreground (person). The consistent pixel map in the figure reveals how the mechanisms in our algorithm can reliably deal with (fast) large occlusion.\n        ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n        Fast Burst Images Denoising\n        ?\n        232:7\n        \n          \n        \n      \n      \n        (a) input burst images (b) reference image (c) spatial-temporal filtering (d) lucky imaging (e) optical flow + median\n        \n          \n        \n      \n      \n        (f) VBM3D (g) BM4D (h) our result\n         Figure 10 : Complex scene motion I.\n        \n          \n        \n        (a) input burst images (b) reference image\n        \n          \n        \n        (e) optical flow + median (f) VBM3D\n        \n          Figure 11: Complex scene motion II.\n        \n        We run our method on an Intel i5 3.2GHZ machine with 16G RAM. Our unoptimized C++ implementation (single core, no SSE SIMD acceleration) takes 920 ms on average to process 10 frames of 5Mpixel image. Specifically, our method takes 82ms, 177ms, 51ms, 30ms, 253ms, 328ms to build pyramid, extract and match sparse features, estimate homography flow, select consistent pixels, run temporal fusion, and execute multi-scale fusion. Our prototype on a smartphone (Nokia Lumia 920) costs about 4.7 seconds on average, without using multi-core or NEON instructions or GPU acceleration. As our solution is mainly based on point-wise operations, we expect it can be significantly accelerated. Table 1 further demonstrates the processing time of different methods on the same machine for Figure 8 , 9, 10, and 11.\n      \n      \n        5 Concluding Remarks\n        In an image burst, we expect camera motion from hand shake and small/moderate motion of the main subject(s). Our method is not designed for handling dramatic motion (e.g., in sports), or denois-\n      \n      \n        close-up of views\n        (c) spatial-temporal filtering (d) lucky imaging\n        (g) BM4D (h) our result\n        \n          \n            \n              \n                \n                   4.3 Time complexity.\n                  \n                   Figure 8\n                   Figure 9\n                   Figure 10\n                   Figure 11\n                \n              \n              \n                \n                  \n                   Size\n                   1520 ? 2688\n                   1600 ? 1200\n                   2352 ? 1568\n                   1280 ? 720\n                \n                \n                  \n                   (c)\n                   337.85\n                   139.88\n                   304.75\n                   81.54\n                \n                \n                  \n                   (d)\n                   74.06\n                   40.27\n                   64.94\n                   28.46\n                \n                \n                  \n                   (e)\n                   577.39\n                   298.73\n                   538.02\n                   126.63\n                \n                \n                  \n                   (f)\n                   214.83\n                   123.42\n                   198.36\n                   53.92\n                \n                \n                  \n                   (g)\n                   1867.27\n                   1019.31\n                   1576.49\n                   517.83\n                \n                \n                  \n                   (h)\n                   0.80\n                   0.48\n                   0.77\n                   0.23\n                \n              \n            \n          \n          4.3 Time complexity.\n           Figure 8 Figure 9 Figure 10 Figure 11 Size 1520 ? 2688 1600 ? 1200 2352 ? 1568 1280 ? 720 (c) 337.85 139.88 304.75 81.54 (d) 74.06 40.27 64.94 28.46 (e) 577.39 298.73 538.02 126.63 (f) 214.83 123.42 198.36 53.92 (g) 1867.27 1019.31 1576.49 517.83 (h) 0.80 0.48 0.77 0.23\n          Table 1: Processing times (sec.) of different denoising methods. (c) spatial-temporal filtering, (d) lucky imaging, (e) optical flow + median filtering, (f) VBM3D, (g) BM4D and (h) our method.\n        \n        ing a general video. When the motion between two frames cannot be well represented by our homography flow, such as scene transition or fast camera panning, or even non-rigid deformation (e.g., water wave motion, flag waving), our approach will break.  Besides, there are two cases in which our consistent pixels selection may fail. The first case is when motion blur caused by dynamic objects appears on a majority of frames (more than half of all frames). On the dynamic regions, our pixel selection would automatically\n        ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n        232:8\n        ?\n        Z. Liu et al.\n        \n          \n        \n        (c) frame 5 (reference) (d) our result (use frame 5) close-up views\n        \n          Figure 12: A sequence with motion blurs in individual frames. We respectively use the 4th frame (sharp) or the 5th frame (blurred) as the reference frame. Our solution produces similar results for both.\n          \n        \n      \n      \n        (a) (b) (c)\n        \n          Figure 13: A sequence captured under an extreme low light condition. (a) input image sequence. (b) reference frame after brightness amplification. (c) our result.\n        \n        choose the reference-based strategy. If the reference frame contains blur, our result would retain the blur effect. But if a sharp frame is chosen as the reference, the issue can be avoided. Figure 15 (a) shows such an example. Therefore, we need a better strategy for selecting the reference frame. Besides, fast moving objects would be automatically removed by the median-based strategy for aggressive denoising. To avoid this issue, we may provide another option that allows users to choose the reference frame and constrain the reference region for pixels selection. The second case is when different moving objects and the background may have similar colors in the same pixel locations. Our pixels selection algorithm relies on per-pixel color difference, which is too weak to distinguish such objects. Figure 15 (b) shows an example. Different moving persons have very similar color regions and such ambiguous regions (indicated by highlight box) occur in a majority of frames (i.e., more than half of all frames). Finally, it will lead to ghosting because our selection wrongly regard it as the background. This issue also appears in ghost-free HDR reconstruction [Granados et al. 2013], which requires interactions to exclude these ambiguous regions. Despite the above issues, we believe that our highly efficient solution is practical enough to be deployed for improving the photo experience of users in a broad range of lighting conditions.\n      \n      \n        Acknowledgements\n        We thank all the reviewers for their helpful discussions, Steve Lin and Jiangyu Liu for their help in proofreading.\n        \n          \n          Figure 14: A sequence with large occlusions (moving person). Top: inputs. Bottom: the left map shows the number of selected consistent pixels, and the right image is our result.\n        \n        (a)\n        \n          \n        \n        input images our results generated by different reference\n        (b)\n        \n          \n          Figure 15: Two failure cases.\n        \n        input burst images our results (a) motion blurs on dynamic objects (on a majority of frames). (b) ambiguous regions (with similar colors) from different moving objects.\n      \n      \n        References\n        \n          A DAMS , A., G ELFAND , N., D OLSON , J., AND L EVOY , M. 2009. Gaussian kd-trees for fast high-dimensional filtering. ACM Trans. Graph. (Proc. of SIGGRAPH) 28, 3.\n          B ARNES , C., S HECHTMAN , E., F INKELSTEIN , A., AND G OLD MAN , D. B. 2009. Patchmatch: A randomized correspondence algorithm for structural image editing. SIGGRAPH 28, 3.\n          B ENNETT , E. P., AND M C M ILLAN , L. 2005. Video enhancement using per-pixel virtual exposures. ACM Trans. Graph. (Proc. of SIGGRAPH) 24, 3, 845?852.\n          B RONSHTEIN , I. N., AND S EMENDYAYEV , K. A. 1997. Handbook of Mathematics. Springer-Verlag, New York, NY, USA.\n          B ROX , T., B RUHN , A., P APENBERG , N., AND W EICKERT , J. 2004. High accuracy optical flow estimation based on a theory for warping. In Proc. ECCV.\n          B UADES , A., C OLL , B., AND M OREL , J.-M. 2005. A non-local algorithm for image denoising. In Proc. CVPR.\n          B UADES , A., L OU , Y., M OREL , J.-M., AND T ANG , Z. 2009. A note on multi-image denoising. In In Proceedings of the International Workshop on Local and Non-Local Approximation (LNLA) in Image Processing.\n          B UADES , A., L OU , Y., M OREL , J.-M., AND T ANG , Z. 2010. Multi image noise estimation and denoising. In HAL.\n          C AI , J. F., J I , H., L IU , C., AND S HEN , Z. 2009. Blind motion deblurring using multiple images. J. Comput. Physics 228, 14, 5057?5071.\n          C ALONDER , M., L EPETIT , V., S TRECHA , C., AND F UA , P. 2010. Brief: binary robust independent elementary features. In Proc. ECCV.\n          C HATTERJEE , P., J OSHI , N., K ANG , S. B., AND M ATSUSHITA , Y. 2011. Noise suppression in low-light images through joint denoising and demosaicing. In Proc. CVPR.\n          C HEN , J., AND T ANG , C.-K. 2007. Spatio-temporal markov random field for video denoising. In Proc. CVPR.\n          C HEN , J., T ANG , C.-K., AND W ANG , J. 2009. Noise brush: Interactive high quality image-noise separation. ACM Trans. Graph. (Proc. of SIGGRAPH ASIA) 28, 5.\n          C HEN , X., K ANG , S. B., Y ANG , J., AND Y U , J. 2013. Fast patchbased denoising using approximated patch geodesic paths. In Proc. CVPR.\n          C HO , S., W ANG , J., AND L EE , S. 2012. Vdeo deblurring for hand-held cameras using patch-based synthesis. Proc. ACM SIGGRAPH 31, 4, 64:1?64:9.\n          D ABOV , K., F OI , A., AND E GIAZARIAN , K. 2007. Video denoising by sparse 3d transform-domain collaborative filtering. In Proc. European Signal Process. Conf., EUSIPCO.\n          D ABOV , K., F OI , A., E GIAZARIAN , K., AND E GIAZARIAN , K. 2007. Image denoising by sparse 3-d transform-domain collaborative filtering. IEEE Trans. on Image Processing 16, 8, 2080? 2095.\n          F ARSIU , S., R OBINSON , M. D., E LAD , M., AND M ILANFAR , P. 2004. Fast and robust multiframe super resolution. IEEE Trans. on Image Processing 13, 10, 1327?1344.\n          G ALLO , O., G ELFAND , N., C HEN , W., T ICO , M., AND P ULLI , K. 2009. Artifact-free high dynamic range imaging.\n          G ONZALEZ , R. C., AND W OODS , R. E. 2007. Digital Image Processing. Prentice Hall, 3rd edition.\n          G RANADOS , M., K IM , K. I., T OMPKIN , J., AND T HEOBALT , C. 2013. Automatic noise modeling for ghost-free hdr reconstruction. ACM Trans. Graph. (Proc. of SIGGRAPH ASIA) 32, 6, 1?10.\n          G RUNDMANN , M., K WATRA , V., C ASTRO , D., AND E SSA , I. 2012. Calibration-free rolling shutter removal. In Proc. ICCP.\n          H ARRIS , C., AND S TEPHENS , M. 1988. A combined corner and edge detector. In In Proc. of Fourth Alvey Vision Conference.\n          H ARTLEY , R., AND Z ISSERMAN , A. 2003. Multiple View Geometry in Computer Vision, 2 ed. Cambridge University Press, New York, NY, USA.\n          J ACOBS , D. E., B AEK , J., AND L EVOY , M. 2012. Focal stack compositing for depth of field control. In Stanford Computer Graphics Laboratory Technical Report.\n          J OSHI , N., AND C OHEN , M. F. 2010. Seeing mt. rainier: lucky imaging for multi-image denoising, sharpening, and haze removal. In Proc. ICCP.\n          K ALANTARI , N. K., S HECHTMAN , E., B ARNES , C., D ARABI , S., G OLDMAN , D. B., AND S EN , P. 2013. Patch-based high dynamic range video. ACM Trans. Graph. (Proc. of SIGGRAPH ASIA) 32, 6, 202:1?202:8.\n          L EVIN , A., AND N ADLER , B. 2011. Natural image denoising: Optimality and inherent bounds. In Proc. CVPR, 2833?2840.\n          L IU , C., AND F REEMAN , W. T. 2010. A high-quality video denoising algorithm based on reliable motion estimation. Proc. ECCV, 706?719.\n          L IU , C., S ZELISKI , R., K ANG , S. B., Z ITNICK , C. L., AND F REEMAN , W. T. 2008. Automatic estimation and removal of noise from a single image.\n          L IU , S., Y UAN , L., T AN , P., AND S UN , J. 2013. Bundled camera paths for video stabilization. ACM Trans. Graph. (Proc. of SIGGRAPH) 32, 4, 78:1?78:10.\n          L IU , C. 2009. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. PhD thesis, Massachusetts Institute of Technology.\n          M AGGIONI , M., K ATKOVNIK , V., E GIAZARIAN , K., AND F OI , A. 2013. A nonlocal transform-domain filter for volumetric data denoising and reconstruction. IEEE Trans. on Image Processing, 1, 119?133.\n          M ARTIN , D., F OWLKES , C., T AL , D., AND M ALIK , J. 2001. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proc. ICCV.\n          P ARIS , S., AND D URAND , F. 2009. A fast approximation of the bilateral filter using a signal processing approach. International Journal of Computer Vision 81, 24?52.\n          P ORTILLA , J., S TRELA , V., W AINWRIGHT , M. J., AND S IMON CELLI , E. P. 2003. Image denoising using scale mixtures of gaussians in the wavelet domain. IEEE Trans. on Image Processing 12, 11, 1338?1351.\n          R EINHARD , E., W ARD , G., P ATTANAIK , S. N., D EBEVEC , P. E., AND H EIDRICH , W. 2010. High Dynamic Range Imaging Acquisition, Display, and Image-Based Lighting (2. ed.). Academic Press.\n          R OTH , S., AND B LACK , M. J. 2005. Fields of experts: a framework for learning image priors. In Proc. CVPR.\n          S EN , P., K ALANTARI , N. K., Y AESOUBI , M., D ARABI , S., G OLDMAN , D. B., AND S HECHTMAN , E. 2012. Robust patchbased hdr reconstruction of dynamic scenes. ACM Trans. Graph. (Proc. of SIGGRAPH) 31, 6, 203:1?203:11.\n          T ICO , M. 2008. Multiframe image denoising and stabilization. In EUSIPCO.\n          T OMASI , C., AND M ANDUCHI , R. 1998. Bilateral filtering for gray and color images. In Proc. ICCV, 839?846.\n          V IOLA , P., AND J ONES , M. 2001. Robust real-time object detection. In International Journal of Computer Vision.\n          Z HANG , M., AND G UNTURK , B. K. 2008. Multiresolution bilateral filtering for image denoising. IEEE Trans. on Image Processing 17, 12, 2324?2333.\n          Z HANG , L., AND W U , X. 2005. Color demosaicking via directional linear minimum mean square-error estimation. TIP 14, 12, 2167?2178.\n          Z HANG , L., V ADDADI , S., J IN , H., AND N AYAR , S. K. 2009. Multiple view image denoising. In Proc. CVPR, 1542?1549.\n          Z ONTAK , M., M OSSERI , I., AND I RANI , M. 2013. Separating signal from noise using patch recurrence across scales. In Proc. CVPR.\n        \n        ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n        Fast Burst Images Denoising\n        ?\n        232:9\n        ACM Transactions on Graphics, Vol. 33, No. 6, Article 232, Publication Date: November 2014\n      \n    \n  ",
  "resources" : [ ]
}