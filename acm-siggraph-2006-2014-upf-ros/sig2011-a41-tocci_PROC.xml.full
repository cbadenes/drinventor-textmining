{
  "uri" : "sig2011-a41-tocci_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a41-tocci_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "A Versatile HDR Video Production System",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Michael D.-Tocci",
      "name" : "Michael D.",
      "surname" : "Tocci"
    }, {
      "uri" : "http://drinventor/Chris-Kiser",
      "name" : "Chris",
      "surname" : "Kiser"
    }, {
      "uri" : "http://drinventor/Nora-Tocci",
      "name" : "Nora",
      "surname" : "Tocci"
    }, {
      "uri" : "http://drinventor/Pradeep-Sen",
      "name" : "Pradeep",
      "surname" : "Sen"
    } ]
  },
  "bagOfWords" : [ "bd63fb9c6a4bc8e01f1667efd358b364535a0efa9fc5b6a5d9320a0466834f46", "p0q", "10.1145", "1964921.1964936", "name", "identification", "possible", "Versatile", "HDR", "Video", "Production", "System", "Michael", "D.", "Tocci", "1,2", "Chris", "Kiser", "1,2,3", "Nora", "Tocci", "Contrast", "Optical", "Design", "Engineering", "Inc.", "University", "New", "Mexico", "Figure", "hdr", "image", "acquire", "we", "proposed", "system", "left", "we", "show", "final", "image", "acquire", "we", "camera", "merge", "propose", "algorithm", "inset", "photo", "show", "individual", "ldr", "image", "from", "high", "medium", "low-exposure", "sensor", "respectively", "although", "high", "Dynamic", "Range", "-lrb-", "HDR", "-rrb-", "imaging", "have", "be", "subject", "significant", "research", "over", "past", "fifteen", "year", "goal", "acquire", "cinema-quality", "HDR", "image", "fast-moving", "scene", "use", "available", "component", "have", "yet", "be", "achieve", "work", "we", "present", "optical", "architecture", "HDR", "imaging", "allow", "simultaneous", "capture", "high", "medium", "low-exposure", "image", "three", "sensor", "high", "fidelity", "efficient", "use", "available", "light", "we", "also", "present", "hdr", "merge", "algorithm", "complement", "architecture", "which", "avoid", "undesired", "artifact", "when", "large", "exposure", "difference", "between", "image", "we", "implement", "prototype", "high-definition", "hdr-video", "system", "we", "present", "still", "frame", "from", "acquire", "HDR", "video", "tonemapp", "various", "technique", "cr", "category", "i.", "4.1", "-lsb-", "image", "processing", "computer", "Vision", "-rsb-", "digitization", "image", "capture?radiometry", "keyword", "hdr", "video", "merge", "hdr", "image", "Links", "dl", "pdf", "introduction", "extension", "dynamic", "range", "digital", "image", "have", "be", "subject", "significant", "research", "both", "academia", "industry", "despite", "all", "previous", "work", "however", "currently", "readilyimplemented", "solution", "capture", "high-quality", "hdr", "video", "fast-moving", "scene", "paper", "we", "describe", "end-to-end", "system", "capture", "HDR", "video", "high", "pixel", "fidelity", "use", "lightefficient", "optical", "architecture", "fit", "single", "hand-held", "unit", "ACM", "Reference", "Format", "Tocci", "M.", "Kiser", "C.", "Tocci", "N.", "Sen", "P.", "2011", "Versatile", "HDR", "Video", "Production", "System", "ACM", "Trans", "graph", "30", "Article", "41", "-lrb-", "July", "2011", "-rrb-", "page", "dous", "10.1145", "1964921.1964936", "http://doi.acm.org/10.1145/1964921.1964936", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "212", "869-0481", "permissions@acm.org", "2011", "ACM", "0730-0301/2011", "07-art41", "10.00", "DOI", "10.1145", "1964921.1964936", "http://doi.acm.org/10.1145/1964921.1964936", "Pradeep", "Sen", "2,3", "Advanced", "Graphics", "Lab", "we", "proposed", "system", "simple", "use", "only", "off-the-shelf", "technology", "flexible", "term", "sensor", "use", "specifically", "we", "HDR", "optical", "architecture", "-lrb-", "-rrb-", "capture", "optically-aligned", "multipleexposure", "image", "simultaneously", "do", "need", "image", "manipulation", "account", "motion", "-lrb-", "-rrb-", "extend", "dynamic", "range", "available", "image", "sensor", "-lrb-", "over", "photographic", "stop", "we", "current", "prototype", "-rrb-", "-lrb-", "-rrb-", "inexpensive", "implement", "-lrb-", "-rrb-", "utilize", "single", "standard", "camera", "lens", "-lrb-", "-rrb-", "efficiently", "use", "light", "from", "lens", "complement", "we", "system", "we", "also", "propose", "novel", "hdr", "imagemerge", "algorithm", "-lrb-", "-rrb-", "combine", "image", "separate", "more", "than", "stop", "exposure", "-lrb-", "-rrb-", "spatially", "blend", "pre-demosaiced", "pixel", "datum", "reduce", "unwanted", "artifact", "-lrb-", "-rrb-", "produce", "HDR", "image", "radiometrically", "correct", "-lrb-", "-rrb-", "use", "highest-fidelity", "-lrb-", "lowest", "quantized-noise", "-rrb-", "pixel", "datum", "available", "we", "demonstrate", "work", "prototype", "present", "image", "video", "acquire", "system", "previous", "work", "2.2", "algorithm", "merge", "hdr", "image", "common", "method", "merge", "multiple", "ldr", "image", "single", "composite", "HDR", "image", "one", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "which", "first", "solve", "camera", "response", "curve", "translate", "pixel", "value", "log", "scene", "irradiance", "blend", "irradiance", "from", "image", "together", "during", "merging", "process", "algorithm", "combine", "value", "from", "every", "exposure", "weighting", "each", "contribution", "triangle", "filter", "fall", "off", "pixel", "value", "approach", "cutoff", "saturation", "peak", "middle", "idea", "give", "more", "weight", "pixel", "work", "range", "camera", "less", "one", "near", "extrema", "camera?s", "operate", "range", "we", "describe", "Sec", "however", "approach", "can", "suffer", "from", "undesirable", "artifact", "when", "apply", "widely-separated", "ldr", "image", "due", "blending", "between", "exposure", "follow", "work", "Debevec", "Malik", "other", "researcher", "have", "propose", "different", "weighting", "function", "merge", "differentlyexpose", "ldr", "image", "reduce", "noise", "improve", "result", "-lrb-", "e.g.", "-lsb-", "Mitsunaga", "Nayar", "1999", "Robertson", "et", "al.", "2003", "Kao", "2008", "Granados", "et", "al.", "2010", "-rsb-", "-rrb-", "approach", "typically", "work", "each", "pixel", "final", "HDR", "image", "independently", "use", "only", "information", "contain", "within", "respective", "pixel", "each", "ldr", "image", "unlike", "approach", "we", "propose", "use", "additional", "information", "available", "neighborhood", "pixel", "reduce", "noise", "we", "final", "irradiance", "estimate", "finally", "other", "have", "present", "algorithm", "fuse", "ldr", "image", "together", "without", "explicitly", "create", "hdr", "image", "first", "-lrb-", "e.g.", "-lsb-", "Agarwala", "et", "al.", "2004", "Mertens", "et", "al.", "2008", "-rsb-", "-rrb-", "method", "do", "produce", "true", "radiometrically-correct", "hdr", "image", "so", "result", "can", "incorporate", "hdr", "production", "workflow", "2.1", "HDR", "Acquisition", "system", "process", "capture", "hdr", "image", "have", "be", "focus", "work", "dozen", "researcher", "hundred", "artist", "photographer", "result", "many", "publish", "papers", "patent", "describe", "method", "system", "capture", "hdr", "image", "because", "space", "limit", "we", "focus", "only", "principal", "technology", "currently", "available", "HDR", "video", "refer", "interested", "reader", "text", "subject", "-lrb-", "e.g.", "-lsb-", "Myszkowski", "et", "al.", "2008", "-rsb-", "-rrb-", "more", "information", "simplest", "approach", "HDR", "imaging", "involve", "take", "series", "image", "different", "exposure", "time", "-lrb-", "e.g.", "-lsb-", "Mann", "Picard", "1995", "Debevec", "Malik", "1997", "-rsb-", "-rrb-", "although", "method", "work", "well", "static", "scene", "well-suited", "video", "because", "different", "moment", "time", "exposure", "length", "each", "photograph", "which", "result", "vary", "amount", "motion", "blur", "other", "timerelated", "effect", "nevertheless", "researcher", "have", "extend", "approach", "video", "capture", "frame", "alternate", "bright", "dark", "exposure", "-lsb-", "Ginosar", "et", "al.", "1992", "Kang", "et", "al.", "2003", "-rsb-", "use", "roll", "shutter", "vary", "exposure", "-lsb-", "Unger", "Gustavson", "2007", "Krymski", "2008", "-rsb-", "approach", "require", "image", "manipulation", "register", "image", "which", "also", "introduce", "artifact", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "41:2", "M.", "Tocci", "et", "al.", "he", "sensor", "0.33", "Figure", "0.33", "0.33", "ME", "prism", "sensor", "0.33", "0.0272", "0.0016", "LE", "sensor", "traditional", "beamsplitting", "hdr", "optical", "system", "here", "beamsplitt", "prism", "break", "up", "light", "three", "part", "one", "each", "sensor", "fit", "different", "filter", "design", "use", "absorptive", "filter", "like", "one", "make", "inefficient", "use", "light", "other", "researcher", "have", "propose", "new", "camera", "sensor", "HDR", "imaging", "some", "approach", "place", "array", "neutral-density", "filter", "over", "individual", "pixel", "sensor", "vary", "amount", "absorption", "-lrb-", "e.g.", "-lsb-", "Nayar", "Mitsunaga", "2000", "-rsb-", "-rrb-", "which", "can", "require", "complex", "demosaicing", "algorithm", "approach", "also", "wasteful", "light", "enter", "camera", "sensor", "have", "filter", "pattern", "four", "differently-exposed", "pixel", "-lrb-", "one", "four", "fully", "expose", "-rrb-", "only", "pixel", "would", "receive", "full", "exposure", "level", "from", "scene", "other", "propose", "HDR", "sensor", "have", "unique", "response", "light", "either", "adapt", "sensitivity", "-lrb-", "e.g.", "-lsb-", "Nayar", "Branzoi", "2003", "-rsb-", "-rrb-", "measure", "pixel", "saturation", "time", "-lrb-", "e.g.", "-lsb-", "Brajovic", "Kanade", "1996", "-rsb-", "-rrb-", "have", "logarithmic", "response", "mimic", "human", "eye", "-lrb-", "e.g.", "-lsb-", "Seger", "et", "al.", "1999", "-rsb-", "-rrb-", "primary", "problem", "all", "approach", "require", "production", "new", "type", "camera", "sensor", "although", "commercial-scale", "production", "sensor", "may", "someday", "realize", "currently", "expensive", "manufacture", "render", "method", "unusable", "most", "researcher", "today", "we", "propose", "architecture", "other", "hand", "perform", "HDR", "imaging", "independent", "sensor", "use", "which", "make", "realizable", "use", "today?s", "technology", "allow", "we", "adopt", "better", "sensor", "technology", "-lrb-", "low-light", "level", "response", "faster", "framerate", "wider", "spectral", "response", "etc.", "-rrb-", "develop", "future", "approach", "similar", "we", "own", "light", "camera", "split", "pyramid-shaped", "mirror", "refract", "prism", "redirect", "toward", "set", "sensor", "fit", "absorptive", "filter", "produce", "image", "different", "exposure", "-lrb-", "e.g.", "Harvey", "-lsb-", "1998", "-rsb-", "Aggarwal", "Ahuja", "-lsb-", "2001", "2004", "-rsb-", "Wang", "et", "al.", "-lsb-", "2005", "-rsb-", "-rrb-", "design", "previous", "system", "all", "suffer", "from", "parallax", "error", "due", "fact", "image-forming", "beam", "split", "spatially-distinct", "subsection", "each", "individual", "sensor", "look", "through", "camera", "lens", "from", "slightly", "different", "angle", "show", "recent", "work", "handheld", "plenoptic", "camera", "-lrb-", "e.g.", "-lsb-", "Ng", "et", "al.", "2005", "-rsb-", "-rrb-", "provide", "each", "sensor", "slightly", "different", "information", "which", "significantly", "affect", "imaging", "scene", "close", "camera", "previous", "spatial-beamsplitting", "method", "also", "wasteful", "light", "absorptive", "filter", "use", "achieve", "dynamic", "range", "allow", "only", "fraction", "incoming", "light", "sensor", "Watts", "radiative", "power", "enter", "aperture", "camera", "three-way", "system", "show", "fig.", "-lrb-", "configure", "same", "dynamic", "range", "ours", "-rrb-", "allow", "only", "0.3622", "Watts", "sensor", "waste", "almost", "available", "light", "Aggarwal", "Ahuja", "-lsb-", "2004", "-rsb-", "point", "out", "possible", "vary", "amount", "light", "each", "sensor", "move", "beamsplitt", "prism", "away", "from", "optical", "axis", "instead", "use", "filter", "effectively", "change", "size", "shape", "aperture", "stop", "each", "sensor", "exacerbate", "problem", "each", "sensor", "get", "different", "view", "scene", "furthermore", "shiftedoptical-axis", "spatial-beamsplitting", "method", "easily", "integrate", "standard", "camera", "lens", "require", "either", "custom", "lens", "manufacture", "lens", "modification", "work", "correctly", "another", "option", "split", "incoming", "light", "beamsplitter", "prior", "lens", "example", "McGuire", "et", "al.", "-lsb-", "2007", "-rsb-", "present", "design", "tool", "create", "efficient", "beamsplitt", "tree", "separate", "lens", "each", "sensor", "show", "example", "HDR", "imaging", "same", "concept", "demonstrate", "soviet", "Montage", "Productions", "-lsb-", "Cole", "Safai", "2010", "-rsb-", "which", "can", "implement", "3d", "film", "rig", "intraocular", "distance", "zero", "use", "beamsplitter", "provide", "different", "light", "transmission", "two", "identical", "lens", "one", "each", "sensor", "two", "lens", "must", "perfectly", "match", "however", "zoom", "focus", "iris-tracking", "can", "difficult", "maintain", "between", "they", "addition", "put", "beamsplitter", "front", "camera", "lens", "place", "limit", "field", "view", "finally", "unclear", "how", "system", "could", "develop", "single", "hand-held", "unit", "we", "system", "place", "beamsplitter", "behind", "single", "camera", "lens", "so", "do", "suffer", "from", "limitation", "finally", "early", "prototype", "hdr", "system", "industry", "eventually", "intend", "commercial", "use", "spheronvr", "hdrv", "camera", "-lsb-", "spheron", "vr", "2011", "-rsb-", "however", "method", "achieve", "HDR", "capture", "have", "be", "publish", "while", "all", "system", "we", "mention", "section", "capable", "produce", "hdr", "video", "date", "method", "produce", "high-quality", "hdr", "video", "have", "be", "demonstrate", "robust", "yet", "simple", "enough", "readily", "introduce", "wide", "commercial", "audience", "implement", "modern", "optics", "laboratory", "goal", "work", "present", "system", "efficient", "Optical", "Architecture", "HDR", "Video", "we", "optical", "architecture", "base", "beamsplitter", "located", "between", "camera", "lens", "sensor", "which", "have", "be", "use", "previous", "HDR", "camera", "design", "well", "3-sensor", "color-splitting", "camera", "-lsb-", "Kell", "Sziklai", "1951", "-rsb-", "like", "previous", "method", "we", "optical", "system", "use", "set", "partially-reflecting", "surface", "split", "light", "from", "single", "photographic", "lens", "so", "focus", "onto", "three", "optical", "splitting", "prism", "use", "3ccd", "camera", "employ", "dichroic", "filter", "split", "incoming", "light", "from", "camera", "lens", "red", "green", "blue", "portion", "so", "each", "color", "image", "onto", "its", "own", "image", "sensor", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "Versatile", "HDR", "Video", "Video", "Production", "System", "41:3", "me", "sensor", "0.075", "beam", "splitter", "-lrb-", "94/6", "-rrb-", "he", "0.92", "sensor", "0.0044", "LE", "sensor", "beam", "splitter", "-lrb-", "92/8", "-rrb-", "Figure", "illustration", "we", "optical", "architecture", "we", "also", "use", "beamsplitter", "between", "lens", "sensor", "key", "difference", "we", "re-use", "optical", "path", "improve", "we", "light", "efficiency", "end", "99.96", "light", "enter", "aperture", "arrive", "sensor", "light", "efficiency", "important", "all", "imaging", "application", "imaging", "sensor", "simultaneously", "we", "architecture", "however", "light", "direct", "back", "through", "one", "beamsplitter", "second", "time", "three", "sub-image", "split", "red", "green", "blue", "instead", "optically", "identical", "except", "light", "level", "design", "show", "fig.", "allow", "we", "capture", "HDR", "image", "use", "most", "light", "enter", "camera", "we", "use", "term", "high", "medium", "low", "exposure", "-lrb-", "he", "ME", "LE", "respectively", "-rrb-", "refer", "sensor", "base", "amount", "light", "each", "one", "receive", "optical", "splitting", "system", "we", "current", "implementation", "use", "two", "uncoated", "2-micron", "thick", "plastic", "beamsplitter", "which", "rely", "fresnel", "reflection", "air/plastic", "interface", "so", "actual", "transmittance/reflectance", "-lrb-", "t/r", "-rrb-", "value", "function", "angle", "we", "arrangement", "first", "beamsplitter", "45", "angle", "have", "approximate", "t/r", "ratio", "92/8", "which", "mean", "92", "light", "from", "camera", "lens", "transmit", "through", "first", "beamsplitter", "focus", "directly", "onto", "high-exposure", "-lrb-", "he", "-rrb-", "sensor", "beamsplitter", "reflect", "light", "from", "lens", "upward", "show", "fig.", "toward", "second", "uncoated", "beamsplitter", "which", "have", "same", "optical", "property", "first", "position", "90", "angle", "light", "path", "have", "approximate", "t/r", "ratio", "94/6", "total", "light", "reflect", "upward", "94", "-lrb-", "7.52", "total", "light", "-rrb-", "transmit", "through", "second", "beamsplitter", "focus", "onto", "medium-exposure", "-lrb-", "me", "-rrb-", "sensor", "other", "upward-reflected", "light", "-lrb-", "0.48", "total", "light", "-rrb-", "reflect", "back", "down", "second", "beamsplitter", "toward", "first", "one", "-lrb-", "which", "again", "45", "-rrb-", "through", "which", "92", "-lrb-", "0.44", "total", "light", "-rrb-", "transmit", "focus", "onto", "low-exposure", "-lrb-", "le", "-rrb-", "sensor", "arrangement", "he", "ME", "LE", "sensor", "capture", "image", "92", "7.52", "0.44", "total", "light", "gather", "camera", "lens", "respectively", "therefore", "he", "me", "exposure", "separate", "12.2", "-lrb-", "3.61", "stop", "-rrb-", "me", "le", "separate", "17.0", "-lrb-", "4.09", "stop", "-rrb-", "which", "mean", "configuration", "design", "extend", "dynamic", "range", "sensor", "7.7", "stop", "beamsplitter", "arrangement", "make", "we", "design", "light", "efficient", "negligible", "0.04", "total", "light", "gather", "lens", "waste", "also", "allow", "all", "three", "sensor", "see", "same", "scene", "so", "all", "three", "image", "optically", "identical", "except", "light", "level", "course", "ME", "image", "have", "undergo", "odd", "number", "reflection", "so", "flip", "left-right", "compare", "other", "image", "fix", "easily", "software", "three", "sensor", "gen-locked", "capture", "perfectly", "synchronize", "video", "frame", "identical", "exposure", "time", "since", "t/r", "also", "dependent", "wavelength", "light", "we", "calculate", "t/r", "value", "full", "visible", "spectrum", "integrate", "over", "filter", "spectrum", "Bayer", "pattern", "arrive", "separate", "t/r", "value", "each", "color", "channel", "use", "we", "design", "implementation", "simplify", "discussion", "paper", "however", "we", "simply", "state", "single", "average", "value", "transmittance", "he", "sensor", "5.45", "mm", "41", "mm", "exit", "pupil", "115", "mm", "lens", "figure", "Scale", "drawing", "optical", "path", "first", "sensor", "since", "uncoated", "beamsplitter", "like", "ours", "can", "vary", "transmittance", "function", "angle", "we", "examine", "exact", "geometrical", "configuration", "we", "system", "f/2", ".8", "determine", "range", "transmittance", "value", "across", "we", "sensor", "case", "46.4", "43.7", "which", "result", "transmittance", "value", "91.85", "92.38", "respectively", "3.1", "analysis", "Optical", "System", "because", "exact", "transmission/reflection", "property", "we", "beamsplitter", "vary", "angle", "we", "examine", "how", "might", "vary", "over", "area", "sensor", "simulate", "propose", "optical", "architecture", "ZEMAX", "-lsb-", "2011", "-rsb-", "calculate", "range", "transmittance", "value", "function", "angle", "we", "examine", "largest", "angular", "variation", "possible", "pellicle", "beamsplitter", "approach", "place", "beamsplitter", "outside", "lens", "optical", "tree", "McGuire", "et", "al.", "-lsb-", "2007", "-rsb-", "can", "have", "large", "range", "incident", "angle", "which", "result", "significant", "variation", "transmission", "over", "field", "view", "unlike", "approach", "we", "system?s", "internal", "beamsplitter", "receive", "light", "much", "smaller", "range", "field", "angle", "because", "geometrical", "configuration", "system", "show", "scale", "Fig.", "we", "case", "top-left", "bottom-right", "corner", "point", "sensor", "have", "chief-ray", "angle", "pellicle", "46.4", "43.7", "difference", "2.7", "f/2", ".8", "each", "two", "point", "receive", "10", "cone", "ray", "from", "lens", "show", "blue", "red", "-lrb-", "cone", "constant", "angle", "over", "entire", "sensor", "-rrb-", "we", "calculate", "transmittance", "beamsplitter", "integrate", "over", "cone", "ray", "use", "ZEMAX", "simulation", "million", "random", "ray", "2-micron", "thick", "uncoated", "plastic", "pellicle", "beamsplitter", "which", "yield", "transmittance", "91.85", "top-left", "92.38", "bottom-right", "point", "difference", "about", "0.5", "close", "92", "value", "we", "use", "we", "design", "calculation", "therefore", "variation", "transmittance", "across", "sensor", "major", "issue", "we", "system", "polarization", "incident", "light", "might", "affect", "transmission", "property", "beamsplitter", "well", "although", "we", "simulation", "be", "all", "do", "unpolarized", "light", "possible", "encounter", "linearly", "polarize", "light", "outdoor", "scene", "-lrb-", "e.g.", "from", "glance", "reflection", "off", "water", "-rrb-", "which", "may", "change", "exposure", "difference", "between", "sensor", "however", "practice", "we", "do", "see", "polarization", "effect", "scene", "we", "capture", "we", "note", "all", "effect", "may", "reduce", "eliminate", "use", "thin-film", "coating", "beamsplitter", "thin-film", "coating", "could", "design", "have", "more", "constant", "transmission", "property", "over", "range", "angle", "system", "reduce", "polarization", "effect", "examination", "different", "beamsplitter", "coating", "address", "factor", "topic", "future", "work", "advantage", "propose", "optical", "splitting", "system", "its", "cost", "complexity", "relatively", "low", "compatible", "standard", "camera", "lens", "compact", "light", "path", "allow", "integration", "single", "hand-held", "unit", "something", "difficult", "do", "design", "place", "beamsplitter", "outside", "lens", "-lsb-", "McGuire", "et", "al.", "2007", "Cole", "Safai", "2010", "-rsb-", "optical", "architecture", "also", "flexible", "term", "kind", "sensor", "use", "use", "low-cost", "sensor", "example", "could", "allow", "design", "integrate", "consumer", "electronics", "bring", "HDR", "video", "wide", "audience", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "41:4", "M.", "Tocci", "et", "al.", "sample", "he", "input", "sample", "LE", "input", "Figure", "problem", "traditional", "merging", "algorithm", "when", "apply", "widely-separated", "ldr", "image", "Debevec", "Malik?s", "merge", "algorithm", "can", "produce", "artifact", "tone", "map", "result", "see", "top", "row", "inset", "we", "propose", "merge", "algorithm", "-lrb-", "left", "image", "bottom", "inset", "-rrb-", "use", "exclusively", "datum", "finest", "quantization", "available", "-lrb-", "from", "brightest-exposed", "sensor", "-rrb-", "blend", "datum", "from", "next", "darker", "sensor", "when", "necessary", "we", "show", "input", "he", "LE", "image", "sample", "inset", "LE", "image", "brightness", "scale", "show", "noise", "HDR", "image", "have", "be", "tonemapp", "FDR", "tool", "use", "same", "setting", "both", "merged", "image", "new", "Algorithm", "merge", "HDR", "Images", "addition", "optical", "architecture", "we", "also", "propose", "novel", "algorithm", "merge", "together", "image", "acquire", "we", "system", "order", "automatically", "create", "hdr", "image", "from", "widely-separated", "component", "ldr", "image", "without", "artifact", "we", "find", "necessary", "depart", "from", "standard", "method", "merge", "hdr", "image", "4.1", "Limitations", "current", "approach", "most", "previous", "algorithm", "merge", "hdr", "image", "from", "set", "ldr", "image", "different", "exposure", "typically", "do", "so", "after", "demosaice", "ldr", "image", "merge", "datum", "pixel-by-pixel", "without", "take", "neighboring", "pixel", "information", "account", "discussion", "we", "focus", "original", "merge", "algorithm", "propose", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "although", "algorithm", "work", "well", "when", "ldr", "image", "have", "small", "exposure", "separation", "we", "find", "quality", "final", "HDR", "image", "degrade", "when", "ldr", "image", "separate", "more", "than", "photographic", "stop", "capture", "widest", "dynamic", "range", "possible", "smallest", "number", "camera", "sensor", "necessary", "position", "ldr", "image", "further", "apart", "exposure", "we", "system", "than", "traditional", "hdr", "acquisition", "method", "devebec", "malik?s", "merge", "algorithm", "can", "yield", "undesired", "artifact", "when", "apply", "we", "camera?s", "datum", "because", "quantization", "noise", "effect", "show", "Fig.", "problem", "exacerbate", "when", "we", "apply", "certain", "tone", "mapping", "operator", "-lrb-", "tmo?s", "-rrb-", "Fattal", "et", "al.", "local-contrast", "operator", "-lsb-", "2002", "-rsb-", "tmo?s", "amplify", "small", "gradient", "difference", "image", "make", "they", "visible", "when", "dynamic", "range", "compress", "amplify", "merge", "artifact", "well", "course", "when", "ldr", "image", "very", "close", "exposure", "artifact", "considerably", "reduce", "method", "like", "Debevec", "Malik?s", "work", "quite", "well", "however", "hdr", "video", "system", "small", "set", "sensor", "become", "more", "widely", "use", "issue", "become", "more", "important", "fig.", "show", "underlie", "cause", "artifact", "present", "range", "scene", "illumination", "measure", "three", "sensor", "we", "architecture", "illustration", "we", "simplify", "system", "4-bit", "sensor", "-lrb-", "oppose", "12-bit", "sensor", "we", "implementation", "-rrb-", "which", "measure", "only", "16", "unique", "brightness", "value", "we", "separate", "sensor", "only", "stop", "-lrb-", "factor", "-rrb-", "exposure", "since", "cmo", "sensor", "exhibit", "approximately", "linear", "relationship", "between", "incident", "exposure", "output", "value", "we", "graph", "value", "from", "three", "sensor", "linear", "function", "incident", "irradiance", "instead", "traditional", "logarithmic", "scale", "figure", "we", "see", "LE", "sensor", "measure", "scene", "irradiance", "more", "coarsely", "than", "other", "two", "sensor", "example", "he", "sensor", "may", "measure", "different", "pixel", "value", "gradient", "merge", "result", "Debevec", "Malik?s", "algorithm", "we", "propose", "merge", "approach", "sample", "illumination", "scene", "illumination", "darker", "brighter", "weighting", "factor", "he", "sensor", "10", "11", "12", "13", "14", "15", "saturation", "Debevec", "Malik", "we", "approach", "me", "sensor", "10", "11", "12", "13", "14", "15", "saturation", "LE", "sensor", "10", "11", "12", "13", "14", "15", "saturation", "we", "merged", "HDR", "image", "-lrb-", "approx", "-rrb-", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "saturation", "figure", "quantization", "problem", "when", "merge", "ldr", "image", "figure", "show", "source", "artifact", "when", "merge", "we", "LDR", "image", "-lrb-", "simplify", "here", "4-bit", "sensor", "-rrb-", "algorithm", "always", "use", "datum", "from", "all", "three", "sensor", "simple", "weighting", "function", "Debevec", "Malik", "case", "datum", "from", "each", "sensor", "weight", "triangle", "function", "show", "so", "non-zero", "contribution", "from", "LE", "sensor", "low", "brightness", "value", "-lrb-", "like", "sample", "illumination", "level", "indicate", "-rrb-", "even", "though", "datum", "from", "LE", "sensor", "quantize", "more", "coarsely", "than", "he", "sensor", "we", "approach", "contrast", "use", "datum", "from", "higherexposure", "sensor", "much", "possible", "blend", "datum", "from", "next", "darker", "sensor", "when", "near", "saturation", "fore", "LE", "sensor", "record", "single", "increment", "addition", "always", "some", "small", "amount", "noise", "pixel", "value", "error", "LE", "sensor", "span", "12", "value", "range", "he", "sensor", "example", "although", "Debevec", "Malik?s", "algorithm", "blend", "value", "together", "we", "propose", "instead", "use", "pixel", "value", "from", "only", "longest-exposure", "sensor", "-lrb-", "which", "less", "noisy", "-rrb-", "wherever", "possible", "blend", "next", "darker", "exposure", "when", "pixel", "approach", "saturation", "more", "importantly", "we", "algorithm", "only", "examine", "individual", "pixel", "when", "merge", "ldr", "image", "also", "take", "account", "neighbor", "pixel", "might", "provide", "additional", "information", "help", "denoising", "process", "4.2", "we", "propose", "hdr-merging", "algorithm", "while", "new", "idea", "one", "key", "aspect", "we", "image-merging", "algorithm", "use", "pixel", "datum", "exclusively", "from", "brightest", "most", "well-exposed", "sensor", "possible", "therefore", "we", "use", "pixel", "from", "he", "image", "much", "possible", "only", "use", "pixel", "me", "image", "he", "pixel", "close", "saturation", "corresponding", "me", "pixel", "below", "saturation", "level", "multiply", "factor", "adjust", "relation", "he", "pixel", "base", "camera?s", "response", "curve", "-lrb-", "fig.", "-rrb-", "give", "me", "pixel", "receive", "12.2", "less", "irradiance", "than", "he", "pixel", "corresponding", "me", "pixel", "above", "saturation", "level", "we", "simply", "apply", "similar", "process", "same", "pixel", "low-exposure", "LE", "image", "when", "merge", "datum", "between", "sensor", "may", "seem", "sufficient", "follow", "na?ve", "winner", "take", "all", "approach", "exclusively", "use", "val", "ue", "from", "he", "sensor", "until", "become", "saturated", "simply", "switch", "next", "sensor", "-lsb-", "JAI", "2009", "-rsb-", "we", "find", "do", "work", "well", "practice", "because", "result", "banding", "artifact", "where", "transition", "occur", "instead", "we", "propose", "gracefully", "transition", "from", "one", "sensor", "next", "spatially", "blend", "pixel", "value", "between", "two", "sensor", "do", "we", "algorithm", "scan", "region", "around", "pixel", "be", "evaluate", "any", "neighbor", "pixel", "region", "saturate", "pixel", "under", "consideration", "may", "subject", "pixel", "crosstalk", "leakage", "algorithm", "estimate", "value", "pixel", "base", "its", "neighbor", "describe", "below", "algorithm", "detail", "we", "approach", "hdr-merging", "perform", "prior", "demosaice", "individual", "Bayer", "color", "filter", "array", "image", "because", "we", "find", "demosaicing", "can", "corrupt", "color", "saturated", "region", "-lrb-", "also", "note", "Ajdin", "et", "al.", "-lsb-", "2008", "-rsb-", "-rrb-", "example", "bright", "orange", "section", "scene", "might", "have", "red", "pixel", "saturate", "while", "green", "blue", "pixel", "image", "demosaice", "before", "be", "merge", "HDR", "demosaiced", "orange", "color", "compute", "from", "saturated", "red-pixel", "datum", "non-saturated", "green/blue-pixel", "datum", "result", "hue", "orange", "section", "incorrectly", "reproduce", "only", "way", "avoid", "artifact", "perform", "hdr-merging", "prior", "demosaicing", "since", "image", "merge", "prior", "demosaicing", "step", "we", "algorithm", "work", "pixel", "value", "instead", "irradiance", "produce", "radiometrically-correct", "hdr", "image", "we", "must", "correctly", "match", "irradiance", "level", "he", "ME", "LE", "sensor", "use", "appropriate", "beamsplitter", "transmittance", "value", "each", "pixel", "color", "since", "change", "slightly", "function", "wavelength", "although", "we", "use", "different", "value", "match", "each", "color", "channel", "simplicity", "we", "explain", "process", "average", "value", "we", "consider", "convert", "pixel", "value", "through", "camera", "response", "curve", "where", "result", "irradiance", "adjust", "exposure", "level", "ratio", "-lrb-", "average", "12.2", "he/me", "-rrb-", "new", "irradiance", "value", "convert", "back", "through", "camera", "response", "curve", "new", "pixel", "value", "fig.", "show", "3-step", "process", "graphically", "conversion", "process", "may", "next", "do", "all", "he", "pixel", "value", "-lrb-", "from", "through", "4096", "-rrb-", "arrive", "pixel-ratio", "curve", "which", "give", "scaling", "factor", "convert", "each", "me", "pixel?s", "value", "corresponding", "pixel", "value", "he", "sensor", "same", "irradiance", "-lrb-", "fig.", "-rrb-", "practice", "separate", "pixel-ratio", "curve", "calculate", "each", "color", "-lrb-", "-rrb-", "Bayer", "pattern", "when", "compare", "pixel", "value", "between", "he", "me", "image", "-lrb-", "between", "me", "le", "image", "-rrb-", "we", "use", "pixel-ratio", "curve", "lookup", "table", "-lrb-", "lut", "-rrb-", "convert", "he", "pixel", "value", "less", "than", "4096", "me", "pixel", "value", "vice", "versa", "when", "he", "pixel", "value", "saturate", "we", "simply", "extend", "pixelratio", "curve", "use", "last", "value", "obtain", "-lrb-", "approximately", "-rrb-", "camera", "response", "curve", "can", "measure", "method", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "take", "set", "bracket", "exposure", "solve", "monotonically-increasing", "function", "relate", "exposure", "pixel", "value", "-lrb-", "within", "scale", "constant", "linear", "domain", "-rrb-", "fig.", "show", "curve", "compute", "from", "raw", "camera", "datum", "although", "curve", "compute", "from", "linear", "best-fit", "could", "also", "use", "we", "case", "we", "can", "factor", "out", "exposure", "time", "-lrb-", "since", "constant", "all", "three", "image", "-rrb-", "produce", "curve", "map", "pixel", "value", "directly", "scene", "irradiance", "we", "call", "function", "-lrb-", "-rrb-", "where", "we", "pixel", "value", "three-step", "process", "describe", "earlier", "can", "reverse", "map", "me", "pixel", "value", "he", "pixel", "value", "write", "me?he", "-lrb-", "-rrb-", "-lrb-", "12.2", "-lrb-", "-rrb-", "-rrb-", "function", "me?he", "-lrb-", "-rrb-", "use", "blend", "pixel", "value", "between", "me", "he", "sensor", "similar", "function", "le?he", "-lrb-", "-rrb-", "use", "blend", "between", "LE", "he", "sensor", "once", "irradiance", "level", "three", "image", "have", "be", "match", "we", "ready", "begin", "merging", "process", "explain", "we", "merge", "approach", "we", "assume", "two", "registered", "ldr", "image", "-lrb-", "one", "high-exposure", "image", "he", "second", "medium", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "Versatile", "HDR", "Video", "Video", "Production", "System", "41:5", "2.5", "-lrb-", "-rrb-", "1.5", "irradiance", "0.5", "figure", "500 1000 1500 2000", "2500 3000 3500 4000", "4500", "Pixel", "Value", "Camera", "response", "curve", "curve", "show", "how", "camera", "convert", "scene", "irradiance", "pixel", "value", "compute", "what", "me", "pixel", "value", "should", "give", "he", "value", "he", "pixel", "value", "-lrb-", "-rrb-", "first", "convert", "scene", "irradiance", "-lrb-", "-rrb-", "which", "next", "divide", "we", "he/me", "attenuation", "ratio", "12.2", "new", "irradiance", "value", "-lrb-", "-rrb-", "convert", "through", "camera", "response", "curve", "expect", "me", "pixel", "value", "-lrb-", "-rrb-", "although", "graph", "approximately", "linear", "perfectly", "so", "because", "compute", "from", "raw", "datum", "without", "significant", "smoothing", "apply", "linear", "fit", "exposure", "image", "me", "-rrb-", "merge", "hdr", "image", "hdr", "we", "approach", "information", "high-exposure", "image", "he", "combine", "datum", "from", "next", "darker-exposure", "image", "me", "need", "reduce", "transition", "artifact", "describe", "earlier", "we", "algorithm", "work", "each", "pixel", "location", "-lrb-", "-rrb-", "look", "information", "from", "surrounding", "-lrb-", "2k", "-rrb-", "-lrb-", "2k", "-rrb-", "pixel", "neighborhood", "denote", "-lrb-", "-rrb-", "we", "implementation", "we", "use", "pixel", "neighborhood", "-lrb-", "-rrb-", "we", "define", "pixel", "saturate", "its", "value", "greater", "than", "90", "maximum", "pixel", "value", "-lrb-", "4096", "we", "sensor", "-rrb-", "we", "now", "specify", "algorithm", "each", "follow", "state", "pixel", "its", "neighborhood", "Case", "pixel", "under", "consideration", "he", "-lrb-", "-rrb-", "saturate", "he", "-lrb-", "-rrb-", "have", "saturated", "pixel", "so", "pixel", "value", "use", "as-is", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "case", "he", "-lrb-", "-rrb-", "saturate", "he", "-lrb-", "-rrb-", "have", "more", "saturated", "pixel", "although", "most", "hdr", "merge", "algorithm", "would", "use", "pixel", "as-is", "we", "find", "call", "question", "actual", "value", "we", "pixel", "due", "proximity", "effect", "-lrb-", "e.g.", "leakage", "pixel", "cross-talk", "sensor", "-rrb-", "we", "therefore", "blend", "between", "pixel", "value", "he", "-lrb-", "-rrb-", "one", "next", "darker-exposure", "me", "-lrb-", "-rrb-", "depend", "amount", "saturation", "present", "neighborhood", "do", "three", "step", "let", "set", "unsaturated", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "where", "number", "unsaturated", "pixel", "let", "he", "-lrb-", "-rrb-", "number", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "interpolation", "coefficient", "can", "compute", "he", "which", "represent", "fraction", "unsaturated", "pixel", "neighborhood", "output", "pixel", "give", "blended", "value", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "-lrb-", "??", "-rrb-", "me?he", "-lrb-", "me", "-lrb-", "-rrb-", "-rrb-", "where", "we", "use", "pixel-ratio", "lut", "map", "me", "value", "he", "range", "blend", "measurement", "higher", "exposure", "he", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "base", "number", "saturated", "pixel", "neighborhood", "he", "-lrb-", "-rrb-", "Case", "he", "-lrb-", "-rrb-", "saturated", "he", "-lrb-", "-rrb-", "have", "more", "nonsaturated", "pixel", "which", "can", "use", "better", "estimate", "value", "he", "-lrb-", "-rrb-", "we", "calculate", "ratio", "pixel", "value", "me", "image", "between", "unsaturated", "pixel", "neighborhood", "center", "pixel", "use", "map", "me", "ratio", "estimate", "actual", "value", "saturated", "pixel", "under", "consideration", "do", "four", "step", "case", "compute", "coefficient", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "41:6", "M.", "Tocci", "et", "al.", "10", "value", "calculate", "ra", "from", "camera", "calibra", "curve", "measure", "ra", "from", "image", "figure", "500 1000 1500 2000", "2500 3000 3500 4000", "4500", "he", "Pixel", "Value", "Pixel", "ratio", "curve", "blue", "curve", "obtain", "when", "3-step", "process", "-lrb-", "fig.", "-rrb-", "apply", "all", "red", "pixel", "value", "he", "image", "use", "camera", "response", "curve", "ratio", "between", "he", "me", "pixel", "value", "calculate", "red", "curve", "measure", "experimentally", "take", "ratio", "large", "set", "red", "he", "me", "pixel", "actual", "image", "capture", "we", "system", "two", "match", "reasonably", "validate", "we", "approach", "red", "curve", "use", "algorithm", "look-up", "table", "-lrb-", "lut", "-rrb-", "scale", "factor", "convert", "red", "me", "pixel", "value", "blend", "they", "smoothly", "red", "he", "pixel", "value", "similar", "curve", "be", "compute", "green", "blue", "channel", "compute", "ratio", "map", "ratio", "between", "center", "pixel", "each", "pixel", "neighborhood", "from", "me", "image", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "all", "pixel", "ME", "neighborhood", "me", "-lrb-", "-rrb-", "now", "compute", "estimate", "he", "-lrb-", "-rrb-", "saturated", "pixel", "scale", "unsaturated", "pixel", "value", "neighborhood", "he", "he", "-lrb-", "-rrb-", "ratio", "i?u", "compute", "he", "-lrb-", "step", "-rrb-", "finally", "blend", "estimate", "he", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "use", "equation", "similar", "case", "step", "hdr", "-lrb-", "-rrb-", "he", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "me?he", "-lrb-", "me", "-lrb-", "-rrb-", "-rrb-", "Case", "he", "-lrb-", "-rrb-", "saturated", "all", "pixel", "he", "-lrb-", "-rrb-", "saturate", "so", "we", "do", "have", "any", "valid", "information", "from", "highexposure", "image", "case", "we", "simply", "use", "me", "image", "set", "hdr", "-lrb-", "-rrb-", "me", "-lrb-", "-rrb-", "we", "algorithm", "combine", "datum", "from", "two", "sensor", "when", "three", "LDR", "image", "process", "above", "simply", "repeat", "second", "iteration", "substitute", "hdr", "he", "le", "me", "manner", "we", "merge", "datum", "from", "higher", "exposure", "we", "work", "we", "way", "toward", "lowest", "exposure", "only", "use", "datum", "from", "lower", "exposure", "when", "higher-exposure", "datum", "near", "saturation", "we", "find", "algorithm", "reduce", "artifact", "merged", "image", "considerably", "show", "Fig.", "output", "we", "algorithm", "hdr", "image", "can", "demosaice", "convert", "from", "pixel", "value", "irradiance", "use", "camera", "response", "curve", "similar", "fig.", "accounting", "all", "color", "channel", "final", "HDR", "full-color", "image", "may", "tonemapp", "commercial", "software", "package", "-lrb-", "e.g.", "fdrtool", "HDR", "expose", "Photomatix", "etc.", "-rrb-", "describe", "literature", "-lrb-", "e.g.", "-lsb-", "Smith", "et", "al.", "2006", "-rsb-", "-rrb-", "System", "Implementation", "we", "implement", "prototype", "we", "design", "use", "three", "Silicon", "Imaging", "si-1920hd", "high-end", "cinema", "cmo", "sensor", "mount", "inside", "custom", "camera", "body", "each", "12-bit", "sensor", "connect", "Dalsa/Coreco", "x64-cl", "ipro", "frame", "grabber", "pc", "sensor", "have", "1920", "1080", "pixel", "-lrb-", "micron", "square", "-rrb-", "standard", "Bayer", "color", "filter", "array", "can", "measure", "dynamic", "range", "around", "10", "stop", "-lrb-", "exclude", "noise", "-rrb-", "sensor", "be", "align", "aim", "camera", "small", "pinhole", "light", "source", "lock", "down", "he", "sensor", "adjust", "setscrew", "align", "me", "le", "sensor", "we", "prototype", "final", "image", "registration", "accurate", "less", "than", "micron", "rotation", "error", "less", "than", "0.1", "note", "general", "require", "registration", "accuracy", "modulo", "pixel", "array", "Bayer", "pattern", "because", "while", "pixel", "one", "color", "sensor", "must", "match", "same", "color", "another", "shift", "any", "number", "Bayer", "pattern", "can", "handle", "software", "we", "discuss", "how", "registration", "tolerance", "affect", "we", "result", "Sec", "camera", "body", "have", "Hasselblad", "lens", "mount", "allow", "use", "high-performance", "interchangeable", "commercial", "lens", "beamsplitter", "we", "current", "prototype", "system", "employ", "uncoated", "pellicle", "beamsplitter", "one", "sell", "Edmund", "Optics", "-lsb-", "part", "number", "nt39-482", "-rsb-", "although", "pellicle", "beamsplitter", "ideal", "commercial", "product", "-lrb-", "can", "delicate", "difficult", "clean", "maintain", "-rrb-", "we", "use", "they", "because", "low", "cost", "accessibility", "however", "we", "find", "they", "robust", "we", "application", "we", "be", "able", "use", "we", "system", "successfully", "various", "location", "environmental", "condition", "picture", "complete", "camera", "prototype", "which", "cost", "less", "than", "us$", "15k", "part", "show", "Fig.", "we", "calculate", "camera?s", "response", "curve", "describe", "devebec", "Malik", "-lsb-", "1997", "-rsb-", "set", "bracket", "exposure", "from", "he", "sensor", "-lrb-", "fig.", "-rrb-", "we", "also", "measure", "ratio", "between", "he", "me", "pixel", "value", "experimentally", "each", "color", "channel", "observe", "match", "curve", "predict", "we", "3-step", "process", "-lrb-", "fig.", "-rrb-", "curve", "implement", "LUT", "translate", "value", "from", "me", "he", "sensor", "range", "merge", "similar", "process", "use", "obtain", "implement", "he/le", "ratio", "curve", "LE", "sensor", "value", "also", "adjust", "slight", "offset", "before", "be", "use", "merge", "algorithm", "account", "small", "amount", "stray", "light", "workflow", "process", "datum", "come", "from", "camera", "show", "alg", "most", "step", "self-explanatory", "typical", "hdr", "system", "non-uniform", "correction", "-lrb-", "nuc", "-rrb-", "step", "apply", "each", "image", "correct", "non-uniformity", "sensor", "2-point", "correction", "which", "involve", "take", "dark", "field", "image", "offset", "uniformly-lit", "diffuse", "scene", "gain", "HDR", "merge", "algorithm", "present", "Sec", "implement", "MATLAB", "demosaicing", "step", "use", "algorithm", "Malvar", "et", "al.", "-lsb-", "2004", "-rsb-", "we", "tonemapp", "final", "hdr", "image", "use", "hdr", "processing", "tool", "HDR", "expose", "Photomatix", "Figure", "complete", "camera", "prototype", "-lrb-", "left", "-rrb-", "optical", "path", "we", "complete", "implementation", "-lrb-", "right", "-rrb-", "finish", "prototype", "use", "acquire", "video", "image", "paper", "result", "once", "system", "build", "we", "perform", "radiometric", "calibration", "test", "measure", "its", "dynamic", "range", "do", "we", "aim", "camera", "step", "array", "neutral", "density", "filter", "step", "stop", "each", "focus", "down", "sun?s", "light", "integrate", "sphere", "behind", "array", "provide", "uniform", "bright", "light", "source", "single", "frame", "image", "datum", "capture", "all", "sensor", "simultaneously", "exposure", "time", "1/30", "second", "fig.", "10", "show", "result", "experiment", "we", "current", "prototype", "we", "able", "clearly", "measure", "dynamic", "range", "over", "17", "equivalent", "over", "17", "photographic", "stop", "over", "100db", "more", "importantly", "we", "demonstrate", "we", "optical", "architecture", "can", "increase", "dynamic", "range", "partic", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "Versatile", "HDR", "Video", "Video", "Production", "System", "41:7", "algorithm", "HDR", "Processing", "Workflow", "read", "he", "ME", "LE", "raw", "image", "from", "camera", "apply", "nuc", "-lrb-", "point", "correction", "-rrb-", "fix", "registration", "shift", "modulo", "Bayer", "pattern", "merge", "hdr", "image", "use", "algorithm", "Sec", "demosaic", "hdr", "image", "produce", "full-color", "image", "White", "balancing", "color", "correction", "conversion", "hdr", "format", "tonemapping", "filter", "-lrb-", "saturation", "brightness", "sharpness", "-rrb-", "build", "final", "video", "from", "individual", "frame", "satura", "irradiance", "measure", "10", "11", "transus", "12", "13", "discernible", "14", "15", "16", "17", "posus", "filter", "array", "figure", "10", "Dynamic", "range", "test", "dynamic", "range", "we", "cam", "era", "test", "imaging", "set", "neutral", "density", "filter", "vary", "transmittance", "power", "we", "focus", "sun", "onto", "integrate", "sphere", "behind", "filter", "uniform", "illumination", "left", "tonemapped", "hdr", "image", "filter", "take", "we", "prototype", "right", "log", "plot", "measure", "intensity", "across", "HDR", "image", "-lrb-", "kink", "curve", "step", "10", "due", "wrap-around", "image", "-rrb-", "dynamic", "range", "camera", "can", "measure", "find", "last", "pair", "filter", "where", "noticeable", "difference", "measure", "intensity", "happen", "after", "filter", "17", "which", "mean", "we", "prototype", "can", "capture", "over", "17", "stop", "dynamic", "range", "ular", "camera", "sensor", "stop", "-lrb-", "7.7", "calculate", "from", "design", "-rrb-", "use", "available", "optical", "electronic", "component", "we", "also", "test", "we", "propose", "merge", "algorithm", "against", "Debevec", "Malik", "-lsb-", "1997", "-rsb-", "every", "scene", "we", "test", "we", "approach", "able", "produce", "higher", "quality", "image", "than", "algorithm", "because", "quantization", "artifact", "introduce", "widely-separated", "exposure", "image", "-lrb-", "see", "Figs.", "11", "-rrb-", "we", "find", "useful", "visualize", "merging", "process", "indicate", "which", "part", "image", "be", "be", "select", "from", "different", "ldr", "image", "-lrb-", "fig.", "11", "-rrb-", "finally", "fig.", "11", "13", "show", "frame", "from", "video", "capture", "we", "prototype", "tonemapped", "different", "way", "all", "video", "be", "capture", "1920", "1080", "24", "30", "fp", "show", "wide", "variety", "hdr", "scene", "complex", "movement", "lens", "effect", "UT", "TING", "orch", "scene", "fig.", "show", "oxy-acetylene", "torch", "cut", "steel", "plate", "tonemapp", "Photomatix", "spark", "correctly", "motion-blurred", "because", "simultaneous", "capture", "all", "sensor", "we", "also", "able", "capture", "detail", "bright", "molten", "metal", "dark", "surroundings", "same", "time", "suggest", "proposed", "system", "could", "use", "industrial", "manufacturing", "application", "ALLS", "ATER", "scene", "Fig.", "11", "show", "rapidly-moving", "color", "object", "all", "correctly", "motion-blurred", "bright", "sunlight", "uniform-color", "sphere", "present", "challenging", "gradient", "merge", "algorithm", "first", "two", "image", "Fig.", "13", "now", "og", "ash", "be", "capture", "snow", "direct", "sunlight", "film", "snowy", "scene", "bright", "sunlight", "notoriously", "difficult", "especially", "dark-colored", "subject", "yet", "we", "system", "capture", "detail", "both", "snow", "dark", "area", "hip", "lame", "scene", "show", "intense", "flame", "set", "object", "direct", "sunlight", "deep", "shadow", "elting", "now", "very", "challenging", "scene", "impossible", "capture", "conventional", "camera", "because", "take", "from", "inside", "dimly-lit", "room", "look", "out", "bright", "outdoor", "scene", "we", "system", "can", "simultaneously", "capture", "both", "wall", "decoration", "inside", "structure", "cloud", "outside", "which", "several", "order", "magnitude", "brighter", "scene", "also", "feature", "focus-pull", "during", "shot", "from", "distant", "mountain", "foreground", "which", "difficult", "do", "system", "split", "beam", "before", "lens", "-lrb-", "e.g.", "-lsb-", "Cole", "Safai", "2010", "-rsb-", "-rrb-", "onkey", "show", "stuff", "animal", "bright", "white", "fur", "direct", "sunlight", "fine", "detail", "fur", "dark", "bucket", "accurately", "capture", "finally", "ighting", "orch", "scene", "show", "focus", "pull", "from", "bright", "background", "dimly-lit", "subject", "foreground", "bright", "torch", "flame", "add", "which", "provide", "flicker", "illumination", "subject?s", "otherwise", "darken", "face", "result", "all", "demonstrate", "quality", "image", "we", "prototype", "system", "can", "capture", "Figure", "11", "another", "comparison", "we", "merge", "algorithm", "top", "image", "show", "HDR", "image", "merge", "we", "algorithm", "after", "tonemapp", "Photomatix", "ldr", "input", "side", "bottom", "two", "row", "show", "comparison", "between", "Debevec", "Malik?s", "algorithm", "-lrb-", "left", "-rrb-", "ours", "-lrb-", "middle", "-rrb-", "along", "visualization", "show", "how", "we", "select", "pixel", "value", "-lrb-", "right", "-rrb-", "Discussion", "Sensor", "alignment", "merge", "we", "prototype", "align", "so", "mis-registration", "between", "image", "less", "than", "micron", "we", "note", "mis-registration", "could", "cause", "sub-pixel", "artifact", "region", "sharp", "detail", "when", "datum", "merge", "HDR", "prior", "demosaicing", "process", "however", "general", "we", "spatially-blending", "merge", "algorithm", "help", "reduce", "use", "value", "from", "single", "sensor", "much", "possible", "only", "blend", "datum", "from", "other", "sensor", "when", "approach", "saturation", "limit", "region", "where", "blend", "mis-registered", "value", "would", "cause", "problem", "those", "near", "saturated", "pixel", "have", "sharp", "detail", "furthermore", "although", "can", "artifact", "from", "merge", "datum", "prior", "demosaicing", "color", "artifact", "cause", "merge", "datum", "postdemosaic", "-lrb-", "which", "happen", "area", "where", "only", "some", "color", "channel", "saturate", "-rrb-", "more", "serious", "because", "can", "happen", "large", "smooth", "region", "image", "overall", "mis-registration", "artifact", "can", "mitigate", "simply", "use", "sensor", "larger", "pixel", "also", "same", "challenge", "sensor", "alignment", "address", "3ccd", "system", "full-resolution", "color", "commercial", "product", "typically", "align", "very", "tight", "tolerance", "-lrb-", "much", "smaller", "than", "single", "pixel", "-rrb-", "we", "expect", "production", "version", "we", "proposed", "system", "benefit", "from", "similar", "commercial", "alignment", "technique", "merge", "algorithm", "noise", "reduction", "although", "we", "algorithm", "rely", "datum", "from", "only", "single", "sensor", "until", "approach", "saturation", "reduce", "quantization", "artifact", "noise", "from", "darker", "sensor", "might", "possible", "leverage", "information", "from", "other", "sensor", "appropriately", "further", "noise", "reduction", "course", "we", "image", "be", "space", "very", "closely", "together", "simple", "approach", "like", "Debevec", "Malik", "would", "reduce", "noise", "we", "spacing", "might", "possible", "combine", "we", "spatial-blending", "approach", "-lrb-", "which", "look", "neighborhood", "pixel", "just", "pixel", "be", "merge", "-rrb-", "merge", "algorithm", "weigh", "other", "image", "base", "noise", "content", "-lrb-", "e.g.", "-lsb-", "Granados", "et", "al.", "2010", "Hasinoff", "et", "al.", "2010", "-rsb-", "-rrb-", "produce", "better", "result", "interesting", "subject", "future", "work", "flexible", "design", "we", "present", "simple", "blueprint", "hdr", "video", "imaging", "system", "use", "off-the-shelf", "component", "can", "produce", "high-quality", "hdr", "video", "footage", "use", "commerciallyavailable", "sensor", "make", "easy", "change", "they", "extend", "capability", "camera", "furthermore", "number", "sensor", "can", "extend", "minor", "modification", "optical", "path", "fig.", "12", "show", "optical", "design", "configure", "sensor", "which", "would", "allow", "camera", "have", "even", "higher", "dynamic", "range", "produce", "higher", "quality", "image", "same", "dynamic", "range", "toward", "better", "low-cost", "consumer", "camera", "although", "system", "like", "ours", "could", "use", "high-end", "imaging", "application", "feature", "film", "production", "could", "also", "impact", "consumer", "camera", "market", "first", "propose", "architecture", "might", "enable", "high-quality", "low-cost", "imaging", "because", "allow", "three", "cheap", "sensor", "limited", "dynamic", "range", "capture", "image", "dynamic", "range", "comparable", "single", "high-end", "sensor", "second", "hdr", "imaging", "would", "arguably", "benefit", "consumer", "more", "than", "professional", "cinematographer", "because", "typical", "consumer", "do", "have", "lighting", "rig", "training", "need", "achieve", "HDR", "effect", "ldr", "system", "Limitations", "any", "hdr", "system", "-lrb-", "include", "human", "eye", "-rrb-", "ultimately", "limit", "its", "ability", "capture", "wide", "dynamic", "range", "veil", "glare", "optics", "-lsb-", "McCann", "Rizzi", "2007", "-rsb-", "we", "design", "compatible", "higher-performance", "lens", "which", "may", "help", "reduce", "veil", "glare", "we", "optical", "architecture", "also", "provide", "highexposure", "sensor", "only", "92", "light", "scene", "which", "compare", "favorably", "33", "provide", "previous", "internal", "beamsplitting", "system", "less", "light", "than", "would", "capture", "single", "sensor", "lens", "traditional", "camera", "future", "work", "term", "optical", "system", "study", "thinfilm", "coating", "beamsplitter", "reduce", "angleor", "polarizationdependent", "effect", "-lrb-", "discuss", "Sec", "3.1", "-rrb-", "could", "explore", "optical", "system", "would", "also", "benefit", "from", "improvement", "sensor", "alignment", "term", "propose", "merge", "algorithm", "study", "noise", "optimization", "during", "image", "merge", "process", "conjunction", "we", "spatial-blending", "method", "would", "make", "interesting", "subject", "future", "work", "finally", "would", "useful", "develop", "software", "package", "hdr", "cinematography", "use", "system", "like", "one", "propose", "would", "give", "director", "photography", "same", "kind", "tool", "post-process", "use", "have", "disposal", "when", "light", "set", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "41:8", "M.", "Tocci", "et", "al.", "sensor", "sensor", "sensor", "sensor", "main", "lens", "sensor", "Figure", "12", "Extension", "we", "optical", "architecture", "we", "optical", "architecture", "can", "extend", "more", "sensor", "better", "image", "quality", "higher", "dynamic", "range", "here", "we", "show", "design", "system", "sensor", "where", "we", "essentially", "replace", "he", "me", "sensor", "two", "other", "beamsplitter", "pair", "sensor", "each", "design", "simulated", "zemax", "-lsb-", "2011", "-rsb-", "conclusion", "conclusion", "we", "have", "present", "optical", "architecture", "highdynamic", "range", "video", "make", "efficient", "use", "incoming", "light", "produce", "cinema-quality", "image", "we", "also", "propose", "novel", "hdr", "merge", "algorithm", "minimize", "quantization", "artifact", "from", "LDR", "datum", "use", "highest-exposure", "datum", "until", "saturation", "before", "spatially-blending", "darker-exposure", "datum", "test", "we", "design", "we", "build", "work", "prototype", "system", "show", "image", "from", "video", "acquire", "system", "Acknowledgment", "we", "thank", "anonymous", "reviewer", "helpful", "comment", "improve", "final", "version", "paper", "reference", "garwalum", "a.", "ONTCHEVA", "M.", "GRAWALA", "M.", "RUCKER", "S.", "OLBURN", "A.", "URLESS", "B.", "ALESIN", "D.", "OHEN", "M.", "2004", "interactive", "digital", "photomontage", "ACM", "Trans", "graph", "23", "-lrb-", "August", "-rrb-", "294", "302", "GGARWAL", "M.", "HUJA", "N.", "2001", "split", "aperture", "imaging", "high", "dynamic", "range", "Proceedings", "ICCV", "2001", "10", "17", "GGARWAL", "M.", "HUJA", "N.", "2004", "split", "aperture", "imaging", "high", "dynamic", "range", "International", "Journal", "Computer", "Vision", "58", "17", "jdin", "B.", "ULLIN", "M.", "uch", "C.", "eidel", "h.-p.", "ensch", "H.", "2008", "demosaice", "smooth", "along", "1d", "feature", "Proceedings", "CVPR", "2008", "rajovic", "V.", "ANADE", "T.", "1996", "sort", "image", "sensor", "example", "massively", "parallel", "intensity-to-time", "processing", "low-latency", "computational", "sensor", "Proceedings", "ICRA", "1996", "vol", "1638", "1643", "ole", "a.", "afaus", "M.", "2010", "soviet", "Montage", "Productions", "http://www.sovietmontage.com/", "ebevec", "P.", "E.", "ALIK", "J.", "1997", "recover", "high", "dynamic", "range", "radiance", "map", "from", "photograph", "Proceedings", "ACM", "SIGGRAPH", "1997", "369", "378", "attal", "R.", "ISCHINSKI", "D.", "ERMAN", "M.", "2002", "gradient", "domain", "high", "dynamic", "range", "compression", "ACM", "Trans", "graph", "21", "-lrb-", "July", "-rrb-", "249", "256", "inosar", "R.", "ILSENRATH", "O.", "eevus", "Y.", "1992", "wide", "dynamic", "range", "camera", "United", "States", "Patent", "5,144,442", "ranado", "M.", "JDIN", "B.", "M.", "heobalt", "C.", "eus", "del", "h.-p.", "ensch", "H.", "2010", "optimal", "hdr", "reconstruction", "linear", "digital", "camera", "Proceedings", "CVPR", "2010", "arvey", "R.", "1998", "Optical", "beam", "splitter", "electronic", "high", "speed", "camera", "incorporate", "beam", "splitter", "United", "States", "Patent", "5,734,507", "asinoff", "S.", "W.", "URAND", "F.", "reeman", "W.", "T.", "2010", "noise-optimal", "capture", "high", "dynamic", "range", "photography", "Proceedings", "CVPR", "2010", "JAI", "2009", "use", "image", "fusion", "capture", "high-dynamic", "range", "-lrb-", "hdr", "-rrb-", "scene", "Tech", "Rep.", "TN-0903", "JAI", "Inc.", "Japan", "ang", "S.", "B.", "YTTENDAELE", "M.", "INDER", "S.", "ZELISKI", "R.", "2003", "high", "dynamic", "range", "video", "Proceedings", "ACM", "SIGGRAPH", "2003", "ACM", "319", "325", "ao", "w.-c", "2008", "high", "dynamic", "range", "imaging", "fuse", "multiple", "raw", "image", "tone", "reproduction", "IEEE", "transaction", "Consumer", "Electronics", "54", "-lrb-", "Feb.", "-rrb-", "10", "15", "ELL", "R.", "ZIKLAI", "G.", "1951", "simultaneous", "color", "television", "United", "States", "Patent", "2,560,351", "RYMSKI", "a.", "2008", "high", "dynamic", "range", "imager", "rolling", "shutter", "United", "States", "Patent", "7,397,509", "alvar", "H.", "WEI", "L.", "utler", "R.", "2004", "high-quality", "linear", "interpolation", "demosaicing", "bayer-patterned", "color", "image", "Proceedings", "ICASSP", "2004", "vol", "485", "488", "ANN", "S.", "icard", "R.", "W.", "1995", "be", "undigital", "digital", "camera", "extend", "dynamic", "range", "combine", "differently", "expose", "picture", "Proceedings", "Society", "Imaging", "Science", "Technology?s", "48th", "annual", "conference", "442", "448", "ANN", "J.", "izzus", "a.", "2007", "veil", "glare", "dynamic", "range", "limit", "hdr", "image", "human", "Vision", "Electronic", "Imaging", "XII", "SPIE", "uire", "m.", "atusik", "W.", "fister", "H.", "HEN", "B.", "UGHES", "J.", "AYAR", "S.", "2007", "Optical", "splitting", "tree", "high-precision", "monocular", "imaging", "IEEE", "Computer", "Graphics", "application", "27", "-lrb-", "march-april", "-rrb-", "32", "42", "erten", "T.", "AUTZ", "J.", "eeth", "F.", "V.", "2008", "exposure", "fusion", "simple", "practical", "alternative", "high", "dynamic", "range", "photography", "Computer", "Graphics", "Forum", "28", "161", "171", "itsunaga", "T.", "AYAR", "S.", "1999", "radiometric", "self", "calibration", "Proceedings", "CVPR", "1999", "vol", "374", "380", "yszkowskus", "K.", "ANTIUK", "R.", "RAWCZYK", "G.", "2008", "high", "Dynamic", "Range", "Video", "Morgan", "Claypool", "ayar", "S.", "RANZOI", "V.", "2003", "adaptive", "dynamic", "range", "imaging", "optical", "control", "pixel", "exposure", "over", "space", "time", "Proceedings", "ICCV", "2003", "1168", "1175", "ayar", "S.", "ITSUNAGA", "T.", "2000", "high", "dynamic", "range", "imaging", "spatially", "vary", "pixel", "exposure", "Proceedings", "CVPR", "2000", "472", "479", "R.", "EVOY", "M.", "DIF", "M.", "UVAL", "G.", "OROWITZ", "M.", "ANRAHAN", "P.", "2005", "light", "field", "photography", "hand-held", "plenoptic", "camera", "Tech", "Rep.", "CSTR", "2005-02", "Stanford", "University", "obertson", "M.", "A.", "ORMAN", "S.", "TEVENSON", "R.", "L.", "2003", "estimation-theoretic", "approach", "dynamic", "range", "enhancement", "use", "multiple", "exposure", "Journal", "Electronic", "Imaging", "12", "-lrb-", "april", "-rrb-", "219", "228", "eger", "U.", "pel", "U.", "offlinger", "B.", "1999", "hdrcimager", "natural", "visual", "perception", "handbook", "computer", "Vision", "application", "B.", "J?hne", "H.", "Hau?ecker", "P.", "Gei?ler", "Eds.", "vol", "Academic", "Press", "223", "235", "mith", "K.", "RAWCZYK", "G.", "YSZKOWSKI", "K.", "EIDEL", "h.-p", "2006", "beyond", "tone", "mapping", "enhance", "depiction", "tone", "map", "HDR", "image", "Computer", "Graphics", "Forum", "25", "427", "438", "pheron", "vr", "2011", "http://www.spheron.com/", "NGER", "J.", "USTAVSON", "S.", "2007", "high-dynamic-range", "video", "photometric", "measurement", "illumination", "SPIE", "vol", "6501", "65010e", "ang", "H.", "ASKAR", "R.", "HUJA", "N.", "2005", "high", "dynamic", "range", "video", "use", "split", "aperture", "camera", "Proceedings", "OMNIVIS", "2005", "ZEMAX", "2011", "Optical", "design", "software", "http://www.zemax.com/", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "Versatile", "HDR", "Video", "Video", "Production", "System", "41:9", "Figure", "13", "NOW", "ONKEY", "IGHTING", "ORCH", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "41", "publication", "date", "July", "2011", "image", "from", "video", "capture", "we", "camera", "sample", "tonemapp", "image", "demonstrate", "different", "imaging", "hdr", "effect", "ldr", "image", "capture", "three", "sensor", "we", "system", "show", "right", "scene", "from", "top", "bottom", "now", "OG", "ASH", "hip", "lame", "elting" ],
  "content" : "\n  \n    bd63fb9c6a4bc8e01f1667efd358b364535a0efa9fc5b6a5d9320a0466834f46\n    p0q\n    10.1145/1964921.1964936\n    Name identification was not possible. \n  \n  \n    \n      \n        A Versatile HDR Video Production System\n      \n      Michael D. Tocci 1,2 Chris Kiser 1,2,3 Nora Tocci 1 1 Contrast Optical Design & Engineering, Inc. 2 University of New Mexico\n      \n        \n        Figure 1: HDR image acquired with our proposed system. On the left we show the final image acquired with our camera and merged with the proposed algorithm. The inset photos show the individual LDR images from the high, medium, and low-exposure sensors, respectively.\n      \n      Although High Dynamic Range (HDR) imaging has been the subject of significant research over the past fifteen years, the goal of acquiring cinema-quality HDR images of fast-moving scenes using available components has not yet been achieved. In this work, we present an optical architecture for HDR imaging that allows simultaneous capture of high, medium, and low-exposure images on three sensors at high fidelity with efficient use of the available light. We also present an HDR merging algorithm to complement this architecture, which avoids undesired artifacts when there is a large exposure difference between the images. We implemented a prototype high-definition HDR-video system and we present still frames from the acquired HDR video, tonemapped with various techniques. CR Categories: I.4.1 [Image Processing and Computer Vision]: Digitization and Image capture?Radiometry Keywords: HDR video, merging HDR images Links: DL PDF\n    \n    \n      \n        1 Introduction\n      \n      The extension of the dynamic range of digital images has been the subject of significant research in both academia and industry. Despite all this previous work, however, there are currently no readilyimplemented solutions for capturing high-quality HDR video of fast-moving scenes. In this paper, we describe an end-to-end system for capturing HDR video with high pixel fidelity, using a lightefficient optical architecture that fits into a single hand-held unit.\n      \n        ACM Reference Format\n      \n      Tocci, M., Kiser, C., Tocci, N., Sen, P. 2011. A Versatile HDR Video Production System. ACM Trans. Graph. 30, 4, Article 41 (July 2011), 9 pages. DOI = 10.1145/1964921.1964936 http://doi.acm.org/10.1145/1964921.1964936.\n      \n        Copyright Notice\n      \n      Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1\n      \n        212\n        869-0481, or permissions@acm.org.\n      \n      ? 2011 ACM 0730-0301/2011/07-ART41 $10.00 DOI 10.1145/1964921.1964936 http://doi.acm.org/10.1145/1964921.1964936\n      Pradeep Sen 2,3 3 Advanced Graphics Lab\n      Our proposed system is simple, uses only off-the-shelf technology, and is flexible in terms of the sensors that are used. Specifically, our HDR optical architecture: (1) captures optically-aligned, multipleexposure images simultaneously that do not need image manipulation to account for motion, (2) extends the dynamic range of available image sensors (by over 7 photographic stops in our current prototype), (3) is inexpensive to implement, (4) utilizes a single, standard camera lens, and (5) efficiently uses the light from the lens. To complement our system, we also propose a novel HDR imagemerging algorithm that: (1) combines images separated by more than 3 stops in exposure, (2) spatially blends pre-demosaiced pixel data to reduce unwanted artifacts, (3) produces HDR images that are radiometrically correct, and (4) uses the highest-fidelity (lowest quantized-noise) pixel data available. We demonstrate a working prototype and present images and video acquired with this system.\n      \n        2 Previous Work\n        \n          2.2 Algorithms for merging HDR images\n          A common method for merging multiple LDR images into a single composite HDR image is the one of Debevec and Malik [1997], which first solves for the camera response curve that translates pixel values to the log of scene irradiance and then blends irradiances from the images together. During the merging process, the algorithm combines values from every exposure by weighting each contribution by a triangle filter that falls off as the pixel value approaches cutoff or saturation and peaks in the middle. The idea is to give more weight to pixels in the ?working range? of the camera, and less to the ones near the extrema of the camera?s operating range. As we describe in Sec. 4, however, this approach can suffer from undesirable artifacts when applied to widely-separated LDR images due to the blending between exposures. Following the work of Debevec and Malik, other researchers have proposed different weighting functions for merging differentlyexposed LDR images to reduce noise and improve the result (e.g., [Mitsunaga and Nayar 1999; Robertson et al. 2003; Kao 2008; Granados et al. 2010]). These approaches typically work on each pixel of the final HDR image independently and use only the information contained within the respective pixel in each of the LDR images. Unlike these approaches, we propose to use additional information available in the neighborhood of a pixel to reduce the noise in our final irradiance estimate. Finally, others have presented algorithms for fusing the LDR images together without explicitly creating an HDR image first (e.g., [Agarwala et al. 2004; Mertens et al. 2008]). These methods do not produce a true, radiometrically-correct HDR image, so the results cannot be incorporated into an HDR production workflow.\n        \n      \n      2.1 HDR Acquisition systems The process of capturing HDR images has been the focus of work by dozens of researchers and hundreds of artists and photographers. As a result, there are many published papers and patents describing methods and systems for capturing HDR images. Because of space limits, we focus only on the principal technologies currently available for HDR video and refer interested readers to texts on the subject (e.g., [Myszkowski et al. 2008]) for more information. The simplest approach for HDR imaging involves taking a series of images with different exposure times (e.g., [Mann and Picard 1995; Debevec and Malik 1997]). Although this method works well for static scenes, it is not well-suited for video because of the different moments in time and exposure lengths for each photograph, which result in varying amounts of motion blur and other timerelated effects. Nevertheless, researchers have extended this approach to video, by capturing frames with alternating bright and dark exposures [Ginosar et al. 1992; Kang et al. 2003] or using a rolling shutter with varying exposures [Unger and Gustavson 2007; Krymski 2008]. These approaches require image manipulation to register the images, which also introduces artifacts.\n      ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n      41:2\n      ?\n      M. Tocci et al.\n      HE sensor 0.33 Q\n      \n        Figure 2:\n      \n      0.33 Q Q 0.33 Q ME prism sensor 0.33 Q 0.0272 Q 0.0016 Q LE sensor A traditional beamsplitting HDR optical system. Here a beamsplitting prism breaks up the light into three parts, one for each sensor fitted with different filters. Designs that use absorptive filters like this one make inefficient use of light.\n      Other researchers have proposed new camera sensors for HDR imaging. Some approaches place an array of neutral-density filters over the individual pixels of the sensor with varying amounts of absorption (e.g., [Nayar and Mitsunaga 2000]), which can require complex demosaicing algorithms. These approaches are also wasteful of light entering the camera. If the sensor has a filter pattern with four differently-exposed pixels (one of the four fully exposed), then only 1 4 pixels would receive the full exposure level from the scene. Other proposed HDR sensors have a unique response to light, either by adapting their sensitivity (e.g., [Nayar and Branzoi 2003]), measuring the pixel saturation time (e.g., [Brajovic and Kanade 1996]), or having a logarithmic response to mimic the human eye (e.g., [Seger et al. 1999]). The primary problem with all of these approaches is that they require the production of a new type of camera sensor. Although commercial-scale production of these sensors may someday be realized, they are currently expensive to manufacture, rendering these methods unusable by most researchers today. Our proposed architecture, on the other hand, performs HDR imaging independent of the sensor used, which makes it realizable using today?s technology and allows us to adopt better sensor technologies (with low-light level response, faster framerates, wider spectral response, etc.), as they are developed in the future. In approaches similar to our own, the light in the camera is split with a pyramid-shaped mirror or refracting prism and redirected toward a set of sensors fitted with absorptive filters to produce images with different exposures (e.g., Harvey [1998], Aggarwal and Ahuja [2001; 2004], and Wang et al. [2005]). The designs of these previous systems all suffer from parallax error, due to the fact that the image-forming beam is split into spatially-distinct subsections; each individual sensor ?looks? through the camera lens from a slightly different angle. As shown in recent work on handheld plenoptic cameras (e.g., [Ng et al. 2005]), this provides each of the sensors with slightly different information, which significantly affects the imaging of scenes close to the camera. These previous spatial-beamsplitting methods are also wasteful of light: the absorptive filters used to achieve the dynamic range allow only a fraction of the incoming light to the sensors. If Q Watts of radiative power enters the aperture of the camera, the three-way system shown in Fig. 2 (configured for the same dynamic range as ours) allows only 0.3622Q Watts to the sensors, wasting almost 2 3 of the available light. As Aggarwal and Ahuja [2004] point out, it is possible to vary the amount of light to each sensor by moving the beamsplitting prism away from the optical axis instead of using filters. This effectively changes the size and shape of the aperture stop for each sensor, exacerbating the problem of each sensor getting different views of the scene. Furthermore, these shiftedoptical-axis spatial-beamsplitting methods are not easily integrated with standard camera lenses, and require either custom lens manufacture or lens modification to work correctly. Another option is to split the incoming light with beamsplitters prior to the lens. For example, McGuire et al. [2007] present a design tool to create efficient beamsplitting trees with separate lenses for each sensor, and show examples of HDR imaging. This same concept was demonstrated by Soviet Montage Productions [Cole and Safai 2010], which can be implemented on a 3D filming rig with an intraocular distance of zero by using a beamsplitter to provide different light transmission to two identical lenses, one for each sensor. The two lenses must be perfectly matched, however, and zoom-, focus-, and iris-tracking can be difficult to maintain between them. In addition, putting the beamsplitter in front of the camera lens places a limit on the field of view. Finally, it is unclear how such as system could be developed into a single, hand-held unit. Our system places the beamsplitter behind a single camera lens, so it does not suffer from these limitations. Finally, there are early prototype HDR systems in industry eventually intended for commercial use, such as the SpheronVR HDRv camera [Spheron VR 2011 ]. However, their method for achieving HDR capture has not been published. While all the systems we mention in this section are capable of producing HDR video, to date no method for producing high-quality HDR video has been demonstrated that is robust and yet simple enough to be readily introduced to a wide commercial audience or implemented in a modern optics laboratory. The goal of this work is to present such a system.\n      \n        3 Efficient Optical Architecture for HDR Video\n        Our optical architecture is based on beamsplitters located between the camera lens and the sensors, which have been used in previous HDR camera designs as well as in 3-sensor color-splitting cameras [Kell and Sziklai 1951] 1 . Like these previous methods, our optical system uses a set of partially-reflecting surfaces to split the light from a single photographic lens so that it is focused onto three\n        1 The optical splitting prisms used in 3CCD cameras employ dichroic filters to split the incoming light from a camera lens into red, green, and blue portions so that each color is imaged onto its own image sensor.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n        A Versatile HDR Video Video Production System\n        ?\n        41:3\n        ME sensor 0.075 Q beam splitter 2 (94/6) Q HE 0.92 Q sensor 0.0044 Q LE sensor beam splitter 1 (92/8)\n        \n          Figure 3: Illustration of our optical architecture. We also use beamsplitters between the lens and sensors, but the key difference is that we re-use the optical path to improve our light efficiency. In the end, 99.96% of light entering the aperture arrives at the sensors. Light efficiency is important in all imaging applications.\n        \n        imaging sensors simultaneously. In our architecture, however, the light is directed back through one of the beamsplitters a second time, and the three sub-images are not split into red, green, and blue but instead are optically identical except for their light levels. This design, shown in Fig. 3 , allows us to capture HDR images using most of the light entering the camera. We use the terms ?high,? ?medium,? and ?low? exposure (HE, ME, LE, respectively) to refer to the sensors based on the amount of light each one receives. The optical splitting system in our current implementation uses two uncoated, 2-micron thick plastic beamsplitters which rely on Fresnel reflections at air/plastic interfaces so their actual transmittance/reflectance (T/R) values are a function of angle. In our arrangement, the first beamsplitter is at a 45 ? angle and has an approximate T/R ratio of 92/8, which means that 92% of the light from the camera lens is transmitted through the first beamsplitter and focused directly onto the high-exposure (HE) sensor 2 . This beamsplitter reflects 8% of the light from the lens upwards, as shown in Fig. 3 , toward the second uncoated beamsplitter, which has the same optical properties as the first but is positioned at a 90 ? angle to the light path and has an approximate T/R ratio of 94/6. Of the 8% of the total light that is reflected upwards, 94% (or 7.52% of the total light) is transmitted through the second beamsplitter and focused onto the medium-exposure (ME) sensor. The other 6% of this upward-reflected light (or 0.48% of the total light) is reflected back down by the second beamsplitter toward the first one (which is again at 45 ? ), through which 92% (or 0.44% of the total light) is transmitted and focused onto the low-exposure (LE) sensor. With this arrangement, the HE, ME and LE sensors capture images with 92%, 7.52%, and 0.44% of the total light gathered by the camera lens, respectively. Therefore, the HE and ME exposures are separated by 12.2? (3.61 stops) and the ME and LE are separated by 17.0? (4.09 stops), which means that this configuration is designed to extend the dynamic range of the sensor by 7.7 stops. This beamsplitter arrangement makes our design light efficient: a negligible 0.04% of the total light gathered by the lens is wasted. It also allows all three sensors to ?see? the same scene, so all three images are optically identical except for their light levels. Of course, the ME image has undergone an odd number of reflections and so it is flipped left-right compared to the other images, but this is fixed easily in software. The three sensors are gen-locked to capture perfectly synchronized video frames with identical exposure times.\n        2 Since T/R is also dependent on the wavelength of the light, we calculate T/R values for the full visible spectrum and integrate over the R, G, and B filter spectra in the Bayer pattern to arrive at separate T/R values for each color channel for use in our design and implementation. To simplify the discussion in this paper, however, we simply state a single average value of transmittance.\n        HE sensor ? 1 ? 2 5.45 mm 41 mm exit pupil 115 mm of lens\n        \n          Figure 4: Scale drawing of optical path of first sensor. Since uncoated beamsplitters like ours can vary in transmittance as a function of angle, we examine the exact geometrical configuration of our system at f/2.8 to determine the range of transmittance values across our sensor. In this case, ? 1 = 46.4 ? and ? 2 = 43.7 ? , which\n        \n        result in transmittance values of 91.85% and 92.38%, respectively.\n        \n          3.1 Analysis of Optical System\n          Because the exact transmission/reflection properties of our beamsplitters vary with angle, we examine how these might vary over the area of the sensor by simulating the proposed optical architecture in ZEMAX [2011]. To calculate the range of transmittance values as a function of angle, we examine the largest angular variation possible on the pellicle beamsplitter. Approaches that place the beamsplitters outside the lens, such as the optical trees of McGuire et al. [2007], can have a large range of incident angles which results in significant variation in transmission over the field of view. Unlike these approaches, our system?s internal beamsplitters receive light in a much smaller range of field angles because of the geometrical configuration of the system, shown to scale in Fig. 4 . In our case, the top-left and bottom-right corner points on the sensor have chief-ray angles at the pellicle of 46.4 ? and 43.7 ? , a difference of 2.7 ? . At f/2.8, each of these two points receives a ?10 ? cone of rays from the lens, shown in blue and red (these cones are constant in angle over the entire sensor). We calculate the transmittance of the beamsplitter by integrating over this cone of rays using a ZEMAX simulation with 1 million random rays on a 2-micron thick, uncoated plastic pellicle beamsplitter, which yields a transmittance of 91.85% for the top-left and 92.38% for the bottom-right points, a difference of about 0.5%, and close to the 92% value we used in our design calculations. Therefore variation in transmittance across the sensor is not a major issue in our system. Polarization of the incident light might affect the transmission properties of the beamsplitter as well. Although our simulations were all done with unpolarized light, it is possible to encounter linearly polarized light in outdoor scenes (e.g., from glancing reflections off water), which may change the exposure difference between sensors. However, in practice we did not see such polarization effects in the scenes we captured. We note that all of these effects may be reduced or eliminated by using a thin-film coating on the beamsplitter. This thin-film coating could be designed to have more constant transmission properties over the range of angles in the system or to reduce polarization effects. An examination of different beamsplitter coatings to address these factors is a topic for future work. Advantages of the proposed optical splitting system are that its cost and complexity are relatively low, and it is compatible with standard camera lenses. The compact light path allows integration into a single hand-held unit, something difficult to do with designs that place the beamsplitters outside the lens [McGuire et al. 2007; Cole and Safai 2010]. The optical architecture is also flexible in terms of the kind of sensor used. The use of low-cost sensors, for example, could allow the design to be integrated into consumer electronics and bring HDR video to a wide audience.\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n          41:4\n          ?\n          M. Tocci et al.\n          sample HE input\n          \n            \n          \n          sample LE input\n          \n            Figure 5: The problem with traditional merging algorithms. When applied to widely-separated LDR images, Debevec and Malik?s merging algorithm can produce artifacts in the tone mapped results, as seen in the top row of insets. Our proposed merging algorithm (left image and bottom insets) uses exclusively the data with the finest quantization available (from the brightest-exposed sensor) and blends in data from the next darker sensor when necessary. We show the input HE and LE images for a sample inset, with the LE image brightness scaled by 3? to show the noise. The HDR image has been tonemapped with FDR Tools using the same settings for both merged images.\n          \n        \n      \n      \n        4 A New Algorithm for Merging HDR Images\n        In addition to the optical architecture, we also propose a novel algorithm for merging together images acquired with our system. In order to automatically create HDR images from the widely-separated component LDR images without artifacts, we found it necessary to depart from the standard methods for merging HDR images.  4.1 Limitations of current approaches Most previous algorithms for merging HDR images from a set of LDR images with different exposures typically do so after demosaicing the LDR images and merge data pixel-by-pixel without taking neighboring pixel information into account. For this discussion, we focus on the original merging algorithm proposed by Debevec and Malik [1997]. Although this algorithm works well when the LDR images have a small exposure separation, we found the quality of the final HDR image degrades when the LDR images are separated by more than 3 photographic stops. To capture the widest dynamic range possible with the smallest number of camera sensors, it is necessary to position the LDR images further apart in exposure in our system than with traditional HDR acquisition methods. Devebec and Malik?s merging algorithm can yield undesired artifacts when applied to our camera?s data because of quantization and noise effects, as shown in Fig. 5 . This problem is exacerbated when we apply certain tone mapping operators (TMO?s), such as Fattal et al.?s local-contrast operator [2002]. These TMO?s amplify small gradient differences in the image to make them visible when the dynamic range is compressed, amplifying merging artifacts as well. Of course, when the LDR images are very close in exposure, these artifacts are considerably reduced and methods like Debevec and Malik?s work quite well. However, as HDR video systems with a small set of sensors become more widely used, this issue will become more important. Fig. 6 shows the underlying cause for these artifacts by presenting the range of scene illumination measured by the three sensors in our architecture. For illustration, we simplify the system with 4-bit sensors (as opposed to the 12-bit sensors in our implementation), which measure only 16 unique brightness values and we separate the sensors by only 1 stop (a factor of 2) in exposure. Since CMOS sensors exhibit an approximately linear relationship between incident exposure and their output value, we graph the values from the three sensors as a linear function of incident irradiance instead of the traditional logarithmic scale. In this figure, we see that the LE sensor measures the scene irradiance more coarsely than the other two sensors. For example, the HE sensor may measure 4 different pixel values in a gradient be-\n        merged results with Debevec and Malik?s algorithm\n        our proposed merging approach\n        sample illumination scene illumination darker brighter weighting factor HE sensor 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 saturation Debevec and Malik our approach ME sensor 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 saturation LE sensor 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 saturation our merged HDR image (approx) 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 saturation\n        \n          Figure 6:\n        \n        Quantization problems when merging LDR images. This figure shows the source of artifacts when merging our LDR images (simplified here to 4-bit sensors) with algorithms that always use data from all three sensors with simple weighting functions, such as that of Debevec and Malik. In this case, data from each sensor is weighted with a triangle function as shown, so there are non-zero contributions from the LE sensor at low brightness values (like the sample illumination level indicated), even though the data from the LE sensor is quantized more coarsely than that of the HE sensor. Our approach, in contrast, uses data from the higherexposure sensor as much as possible and blends in data from the next darker sensor when near saturation.\n        fore the LE sensor records a single increment. In addition, there is always some small amount of noise in the pixel values, and an error of ?1 in the LE sensor spans a 12 value range in the HE sensor for this example. Although Debevec and Malik?s algorithm blends these values together, we propose instead to use pixel values from only the longest-exposure sensor (which is less noisy) wherever possible, and blend in the next darker exposure when pixels approach saturation. More importantly, our algorithm not only examines individual pixels when merging the LDR images, but also takes into account neighboring pixels that might provide additional information to help in the denoising process.\n        \n          4.2 Our proposed HDR-merging algorithm\n          While not a new idea, one key aspect of our image-merging algorithm is to use pixel data exclusively from the brightest, most well-exposed sensor possible. Therefore, we use pixels from the HE image as much as possible, and only use pixels in the ME image if the HE pixel is close to saturation. If the corresponding ME pixel is below the saturation level, it is multiplied by a factor that adjusts it in relation to the HE pixel based on the camera?s response curve ( Fig. 7 ), given that the ME pixel receives 12.2? less irradiance than the HE pixel. If the corresponding ME pixel is above the saturation level, then we simply apply a similar process to the same pixel in the low-exposure LE image. When merging data between sensors, it may seem sufficient to follow a na?ve ?winner take all? approach and exclusively use the val- ues from the HE sensor until they become saturated and then simply switch to the next sensor [ JAI 2009 ]. We found that this does not work well in practice because it results in banding artifacts where transitions occur. Instead, we propose to gracefully transition from one sensor to the next by spatially blending pixel values between the two sensors. To do this, our algorithm scans a region around the pixel being evaluated. If any neighboring pixels in this region are saturated, then the pixel under consideration may be subject to pixel crosstalk or leakage, and the algorithm will estimate a value for the pixel based on its neighbors as described below. Algorithm details ? In our approach, HDR-merging is performed prior to demosaicing the individual Bayer color filter array images, because we found that demosaicing can corrupt colors in saturated regions (also noted by Ajdin et al. [2008]). For example, a bright orange section of a scene might have red pixels that are saturated while the green and blue pixels are not. If the image is demosaiced before being merged into HDR, the demosaiced orange color will be computed from saturated red-pixel data and non-saturated green/blue-pixel data. As a result, the hue of the orange section will be incorrectly reproduced. The only way to avoid these artifacts is to perform HDR-merging prior to demosaicing. Since the images are merged prior to the demosaicing step, our algorithm works with pixel values instead of irradiance. To produce a radiometrically-correct HDR image, we must correctly match the irradiance levels of the HE, ME, and LE sensors using the appropriate beamsplitter transmittance values for each pixel color, since these change slightly as a function of wavelength. Although we use different values to match each of the color channels, for simplicity we explain the process with average values. We consider converting a pixel value through the camera response curve, where the resulting irradiance is adjusted by the exposure level ratio (average of 12.2? for HE/ME), and this new irradiance value is converted back through the camera response curve to a new pixel value. Fig. 7 shows this 3-step process graphically. This conversion process may next be done for all HE pixel values (from 1 through 4096), to arrive at a pixel-ratio curve, which gives the scaling factor for converting each ME pixel?s value to the corresponding pixel value on the HE sensor for the same irradiance ( Fig. 8 ). In practice, separate pixel-ratio curves are calculated for each color (R,G,B) in the Bayer pattern. When comparing pixel values between HE and ME images (or between ME and LE images), we use the pixel-ratio curves as lookup tables (LUTs) to convert HE pixel values less than 4096 into ME pixel values, or vice versa. When the HE pixel values are saturated, we simply extend the pixelratio curve using the last value obtained there (approximately 8). The camera response curve can be measured with the method of Debevec and Malik [1997], by taking a set of bracketed exposures and solving for a monotonically-increasing function that relates exposure to pixel value (to within a scale constant in the linear domain). Fig. 7 shows the curve computed from the raw camera data, although a curve computed from a linear best-fit could also be used. In our case, we can factor out the exposure time (since it is constant for all three images) and produce a curve that maps pixel value directly to scene irradiance. If we call this function f (x), where x is our pixel value, then the three-step process described earlier can be reversed to map ME pixel values to HE pixel values, written as g ME?HE (x) = f ?1 (12.2f (x)). The function g ME?HE (x) is used to blend pixel values between the ME and HE sensors, and a similar function g LE?HE (x) is used to blend between the LE and the HE sensors. Once the irradiance levels of the three images have been matched, we are ready to begin the merging process. To explain our merging approach, we assume two registered LDR images (one high-exposure image I HE and a second medium-\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n          A Versatile HDR Video Video Production System\n          ?\n          41:5\n          2.5 2 2 (E) 1.5 Irradiance 1 0.5\n          \n            Figure 7:\n          \n          3 0 4 1 0 500 1000 1500 2000 2500 3000 3500 4000 4500 Pixel Value Camera response curve. This curve shows how the camera converts scene irradiance into pixel values. To compute what the ME pixel value should be for a given HE value, the HE pixel value (1) is first converted to a scene irradiance (2), which is next divided by our HE/ME attenuation ratio of 12.2. This new irradiance value (3) is converted through the camera response curve into the expected ME pixel value (4). Although this graph is approximately linear, it is not perfectly so because it is computed from the raw data, without significant smoothing or applying a linear fit.\n          exposure image I ME ) that are to be merged into an HDR image I HDR . Our approach starts with the information in the high-exposure image I HE and then combines in data from the next darker-exposure image I ME , as needed. To reduce the transition artifacts described earlier, our algorithm works on each pixel location (x, y) by looking at the information from the surrounding (2k + 1) ? (2k + 1) pixel neighborhood, denoted as N (x, y). In our implementation, we used a 5 ? 5 pixel neighborhood (k = 2), and we define a pixel to be saturated if its value is greater than 90% of the maximum pixel value (4096, for our sensor). We now specify the algorithm for each of the following 4 states for the pixel and its neighborhood: Case 1: The pixel under consideration I HE (x, y) is not saturated and N HE (x, y) has no saturated pixels, so the pixel value is used as-is: I HDR (x, y) = I HE (x, y). Case 2: I HE (x, y) is not saturated, but N HE (x, y) has 1 or more saturated pixels. Although most HDR merging algorithms would use this pixel as-is, we find that this calls into question the actual value of our pixel due to proximity effects (e.g., leakage or pixel cross-talk on the sensor). We therefore blend between the pixel value at I HE (x, y) and the one at the next darker-exposure I ME (x, y) depending on the amount of saturation present in the neighborhood. This is done in three steps: 1. Let U be the set of unsaturated pixels in neighborhood N HE (x, y), where |U | is the number of unsaturated pixels. 2. Let |N HE (x, y)| be the number of pixels in neighborhood N HE (x, y). An interpolation coefficient ? can be computed as ? = |U |/|N HE |, which represents the fraction of unsaturated pixels in the neighborhood. 3. The output pixel is then given by the blended value: I HDR (x, y) = ?I HE (x, y)+(1??)g ME?HE (I ME (x, y)), where we use the pixel-ratio LUT to map the ME value into the HE range. This blends the measurement at the higher exposure I HE (x, y) with I ME (x, y) based on the number of saturated pixels in the neighborhood N HE (x, y). Case 3: I HE (x, y) is saturated but N HE (x, y) has 1 or more nonsaturated pixels, which can be used to better estimate a value for I HE (x, y). We calculate the ratios of pixel values in the ME image between the unsaturated pixels in the neighborhood and the center pixel, and use this map of ME ratios to estimate the actual value of the saturated pixel under consideration. This is done in four steps: 1. As in Case 2, compute U , |U |, and the coefficient ?\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n          41:6\n          ?\n          M. Tocci et al.\n          10 value 8 Calculated ra o from camera calibra on curve Measured ra o from images 6 4 2\n          \n            Figure 8:\n          \n          0 0 500 1000 1500 2000 2500 3000 3500 4000 4500 HE Pixel Value Pixel ratio curve. The blue curve is obtained when the 3-step process ( Fig. 7 ) is applied to all red pixel values in the HE image using the camera response curve, and the ratio between HE and ME pixel values is calculated. The red curve is measured experimentally by taking the ratio of a large set of red HE and ME pixels in actual images captured by our system. The two match reasonably, validating our approach. This red curve is used by the algorithm as a look-up table (LUT) of scale factors to convert red ME pixel values to blend them smoothly with red HE pixel values. Similar curves were computed for the green and blue channels.\n          2. Compute a ratio map R of the ratios between the center pixel and each pixel of the neighborhood from the ME image: R(x, y) i = I ME (x, y)/N ME (x, y) i , for all pixels i in the ME neighborhood N ME (x, y). 3. Now compute an estimated I ? HE (x, y) for the saturated pixel by scaling the unsaturated pixel values in the neighborhood I N ? HE HE (x, with y) the = |U ratios 1 | P i?U computed R i N HE in (x, step y) i 2: .\n          4. Finally, blend estimated I ? HE (x, y) with I ME (x, y) using an equation similar to that of Case 2, step 3: I HDR (x, y) = ? I ? HE (x, y) + (1 ? ?)g ME?HE (I ME (x, y)). Case 4: I HE (x, y) is saturated and all pixels in N HE (x, y) are saturated, so we do not have any valid information from the highexposure image. In this case, we simply use the ME image and set I HDR (x, y) = I ME (x, y). This is our algorithm for combining data from two sensors. When there are three LDR images, the process above is simply repeated in a second iteration, substituting I HDR for I HE and I LE for I ME . In this manner, we merge data from the higher exposures as we work our way toward the lowest exposure, and only use data from lower exposures when the higher-exposure data is at or near saturation. We found that this algorithm reduces artifacts in the merged image considerably, as shown in Fig. 5 . The output of our algorithm is an HDR image that can be demosaiced and converted from pixel values to irradiance using a camera response curve similar to that of Fig. 7 accounting for all 3 color channels. The final HDR full-color image may then be tonemapped with commercial software packages (e.g., FDRTools, HDR Expose, Photomatix, etc.), or as described in the literature (e.g., [Smith et al. 2006]).\n        \n      \n      \n        5 System Implementation\n        We implemented a prototype of our design using three Silicon Imaging SI-1920HD high-end cinema CMOS sensors mounted inside a custom camera body. Each 12-bit sensor was connected to a Dalsa/Coreco x64-CL iPro frame grabber in a PC. These sensors have 1920 ? 1080 pixels (5 microns square) with a standard Bayer color filter array, and can measure a dynamic range of around 10 stops (excluding noise). The sensors were aligned by aiming the camera at small pinhole light sources, locking down the HE sensor and then adjusting setscrews to align the ME and LE sensors. In our prototype, the final image registration was accurate to less than  5 microns with a rotation error of less than 0.1 ? . Note that, in general, the required registration accuracy is modulo the 2 ? 2 pixel array of the Bayer pattern, because while pixels of one color in a sensor must match with the same color in another, a shift of any number of 2 ? 2 Bayer patterns can be handled in software. We discuss how registration tolerances affect our results in Sec. 7. The camera body has a Hasselblad lens mount to allow the use of high-performance, interchangeable commercial lenses. For beamsplitters, our current prototype system employs uncoated pellicle beamsplitters, such as the ones sold by Edmund Optics [part number NT39-482]. Although pellicle beamsplitters are not ideal for commercial products (they can be delicate and difficult to clean and maintain), we used them because of their low cost and accessibility. However, we found them to be robust for our application, as we were able to use our system successfully in various locations and environmental conditions. A picture of the completed camera prototype, which cost less than US$15k in parts, is shown in Fig. 9 . We calculated the camera?s response curve as described by Devebec and Malik [1997] with a set of bracketed exposures from the HE sensor ( Fig. 7 ). We also measured the ratio between the HE and ME pixel values experimentally for each color channel and observed that it matched the curve predicted by our 3-step process ( Fig. 8 ). This curve was implemented as a LUT to translate values from the ME to the HE sensor range for merging, and a similar process was used to obtain and implement the HE/LE ratio curve. The LE sensor values are also adjusted with a slight offset before being used in the merging algorithm to account for a small amount of stray light. The workflow for processing the data coming from the camera is shown in Alg. 1. Most of the steps are self-explanatory and are typical for HDR systems. The non-uniform correction (NUC) step is applied to each image to correct for non-uniformities in the sensor. This is a 2-point correction, which involves taking a dark field image for offset and a uniformly-lit diffuse scene for gain. The HDR merging algorithm presented in Sec. 4 was implemented in MATLAB. The demosaicing step used the algorithm of Malvar et al. [2004], and we tonemapped the final HDR images using HDR processing tools such as HDR Expose and Photomatix.\n        \n          \n          Figure 9: Completed camera prototype. (left) The optical path of our completed implementation. (right) The finished prototype used to acquire the video images in this paper.\n        \n      \n      \n        6 Results\n        Once the system was built, we performed a radiometric calibration test to measure its dynamic range. To do this, we aimed the camera at a stepped array of neutral density filters, in steps of 1 stop each, and focused down the sun?s light to an integrating sphere behind the array to provide a uniform, bright light source. A single frame of image data was captured with all 3 sensors simultaneously at an exposure time of 1/30 second. Fig. 10 shows the result of this experiment. In our current prototype, we are able to clearly measure a dynamic range of over 2 17 to 1, equivalent to over 17 photographic stops or over 100dB. More importantly, we demonstrate that our optical architecture can increase the dynamic range of these partic-\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n        A Versatile HDR Video Video Production System\n        ?\n        41:7\n        Algorithm 1 HDR Processing Workflow 1: Read in HE, ME, and LE raw images from the camera 2: Apply NUC (2 point correction) and fix registration shifts modulo Bayer pattern 3: Merge HDR image using algorithm in Sec. 4 4: Demosaic HDR image to produce full-color image 5: White balancing, color correction, conversion to HDR format 6: Tonemapping and filtering (saturation, brightness, sharpness) 7: Build final video from individual frames\n        satura on 1 2 3 irradiance 4 5 6 7 Measured 8 9 10 11 transi on not 12 13 discernible 14 15 16 17\n        \n          \n        \n        Posi on on filter array\n        \n          Figure 10: Dynamic range test. The dynamic range of our cam-\n        \n        era was tested by imaging a set of neutral density filters varying in transmittance by powers of 2. We focused the sun onto an integrating sphere behind the filters for uniform illumination. On the left is a tonemapped HDR image of the filters taken by our prototype. On the right is a log plot of the measured intensity across the HDR image (the kink in the curve at step 10 is due to the wrap-around in the image). The dynamic range of the camera can be measured by finding the last pair of filters where there is no noticeable difference in the measured intensity. This happens after filter 17, which means our prototype can capture over 17 stops of dynamic range.\n        ular camera sensors by 7 stops (7.7 was calculated from the design) using available optical and electronic components. We also tested our proposed merging algorithm against that of Debevec and Malik [1997]. In every scene we tested, our approach was able to produce higher quality images than this algorithm because of the quantization artifacts it introduces with widely-separated exposure images (see Figs. 5 and 11). We found it useful to visualize the merging process by indicating which parts of the image were being selected from the different LDR images ( Fig. 11 ). Finally, Figs. 1, 11, and 13 show frames from video captured with our prototype and tonemapped in different ways. All videos were captured at 1920 ? 1080 at 24 or 30 fps and show a wide variety of HDR scenes with complex movement and lens effects. The C UT TING T ORCH scene in Fig. 1 shows an oxy-acetylene torch cutting a steel plate, tonemapped with Photomatix. The sparks are correctly motion-blurred because of the simultaneous capture on all sensors. We are also able to capture detail in the bright molten metal and the dark surroundings at the same time, suggesting that the proposed system could be used in industrial and manufacturing applications. The B ALLS & W ATER scene in Fig. 11 shows rapidly-moving, colored objects all correctly motion-blurred in bright sunlight. The uniform-color spheres present a challenging gradient for the merging algorithm. The first two images of Fig. 13 , S NOW D OG and W ASH M E , were captured in the snow in direct sunlight. Filming snowy scenes in bright sunlight is notoriously difficult, especially with dark-colored subjects, yet our system captures detail in both the snow and the dark areas. The G O -C HIPS & F LAME scene shows an intense flame and a set of objects in direct sunlight and deep shadow. M ELTING S NOW is a very challenging scene that is impossible to capture with conventional cameras, because it was taken from inside a dimly-lit room looking out at a bright outdoor scene. Our system can simultaneously capture both the wall decoration inside and the structure of the clouds outside which are several orders of magnitude brighter. This scene also features a focus-pull during the shot from the distant mountains to the foreground, which is difficult to do with systems that split the beam before the lens (e.g., [Cole and Safai 2010]). M ONKEY shows a stuffed animal with bright white fur in direct sunlight. Fine details in the fur and the dark bucket are accurately captured. Finally, the L IGHTING T ORCH scene shows a focus pull from a bright background to a dimly-lit subject in the foreground. A bright torch flame is added, which provides flickering illumination to the subject?s otherwise darkened face. These results all demonstrate the quality of images that our prototype system can capture.\n        \n          \n          Figure 11: Another comparison of our merging algorithm. The top image shows the HDR image merged by our algorithm after tonemapping with Photomatix, with the 3 LDR inputs on the side. The bottom two rows show comparisons between Debevec and Malik?s algorithm (left) and ours (middle), along with a visualization showing how we select pixel values (right).\n        \n      \n      \n        7 Discussion\n        Sensor alignment and merging ? Our prototype was aligned so that mis-registration between images was less than 5 microns. We note that mis-registration could cause sub-pixel artifacts in regions of sharp detail when the data is merged into HDR prior to the demosaicing process. However, in general, our spatially-blending merging algorithm helps reduce these by using values from a single sensor as much as possible, and only blending in data from other sensors when it approaches saturation. This limits the regions where blending mis-registered values would cause a problem to those that are near saturated pixels and have sharp detail. Furthermore, although there can be artifacts from merging the data prior to demosaicing, the color artifacts caused by merging data postdemosaic (which happens in areas where only some color channels are saturated) are more serious because they can happen in large, smooth regions in the image. Overall, mis-registration artifacts can be mitigated by simply using sensors with larger pixels. Also, the same challenge of sensor alignment is addressed in 3CCD systems for full-resolution color. These commercial products are typically aligned to very tight tolerances (much smaller than a single pixel), and we expect production versions of our proposed system to benefit from similar commercial alignment techniques. Merging algorithm and noise reduction ? Although our algorithm relies on data from only a single sensor until it approaches saturation to reduce quantization artifacts and noise from darker sensors, it might be possible to leverage information from the other sensors appropriately for further noise reduction. Of course, if our images were spaced very closely together, simple approaches like that of Debevec and Malik would reduce noise. With our spacing, it might  be possible to combine our spatial-blending approach (which looks at a neighborhood of pixels, not just the pixel being merged) with merging algorithms that weigh other images based on their noise content (e.g., [Granados et al. 2010; Hasinoff et al. 2010]) to produce better results. This is an interesting subject for future work. Flexible design ? We present a simple blueprint for an HDR video imaging system using off-the-shelf components that can produce high-quality HDR video footage. The use of commerciallyavailable sensors makes it easy to change them to extend the capabilities of the camera. Furthermore, the number of sensors can be extended with minor modifications to the optical path. Fig. 12 shows an optical design configured with 5 sensors, which would allow the camera to have an even higher dynamic range, or to produce higher quality images at the same dynamic range. Toward better, low-cost consumer cameras ? Although a system like ours could be used in high-end imaging applications such as feature film production, it could also impact the consumer camera market. First, the proposed architecture might enable high-quality, low-cost imaging because it allows three cheap sensors with limited dynamic range to capture images with dynamic range comparable to that of a single, high-end sensor. Second, HDR imaging would arguably benefit consumers more than professional cinematographers because the typical consumer does not have the lighting rigs or training needed to achieve HDR effects with LDR systems. Limitations ? Any HDR system (including the human eye) is ultimately limited in its ability to capture wide dynamic range by veiling glare in the optics [McCann and Rizzi 2007]. Our design is compatible with higher-performance lenses, which may help reduce veiling glare. Our optical architecture also provides the highexposure sensor with only 92% of the light of the scene, which compares favorably with the 33% provided by previous internal beamsplitting systems, but is less light than would be captured with a single sensor and lens in a traditional camera. Future work ? In terms of the optical system, a study of thinfilm coatings on the beamsplitters to reduce angleor polarizationdependent effects (as discussed in Sec. 3.1) could be explored. The optical system would also benefit from improvements in the sensor alignment. In terms of the proposed merging algorithm, studies of noise optimization during the image merging process, in conjunction with our spatial-blending method, would make an interesting subject for future work. Finally, it would be useful to develop software packages for HDR cinematography for use with systems like the one proposed. These would give directors of photography the same kinds of tools in post-process that they are used to having at their disposal when lighting the set.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n        41:8\n        ?\n        M. Tocci et al.\n        sensor 3 sensor 4 sensor 2 sensor 1 main lens sensor 5\n        \n          Figure 12: Extension of our optical architecture. Our optical architecture can be extended to more sensors for a better image quality or higher dynamic range. Here we show a design for a system with 5 sensors, where we essentially replace the HE and ME sensors with two other beamsplitters and a pair of sensors for each. The design was simulated with ZEMAX [2011].\n        \n      \n      \n        8 Conclusion\n        In conclusion, we have presented an optical architecture for highdynamic range video that makes efficient use of the incoming light and produces cinema-quality images. We also proposed a novel HDR merging algorithm that minimizes quantization artifacts from the LDR data by using the highest-exposure data until saturation before spatially-blending in darker-exposure data. To test our design, we built a working prototype of the system and showed images from video acquired with the system. Acknowledgment ? We thank the anonymous reviewers for their helpful comments that improved the final version of the paper.\n      \n      \n        References\n        \n          A GARWALA , A., D ONTCHEVA , M., A GRAWALA , M., D RUCKER , S., C OLBURN , A., C URLESS , B., S ALESIN , D., AND C OHEN , M. 2004. Interactive digital photomontage. ACM Trans. Graph. 23 (August), 294?302.\n          A GGARWAL , M., AND A HUJA , N. 2001. Split aperture imaging for high dynamic range. In Proceedings of ICCV 2001, 10 ? 17.\n          A GGARWAL , M., AND A HUJA , N. 2004. Split aperture imaging for high dynamic range. International Journal of Computer Vision 58, 7?17.\n          A JDIN , B., H ULLIN , M., F UCHS , C., S EIDEL , H.-P., AND L ENSCH , H. 2008. Demosaicing by smoothing along 1D features. In Proceedings of CVPR 2008, 1?8.\n          B RAJOVIC , V., AND K ANADE , T. 1996. A sorting image sensor: an example of massively parallel intensity-to-time processing for low-latency computational sensors. In Proceedings of ICRA, 1996, vol. 2, 1638?1643.\n          C OLE , A., AND S AFAI , M., 2010. Soviet Montage Productions. http://www.sovietmontage.com/.\n          D EBEVEC , P. E., AND M ALIK , J. 1997. Recovering high dynamic range radiance maps from photographs. In Proceedings of ACM SIGGRAPH 1997, 369?378.\n          F ATTAL , R., L ISCHINSKI , D., AND W ERMAN , M. 2002. Gradient domain high dynamic range compression. ACM Trans. Graph. 21 (July), 249?256.\n          G INOSAR , R., H ILSENRATH , O., AND Z EEVI , Y., 1992. Wide dynamic range camera. United States Patent #5,144,442.\n          G RANADOS , M., A JDIN , B., W AND , M., T HEOBALT , C., S EI DEL , H.-P., AND L ENSCH , H. 2010. Optimal HDR reconstruction with linear digital cameras. In Proceedings of CVPR 2010.\n          H ARVEY , R., 1998. Optical beam splitter and electronic high speed camera incorporating such a beam splitter. United States Patent #5,734,507.\n          H ASINOFF , S. W., D URAND , F., AND F REEMAN , W. T. 2010. Noise-optimal capture for high dynamic range photography. In Proceedings of CVPR 2010.\n          JAI. 2009. Using image fusion to capture high-dynamic range (HDR) scenes. Tech. Rep. TN-0903. JAI, Inc., Japan.\n          K ANG , S. B., U YTTENDAELE , M., W INDER , S., AND S ZELISKI , R. 2003. High dynamic range video. In Proceedings of ACM SIGGRAPH 2003, ACM, 319?325.\n          K AO , W.-C. 2008. High dynamic range imaging by fusing multiple raw images and tone reproduction. IEEE Transactions on Consumer Electronics 54, 1 (Feb.), 10 ? 15.\n          K ELL , R., AND S ZIKLAI , G., 1951. Simultaneous color television. United States Patent #2,560,351.\n          K RYMSKI , A., 2008. High dynamic range imager with a rolling shutter. United States Patent #7,397,509.\n          M ALVAR , H., WEI H E , L., AND C UTLER , R. 2004. High-quality linear interpolation for demosaicing of Bayer-patterned color images. In Proceedings of ICASSP 2004, vol. 3, 485?488.\n          M ANN , S., AND P ICARD , R. W. 1995. On being ?undigital? with digital cameras: Extending dynamic range by combining differently exposed pictures. In Proceedings of Society for Imaging Science and Technology?s 48th Annual Conference, 442?448.\n          M C C ANN , J., AND R IZZI , A. 2007. Veiling glare: the dynamic range limit of HDR images. In Human Vision and Electronic Imaging XII, SPIE.\n          M C G UIRE , M., M ATUSIK , W., P FISTER , H., C HEN , B., H UGHES , J., AND N AYAR , S. 2007. Optical splitting trees for high-precision monocular imaging. IEEE Computer Graphics and Applications 27, 2 (march-april), 32?42.\n          M ERTENS , T., K AUTZ , J., AND R EETH , F. V. 2008. Exposure fusion: A simple and practical alternative to high dynamic range photography. Computer Graphics Forum 28, 1, 161?171.\n          M ITSUNAGA , T., AND N AYAR , S. 1999. Radiometric self calibration. In Proceedings of CVPR 1999, vol. 1, 374?380.\n          M YSZKOWSKI , K., M ANTIUK , R., AND K RAWCZYK , G. 2008. High Dynamic Range Video. Morgan & Claypool.\n          N AYAR , S., AND B RANZOI , V. 2003. Adaptive dynamic range imaging: optical control of pixel exposures over space and time. In Proceedings of ICCV 2003, 1168 ? 1175.\n          N AYAR , S., AND M ITSUNAGA , T. 2000. High dynamic range imaging: spatially varying pixel exposures. In Proceedings of CVPR 2000, 472 ? 479.\n          N G , R., L EVOY , M., B R ? DIF , M., D UVAL , G., H OROWITZ , M., AND H ANRAHAN , P. 2005. Light field photography with a hand-held plenoptic camera. Tech. Rep. CSTR 2005-02, Stanford University.\n          R OBERTSON , M. A., B ORMAN , S., AND S TEVENSON , R. L. 2003. Estimation-theoretic approach to dynamic range enhancement using multiple exposures. Journal of Electronic Imaging 12, 2 (april), 219?228.\n          S EGER , U., A PEL , U., AND H OFFLINGER  ? , B. 1999. HDRCImagers for natural visual perception. In Handbook of Computer Vision and Application, B. J?hne, H. Hau?ecker, and P. Gei?ler, Eds., vol. 1. Academic Press, 223?235.\n          S MITH , K., K RAWCZYK , G., M YSZKOWSKI , K., AND S EIDEL , H.-P. 2006. Beyond tone mapping: Enhanced depiction of tone mapped HDR images. Computer Graphics Forum 25, 3, 427? 438.\n          S PHERON VR, 2011. http://www.spheron.com/.\n          U NGER , J., AND G USTAVSON , S. 2007. High-dynamic-range video for photometric measurement of illumination. SPIE, vol. 6501, 65010E.\n          W ANG , H., R ASKAR , R., AND A HUJA , N. 2005. High dynamic range video using split aperture camera. In Proceedings of OMNIVIS 2005.\n          ZEMAX, 2011. Optical design software. http://www.zemax.com/.\n        \n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n        A Versatile HDR Video Video Production System\n        ?\n        41:9\n        \n          \n          Figure 13:\n        \n        NOW ONKEY IGHTING ORCH ACM Transactions on Graphics, Vol. 30, No. 4, Article 41, Publication date: July 2011.\n        Images from videos captured with our camera. These are sample tonemapped images that demonstrate different imaging HDR effects. The LDR images captured by three sensors in our system are shown on the right. The scenes are, from top to bottom, S NOW D OG , W ASH M E , G O C HIPS & F LAME , M ELTING S , M , and L T .\n      \n    \n  ",
  "resources" : [ ]
}