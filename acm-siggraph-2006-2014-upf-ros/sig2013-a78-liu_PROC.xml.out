{
  "uri" : "sig2013-a78-liu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a78-liu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Bundled Camera Paths for Video Stabilization",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shuaicheng-Liu",
      "name" : "Shuaicheng",
      "surname" : "Liu"
    }, {
      "uri" : "http://drinventor/Lu-Yuan",
      "name" : "Lu",
      "surname" : "Yuan"
    }, {
      "uri" : "http://drinventor/Ping-Tan",
      "name" : "Ping",
      "surname" : "Tan"
    }, {
      "uri" : "http://drinventor/Jian Sun-null",
      "name" : "Jian Sun",
      "surname" : null
    } ]
  },
  "bagOfWords" : [ "evaluation", "large", "variety", "consumer", "video", "demonstrate", "merit", "we", "method", "specifically", "we", "present", "bundle", "camera", "path", "model", "which", "maintain", "multiple", "spatially-variant", "camera", "path", "we", "bundle", "camera", "path", "model", "build", "two", "novel", "component", "warping-based", "motion", "representation", "-lrb-", "estimation", "-rrb-", "adaptive", "space-time", "path", "smoothing", "Notice", "as-similar-as-possible", "warp", "use", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "-rsb-", "video", "stabilization", "we", "second", "component", "smooth", "all", "bundle", "camera", "path", "whole", "maintain", "both", "spatial", "temporal", "coherence", "quantitative", "evaluation", "we", "provide", "comprehensive", "dataset", "-lrb-", "include", "both", "public", "example", "we", "own", "video", "clip", "different", "kind", "motion", "-rrb-", "earlier", "work", "-lsb-", "Morimoto", "Chellappa", "1998", "Matsushita", "et", "al.", "2006", "-rsb-", "apply", "low-pass", "filter", "individual", "model", "parameter", "we", "use", "spatially-variant", "model", "represent", "motion", "between", "video", "frame", "design", "appropriate", "smoothing", "technique", "model", "still", "require", "moderate", "feature", "track", "length", "-lrb-", "typically", "over", "20", "frame", "-rrb-", "Optical", "flow", "algorithm", "-lsb-", "Lucas", "Kanade", "1981", "-rsb-", "model", "transition", "individual", "displacement", "vector", "every", "pixel", "we", "motion", "model", "essentially", "mesh-based", "spatially-variant", "homography", "model", "inspire", "recent", "image", "warping", "technique", "-lsb-", "Igarashi", "et", "al.", "2005", "Schaefer", "et", "al.", "2006", "Liu", "et", "al.", "2009", "-rsb-", "similar", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "method", "correct", "roll", "shutter", "effect", "without", "any", "prior", "calibration", "we", "warping-based", "model", "naturally", "handle", "rolling", "shutter", "effect", "special", "kind", "spatially", "variant", "motion", "we", "adopt", "warping", "model", "-lsb-", "Igarashi", "et", "al.", "2005", "Liu", "et", "al.", "2009", "-rsb-", "Model", "each", "frame", "we", "define", "uniform", "grid", "mesh", "illustrate", "figure", "we", "require", "match", "feature", "-lrb-", "e.g.", "figure", "-rrb-", "share", "same", "bilinear", "interpolation", "four", "corner", "enclose", "grid", "cell", "after", "warp", "thus", "warping-based", "motion", "model", "actually", "set", "spatially-variant", "homography", "2d", "grid", "note", "highly", "flexible", "model", "able", "handle", "parallax", "between", "global", "homography", "per-pixel", "optical", "flow", "combination", "shape-preserving", "mesh", "representation", "together", "provide", "two", "kind", "regularization", "-rrb-", "each", "cell", "fitted", "homography", "should", "bias", "toward", "reduce", "similarity", "-lrb-", "rigid", "-rrb-", "transformation", "-rrb-", "intrinsic", "connection", "mesh", "-lrb-", "two", "neighbor", "mesh", "cell", "share", "two", "vertex", "-rrb-", "enforce", "first-order", "continuity", "constraint", "finally", "we", "estimate", "motion", "minimize", "two", "energy", "term", "datum", "term", "match", "feature", "shape-preserving", "term", "enforce", "regularization", "we", "first", "describe", "we", "basic", "method", "follow", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "later", "extend", "better", "robustness", "next", "subsection", "we", "expect", "corresponding", "feature", "can", "represent", "same", "weight", "warped", "grid", "vertex", "-lsb-", "-rsb-", "here", "contain", "all", "warped", "grid", "vertex", "shape-preserving", "term", "we", "use", "same", "shape-preserving", "term", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "involve", "all", "vertex", "-lrb-", "-rrb-", "sr", "90", "-lrb-", "-rrb-", "90", "-lrb-", "-rrb-", "where", "known", "scalar", "compute", "from", "initial", "mesh", "shape-preserving", "term", "require", "triangle", "neighbor", "vertex", "follow", "similarity", "transformation", "estimate", "homography", "after", "have", "new", "mesh", "we", "can", "estimate", "each", "local", "homography", "-lrb-", "-rrb-", "grid", "cell", "frame", "solve", "linear", "equation", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "four", "vertex", "before", "after", "warping", "Figure", "show", "warped", "mesh", "grid", "accord", "estimated", "motion", "leave", "right", "result", "without", "shapepreserving", "term", "clear", "regularization", "term", "help", "maintain", "smooth", "vary", "mesh", "representation", "outlier", "rejection", "we", "reject", "incorrectly", "match", "feature", "two", "scale", "coarse", "scale", "-lrb-", "whole", "image", "-rrb-", "we", "apply", "ransac", "algorithm", "-lsb-", "fischler", "Bolles", "1981", "-rsb-", "fit", "global", "homography", "-lrb-", "-rrb-", "discard", "feature", "relatively", "large", "threshold", "fitting", "error", "-lrb-", "image", "width", "-rrb-", "fine", "scale", "-lrb-", "sub-image", "-rrb-", "we", "apply", "ransac", "again", "reject", "feature", "relatively", "small", "threshold", "-lrb-", "image", "width", "-rrb-", "pre-warping", "facilitate", "warping", "estimation", "we", "use", "global", "homography", "-lrb-", "-rrb-", "bring", "match", "feature", "closer", "we", "solve", "warping", "estimate", "residual", "motion", "which", "generate", "homography", "-lrb-", "-rrb-", "each", "grid", "cell", "final", "homography", "-lrb-", "-rrb-", "simply", "compute", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "note", "coarse-to-fine", "strategy", "have", "be", "use", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "image", "synthesis", "proven", "effective", "motion", "estimation", "literature", "-lsb-", "Brox", "et", "al.", "2004", "-rsb-", "different", "scene", "-lrb-", "-rrb-", "scene", "free", "occlusion", "-lrb-", "-rrb-", "scene", "severe", "occlusion", "when", "occlusion", "insufficient", "feature", "we", "prefer", "stronger", "regularization", "datum", "term", "less", "reliable", "implement", "strategy", "we", "adaptively", "set", "per", "frame", "base", "two", "error", "fitting", "error", "smoothness", "error", "fitting", "error", "average", "residual", "feature", "match", "under", "estimate", "homography", "i.e.", "where", "homography", "cell", "contain", "number", "feature", "pair", "here", "homography", "matrix", "normalize", "so", "sum", "all", "its", "element", "one", "we", "empirically", "set", "0.01", "since", "make", "scale", "similar", "most", "example", "we", "equally", "discretize", "10", "value", "between", "0.3", "we", "perform", "model", "estimation", "use", "every", "discretize", "value", "select", "model", "minimum", "error", "so", "we", "choose", "small", "-lrb-", "0.9", "-rrb-", "better", "minimize", "datum", "error", "contrary", "scene", "large", "occlusion", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "neighbor", "local", "homography", "less", "similar", "finally", "we", "show", "example", "Figure", "verify", "strength", "regularization", "we", "method", "example", "we", "compare", "two", "mesh", "estimate", "use", "all", "feature", "subset", "feature", "two", "similar", "result", "indicate", "we", "method", "can", "robustly", "deal", "region", "insufficient", "feature", "estimate", "local", "homography", "we", "can", "define", "bundle", "spatially-variant", "camera", "path", "whole", "video", "let", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "Relationships", "among", "original", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "smooth", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "transformation", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "camera", "pose", "grid", "cell", "frame", "t.", "-lrb-", "-rrb-", "-rcb-", "estimate", "local", "homography", "same", "grid", "cell", "show", "Figure", "-lrb-", "-rrb-", "we", "call", "spatially-variant", "path", "bundle", "camera", "path", "next", "section", "we", "describe", "how", "we", "smoothen", "bundle", "path", "video", "stabilization", "we", "first", "describe", "we", "smoothing", "method", "single", "camera", "path", "extend", "bundle", "camera", "path", "good", "camera", "path", "smoothing", "should", "consider", "multiple", "compete", "factor", "remove", "jitters", "avoid", "excessive", "crop", "minimize", "various", "geometrical", "distortion", "-lrb-", "shearing/skewing", "wobble", "-rrb-", "Formulation", "give", "original", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "we", "seek", "optimize", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "minimize", "follow", "function", "-lrb-", "-rrb-", "where", "neighborhood", "frame", "other", "term", "datum", "term", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "enforce", "new", "camera", "path", "close", "original", "one", "reduce", "crop", "distortion", "smoothness", "term", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "stabilize", "path", "weight", "-lrb-", "-rrb-", "preserve", "motion", "discontinuity", "under", "fast", "panning/rotation", "scene", "transition", "video", "rapid", "camera", "panning", "camera", "path", "top", "plot", "x-translation", "over", "time", "parameter", "balance", "above", "two", "term", "since", "equation", "quadratic", "we", "can", "solve", "any", "linear", "system", "solver", "initialization", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "once", "we", "obtain", "optimize", "path", "we", "compute", "warping", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "warp", "original", "video", "frame", "stabilize", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "discontinuity-preserving", "adaptive", "weight", "important", "preserve", "motion", "discontinuity", "-lrb-", "-rrb-", "measure", "change", "two", "camera", "pose", "we", "use", "large", "kernel", "ensure", "successful", "suppression", "both", "highfrequency", "jitters", "-lrb-", "e.g.", "handshake", "-rrb-", "low-frequency", "bounce", "-lrb-", "e.g.", "walk", "-rrb-", "we", "implementation", "we", "set", "60", "neighbor", "frame", "standard", "deviation", "-lrb-", "-rrb-", "10", "contrast", "previous", "low-pass", "filter", "base", "method", "-lsb-", "Matsushita", "et", "al.", "2006", "-rsb-", "typically", "need", "smaller", "amount", "support", "-lrb-", "e.g.", "10", "frame", "-rrb-", "avoid", "aggressive", "crop", "distortion", "small", "kernel", "often", "insufficient", "suppress", "low", "frequency", "bounce", "reason", "why", "we", "can", "use", "larger", "kernel", "lie", "-lrb-", "-rrb-", "video", "stabilization", "rapid", "camera", "motion", "-lrb-", "e.g", "cause", "fast", "panning", "scene", "transition", "-rrb-", "inappropriate", "amount", "smoothing", "may", "lead", "excessive", "crop", "show", "figure", "case", "camera", "pan", "quickly", "na?ve", "gaussian", "smoothing", "-lrb-", "second", "row", "-rrb-", "cause", "camera", "path", "significantly", "deviate", "from", "its", "original", "path", "indicate", "dash", "line", "left", "plot", "top", "corresponding", "frame", "show", "second", "row", "require", "large", "crop", "we", "adaptive", "term", "-lrb-", "-rrb-", "preserve", "sudden", "camera", "motion", "certain", "degree", "result", "from", "we", "adaptive", "smoothing", "-lrb-", "bottom", "row", "-rrb-", "produce", "much", "less", "crop", "measure", "camera", "motion", "we", "use", "change", "translation", "component", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "extract", "from", "camera", "pose", "-lrb-", "-rrb-", "namely", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "frame", "translation", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "can", "describe", "most", "camera", "motion", "practice", "except", "in-plane", "rotation", "scale", "around", "principal", "axis", "principle", "we", "could", "formulate", "constrain", "optimization", "address", "issue", "we", "first", "run", "optimization", "global", "fix", "-lrb-", "empirically", "set", "-rrb-", "check", "crop", "ratio", "distortion", "every", "frame", "note", "accord", "equation", "smaller", "make", "optimize", "path", "closer", "original", "one", "which", "have", "less", "crop", "distortion", "procedure", "iterate", "until", "all", "frame", "satisfy", "requirement", "we", "measure", "crop", "ratio", "distortion", "from", "warping", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "anisotropic", "scaling", "-lrb-", "-rrb-", "measure", "distortion", "we", "use", "-lrb-", "-rrb-", "compute", "overlap", "area", "original", "video", "frame", "stabilize", "frame", "crop", "ratio", "ratio", "area", "original", "frame", "area", "we", "experiment", "we", "require", "crop", "ratio", "larger", "than", "0.8", "distortion", "score", "larger", "than", "0.95", "all", "example", "principle", "we", "can", "further", "measure", "perspective", "distortion", "two", "perspective", "component", "-lrb-", "-rrb-", "we", "empirically", "find", "always", "too", "small", "when", "compare", "affine", "component", "do", "include", "they", "we", "motion", "model", "generate", "bundle", "camera", "path", "path", "optimize", "independently", "neighbor", "path", "could", "less", "consistent", "which", "may", "generate", "distortion", "final", "render", "video", "hence", "we", "do", "space-time", "optimization", "all", "path", "minimize", "follow", "objective", "function", "-lrb-", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "j?n", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "include", "eight", "neighbor", "grid", "cell", "first", "term", "objective", "function", "equation", "each", "single", "path", "second", "term", "enforce", "smoothness", "between", "neighbor", "path", "optimization", "also", "quadratic", "optimum", "result", "can", "obtain", "solve", "large", "sparse", "linear", "system", "again", "we", "solution", "update", "jacobi-based", "iteration", "-lsb-", "bronshtein", "Semendyayev", "1997", "-rsb-", "-lrb-", "+1", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "2p", "-lrb-", "-rrb-", "-rrb-", "??", "j?n", "-lrb-", "-rrb-", "where", "2n", "-lrb-", "-rrb-", "??", "comparison", "determine", "from", "global", "path", "-lrb-", "generate", "concatenate", "pre-warping", "global", "homography", "-rrb-", "because", "control", "overall", "crop", "distortion", "we", "use", "optimize", "camera", "path", "all", "cell", "result", "synthesis", "after", "path", "optimization", "we", "compute", "warp", "matrix", "-lrb-", "-rrb-", "each", "cell", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "apply", "-lrb-", "-rrb-", "warp", "i-th", "cell", "t-th", "frame", "generate", "final", "output", "video", "usually", "apply", "-lrb-", "-rrb-", "directly", "generate", "good", "result", "because", "we", "motion", "estimation", "ensure", "first", "order", "smoothness", "original", "path", "furthermore", "bundle", "optimization", "equation", "require", "nearby", "optimize", "path", "similar", "thus", "smoothness", "naturally", "satisfy", "-lrb-", "-rrb-", "most", "time", "we", "bundle", "path", "model", "can", "naturally", "handle", "roll", "shutter", "effect", "without", "pre-calibration", "principle", "we", "method", "similar", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "system", "do", "roll", "shutter", "correction", "while", "simultaneously", "stabilize", "video", "shaky", "video", "rolling", "shutter", "cause", "spatially", "variant", "high", "frequency", "jitters", "when", "smooth", "camera", "path", "we", "simultaneously", "rectify", "roll", "shutter", "effect", "other", "jitters", "cause", "camera", "shake", "we", "run", "we", "method", "Intel", "i7", "3.2", "GHZ", "Quad-Core", "machine", "8g", "RAM", "we", "extract", "400-600", "surf", "feature", "-lsb-", "Bay", "et", "al.", "2008", "-rsb-", "per", "frame", "video", "1280", "720", "resolution", "we", "unoptimized", "system", "take", "392", "millisecond", "process", "frame", "-lrb-", "around", "2.5", "fp", "-rrb-", "specifically", "we", "spend", "300m", "50m", "12m", "30m", "extract", "feature", "estimate", "motion", "optimize", "camera", "path", "render", "final", "result", "all", "original", "result", "video", "provide", "we", "webpage", "we", "first", "verify", "effectiveness", "different", "component", "propose", "approach", "global", "path", "vs.", "bundle", "path", "example", "Figure", "result", "accord", "global", "path", "have", "remain", "jitters", "some", "image", "region", "we", "bundle", "path", "can", "handle", "kind", "typical", "situation", "please", "refer", "we", "accompany", "video", "visual", "comparison", "spatially-variant", "homography", "vs.", "Homography", "Mixture", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "homography", "mixture", "model", "roll", "shutter", "correction", "model", "beyond", "single", "2d", "transformation", "able", "partially", "handle", "parallax", "http://www.ece.nus.edu.sg/stfpage/eletp/projects/stabilization/stabili", "zationsig13.html", "compare", "we", "2d", "mesh-based", "spatially-variant", "homography", "model", "have", "two", "limitation", "-rrb-", "do", "address", "horizontal", "depth", "variation", "-rrb-", "use", "weaker", "feature", "point", "-lrb-", "which", "apply", "lower", "threshold", "level", "feature", "detection", "-rrb-", "simple", "gaussian", "mixture", "regularization", "weaker", "feature", "point", "may", "result", "larger", "fitting", "error", "ability", "use", "simple", "gaussian", "smoothing", "limit", "Figure", "show", "comparison", "two", "model", "example", "scene", "have", "horizontal", "depth", "variation", "sky", "region", "lack", "feature", "point", "Figure", "-lrb-", "-rrb-", "result", "use", "YouTube", "Stabilizer", "-lrb-", "integrate", "Homography", "Mixture", "feature", "-rrb-", "we", "can", "observe", "severe", "geometrical", "distortion", "further", "verify", "we", "observation", "we", "replace", "we", "spatially-variant", "model", "homography", "mixture", "model", "-lrb-", "we", "implementation", "-rrb-", "we", "framework", "generate", "result", "Figure", "-lrb-", "-rrb-", "where", "we", "observe", "similar", "distortion", "comparison", "we", "warping-based", "motion", "estimation", "can", "fundamentally", "handle", "depth", "variation", "-lrb-", "limit", "vertical", "direction", "-rrb-", "we", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "do", "suffer", "from", "distortion", "please", "also", "see", "comparison", "accompany", "video", "Rolling", "Shutter", "Handling", "Figure", "compare", "we", "method", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "two", "example", "video", "from", "paper", "we", "model", "account", "frame", "distortion", "skew", "-lrb-", "leave", "example", "-rrb-", "local", "wobble", "-lrb-", "right", "example", "-rrb-", "more", "example", "include", "supplementary", "video", "which", "show", "we", "achieve", "similar", "result", "correct", "rolling", "shutter", "distortion", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "quantitatively", "evaluate", "measure", "result", "from", "different", "aspect", "we", "define", "three", "objective", "metric", "crop", "distortion", "we", "first", "two", "metric", "measure", "crop", "ratio", "global", "distortion", "we", "first", "fit", "global", "homography", "each", "frame", "between", "input", "video", "output", "video", "we", "compute", "crop", "ratio", "distortion", "each", "frame", "one", "global", "crop", "ratio", "whole", "sequence", "each", "frame", "provide", "estimation", "we", "average", "estimation", "all", "frame", "final", "metric", "distortion", "compute", "define", "section", "4.1", "because", "any", "distortion", "single", "frame", "destroy", "perfection", "whole", "result", "we", "choose", "minimum", "across", "whole", "sequence", "final", "metric", "worst-case", "metric", "allow", "we", "easily", "see", "whether", "whole", "result", "video", "completely", "successful", "good", "result", "both", "metric", "should", "close", "stability", "third", "metric", "measure", "stability", "result", "we", "suggest", "empirically", "good", "metric", "use", "frequency", "analysis", "estimate", "2d", "motion", "from", "video", "we", "basic", "assumption", "more", "energy", "contain", "low", "frequency", "part", "motion", "more", "stable", "video", "computationally", "we", "estimate", "we", "bundle", "camera", "path", "approximate", "true", "motion", "-lrb-", "optical", "flow", "-rrb-", "video", "we", "do", "smooth", "out", "anything", "after", "estimation", "we", "extract", "translation", "rotation", "component", "from", "each", "path", "each", "component", "1d", "temporal", "signal", "finally", "we", "evaluate", "energy", "percentage", "low", "frequency", "component", "-lrb-", "expect", "dc", "component", "-rrb-", "1d", "signal", "measure", "stability", "specifically", "we", "take", "few", "lowest", "-lrb-", "empirically", "set", "from", "2nd", "6th", "-rrb-", "frequency", "calculate", "energy", "percentage", "over", "full", "frequency", "-lrb-", "exclude", "dc", "component", "-rrb-", "similar", "distortion", "we", "take", "smallest", "measurement", "among", "translation", "rotation", "final", "metric", "good", "result", "metric", "should", "approach", "here", "well", "purpose", "comparison", "test", "whether", "we", "result", "comparable", "-lrb-", "better", "than", "-rrb-", "previous", "successful", "result", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "Grundmann", "et", "al.", "2011", "-rsb-", "we", "collect", "eleven", "test", "video", "from", "papers", "-lrb-", "thumbnail", "Figure", "10", "-rrb-", "compare", "we", "result", "publish", "result", "-lrb-", "all", "from", "author", "project", "webpage", "-rrb-", "overall", "all", "method", "generate", "similar", "stability", "both", "subjectively", "quantitatively", "-lrb-", "Figure", "10", "-rrb-", "example", "while", "we", "result", "slightly", "better", "some", "video", "term", "crop", "ratio", "distortion", "video", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3d", "stabilization", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "achieve", "best", "stability", "distortion", "score", "although", "we", "result", "slightly", "worse", "stability", "visual", "difference", "quite", "small", "-lrb-", "please", "verify", "from", "supplementary", "video", "-rrb-", "furthermore", "aggressive", "smoothing", "3d", "method", "sometimes", "lead", "output", "fov", "too", "small", "demonstrate", "crop", "score", "we", "method", "manage", "provide", "good", "trade-off", "video", "-lrb-", "5-9", "-rrb-", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "-lsb-", "Goldstein", "Fattal", "2012", "-rsb-", "we", "method", "achieve", "similar", "stability", "while", "we", "method", "slightly", "better", "crop", "distortion", "video", "-lrb-", "10-11", "-rrb-", "we", "method", "outperform", "l1optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "stability", "-lrb-", "slightly", "-rrb-", "crop", "ratio", "distortion", "score", "Figure", "11", "highlight", "most", "challenging", "video", "-lrb-", "10", "-rrb-", "dataset", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "refer", "example", "failure", "case", "because", "single", "subspace", "can", "account", "feature", "trajectory", "both", "face", "background", "result", "have", "visible", "distortion", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "produce", "better", "result", "example", "video", "result", "we", "still", "observe", "large", "temporal", "distortion", "background", "region", "-lrb-", "see", "we", "accompany", "video", "-rrb-", "comparison", "we", "method", "can", "successfully", "handle", "example", "-lrb-", "achieve", "best", "term", "all", "three", "metric", "-rrb-", "because", "warping-based", "motion", "model", "can", "represent", "complicated", "motion", "due", "publicly", "available", "implementation", "previous", "work", "we", "compare", "we", "system", "two", "well-known", "commercial", "system", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "after", "Effects", "CS6", "YouTube", "Stabilizer", "base", "combination", "norm", "path", "optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "homography", "mixture", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "Warp", "Stabilizer", "Adobe", "after", "Effects", "largely", "base", "subspace", "stabilization", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "we", "understand", "commercial", "product", "often", "different", "from", "give", "research", "system", "we", "believe", "two", "system", "represent", "essential", "element", "research", "conduct", "field", "comparison", "make", "sense", "examine", "strength", "weakness", "robustness", "-lrb-", "various", "video", "use", "set", "fix", "parameter", "-rrb-", "we", "system", "Dataset", "we", "assemble", "comprehensive", "dataset", "174", "short", "video", "-lrb-", "10", "60", "seconds", "-rrb-", "from", "previous", "publication", "internet", "we", "own", "capture", "-lrb-", "-rrb-", "simple", "-lrb-", "II", "-rrb-", "quick", "rotation", "-lrb-", "III", "-rrb-", "zoom", "-lrb-", "iv", "-rrb-", "large", "parallax", "-lrb-", "-rrb-", "driving", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "running", "better", "measure", "stability", "background", "motion", "-lrb-", "cause", "camera", "shake", "-rrb-", "we", "use", "manual", "foreground", "mask", "exclude", "foreground", "motion", "YouTube", "Stabilizer", "parameter-free", "online", "tool", "Warp", "Stabilizer", "interactive", "system", "user", "might", "carefully", "tune", "few", "parameter", "we", "use", "example", "video", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "decide", "best", "parameter", "finally", "we", "choose", "default", "parameter", "-lrb-", "smoothness", "50", "smooth", "Motion", "Subspace", "Warp", "-rrb-", "produce", "result", "quantitative", "comparison", "each", "category", "we", "compute", "average", "metric", "standard", "deviation", "three", "system", "-lrb-", "figure", "12", "-lrb-", "-rrb-", "-rrb-", "we", "discuss", "result", "regard", "each", "system", "detail", "below", "all", "three", "system", "perform", "well", "category", "-lrb-", "-rrb-", "simple", "since", "category", "contain", "video", "relatively", "smooth", "camera", "motion", "mild", "depth", "variation", "though", "we", "method", "have", "minor", "advantage", "user", "can", "safely", "choose", "any", "three", "get", "desire", "result", "among", "remain", "category", "we", "want", "highlight", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "three", "system", "achieve", "similar", "stability", "while", "we", "system", "clearly", "better", "term", "distortion", "we", "show", "two", "example", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "visual", "comparison", "we", "system", "YouTube", "Stabilizer", "example", "show", "limitation", "1d", "array", "homography", "mixture", "can", "model", "depth", "change", "horizontal", "direction", "Warp", "Stabilizer", "also", "generate", "some", "shearing/skewing", "artifact", "some", "video", "frame", "though", "principle", "3d", "method", "should", "able", "handle", "parallax", "Figure", "12", "-lrb-", "-rrb-", "show", "example", "-lrb-", "please", "note", "shearing", "bookshelf", "-rrb-", "probably", "due", "subspace", "analysis", "failure", "cause", "occlusion", "we", "method", "succeed", "all", "example", "comparison", "category", "clearly", "demonstrate", "advantage", "we", "warping-based", "motion", "model", "deal", "large", "parallax", "Warp", "Stabilizer", "often", "generate", "significant", "crop", "Figure", "12", "-lrb-", "-rrb-", "example", "when", "apply", "weaker", "smoothing", "however", "we", "find", "its", "result", "become", "shaky", "comparison", "we", "method", "generate", "stable", "result", "much", "less", "crop", "category", "-lrb-", "v?vii", "-rrb-", "three", "system", "generate", "similar", "stability", "level", "-lrb-", "Warp", "Stabilizer", "slightly", "better", "category", "vii", "-rrb-", "while", "we", "sy", "tem", "consistently", "better", "respect", "either", "crop", "ratio", "distortion", "control", "we", "notice", "we", "method", "generate", "relatively", "smaller", "standard", "deviation", "three", "metric", "all", "category", "suggest", "we", "method", "generate", "more", "consistent", "result", "from", "various", "input", "user", "study", "we", "further", "conduct", "user", "study", "40", "participant", "evaluate", "compare", "we", "method", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "AfterEffects", "CS6", "every", "participant", "require", "evaluate", "result", "28", "different", "input", "video", "-lrb-", "randomly", "sample", "from", "we", "dataset", "-rrb-", "which", "video", "each", "category", "mention", "above", "-lrb-", "video", "prepare", "way", "two", "they", "compare", "we", "result", "YouTube", "Stabilizer", "other", "two", "Warp", "Stabilizer", "-rrb-", "user", "study", "we", "use", "scheme", "forced", "two-alternative", "choice", "every", "participant", "ask", "pick", "better", "one", "between", "result", "we", "method", "YouTube", "Stabilizer", "between", "result", "we", "method", "Warp", "Stabilizer", "video", "display", "subject", "random", "order", "subject", "unaware", "video", "category", "Figure", "13", "-lrb-", "-rrb-", "show", "interface", "user", "study", "original", "video", "display", "top", "two", "stabilize", "one", "show", "side-by-side", "below", "user", "can", "simultaneously", "play", "input", "video", "both", "two", "result", "better", "examine", "difference", "video", "can", "play", "back", "forth", "pause", "certain", "frame", "help", "user", "carefully", "make", "decision", "user", "can", "also", "play", "each", "video", "individually", "examine", "quality", "without", "other", "distraction", "user", "study", "result", "show", "13", "-lrb-", "-rrb-", "each", "category", "we", "show", "average", "percentage", "user", "preference", "general", "majority", "all", "user", "show", "significant", "preference", "towards", "we", "result", "when", "compare", "any", "other", "two", "system", "respectively", "particular", "participant", "prefer", "overall", "quality", "we", "result", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "over", "YouTube", "Stabilizer", "-lrb-", "72", "vs.", "28", "-rrb-", "Warp", "Stabilizer", "-lrb-", "69", "vs.", "31", "-rrb-", "result", "consistent", "we", "metric", "evaluation", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "user", "show", "strong", "bias", "preference", "toward", "we", "result", "over", "Warp", "Stabilizer", "-lrb-", "93", "vs.", "rotation", "83", "vs.", "17", "zoom", "-rrb-", "possibly", "due", "significant", "crop", "result", "Warp", "Stabilizer", "category", "-lrb-", "v?vii", "-rrb-", "more", "participant", "prefer", "we", "result", "other", "two", "system", "although", "three", "system", "generate", "similar", "stability", "level", "accord", "we", "stability", "metric", "likely", "because", "superior", "distortion", "crop", "control", "we", "method", "category", "-lrb-", "-rrb-", "simple", "user", "express", "similar", "preference", "toward", "three", "result", "after", "user", "study", "we", "also", "ask", "all", "participant", "articulate", "criterion", "feedback", "we", "conclude", "main", "criterion", "unacceptable", "video", "-rrb-", "video", "get", "smaller", "field", "view", "even", "contain", "frame", "visible", "empty", "-lrb-", "black", "-rrb-", "area", "-rrb-", "video", "present", "structure", "distortion", "individual", "frame", "-rrb-", "motion", "some", "video", "frame", "vibrate", "oscillate", "-rrb-", "scene", "transition", "look", "abrupt", "smooth", "video", "from", "criterion", "we", "propose", "metric", "can", "partially", "relate", "human", "preference", "both", "quantitative", "evaluation", "user", "study", "result", "consistently", "indicate", "we", "system", "perform", "better", "than", "other", "two", "system", "we", "find", "when", "3d", "reconstruction", "successful", "3d", "method", "often", "generate", "best", "result", "however", "we", "system", "more", "robust", "we", "do", "require", "feature", "tracking", "produce", "comparable", "only", "slightly", "worse", "result", "interesting", "note", "we", "adaptive", "path", "optimization", "can", "also", "apply", "path", "smoothing", "3d", "method", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "-rsb-", "which", "often", "use", "low-pass", "filter", "-lrb-", "gaussian", "smoothing", "-rrb-", "curve", "fitting", "path", "planning", "we", "show", "example", "video", "we", "project", "webpage", "case", "where", "warping-based", "motion", "model", "fail", "handle", "severe", "occlusion", "dis-occlusion", "especially", "when", "combine", "roll", "shutter", "effect", "Figure", "14", "show", "two", "example", "we", "warping-based", "motion", "model", "choose", "large", "enforce", "strong", "coherence", "between", "grid", "cell", "way", "we", "can", "minimize", "geometrical", "distortion", "same", "time", "we", "sacrifice", "motion", "accuracy", "eventually", "stability", "result", "general", "we", "find", "geometrical", "distortion", "more", "disruptive", "than", "some", "slight", "remain", "jitters", "we", "discontinuity-preservation", "optimization", "produce", "visually", "please", "result", "most", "example", "we", "also", "do", "deal", "motion", "blur", "sometimes", "stabilize", "result", "contain", "visible", "blur", "artifact", "we", "have", "present", "new", "2d", "video", "stabilization", "method", "bundle", "camera", "path", "model", "propose", "method", "can", "simultaneously", "generate", "comparable", "result", "3d", "method", "while", "keep", "merit", "2d", "method", "use", "image", "warping", "technique", "motion", "representation", "interesting", "finding", "paper", "extend", "kind", "representation", "other", "video-based", "application", "we", "thank", "all", "reviewer", "helpful", "discussion", "Kaimo", "Lin", "other", "subject", "help", "user", "study", "Nathan", "Holdstein", "Jiangyu", "Liu", "help", "proofread", "work", "also", "partially", "support", "Singapore", "project", "r-263-000-620-112", "hen", "b.-y.", "ee", "k.-y.", "uang", "w.-t.", "j.-s", "2008", "capture", "intention-based", "full-frame", "video", "stabilization", "Computer", "Graphics", "Forum", "27", "1805", "1814", "ho", "S.", "ang", "J.", "ee", "S.", "2012", "Video", "deblurr", "hand-held", "camera", "use", "patch-based", "synthesis", "ACM", "Trans", "SIGGRAPH", "-rrb-", "31", "ischler", "M.", "A.", "OLLES", "R.", "C.", "1981", "Random", "sample", "consensus", "paradigm", "model", "fitting", "application", "image", "analysis", "automate", "cartography", "ACM", "24", "381", "395", "orss", "p.-e.", "ingaby", "E.", "2010", "rectify", "rolling", "shutter", "video", "from", "hand-held", "device", "ao", "J.", "IM", "S.", "J.", "ROWN", "M.", "S.", "2011", "construct", "image", "panorama", "use", "dual-homography", "warping", "leicher", "M.", "L.", "iu", "F.", "2007", "re-cinematography", "improving", "camera", "dynamics", "casual", "video", "oldstein", "a.", "attal", "R.", "2012", "Video", "stabilization", "use", "epipolar", "geometry", "ACM", "Trans", "-lrb-", "tog", "-rrb-", "31", "126:1", "126:10", "rundmann", "M.", "WATRA", "V.", "ssa", "i.", "2011", "autodirect", "video", "stabilization", "robust", "l1", "optimal", "camera", "path", "rundmann", "M.", "WATRA", "V.", "ASTRO", "D.", "ssa", "i.", "2012", "calibration-free", "rolling", "shutter", "removal", "artley", "R.", "isserman", "a.", "2003", "multiple", "View", "Geometry", "Computer", "Vision", "ed", "Cambridge", "University", "Press", "New", "York", "NY", "USA", "garashi", "T.", "OSCOVICH", "T.", "UGHES", "J.", "F.", "2005", "asrigid-as-possible", "shape", "manipulation", "ACM", "Trans", "SIGGRAPH", "-rrb-", "24", "1134", "1141", "ARPENKO", "A.", "ACOBS", "D.", "AEK", "J.", "EVOY", "M.", "2011", "Digital", "video", "stabilization", "roll", "shutter", "correction", "use", "gyroscope", "Stanford", "CS", "Tech", "Report", "ee", "k.-y.", "huang", "y.-y.", "hen", "b.-y.", "uhyoung", "M.", "2009", "Video", "stabilization", "use", "robust", "feature", "trajectory", "iang", "c.-k.", "hang", "l.-w.", "hen", "H.", "H.", "2008", "analysis", "compensation", "roll", "shutter", "effect", "IEEE", "Trans", "W.-Y.", "IU", "S.", "atsushita", "Y.", "T.-T.", "heong", "l.-f", "2011", "smoothly", "vary", "affine", "stitching", "iu", "F.", "LEICHER", "M.", "H.", "garwalum", "a.", "2009", "content-preserving", "warp", "3d", "video", "stabilization", "iu", "F.", "LEICHER", "M.", "ang", "J.", "H.", "garwalum", "a.", "2011", "Subspace", "video", "stabilization", "ACM", "Trans", "iu", "S.", "ang", "Y.", "uan", "L.", "J.", "P.", "UN", "J.", "2012", "Video", "stabilization", "depth", "camera", "uca", "B.", "D.", "ANADE", "T.", "1981", "iterative", "image", "registration", "technique", "application", "stereo", "vision", "-lrb-", "ijcaus", "-rrb-", "674", "679", "atsushita", "Y.", "FEK", "E.", "W.", "ang", "X.", "HUM", "H.Y.", "2006", "full-frame", "video", "stabilization", "motion", "inpainting", "IEEE", "Trans", "pattern", "Anal", "28", "1150", "1163", "orimoto", "C.", "hellappa", "R.", "1998", "evaluation", "image", "stabilization", "algorithm", "IEEE", "International", "Conference", "Acoustics", "speech", "signal", "processing", "2789", "2792", "akamura", "J.", "2005", "image", "sensor", "signal", "processing", "Digital", "still", "Cameras", "crc", "Press", "Inc.", "ir", "T.", "RUCKSTEIN", "A.", "M.", "IMMEL", "R.", "2008", "overparameterize", "variational", "optical", "flow", "J.", "Comput", "Vision", "-lrb-", "IJCV", "-rrb-", "76", "205", "216", "chaefer", "S.", "hail", "T.", "ARREN", "J.", "2006", "image", "deformation", "use", "move", "least", "square", "ACM", "Trans", "SIGGRAPH", "-rrb-", "25", "533", "540", "hum", "h.-y.", "zeliskus", "R.", "2000", "construction", "panoramic", "image", "mosaic", "global", "local", "alignment", "J.", "Comput", "Vision", "-lrb-", "IJCV", "-rrb-", "36", "101", "130", "mith", "B.", "M.", "HANG", "L.", "H.", "garwalum", "a.", "2009", "light", "field", "video", "stabilization", "zeliskus", "R.", "1996", "Motion", "estimation", "quadtree", "spline", "IEEE", "Trans", "pattern", "Anal", "18", "12", "1199", "1210", "omasus", "C.", "anduchus", "R.", "1998", "bilateral", "filter", "gray", "color", "image", "iccv", "839", "846" ],
  "content" : "The evaluation on a large variety of consumer videos demonstrates the merits of our method. Specifically, we present bundled camera paths model which maintains multiple, spatially-variant camera paths. Our bundled camera paths model is built on two novel components: a warping-based motion representation (and estimation), and an adaptive space-time path smoothing. Notice that the ?as-similar-as-possible? warping was used in [Liu et al. 2009; Liu et al. 2011] for video stabilization. Our second component smooths all bundled camera paths as a whole to maintain both spatial and temporal coherences. For a quantitative evaluation, we provide a comprehensive dataset (including both public examples and our own video clips of different kinds of motions). Earlier works [Morimoto and Chellappa 1998; Matsushita et al. 2006] apply low-pass filters to individual model parameters. But we use a spatially-variant model to represent the motion between video frames and design an appropriate smoothing technique for this model. But it still requires moderate feature track length (typically over 20 frames). Optical flow algorithms [Lucas and Kanade 1981] model this transition by individual displacement vectors at every pixel. Our motion model is essentially a mesh-based, spatially-variant homography model, inspired by recent image warping techniques [Igarashi et al. 2005; Schaefer et al. 2006; Liu et al. 2009]. Similar to [Grundmann et al. 2012], our method corrects rolling shutter effects without any prior calibration. Our warping-based model naturally handles the rolling shutter effects as a special kind of spatially variant motion. We adopt the warping model in [Igarashi et al. 2005; Liu et al. 2009], Model At each frame, we define a uniform grid mesh as illustrated in Figure 2 . We require matched features (e.g., p and p in Figure 2 ) to share the same bilinear interpolation of the four corners of the enclosing grid cell after warping. Thus, the warping-based motion model is actually a set of spatially-variant homographies on a 2D grid. Note that this highly flexible model is able to handle parallax. It is between global homography and per-pixel optical flow. The combination of the shape-preserving and mesh representation together provides two kinds of regularizations: 1) for each cell, the fitted homography should be biased toward a reduced similarity (or rigid) transformation; 2) the intrinsic connection of the mesh (two neighboring mesh cells share two vertices) enforces a first-order continuity constraint. Finally, we estimate the motion by minimizing two energy terms: a data term for matching features, and a shape-preserving term for enforcing regularization. We first describe our basic method by following [Liu et al. 2009], and later extend it for better robustness in the next subsection. We expect that the corresponding feature p can be represented by the same weights of the warped grid vertices V ? p = [ v p 1 , v p 2 , v p 3 , v p 4 ]. Here V ? contains all the warped grid vertices. Shape-preserving term We use the same shape-preserving term as [Liu et al. 2009] involving all vertices in V ? ,\n          E s ( V ? ) = v ? v 1 ? sR 90 ( v 0 ? v 1 ) 2 , R 90 = ?1 0 0 1 , (2) v\n          where s = v ? v 1 / v 0 ? v 1 is a known scalar computed from the initial mesh. This shape-preserving term requires the triangle of neighboring vertices v, v 0 , v 1 to follow a similarity transformation. Estimating homographies After having a new mesh, we can estimate each local homography F i (t) in the grid cell i of frame t by solving a linear equation:\n          V ? i = F i (t)V i , (4)\n          where V i and V ? i are the four vertices before and after the warping. Figure 3 shows the warped mesh grid according to the estimated motion. Left and right are the results with and without the shapepreserving term. It is clear that the regularization term helps maintain a smooth varying mesh representation. Outlier rejection We reject incorrectly matched features at two scales. At the coarse scale (the whole image), we apply RANSAC algorithm [Fischler and Bolles 1981] to fit a global homography F  ? (t) and discard features by a relatively large threshold on fitting error (6% image width). At the fine scale (4 ? 4 sub-images), we apply RANSAC again to reject features by a relatively small threshold (2% image width). Pre-warping To facilitate the warping estimation, we use global homography F  ? (t) to bring matching features closer. We then solve the warping to estimate the residual motion, which generates a homography F i (t) at each grid cell. The final homography F i (t) is simply computed as F i (t) ? F  ? (t). Note that this coarse-to-fine strategy has been used in [Liu et al. 2009] for image synthesis and proven effective in motion estimation literature [Brox et al. 2004]. different scenes: (a) a scene free of occlusion; (b) a scene with severe occlusion. But when there is occlusion or insufficient features, we prefer stronger regularization as the data term is less reliable. To implement this strategy, we adaptively set ? per frame, based on two errors: fitting error e h and smoothness error e s . The fitting error e h is the average residual of the feature matching under the estimated homographies, i.e., e h = n 1 p F p ? p ? p 2 , where F p is the homography in the cell containing p, and n is the number of feature pairs. Here, the homography matrix is normalized so that sum of all its elements is one. We empirically set ? = 0.01, since it makes the scale of e h and e s similar on most of the examples. We equally discretize ? into 10 values between 0.3 and 3. We perform the model estimation using every discretized value and select the model with minimum error e. So we choose a small ?(=0.9) to better minimize the data error. On the contrary, for scenes with large occlusion ( Figure 4(b) ), neighboring local homographies are less similar. Finally, we show an example in Figure 5 to verify the strength of the regularization of our method. In this example, we compare two meshes estimated using all features and a subset of features. Two similar results indicate our method can robustly deal with regions of insufficient features. With estimated local homographies, we can define a bundle of spatially-variant camera paths for the whole video. Let C i (t) be (b) Relationships among original path {C(t)}, smoothed path {P (t)}, and transformations {B(t)} the camera pose of the grid cell i at frame t. , F i (t ? 1)} are estimated local homographies at the same grid cell i, as shown in Figure 6 (a). We call these spatially-variant paths as ?bundled camera paths?. In the next section, we describe how we smoothen these bundled paths for video stabilization. We first describe our smoothing method for a single camera path, and extend it to a bundle of camera paths. A good camera path smoothing should consider multiple competing factors: removing jitters, avoiding excessive cropping, and minimizing various geometrical distortions (shearing/skewing, wobble). Formulation Given an original path C = {C(t)}, we seek an optimized path P = {P (t)} by minimizing the following function: t (5) where ? t are the neighborhood at frame t. The other terms are: ? data term P (t) ? C(t) 2 enforcing the new camera path to be close to the original one to reduce cropping and distortion; ? smoothness term P (t) ? P (r) 2 stabilizing the path; ? weight ? t,r (C) to preserve motion discontinuities under fast panning/rotation or scene transition; for a video with rapid camera panning. The camera paths on the top plot the x-translation over time. ? parameter ? t to balance the above two terms. Since Equation 5 is quadratic, we can solve it with any linear system solver. At initialization, P (0) (t) = C(t). Once we obtain the optimized path P, we compute the warping transform B(t) = C ?1 (t)P (t) to warp the original video frame to the stabilized result ( Figure 6(b) ). Discontinuity-preserving The adaptive weight ? t,r is important to preserve motion discontinuity. G m () measures the changes of two camera poses. We use a large kernel to ensure successful suppression of both highfrequency jitters (e.g., handshake) and low-frequency bounces (e.g., walking). In our implementation, we set ? t to 60 neighboring frames and the standard deviation of G t () to 10. In contrast, previous low-pass filtering based methods [Matsushita et al. 2006] typically need a smaller amount of support (e.g., 10 frames) to avoid aggressive cropping and distortion. But such a small kernel is often insufficient in suppressing low frequency bounces. The reason why we can use a larger kernel lies in G m (). In video stabilization, for rapid camera motion (e.g, caused by fast panning or scene transition), an inappropriate amount of smoothing may lead to excessive cropping, as shown in Figure 7 . In this case, the camera pans quickly, and na?ve Gaussian smoothing (second row) causes the camera path to significantly deviate from its original path, as indicated by the dashed lines in the left plot on top. The corresponding frames shown on the second row will require large cropping. Our adaptive term G m () preserves the sudden camera motions to a certain degree. The result from our adaptive smoothing (bottom row) produces much less cropping. To measure the camera motion, we use the change in translation components ? x (t), ? y (t) extracted from the camera pose C(t), namely |? x (t) ? ? x (r)| + |? y (t) ? ? y (r)|. The frame translation ? x (t), ? y (t) can describe most camera motions in practice except for an in-plane rotation or scale around the principal axis. In principle, we could formulate a constrained optimization to address this issue. We first run the optimization with a global fixed ? t = ? (empirically set to 5) and then check the cropping ratio and distortion of every frame. Note, according to Equation 6, a smaller ? will make the optimized path closer to the original one, which has less cropping and distortions. The procedure is iterated until all frames satisfy the requirements. We measure the cropping ratio and distortion from the warping transform B(t) = C ?1 (t)P (t). The anisotropic scaling of B(t) measures the distortion. We use B(t) to compute the overlapping area of the original video frame and the stabilized frame. The cropping ratio is the ratio of this area and the original frame area. In our experiments, we require the cropping ratio to be larger than 0.8, and the distortion score to be larger than 0.95 for all examples. In principle, we can further measure the perspective distortion by the two perspective components in B(t). But we empirically find they are always too small when compared with the affine components and do not include them. Our motion model generates a bundle of camera paths. If these paths are optimized independently, neighboring paths could be less consistent, which may generate distortion in the final rendered video. Hence, we do a space-time optimization of all paths by minimizing the following objective function O ({P i (t)}) + P i (t) ? P j (t) 2 , (8) i t j?N (i) where N (i) includes eight neighbors of the grid cell i. The first term is the objective function in Equation 5 for each single path, and the second term enforces the smoothness between neighboring paths. This optimization is also quadratic and the optimum result can be obtained by solving a large sparse linear system. Again, our solution is updated by a Jacobi-based iteration [Bronshtein and Semendyayev 1997]: (?+1) 1 (?) (?) P i (t) = ? (C i (t)+ 2? t w t,r P i (r)+ 2P j (t)), r?? t j?N (i) r=t j=i where ? = 2? t w t,r + 2N (i) ? 1. r?? t ,r=t In comparison, ? t is determined from the global path (generated by concatenating the pre-warping global homographies), because it controls the overall cropping and distortion. Then, we use ? t to optimize the camera paths in all cells. Result synthesis After path optimization, we compute the warping matrix B i (t) for each cell i by B i (t) = C i ?1 (t)P i (t). We then apply B i (t) to warp the i-th cell at the t-th frame to generate the final output video. Usually, applying B i (t) directly generates good results. This is because our motion estimation ensures first order smoothness of the original paths. Furthermore, the bundled optimization in Equation 8 requires nearby optimized paths to be similar. Thus, the smoothness is naturally satisfied by B i (t) most of the time. Our bundled paths model can naturally handle rolling shutter effects without pre-calibration. The principle of our method is similar to that of [Grundmann et al. 2012]. Our system does rolling shutter correction while simultaneously stabilizing the video. In a shaky video, a rolling shutter causes spatially variant high frequency jitters. When smoothing the camera paths, we simultaneously rectify rolling shutter effects and other jitters caused by camera shake. We run our method on an Intel i7 3.2GHZ Quad-Core machine with 8G RAM. We extract 400-600 SURF features [Bay et al. 2008] per frame. For a video of 1280 ? 720 resolution, our unoptimized system takes 392 milliseconds to process a frame (around 2.5fps). Specifically, we spend 300ms, 50ms, 12ms and 30ms to extract features, estimate motion, optimize camera paths and render the final result. All original and result videos are provided on our webpage 1 . We first verify the effectiveness of different components of the proposed approach. A Global Path vs. Bundled Paths For the example in Figure 1 , the result according to a global path has remaining jitters in some image regions. But our bundled paths can handle this kind of typical situation. Please refer to our accompanying video for a visual comparison. Spatially-variant Homographies vs. Homography Mixture Grundmann et al. [2012] proposed a homography mixture model for rolling shutter correction. This model is beyond a single 2D transformation and able to partially handle parallax. 1 http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Stabilization/Stabili- zationSig13.html Compared with our 2D mesh-based, spatially-variant homographies, this model has two limitations: 1) it does not address horizontal depth variation; 2) it uses weaker feature points (which apply lower threshold level for feature detection) and a simple Gaussian mixture for the regularization. Weaker feature points may result in larger fitting errors and the ability to use simple Gaussian smoothing is limited. Figure 8 shows a comparison of these two models. In this example, the scene has horizontal depth variation and the sky region lacks feature points. Figure 8 (a) is the result of using YouTube Stabilizer (integrated Homography Mixture feature). We can observe severe geometrical distortions. To further verify our observation, we replace our spatially-variant model with the homography mixture model (our implementation) in our framework and generate the result in Figure 8 (d), where we observe similar distortion. In comparison, our warping-based motion estimation can fundamentally handle depth variation (not limited to vertical direction). Our result ( Figure 8 (c)) does not suffer from such distortion. Please also see the comparison in the accompanying video. Rolling Shutter Handling Figure 9 compares our methods with [Grundmann et al. 2012] on two example videos from their paper. Our model accounts for frame distortions such as skew (left example) and local wobble (right example). More examples are included in the supplementary video, which shows we achieve similar results on correcting rolling shutter distortion as [Grundmann et al. 2012]. To quantitatively evaluate and measure the result from different aspects, we define three objective metrics. Cropping and distortion Our first two metrics measure cropping ratio and global distortion. We first fit a global homography at each frame between input video and output video. We then compute the cropping ratio and distortion for each frame. There is one global cropping ratio for the whole sequence, and each frame provides an estimation. We average these estimations at all frames as the final metric. The distortion is computed as defined in Section 4.1. Because any distortion in a single frame will destroy the perfection of the whole result, we choose their minimum across the whole sequence as the final metric. This ?worst-case? metric  allows us to easily see whether the whole result video is completely successful. For a good result, both metrics should be close to 1. Stability The third metric measures the stability of the result. We suggest an empirically good metric using frequency analysis on estimated 2D motion from a video. Our basic assumption is that the more energy is contained in the low frequency part of the motion, the more stable a video is. Computationally, we estimate our bundled camera paths to approximate the true motion (optical flow) in a video. We do not smooth out anything after the estimation. Then, we extract translation and rotation components from each path. Each component is a 1D temporal signal. Finally, we evaluate the energy percentage of the low frequency components (expect for DC component) in these 1D signals to measure the stability. Specifically, we take a few of the lowest (empirically set as from the 2nd to the 6th) frequencies and calculate the energy percentage over full frequencies (excluded by the DC component). Similar to the distortion, we take the smallest measurement among the translation and rotation as the final metric. For a good result, the metric should approach 1 here as well. The purpose of this comparison is to test whether our results are comparable with (if not better than) previous ?successful? results in [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012; Grundmann et al. 2011]. We collect eleven test videos from these papers (thumbnails in Figure 10 ), and compare our results with their published results (all from authors? project webpages). Overall, all methods generate similar stability both subjectively and quantitatively ( Figure 10 ) on these examples, while our results are slightly better on some videos in terms of cropping ratio and distortion. For video (2)-(4), 3D stabilization [Liu et al. 2009] achieves the best stability and distortion scores. Although our results are slightly worse in stability, the visual difference is quite small (please verify from the supplementary video). Furthermore, the aggressive smoothing in 3D methods sometimes leads to an output FOV that is too small as demonstrated by the cropping score. Our method manages to provide a good trade-off. For video (5-9), [Liu et al. 2011], [Goldstein and Fattal 2012], and our method achieve similar stability, while our method is slightly better in cropping and distortion. For video (10-11) 2 , our method outperforms the L1optimization [Grundmann et al. 2011] in stability (slightly), cropping ratio, and distortion scores. Figure 11 highlights the most challenging video (10) in this dataset. Liu et al. [2011] refer this example as a failure case because a single subspace cannot account for the feature trajectories on both the face and the background. Their results have visible distortion. [Grundmann et al. 2011] produced better result on this example. But in the video result, we still observe large temporal distortion on the background region. (See our accompanying video.) In comparison, our method can successfully handle this example (achieve best in terms of all three metrics) because the warping-based motion model can represent this complicated motion. Due to no publicly available implementation of previous works, we compare our system with two well-known commercial systems ? YouTube Stabilizer and ?Warp Stabilizer? in Adobe After Effects CS6. The YouTube Stabilizer is based on the combination of the L 1 -norm path optimization [Grundmann et al. 2011] and homography mixtures [Grundmann et al. 2012]. The ?Warp Stabilizer? in Adobe After Effects is largely based on subspace stabilization [Liu et al. 2011]. We understand that commercial products are often different from a given research system. But we believe these two systems represent the essential elements of research conducted in this field, and the comparison makes sense for examining strengths or weaknesses and robustness (for various videos using a set of fixed parameters) of our system. Dataset We assemble a comprehensive dataset of 174 short videos (10 ? 60 seconds) from previous publications, Internet, and our own captures. They are: (I) simple, (II) quick rotation, (III) zooming, (IV) large parallax, (V) driving, (VI) crowd, and (VII) running. 2 To better measure stability on background motion (caused by camera shake), we use a manual foreground mask to exclude foreground motion. YouTube Stabilizer is a parameter-free online tool. But ?Warp Stabilizer? is an interactive system, and the user might carefully tune a few parameters. We use the example videos in [Liu et al. 2011] to decide the best parameters. Finally, we choose the default parameters (smoothness: 50%, ?Smooth Motion? and ?Subspace Warp?) to produce results. Quantitative Comparison For each category, we compute the average metrics and standard deviation of three systems ( Figure 12 (a)). We discuss the results with regard to each system in detail below. All three systems perform well in category (I) ?simple?, since this category contains videos with relatively smooth camera motion and mild depth variations. Though our method has a minor advantage, the users can safely choose any of three to get a desired result. Among the remaining categories, we want to highlight the category (IV) ?large parallax?. The three systems achieve similar stability, while our system is clearly better in terms of distortion. We show two examples in Figure 12 (b) and (c) for visual comparison of our system and the YouTube Stabilizer. These examples show the limitation of a 1D array of homography mixtures ? it cannot model depth changes in horizontal direction. Warp Stabilizer also generates some shearing/skewing artifacts in some video frames, though in principle this 3D method should be able to handle parallax. Figure 12 (d) shows such an example (please note the shearing of the bookshelf). This is probably due to the subspace analysis failure caused by occlusion. Our method succeeds in all of these examples. Comparison in this category clearly demonstrates the advantages of our warping-based motion model in dealing with a large parallax. ?Warp Stabilizer? often generates significant cropping. Figure 12(e) is such an example. When applying a weaker smoothing, however, we find its result becomes shaky. In comparison, our method generates stable results with much less cropping. For categories (V?VII), the three systems generate similar stability levels (?Warp Stabilizer? is slightly better in category VII), while our sys- tem is consistently better with respect to either cropping ratio or distortion control. We notice that our method generates relatively smaller standard deviations of the three metrics for all categories. It suggests that our method generates more consistent results from various inputs. User Study We further conduct a user study with 40 participants to evaluate and compare our method with the YouTube Stabilizer and the ?Warp Stabilizer? in Adobe AfterEffects CS6. Every participant is required to evaluate results on 28 different input videos (randomly sampled from our dataset), in which there are 4 videos for each category mentioned above (The 4 video are prepared in the way that two of them compare our result to YouTube Stabilizer, and the other two to ?Warp Stabilizer?). In the user study, we use the scheme of forced two-alternative choice. Every participant is asked to pick a better one between the results of our method and YouTube Stabilizer, or between the results of our method and the ?Warp Stabilizer?. These videos are displayed to the subjects in a random order. The subjects are unaware of the video categories. Figure 13 (a) shows such an interface for the user study. The original video is displayed on the top. The two stabilized ones are shown side-by-side below. Users can simultaneously play input video and both two results to better examine the difference. And these videos can be played back and forth, or be paused at a certain frame to help users carefully make their decision. The user can also play each of these videos individually to examine their quality without other distractions. The user study results are shown in 13 (b). For each category, we show the average percentage of user preference. In general, the majority of all users showed significant preference towards our results when compared to any of the other two systems respectively. In particular, the participants prefer the overall quality of our results for category (IV) ?large parallax? over YouTube Stabilizer (72% vs. 28%) and ?Warp Stabilizer? (69% vs. 31%). The result is consistent with our metric evaluation. For category (II?III) containing quick rotation or zooming, users show a strong bias in preference toward our results over ?Warp Stabilizer? (93% vs. 7% for rotation, 83% vs. 17% for zooming). This is possibly due to the significant cropping in the results of ?Warp Stabilizer?. For categories (V?VII), more participants prefer our results to the other two systems, although the three systems generate similar stability levels according to our stability metric. It is likely because of the superior distortion and cropping control in our method. In category (I) ?simple?, users express similar preference toward three results. After the user study, we also ask all participants to articulate the criteria for their feedbacks. We conclude the main criteria for unacceptable videos: 1) the video gets a smaller field of view or even contains frames with visible empty (black) area; 2) the video presents structure distortions in individual frames; 3) the motions in some video frames vibrate or oscillate; 4) the scene transition looks abrupt or not smoothed in the video. From these criteria, our proposed metrics can be partially related with human preferences. And both quantitative evaluation and user study results consistently indicate our system performs better than the other two systems. We find that when 3D reconstruction is successful, 3D methods often generate the best results. However, our system is more robust as we do not require feature tracking, and it produces comparable or only slightly worse results. It is interesting to note that our adaptive path optimization can also be applied to path smoothing for 3D methods [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012], which often use low-pass filtering (Gaussian smoothing), or curve fitting for path planning. We show such an example video on our project webpage. There are cases where the warping-based motion model fails to handle severe occlusions or dis-occlusions, especially when combined with rolling shutter effects. Figure 14 shows two such examples. Our warping-based motion model chooses a large ? to enforce strong coherence between grid cells. In this way, we can minimize the geometrical distortion, but at the same time, we sacrifice motion accuracy and eventually the stability of the result. In general, we find geometrical distortion is more disruptive than some slight remaining jitters. But our discontinuity-preservation optimization produces visually pleasing results in most examples. We also do not deal with motion blur. Sometimes, the stabilized results contain visible blur artifacts. We have presented a new 2D video stabilization method with a bundled camera paths model. The proposed method can simultaneously generate comparable results to 3D methods while keeping merits of 2D methods. Using image warping techniques for motion representation is an interesting finding in this paper. extend this kind of representation to other video-based applications. We thank all the reviewers for their helpful discussions, Kaimo Lin and other subjects for their help in the user study, Nathan Holdstein and Jiangyu Liu for their help in proofreading. This work is also partially supported by the Singapore project R-263-000-620-112. C HEN , B.-Y., L EE , K.-Y., H UANG , W.-T., AND L IN , J.-S. 2008. Capturing intention-based full-frame video stabilization. Computer Graphics Forum 27, 7, 1805?1814. C HO , S., W ANG , J., AND L EE , S. 2012. Video deblurring for hand-held cameras using patch-based synthesis. ACM Trans. of SIGGRAPH) 31, 4. F ISCHLER , M. A., AND B OLLES , R. C. 1981. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. ACM 24, 6, 381?395. F ORSS ? N , P.-E., AND R INGABY , E. 2010. Rectifying rolling shutter video from hand-held devices. G AO , J., K IM , S. J., AND B ROWN , M. S. 2011. Constructing image panoramas using dual-homography warping. G LEICHER , M. L., AND L IU , F. 2007. Re-cinematography: Improving the camera dynamics of casual video. G OLDSTEIN , A., AND F ATTAL , R. 2012. Video stabilization using epipolar geometry. ACM Trans. (TOG) 31, 5, 126:1? 126:10. G RUNDMANN , M., K WATRA , V., AND E SSA , I. 2011. Autodirected video stabilization with robust l1 optimal camera paths. G RUNDMANN , M., K WATRA , V., C ASTRO , D., AND E SSA , I. 2012. Calibration-free rolling shutter removal. H ARTLEY , R., AND Z ISSERMAN , A. 2003. Multiple View Geometry in Computer Vision, 2 ed. Cambridge University Press, New York, NY, USA. I GARASHI , T., M OSCOVICH , T., AND H UGHES , J. F. 2005. Asrigid-as-possible shape manipulation. ACM Trans. of SIGGRAPH) 24, 3, 1134?1141. K ARPENKO , A., J ACOBS , D., B AEK , J., AND L EVOY , M. 2011. Digital video stabilization and rolling shutter correction using gyroscopes. In Stanford CS Tech Report. L EE , K.-Y., C HUANG , Y.-Y., C HEN , B.-Y., AND O UHYOUNG , M. 2009. Video stabilization using robust feature trajectories. L IANG , C.-K., C HANG , L.-W., AND C HEN , H. H. 2008. Analysis and compensation of rolling shutter effect. In IEEE Trans. L IN , W.-Y., L IU , S., M ATSUSHITA , Y., N G , T.-T., AND C HEONG , L.-F. 2011. Smoothly varying affine stitching. In\n        L IU , F., G LEICHER , M., J IN , H., AND A GARWALA , A. 2009. Content-preserving warps for 3d video stabilization. L IU , F., G LEICHER , M., W ANG , J., J IN , H., AND A GARWALA , A. 2011. Subspace video stabilization. ACM Trans. L IU , S., W ANG , Y., Y UAN , L., B U , J., T AN , P., AND S UN , J. 2012. Video stabilization with a depth camera. L UCAS , B. D., AND K ANADE , T. 1981. An iterative image registration technique with an application to stereo vision. (IJCAI), 674?679. M ATSUSHITA , Y., O FEK , E., G E , W., T ANG , X., AND S HUM , H.Y. 2006. Full-frame video stabilization with motion inpainting. IEEE Trans. Pattern Anal. 28, 1150?1163. M ORIMOTO , C., AND C HELLAPPA , R. 1998. Evaluation of image stabilization algorithms. of IEEE International Conference on Acoustics, Speech and Signal Processing, 2789 ? 2792. N AKAMURA , J. 2005. Image Sensors and Signal Processing for Digital Still Cameras. CRC Press, Inc. N IR , T., B RUCKSTEIN , A. M., AND K IMMEL , R. 2008. Overparameterized variational optical flow. J. Comput. Vision (IJCV) 76, 2, 205?216. S CHAEFER , S., M C P HAIL , T., AND W ARREN , J. 2006. Image deformation using moving least squares. ACM Trans. of SIGGRAPH) 25, 3, 533?540. S HUM , H.-Y., AND S ZELISKI , R. 2000. Construction of panoramic image mosaics with global and local alignment. J. Comput. Vision (IJCV) 36, 2, 101?130. S MITH , B. M., Z HANG , L., J IN , H., AND A GARWALA , A. 2009. Light field video stabilization. S ZELISKI , R. 1996. Motion estimation with quadtree splines. IEEE Trans. Pattern Anal. 18, 12, 1199?1210. T OMASI , C., AND M ANDUCHI , R. 1998. Bilateral filtering for gray and color images. ICCV, 839?846.",
  "resources" : [ ]
}