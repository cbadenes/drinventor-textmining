{
  "uri" : "sig2007-a5-cooper_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2007/a5-cooper_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Active Learning for Real-Time Motion Controllers",
    "published" : "2007",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Seth-Cooper",
      "name" : "Seth",
      "surname" : "Cooper"
    }, {
      "uri" : "http://drinventor/Aaron-Hertzmann",
      "name" : "Aaron",
      "surname" : "Hertzmann"
    }, {
      "uri" : "http://drinventor/Zoran-Popovic",
      "name" : "Zoran",
      "surname" : "Popovic"
    } ]
  },
  "bagOfWords" : [ "we", "have", "construct", "three", "example", "demonstrate", "we", "framework", "controller", "demonstrate", "accompany", "video", "each", "controller", "construct", "interactively", "short", "time", "motion", "capture", "lab", "we", "capture", "synthesize", "motion", "between", "20", "25", "frame", "per", "second", "computation", "time", "require", "negligible", "each", "active", "learning", "step", "take", "between", "30", "seconds", "depend", "size", "controller", "involve", "majority", "time", "spend", "capture", "especially", "since", "normally", "take", "two", "three", "try", "perform", "motion", "correct", "state", "match", "task", "we", "first", "example", "parameterized", "walk", "control", "space", "three-dimensional", "control", "stride", "length", "-lrb-", "distance", "between", "heel", "maximum", "spacing", "range", "from", "0.2", "0.9", "meter", "-rrb-", "speed", "-lrb-", "range", "from", "15", "25", "centimeter", "per", "frame", "-rrb-", "turn", "angle", "-lrb-", "change", "horizontal", "orientation", "per", "stride", "range", "from", "??", "radian", "-rrb-", "one", "task", "error", "metric", "each", "feature", "-lrb-", "e.g.", "stride", "length", "-rrb-", "measure", "square", "distance", "between", "desire", "feature", "maximum", "feature", "motion", "order", "determine", "stride", "metric", "assume", "sample", "contain", "complete", "walk", "cycle", "beginning", "right", "foot", "forward", "task", "allow", "motion", "from", "middle", "controller", "task", "produce", "only", "cluster", "use", "12", "example", "total", "843", "frame", "10", "pseudoexample", "we", "limit", "lab", "time", "roughly", "half", "hour", "include", "all", "active", "learning", "user", "interaction", "controller", "achieve", "89", "coverage", "-lrb-", "discuss", "section", "-rrb-", "we", "second", "example", "combine", "parameterized", "walk", "projectile", "dodging", "example", "have", "two", "task", "when", "nothing", "dodge", "control", "space", "one-dimensional", "control", "turn", "angle", "when", "walk", "dodge", "control", "space", "four-dimensional", "control", "turn", "angle", "well", "incoming", "height", "incoming", "angle", "distance", "front", "character", "incoming", "projectile", "walk", "turn", "parameter", "ratio", "radian", "per", "meter", "travel", "range", "from", "??", "task", "metric", "measure", "absolute", "value", "difference", "between", "ratio", "target", "value", "walk", "task", "allow", "motion", "from", "middle", "parameter", "incoming", "projectile", "specify", "relative", "character?s", "local", "coordinate", "system", "incoming", "height", "from", "0.25", "2.0", "meter", "distance", "front", "character", "aim", "from", "0.25", "1.0", "meter", "incoming", "angle", "from", "??", "radian", "parameter", "specify", "trajectory", "projectile", "error", "metric", "inverse", "distance", "between", "projectile", "any", "point", "character", "dodge", "task", "also", "include", "same", "turn", "control", "parameter", "use", "walk", "same", "error", "metric", "well", "synthesize", "controller", "contain", "10", "cluster", "use", "total", "30", "example", "total", "1121", "frame", "19", "pseudoexample", "we", "limit", "lab", "time", "roughly", "hour", "achieve", "76", "coverage", "dodge", "task", "we", "third", "example", "catch", "ball", "example", "have", "two", "task", "when", "catch", "zero-dimensional", "stand", "task", "when", "catch", "control", "space", "three-dimensional", "control", "incoming", "position", "plane", "front", "character", "well", "speed", "parameter", "incoming", "ball", "specify", "relative", "character?s", "local", "coordinate", "system", "incoming", "height", "from", "0.5", "2.0", "meter", "distance", "right", "character", "from", "-2.0", "2.0", "meter", "incoming", "speed", "from", "0.1", "0.2", "meter", "per", "frame", "parameter", "specify", "trajectory", "ball", "task", "error", "metric", "square", "distance", "between", "ball", "character?s", "right", "hand", "example", "we", "remove", "rotation", "invariance", "from", "all", "quantity", "order", "build", "model", "which", "character", "always", "face", "specific", "direction", "while", "wait", "next", "ball", "we", "have", "also", "allow", "user", "load", "several", "motion", "diving", "side", "place", "perform", "motion", "also", "example", "we", "use", "ik", "controller", "so", "we", "have", "can", "have", "greater", "reachability", "result", "motion", "result", "controller", "use", "12", "cluster", "use", "33", "example", "total", "1826", "frame", "23", "pseudoexample", "datum", "include", "diving", "motion", "capture", "lab", "session", "limit", "roughly", "hour", "we", "achieve", "57", "coverage", "catch", "task", "normalize", "manual", "controller", "discuss", "section", "worth", "note", "we", "also", "try", "create", "controller", "simpler", "cluster", "model", "do", "include", "IK", "find", "performance", "significantly", "poorer", "due", "nonlinearity", "underlie", "control", "space", "general", "more", "powerful", "controller", "model", "more", "active", "learning", "can", "take", "advantage", "require", "less", "sample", "produce", "effective", "controller", "we", "have", "introduce", "active", "learning", "framework", "create", "realtime", "motion", "controller", "adaptively", "determine", "which", "motion", "add", "model", "system", "create", "finely-controllable", "motion", "model", "reduce", "number", "datum", "clip", "little", "time", "spend", "motion", "capture", "studio", "addition", "always", "present", "worst-performing", "state", "sample", "user", "have", "continuous", "gauge", "quality", "result", "controller", "active", "learning", "framework", "both", "automatically", "determine", "parameter", "each", "individual", "cluster", "determine", "necessary", "number", "relationship", "between", "different", "cluster", "dynamically", "determine", "controller", "structure", "more", "sample", "appear", "although", "we", "system", "focus", "one", "specific", "model", "motion", "synthesis", "we", "believe", "general", "approach", "adaptive", "modelbuilding", "useful", "many", "type", "animation", "model", "any", "situation", "which", "minimize", "number", "motion", "sample", "important", "can", "potentially", "benefit", "from", "active", "learning", "we", "have", "design", "system", "fast", "flexible", "so", "relatively", "little", "time", "spend", "wait", "next", "candidate", "select", "hence", "we", "employ", "number", "heuristic", "incremental", "learning", "step", "we", "model", "generally", "optimal", "global", "sense", "we", "system", "do", "make", "formal", "guarantee", "be", "able", "perform", "every", "task", "from", "every", "state", "some", "task", "may", "impossible", "-lrb-", "e.g.", "start", "new", "task", "from", "midair", "-rrb-", "other", "may", "have", "be", "capture", "limited", "time", "available", "motion", "capture", "session", "failure", "mode", "controller", "during", "synthesis", "include", "have", "cluster", "appropriate", "state", "determine", "best", "cluster", "use", "determine", "best", "weight", "within", "cluster", "tradeoff", "between", "lab", "time", "number", "sample", "coverage", "user", "must", "evaluate", "we", "example", "we", "show", "possible", "get", "high", "amount", "coverage", "low", "number", "sample", "short", "lab", "time", "some", "difficulty", "arise", "during", "capture", "process", "necessary", "user", "look", "screen", "well", "mentally", "project", "motion", "onto", "on-screen", "visualization", "we", "believe", "difficulty", "can", "overcome", "through", "use", "augmented", "virtual", "reality", "system", "we", "have", "guarantee", "scalability", "we", "believe", "system", "scale", "well", "handle", "many", "different", "task", "perform", "sequentially", "create", "highly", "flexible", "character", "however", "we", "controller", "example", "do", "fully", "test", "effectiveness", "we", "approach", "very", "high", "dimension", "work", "certainly", "do", "solve", "fundamental", "problem", "curse", "dimensionality", "data-driven", "controller", "we", "believe", "active", "learning", "generalize", "controller", "higher", "dimension", "however", "controller", "10", "more", "task", "input", "together", "37", "dof", "character", "state", "space", "even", "though", "active", "learning", "drastically", "reduce", "number", "sample", "number", "require", "sample", "would", "still", "impractical", "very", "high", "dimension", "become", "imperative", "use", "sophisticated", "motion", "model", "accurately", "represent", "nonlinear", "dynamics", "since", "expressive", "power", "model", "greatly", "reduce", "number", "require", "sample" ],
  "content" : "We have constructed three examples to demonstrate our framework. The controllers are demonstrated in the accompanying video. Each of these controllers was constructed interactively in a short time in the motion capture lab. We capture and synthesize motions at between 20 and 25 frames per second. The computation time required is negligible: each active learning step took between 5 and 30 seconds, depending on the size of the controller involved. The majority of the time was spent in capture, especially since it normally takes two or three tries to perform a motion with the correct start state that matches the task. Our first example is a parameterized walk. The control space U is three-dimensional, controlling stride length (the distance between the heels at maximum spacing, ranging from 0.2 ? 0.9 meters), speed (ranging from 15 ? 25 centimeters per frame), and turning angle (change in horizontal orientation per stride, ranging from ??/4 to ?/4 radians). There is one task error metric for each of these features (e.g., stride length), measured as the the squared distance between the desired feature and the maximum feature in the motion. In order to determine strides, the metric assumes that a sample contains a complete walk cycle beginning with the right foot forward. This task allows motions to start from the middle. The controller for this task produced only 1 cluster, using 12 examples totaling 843 frames, and 10 pseudoexamples. We limited lab time to roughly half an hour, including all active learning and user interaction. This controller achieved 89% coverage (discussed in section 6). Our second example combines a parameterized walk with projectile dodging. This example has two tasks. When there is nothing to dodge, the control space is one-dimensional, controlling turning angle. When walking and dodging, the control space is four-dimensional, controlling turning angle as well as the incoming height, incoming angle, and distance in front of the character of the incoming projectile. The walk turning parameter as a ratio of radians per meter traveled, ranging from ??/3 to ?/3. The task metric measures the absolute value of the difference between this ratio and a the target value. The walk task allows motions to start from the middle. The parameters of the incoming projectile are specified relative to the character?s local coordinate system. They are the incoming height, from 0.25 ? 2.0 meters, the distance in front of the character to be aimed at, from 0.25 ? 1.0 meters, and the incoming angle, from ??/2 to ?/2 radians. These parameters specify a trajectory for the projectile. The error metric is the inverse of the distance between the projectile and any point on the character. The dodge task also includes the same turning control parameter as used for walking, with the same error metric as well. The synthesized controller contains 10 clusters, using total of 30 examples totaling 1121 frames, and 19 pseudoexamples. We limited lab time to roughly an hour, and achieved 76% coverage of the dodge task. Our third example is catching a ball. This example has two tasks. When not catching, there is a zero-dimensional stand task. When catching, the control space is three-dimensional, controlling the incoming position on the plane in front of the character as well as  speed. The parameters of the incoming ball are specified relative to the character?s local coordinate system. They are the incoming height, from 0.5 to 2.0 meters, the distance to the right of the character, from -2.0 to 2.0 meters, and the incoming speed, from 0.1 to 0.2 meters per frame. These parameters specify a trajectory for the ball. The task error metric is the squared distance between the ball and the character?s right hand. For this example, we removed rotation invariance from all quantities, in order to build a model in which the character is always facing a specific direction while waiting for the next ball. We have also allowed the user to load in several motions of diving to the side in place of performing these motions. Also, in this example, we use the IK controller so that we have can have greater reachability of the resulting motion. The resulting controller used 12 clusters, using 33 examples totaling 1826 frames, and 23 pseudoexamples. The data, not including the diving motions 4 , was captured in the lab session limited to roughly an hour. We achieved 57% coverage of the catch task, normalized by the manual controllers discussed in section 6. It is worth noting that we also tried creating the controller with a simpler cluster model that did not include IK, and found that the performance was significantly poorer, due to the nonlinearity of the underlying control space. In general, the more powerful the controller model, the more active learning can take advantage of it and require less samples to produce an effective controller. We have introduced an active learning framework for creating realtime motion controllers. By adaptively determining which motions to add to the model, the system creates finely-controllable motion models with a reduced number of data clips and little time spent in the motion capture studio. In addition, by always presenting the worst-performing state samples, the user has a continuous gauge of the quality of the resulting controller. The active learning framework both automatically determines the parameters of each individual cluster and determines the necessary number and relationship between different clusters, dynamically determining the controller structure as more samples appear. Although our system focuses on one specific model for motion synthesis, we believe that the general approach of adaptive modelbuilding will be useful for many types of animation models ? any situation in which minimizing the number of motion samples is important can potentially benefit from active learning. We have designed the system to be fast and flexible, so that relatively little time is spent waiting for the next candidate to be selected. Hence, we employed a number of heuristic, incremental learning steps, and our models are not generally ?optimal? in a global sense. Our system does not make formal guarantees of being able to perform every task from every start state. Some of these tasks may be impossible (e.g., starting a new task from midair); others may not have been captured in the limited time available in a motion capture session. The failure modes of the controller during synthesis include not having a cluster with the appropriate start state, not determining the best cluster to use, and not determining the best weights within a cluster. There is a tradeoff between lab time, number of samples, and coverage that the user must evaluate. In our examples we show that it is possible to get a high amount of coverage with a low number of samples and short lab time. Some difficulties arise during the capture process. It is necessary for the user to look at the screen as well as mentally project their  motion onto the on-screen visualization. We believe that these difficulties can be overcome through the use of an augmented or virtual reality system. We have no guarantee of scalability, but we believe that this system will scale well to handle many different tasks performed sequentially, creating highly flexible characters. However, our controller examples do not fully test the effectiveness of our approach in very high dimensions, and this work certainly does not solve the fundamental problem of ?curse of dimensionality? for data-driven controllers. We believe that active learning will generalize to controllers with higher dimensions. However, for controllers with 10 or more task inputs together with 37 DOF characters state space, even though active learning will drastically reduce the number of samples, the number of required samples would still be impractical. In very high dimensions, it becomes an imperative to use sophisticated motion models that accurately represent nonlinear dynamics, since the expressive power of such models greatly reduces the number of required samples.",
  "resources" : [ ]
}