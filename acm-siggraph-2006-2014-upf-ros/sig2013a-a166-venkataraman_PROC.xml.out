{
  "uri" : "sig2013a-a166-venkataraman_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013a/a166-venkataraman_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "PiCam: An Ultra-Thin High Performance Monolithic Camera Array",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Kartik-Venkataraman",
      "name" : "Kartik",
      "surname" : "Venkataraman"
    }, {
      "uri" : "http://drinventor/Dan-Lelescu",
      "name" : "Dan",
      "surname" : "Lelescu"
    }, {
      "uri" : "http://drinventor/Jacques-Duparr?",
      "name" : "Jacques",
      "surname" : "Duparr?"
    }, {
      "uri" : "http://drinventor/Andrew-McMahon",
      "name" : "Andrew",
      "surname" : "McMahon"
    }, {
      "uri" : "http://drinventor/Gabriel-Molina",
      "name" : "Gabriel",
      "surname" : "Molina"
    }, {
      "uri" : "http://drinventor/Priyam-Chatterjee",
      "name" : "Priyam",
      "surname" : "Chatterjee"
    }, {
      "uri" : "http://drinventor/Robert-Mullis",
      "name" : "Robert",
      "surname" : "Mullis"
    }, {
      "uri" : "http://drinventor/Shree-Nayar",
      "name" : "Shree",
      "surname" : "Nayar"
    } ]
  },
  "bagOfWords" : [ "we", "contribution", "post-processing", "image", "from", "camera", "array", "include", "cost", "function", "parallax", "detection", "integrate", "across", "multiple", "color", "channel", "regularize", "image", "restoration", "-lrb-", "superresolution", "-rrb-", "process", "take", "account", "all", "system", "degradation", "adapt", "range", "practical", "imaging", "condition", "PiCam", "camera", "architecture", "specification", "present", "section", "design", "we", "sensor", "array", "present", "section", "3.2", "eliminate", "color", "crosstalk", "endemic", "all", "sensor", "Bayer", "filter", "each", "camera", "array", "sensitive", "single", "spectral", "color", "optically", "isolate", "from", "its", "neighbor", "eliminate", "color", "crosstalk", "improve", "color", "fidelity", "reduce", "need", "aggressive", "color", "correction", "all", "lighting", "condition", "we", "lens", "array", "design", "module", "integration", "present", "section", "3.1", "finally", "we", "approach", "unique", "provide", "efficient", "software", "imaging", "solution", "capable", "be", "implement", "mobile", "processor", "key", "contribution", "we", "approach", "also", "we", "compute", "scene", "depth", "each", "pixel", "make", "only", "imaging", "system", "also", "potentially", "3d", "acquisition", "system", "we", "approach", "represent", "new", "paradigm", "mobile", "imaging", "provide", "depth", "modality", "small", "form", "factor", "design", "system", "follow", "symbiotic", "design", "approach", "guide", "both", "need", "image", "post-processing", "trade-off", "hardware", "design", "ultimately", "deliver", "ultra-thin", "imaging", "system", "well-balanced", "term", "overall", "resolve", "power", "signal-to-noise", "ratio", "-lrb-", "snr", "-rrb-", "dynamic", "range", "spectral", "fidelity", "depth", "accuracy", "imagebased", "rendering", "technique", "provide", "way", "bridge", "gap", "between", "synthetically", "generate", "scene", "photorealism", "allow", "one", "record", "video", "scene", "which", "appear", "freeze", "time", "ability", "view", "frozen", "scene", "dynamically", "from", "move", "viewpoint", "system", "enable", "compute", "range", "image", "use", "multi-camera", "stereo", "process", "well", "intensity", "image", "work", "Wilburn", "et", "al.", "-lsb-", "2005", "-rsb-", "design", "more", "general", "purpose", "system", "capable", "be", "extend", "large", "array", "camera", "ability", "synchronize", "between", "camera", "well", "carefully", "assign", "spatial", "position", "configuration", "allow", "significant", "creative", "control", "capture", "very", "wide", "field", "view", "from", "which", "interesting", "scene", "collage", "could", "synthesize", "allow", "one", "capture", "sufficient", "context", "become", "easy", "perceive", "overall", "scene", "structure", "also", "important", "acknowledge", "work", "-lsb-", "ng", "et", "al.", "2005", "Georgiev", "et", "al.", "2011", "-rsb-", "plenoptic", "camera", "give", "capture", "light", "field", "contrast", "we", "approach", "extend", "depth", "field", "shrink", "aperture", "focal", "length", "each", "lens", "array", "addition", "since", "we", "approach", "have", "all", "lens", "overlap", "field", "view", "offer", "we", "potential", "recover", "resolution", "encode", "downsampled", "aliased", "image", "array", "post-processing", "software", "use", "iterative", "back-projection", "-lrb-", "ibp", "-rrb-", "technique", "reproduce", "image", "higher", "resolution", "than", "capture", "any", "single", "aperture", "array", "well", "know", "ml-solution", "noise", "unstable", "indeed", "noise", "amplification", "note", "addition", "demonstrate", "resolution", "be", "low", "compare", "today?s", "expectation", "mobile", "imaging", "system", "unlike", "we", "system", "which", "capable", "meeting", "industry", "expectation", "higher", "resolution", "-lrb-", "5mp", "8mp", "-rrb-", "all", "approach", "produce", "low", "output", "resolution", "do", "offer", "full", "complement", "use", "case", "high-resolution", "still", "720p/1080p", "video", "available", "mobile", "camera", "today", "we", "use", "term", "legacy", "camera", "refer", "conventional", "single", "aperture", "design", "camera", "currently", "available", "mobile", "phone", "camera", "sensor", "have", "2x2", "Bayer", "filter", "pattern", "over", "entire", "pixel", "array", "overlay", "top", "sensor", "4or", "5-element", "lens", "voicecoil", "motor", "include", "provide", "autofocus", "contrast", "Figure", "show", "PiCam?s", "focal", "plane", "sensor", "divide", "number", "smaller", "focal", "plane", "which", "turn", "overlay", "lens", "array", "several", "significant", "change", "immediately", "visible", "Optical", "Stack", "Height", "firstly", "height", "optical", "stack", "reduce", "smaller-aperture", "lens", "corresponding", "reduction", "number", "lens", "element", "need", "benefit", "problem", "scale", "lens", "-lrb-", "change", "its", "focal", "length", "constant", "f-number", "field", "view", "-rrb-", "its", "information", "capacity", "fully", "explore", "-lsb-", "Lohmann", "1989", "Fischer", "et", "al.", "2008", "-rsb-", "show", "magnitude", "wavefront", "aberration", "scale", "lens", "size", "up", "power", "picam", "instantiation", "filter", "optical", "stack", "we", "see", "section", "3.1", "reduction", "spectral", "bandwidth", "each", "channel", "enable", "one", "design", "higher", "performance", "lens", "choice", "color", "filter", "pattern", "largely", "decide", "spectral", "coverage", "around", "reference", "camera", "-lrb-", "which", "define", "we", "render", "viewpoint", "-rrb-", "different", "camera", "use", "mode", "still", "capture", "video", "video", "mode", "save", "power", "consumption", "only", "16", "camera", "active", "while", "still", "capture", "mode", "all", "16", "camera", "activate", "increase", "resolution", "requirement", "area", "immediately", "around", "foreground", "object", "visible", "all", "camera", "array", "due", "fact", "different", "camera", "capture", "scene", "from", "different", "viewpoint", "foreground", "object", "occlude", "part", "background", "scene", "therefore", "important", "irrespective", "orientation", "occluded", "zone", "around", "foreground", "object", "area", "sample", "least", "one", "camera", "each", "spectral", "color", "choose", "filter", "pattern", "-lrb-", "show", "figure", "-lrb-", "-rrb-", "-rrb-", "have", "two", "overlap", "sub-group", "-lrb-", "top", "leave", "bottom", "right", "-rrb-", "wherein", "red", "blue", "spectral", "channel", "symmetrically", "dispose", "around", "reference", "green", "camera", "center", "additionally", "also", "allow", "redundancy", "any", "one", "two", "sub-group", "could", "use", "generate", "video", "top", "right", "corner", "blue", "filter", "bottom", "left", "corner", "red", "filter", "complete", "uniformity", "requirement", "16", "camera", "still", "image", "use", "mode", "when", "lens", "focus", "distance", "all", "object", "distance", "from", "half", "hyperfocal", "distance", "out", "infinity", "acceptably", "sharp", "result", "image", "PiCam", "its", "significantly", "shorter", "focal", "length", "all", "focus", "image", "give", "above", "difference", "final", "imaging", "performance", "camera", "array", "function", "-lrb-", "-rrb-", "resolution", "optics", "-lrb-", "-rrb-", "optical", "format", "each", "camera", "array", "-lrb-", "-rrb-", "number", "camera", "array", "-lrb-", "-rrb-", "pixel", "pitch", "sensor", "resolution", "-lrb-", "-rrb-", "superresolution", "factor", "section", "3.1", "3.2", "3.3", "below", "describe", "key", "feature", "lens", "sensor", "array", "how", "trade-off", "impact", "final", "resolution", "performance", "picam", "primary", "challenge", "lens", "design", "array", "camera", "preserve", "sufficient", "signal", "high", "spatial", "frequency", "enable", "effective", "superresolution", "from", "individual", "low", "resolution", "image", "mention", "earlier", "each", "lens", "picam", "design", "only", "relatively", "narrow", "band", "overall", "visible", "spectral", "band", "wavelength", "we", "perform", "analytic", "study", "wherein", "we", "vary", "full", "width", "half", "maximum", "-lrb-", "fwhm", "-rrb-", "spectral", "channel", "bandwidth", "over", "which", "lens", "design", "optimal", "diffraction", "limited", "lens", "be", "design", "fwhm?s", "0nm", "30nm", "60nm", "100nm", "width", "respectively", "result", "lens", "show", "monotonic", "decrease", "both", "on-axis", "off-axis", "mtf", "increase", "fwhm", "spectral", "channel", "substantial", "drop", "greater", "than", "25", "occur", "when", "fwhm", "increase", "from", "60nm", "100nm", "show", "potential", "efficiency", "MTF", "achieve", "when", "design", "lens", "narrow", "spectral", "band", "thus", "give", "lens", "blur", "correspondingly", "less", "achromatization", "effort", "have", "take", "which", "turn", "imply", "lower", "material", "combination", "therefore", "fewer", "lens", "element", "we", "supplementary", "material", "show", "variation", "MTF", "blur", "increase", "FWHM", "diffraction", "limited", "optical", "system", "diameter", "diffraction", "ring", "-lrb-", "characterize", "lens", "psf", "-rrb-", "first", "zero", "function", "wavelength", "light", "-lrb-", "-rrb-", "f-number", "lens", "define", "optical", "cut-off", "frequency", "system", "psf", "square", "fourier", "transform", "lens", "aperture", "mtf", "magnitude", "fourier", "transform", "psf", "monochromatic", "diffraction", "limit", "optical", "mtf", "give", "optics", "-lsb-", "arcco", "-lrb-", "-rrb-", "-rsb-", "where", "normalize", "spatial", "frequency", "define", "absolute", "spatial", "frequency", "diffraction", "cutoff", "spatial", "frequency", "govern", "wavelength", "light", "f-number", "lens", "-lrb-", "-rrb-", "Figure", "show", "diffraction", "limited", "optical", "mtf", "its", "variation", "f-number", "optical", "format", "respectively", "we", "choose", "f3", ".1", "optics", "design", "optical", "format", "equivalent", "1000", "750", "pixel", "-lrb-", "inch", "-rrb-", "reference", "design", "illustrate", "we", "performance", "analysis", "throughout", "paper", "figure", "show", "reference", "design", "solid", "blue", "curve", "graph", "varying", "f-number", "-lrb-", "from", "f3", ".1", "f2", ".6", "f2", ".0", "-rrb-", "while", "hold", "optical", "format", "1000", "750", "pixel", "show", "two", "other", "solid", "curve", "different", "shades", "blue", "finally", "we", "show", "another", "curve", "optical", "format", "800", "600", "use", "1.4", "pixel", "f2", ".4", "lens", "show", "couple", "different", "way", "achieve", "higher", "optical", "cutoff", "mtf", "first", "option", "increase", "optical", "format", "increase", "cost", "xy-size", "camera", "array", "have", "advantage", "optical", "design", "less", "challenging", "second", "option", "decrease", "f-number", "lens", "benefit", "lower", "xy-footprint", "would", "come", "cost", "increase", "lens", "design", "complexity", "nonetheless", "both", "option", "tractable", "give", "camera", "designer", "choice", "base", "various", "mitigating", "design", "factor", "comparison", "we", "show", "mtf", "diffraction", "limited", "f2", ".4", "lens", "iphone5", "wherein", "larger", "optical", "format", "iphone5", "provide", "larger", "space-bandwidth", "product", "permit", "much", "larger", "amount", "information", "transmit", "focal", "plane", "gap", "between", "achievable", "resolution", "large", "aperture", "conventional", "camera", "-lrb-", "iphone5", "-rrb-", "PiCam", "camera", "array", "what", "need", "address", "through", "combination", "carefully", "choose", "parameter", "optics", "sensor", "superresolution", "software", "much", "camera", "array", "functionality", "achieve", "through", "use", "novel", "image", "sensor", "architecture", "under", "certain", "simplify", "assumption", "system", "mtf", "essentially", "product", "optical", "mtf", "geometric", "pixel", "mtf", "wherein", "each", "optics", "sensor", "stage", "lowpass", "filter", "input", "signal", "-lrb-", "scene", "-rrb-", "thereby", "degrade", "response", "higher", "frequency", "-lsb-", "Holst", "1998", "-rsb-", "require", "just", "optics", "mtf", "matter", "overall", "system", "MTF", "have", "maximize", "require", "consideration", "pixel", "architecture", "size", "thus", "image", "sensor", "1000", "750", "pixel", "use", "1.75", "pixel", "Nyquist", "frequency", "286", "line-pair", "per", "mm", "750", "line-width", "per", "picture", "height", "-lrb-", "line-pair", "equivalent", "two", "line-width", "-rrb-", "legacy", "camera", "very", "little", "contrast", "desire", "spatial", "frequency", "larger", "than", "sensor?s", "order", "avoid", "aliasing", "final", "output", "image", "where", "we", "camera", "array", "differ", "from", "legacy", "camera", "here", "desirable", "have", "content", "above", "when", "multiple", "copy", "aliased", "signal", "present", "case", "camera", "array", "possible", "through", "superresolution", "use", "information", "inherently", "present", "aliase", "reconstruct", "higher", "resolution", "signal", "order", "attain", "certain", "minimum", "mtf", "superresolved", "high", "resolution", "-lrb-", "hr", "-rrb-", "image", "we", "need", "provide", "sufficient", "contrast", "aliase", "low", "resolution", "-lrb-", "lr", "-rrb-", "input", "image", "from", "camera", "array", "which", "turn", "imply", "certain", "threshold", "lens", "mtf", "limit", "threshold", "become", "evident", "graph", "exemplify", "optics", "sensor", "design", "figure", "while", "pixel", "size", "influence", "sampling", "rate", "hence", "aliasing", "structure", "pixel", "stack", "itself", "influence", "amount", "pixel", "blur", "pixel", "crosstalk", "pixel", "mtf", "desirable", "see", "what", "efficiency", "can", "enable", "exploit", "camera", "array", "architecture", "develop", "here", "top", "oxide", "layer", "separate", "microlen", "layer", "from", "nitride", "passivation", "layer", "bottom", "oxide", "layer", "provide", "support", "isolation", "metal", "interconnect", "use", "connect", "various", "part", "sensor", "finally", "very", "bottom", "silicon", "substrate", "active", "area", "-lrb-", "photodiode", "-rrb-", "which", "collect", "photon", "incident", "while", "convenient", "think", "pixel", "point", "sample", "reality", "area", "sampling", "construct", "microlen", "spatially", "sample", "light", "over", "give", "area", "focus", "active", "photodiode", "essence", "average", "light", "over", "area", "cover", "microlen", "quite", "simply", "larger", "width", "pixel", "-lrb-", "i.e.", "spatial", "box", "filter", "-rrb-", "larger", "pixel", "blur", "hence", "smaller", "cut-off", "frequency", "frequency", "domain", "actual", "implementation", "pixel", "crosstalk", "reduce", "pixel", "mtf", "from", "define", "its", "ideal", "sinc", "filter", "Figure", "-lrb-", "-rrb-", "angular", "incoming", "light", "focus", "onto", "pixel", "photodiode", "bottom", "stack", "however", "certain", "percentage", "light", "lose", "through", "crosstalk", "neighbor", "pixel", "along", "common", "shared", "boundary", "neighbor", "pixel", "stack", "therefore", "desirable", "reduce", "height", "pixel", "stack", "reduce", "amount", "light", "lose", "due", "crosstalk", "since", "PiCam", "camera", "array", "employ", "monochromatic", "camera", "wherein", "each", "camera", "array", "sensitive", "just", "one", "color", "light", "possible", "move", "color", "filter", "out", "pixel", "stack", "deposit", "optical", "stack", "sensor", "cover-glass", "instead", "advantage", "size", "pixel", "stack", "reduce", "removal", "color", "filter", "from", "pixel", "stack", "from", "Figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "can", "see", "almost", "equivalent", "30", "reduction", "thereby", "reduce", "crosstalk", "significant", "amount", "addition", "bring", "photodiode", "closer", "microlen", "enable", "pixel", "accept", "wider", "cone", "incoming", "light", "thus", "provide", "increase", "design", "degree", "freedom", "optics", "design", "reduce", "light", "sensitivity", "expense", "improve", "pixel", "mtf", "give", "significant", "improvement", "sensitivity", "due", "removal", "color", "filter", "we", "feel", "reasonable", "trade-off", "reference", "design", "do", "front-side", "illuminated", "pixel", "similar", "argument", "hold", "next", "generation", "backside-illuminated", "pixel", "well", "Figure", "show", "overall", "system", "mtf", "when", "both", "mtf", "diffraction", "limited", "optics", "pixel", "mtf", "take", "account", "each", "configuration", "list", "Figure", "we", "show", "corresponding", "system", "mtf", "can", "see", "blur", "induce", "pixel", "area", "sampling", "reduce", "system", "mtf", "from", "corresponding", "optics", "mtf", "alone", "sensor", "array", "design", "single", "substrate", "multiple", "sub-array", "each", "image", "project", "through", "aperture", "lens", "array", "provide", "temporally", "coherent", "capture", "sub-image", "where", "relative", "temporal", "offset/phase", "controllable", "enable", "feature", "single", "shot", "hdr", "where", "different", "subimage", "could", "have", "different", "exposure", "time", "high-speed", "video", "capture", "through", "sequential", "staggering", "capture", "time", "individual", "camera", "give", "plurality", "focal", "plane", "simultaneously", "generate", "pixel", "datum", "also", "necessary", "multiplex", "output", "datum", "onto", "common", "output", "through", "industry", "standard", "camera", "serial", "interface", "-lrb-", "mipus", "csi2", "-rrb-", "we", "develop", "container", "format", "enable", "interleaving", "pixel", "datum", "from", "each", "camera", "-lrb-", "so", "minimize", "on-chip", "storage", "requirement", "-rrb-", "well", "provide", "receiver", "sufficient", "information", "interpret", "receive", "datum", "-lrb-", "example", "which", "camera", "enable", "relative", "readout", "position", "each", "camera", "multiplex", "scheme", "use", "gain", "integration", "time", "-rrb-", "two", "aspect", "differentiate", "picam", "from", "conventional", "camera", "term", "system", "resolution", "first", "explain", "section", "3.1", "requirement", "sufficient", "optical", "mtf", "above", "Nyquist", "sampling", "limit", "sensor", "superresolution", "second", "aspect", "superresolution", "factor", "itself", "how", "much", "superresolution", "can", "we", "expect", "from", "signal", "processing", "perhaps", "most", "relevant", "study", "respect", "-lsb-", "Baker", "Kanade", "2002", "-rsb-", "who", "address", "limit", "resolution", "improvement", "one", "could", "achieve", "use", "linear", "sr", "reconstruction", "here", "we", "investigate", "effect", "aliasing", "analyze", "superresolution", "factor", "while", "take", "account", "amount", "aliase", "present", "signal", "Figure", "-lrb-", "-rrb-", "show", "MTF", "curve", "low", "resolution", "-lrb-", "lr", "-rrb-", "image", "use", "solid", "blue", "line", "corresponding", "high", "resolution", "-lrb-", "hr", "-rrb-", "superresolved", "image", "use", "we", "superresolution", "algorithm", "-lrb-", "section", "4.2", "-rrb-", "use", "solid", "green", "line", "represent", "mtf", "we", "reference", "design", "use", "f3", ".1", "optics", "1000", "750", "1.75", "sensor", "Nyquist", "frequency", "show", "red", "line", "750lw/ph", "signal", "LR", "image", "above", "Nyquist", "aliased", "appear", "signal", "lower", "frequency", "than", "Nyquist", "represent", "dash", "blue", "curve", "reflect", "about", "red", "Nyquist", "line", "750lw/ph", "thus", "Nyquist", "frequency", "all", "lr", "signal", "aliase", "frequency", "below", "Nyquist", "dash", "line", "represent", "percentage", "aliased", "signal", "present", "lr", "signal", "determine", "maximum", "resolvable", "frequency", "lr", "domain", "we", "choose", "frequency", "where", "amount", "aliased", "signal", "more", "than", "20", "total", "signal", "present", "coincide", "frequency", "480lw/ph", "indicate", "vertical", "solid", "blue", "line", "graph", "lower", "part", "solid", "blue", "vertical", "line", "shaded", "red", "indicate", "amount", "aliased", "signal", "present", "correlate", "very", "well", "visually", "LR", "image", "iso12233", "chart", "which", "show", "different", "frequency", "image", "single", "camera", "array", "we", "choose", "similar", "threshold", "20", "contrast", "mtf", "superresolved", "hr", "image", "determine", "maximum", "resolvable", "output", "hr", "frequency", "superresolved", "hr", "image", "correspond", "hr", "mtf", "curve", "show", "figure", "-lrb-", "-rrb-", "indicate", "maximum", "resolvable", "frequency", "1140lw/ph", "ratio", "maximum", "resolvable", "hr", "frequency", "maximum", "resolvable", "input-lr", "frequency", "-lrb-", "determine", "amount", "aliase", "present", "signal", "-rrb-", "take", "true", "superresolution", "factor", "Figure", "show", "software", "processing", "pipeline", "use", "synthesize", "hr", "image", "from", "multiple", "lr", "image", "generate", "from", "camera", "array", "purpose", "illustration", "we", "choose", "aperture", "middle", "array", "-lrb-", "typically", "green", "camera", "closest", "array", "center", "-rrb-", "reference", "viewpoint", "generate", "hr", "image", "see", "viewpoint", "description", "follow", "green", "camera", "correspond", "viewpoint", "denote", "reference", "camera", "section", "below", "outline", "key", "aspect", "above", "stage", "highlight", "how", "we", "approach", "processing", "image", "from", "picam", "differ", "from", "prior", "art", "individual", "camera", "array", "subject", "photometric", "geometric", "variation", "arise", "during", "manufacturing", "module", "assembly", "process", "photometric", "normalization", "step", "correct", "transmission", "difference", "individual", "lens", "capture", "scene", "uniform", "intensity", "-lrb-", "i.e.", "flat", "field", "-rrb-", "each", "camera", "array", "spatially-varying", "correction", "factor", "compute", "which", "reduce", "variation", "photometric", "response", "additionally", "pre-calibrated", "geometric", "normalization", "surface", "characterize", "offset", "geometry", "between", "camera", "due", "difference", "lens", "distortion", "residual", "distortion", "vector", "later", "use", "during", "parallax", "superresolution", "account", "lens", "distortion", "variations.the", "output", "stage", "photonormalized", "set", "LR", "image", "input", "parallax", "detection", "stage", "pipeline", "Parallax", "detection", "parallax", "processing", "determine", "pixel", "correspondence", "non-reference", "image", "respect", "reference", "camera", "we", "approach", "build", "upon", "past", "work", "do", "area", "stereo", "correspondence", "-lsb-", "szeliski", "2010", "-rsb-", "because", "camera", "array", "coplanar", "non-reference", "camera", "scene", "object", "simply", "shift", "along", "epipolar", "line", "direction", "determine", "baseline", "separation", "between", "camera", "discrete", "number", "depths", "search", "distribute", "equal", "increment", "up", "desire", "maximum", "disparity", "number", "depths", "search", "large", "enough", "enable", "detection", "sub-pixel", "level", "accuracy", "essential", "fusion", "superresolution", "step", "later", "stage", "pipeline", "make", "determination", "candidate", "depth", "pixel", "non-reference", "camera", "correspond", "-lrb-", "-rrb-", "use", "calculate", "matching", "cost", "we", "use", "sum-of-absolute-difference", "-lrb-", "sad", "-rrb-", "metric", "where", "lower", "cost", "indicate", "higher", "correspondence", "between", "pixel", "across", "multiple", "viewpoint", "higher", "cost", "indicate", "depth", "likely", "poor", "match", "sad", "matching", "cost", "metric", "apply", "only", "between", "pixel", "from", "camera", "similar", "color", "filter", "green", "camera", "array", "list", "contain", "camera", "compare", "against", "reference", "green", "camera", "list", "rcomb", "enumerate", "combination", "red", "camera", "compare", "other", "red", "camera", "list", "bcomb", "exist", "same", "purpose", "blue", "combination", "camera", "list", "compare", "use", "sad", "metric", "result", "add", "combined", "matching", "score", "below", "even", "distribution", "red", "blue", "camera", "around", "reference", "green", "camera", "represent", "another", "key", "point", "differentiation", "from", "prior", "art", "TOMBO", "approach", "where", "special", "consideration", "give", "placement", "red", "green", "blue", "filter", "-lsb-", "Tanida", "et", "al.", "2003", "-rsb-", "we", "approach", "ensure", "all", "pixel", "capture", "reference", "green", "camera", "also", "see", "least", "one", "red", "one", "blue", "camera", "give", "list", "matching", "score", "calculate", "each", "candidate", "depth", "give", "pixel", "-lrb-", "-rrb-", "reference", "camera", "use", "following", "equation", "sum", "contribution", "from", "red", "green", "blue", "pixel", "correspondence", "across", "multiple", "camera", "coordinate", "-lrb-", "-rrb-", "represent", "pixel", "coordinate", "camera", "correspond", "reference", "pixel", "location", "-lrb-", "-rrb-", "depth", "-lrb-", "-rrb-", "represent", "pixel", "value", "location", "camera", "p.", "similarly", "ref", "-lrb-", "-rrb-", "represent", "intensity", "pixel", "-lrb-", "-rrb-", "reference", "camera", "camera", "combination", "-lcb-", "-rcb-", "represent", "individual", "pair", "camera", "compare", "latter", "two", "summation", "term", "match", "cost", "picam", "have", "intrinsic", "advantage", "over", "stereo", "camera", "increase", "number", "camera", "come", "robustness", "noise", "depth", "search", "additionally", "combine", "matching", "score", "from", "multiple", "orthogonal", "baseline", "explore", "Okutomi", "Kanade", "-lsb-", "1993", "-rsb-", "previously", "can", "resolve", "ambiguity", "case", "traditionally", "problematic", "binocular", "stereo", "detect", "depth", "feature", "orient", "parallel", "baseline", "match", "score", "from", "camera", "pair", "both", "horizontal", "vertical", "baseline", "aggregate", "same", "matching", "score", "situation", "can", "resolve", "because", "camera", "vertical", "baseline", "able", "detect", "depth", "horizontal", "feature", "even", "camera", "horizontal", "baseline", "can", "depth", "map", "provide", "super-resolution", "same", "size", "single", "low-resolution", "array", "image", "reflect", "viewpoint", "reference", "camera", "depth", "map", "contain", "each", "pixel", "depth", "value", "which", "yield", "lowest", "value", "when", "aggregate", "all", "camera", "include", "list", "rcomb", "bcomb", "next", "super-resolution", "step", "depth", "-lrb-", "parallax", "-rrb-", "information", "use", "along", "known", "pixel", "correspondence", "fuse", "restore", "higher", "resolution", "full-color", "version", "reference", "camera", "image", "once", "lr", "image", "register", "reference", "camera", "we", "rely", "superresolution", "-lrb-", "sr", "-rrb-", "process", "obtain", "high-resolution", "image", "from", "capture", "low-resolution", "image", "progression", "process", "illustrate", "Figure", "design", "picam", "system", "allow", "aliased", "frequency", "capture", "each", "LR", "image", "-lrb-", "see", "Section", "3.3", "-rrb-", "serve", "process", "we", "sr", "approach", "design", "bayesian", "framework", "take", "full", "advantage", "we", "knowledge", "system", "parameter", "information", "use", "form", "imaging", "prior", "along", "statistical", "image", "prior", "need", "image", "restoration", "imaging", "prior", "include", "sensor", "analog", "gain", "exposure", "time", "sensor-noise", "characteristic", "optics", "psf", "vignetting", "geometric", "distortion", "knowledge", "array", "camera", "geometry", "-lrb-", "i.e.", "baseline", "-rrb-", "what", "follow", "we", "present", "we", "superresolution", "restoration", "where", "main", "contribution", "include", "-rrb-", "original", "approach", "determine", "likelihood-gradient", "restoration", "-rrb-", "new", "uncertainty", "processing", "approach", "mitigate", "error", "geometric", "registration", "array", "image", "-rrb-", "statistical", "prior", "regularize", "solution", "guide", "restoration", "towards", "noise-suppressed", "hr", "image", "model", "image", "formation", "process", "high", "resolution", "image", "we", "need", "restore", "2d", "projection", "3d", "scene", "onto", "image", "plane", "virtual", "higher", "resolution", "reference", "camera", "let", "desire", "lexicographically", "order", "-lrb-", "vectorize", "-rrb-", "hr", "image", "capture", "high", "resolution", "grid", "correspond", "select", "reference", "LR", "camera", "image", "-lcb-", "-rcb-", "feature", "geometric", "displacement", "respect", "reference", "image", "which", "include", "optical", "distortion", "-lrb-", "scene", "independent", "-rrb-", "parallax", "disparity", "-lrb-", "scene", "dependent", "-rrb-", "mathematically", "we", "can", "define", "forward", "imaging", "model", "generate", "each", "observe", "LR", "image", "where", "shift", "matrix", "capture", "total", "-lrb-", "geometric", "distortion", "parallax", "shift", "-rrb-", "displacement", "image", "see", "from", "th", "camera", "respect", "reference", "camera", "however", "spatial", "variation", "usually", "smooth", "enough", "allow", "we", "apply", "model", "equation", "smaller", "area", "sensor", "high", "resolution", "image", "while", "imaging", "noise", "assume", "i.i.d.", "note", "general", "commutative", "even", "though", "block-toeplitz", "-lrb-", "thus", "can", "make", "block-circulant", "-rrb-", "when", "spatially", "localized", "matrix", "necessarily", "block-toeplitz", "specifically", "target", "sr", "light", "field", "image", "capture", "plenoptic", "camera", "researcher", "have", "recently", "propose", "sr", "algorithm", "exploit", "specific", "image", "capture", "method", "-lsb-", "Wanner", "Goldluecke", "2012", "Georgiev", "et", "al.", "2011", "-rsb-", "exploit", "we", "knowledge", "system", "characteristic", "parameter", "we", "employ", "bayesian", "restoration", "approach", "more", "precisely", "we", "formulate", "maximum-a-posteriorus", "-lrb-", "map", "-rrb-", "approach", "derive", "specifically", "we", "system", "restore", "high", "resolution", "image", "we", "iteratively", "minimize", "convex", "objective", "function", "consist", "likelihood", "-lrb-", "-rrb-", "prior", "-lrb-", "-rrb-", "term", "here", "denote", "norm", "-lrb-", "-rrb-", "statistical", "prior", "gradient", "-lrb-", "-rrb-", "respect", "compute", "here", "gradient", "operator", "initial", "estimate", "obtain", "use", "fusion", "sr", "process", "pixel", "initially", "fuse", "non-integral", "hr", "grid", "position", "interpolate", "use", "any", "interpolation", "technique", "-lrb-", "example", "nearest", "neighbor", "-rrb-", "integer", "hr", "grid", "position", "thus", "outcome", "initial", "fusion", "stack", "variable", "number", "pixel", "-lrb-", "include", "pixel", "-rrb-", "each", "integer", "hr", "grid", "position", "generate", "proper", "initial", "hr", "image", "estimate", "each", "hr", "grid", "position", "stack", "pixel", "interpolate", "generate", "one", "pixel", "value", "keep", "track", "stack", "pixel", "use", "initialize", "each", "hr", "pixel", "we", "introduce", "datum", "structure", "each", "integer", "position", "fuse", "hr", "grid", "store", "information", "about", "pixel?s", "source", "LR", "camera", "-lrb-", "camera", "index", "pixel", "coordinate", "color", "channel", "-rrb-", "since", "we", "determine", "geometric", "registration", "relate", "all", "camera", "array", "reference", "camera", "information", "contain", "also", "capture", "array", "camera", "geometry", "fact", "give", "reference", "camera", "we", "can", "easily", "map", "geometric", "information", "between", "camera", "array", "regardless", "color", "channel", "use", "stack", "calibrate", "baseline", "information", "thus", "essentially", "store", "richer", "geometric", "information", "than", "what", "commonly", "use", "one", "key", "contribution", "we", "sr", "process", "lie", "how", "we", "exploit", "estimate", "new", "likelihood", "gradient", "allow", "we", "perform", "superresolution", "under", "unconstrained", "imaging", "environment", "let", "we", "first", "concentrate", "gradient", "likelihood", "term", "i.e.", "first", "term", "equation", "compute", "gradient", "forward", "transformation", "model", "-lrb-", "equation", "-rrb-", "first", "apply", "hr", "estimate", "obtain", "simulated", "lr", "image", "dhw", "we", "form", "likelihood", "gradient", "lr", "domain", "dhw", "LR", "gradient", "back-projected", "current", "hr", "estimate", "use", "corresponding", "per-camera", "backward", "transformation", "matrix", "-lrb-", "dhw", "-rrb-", "process", "essentially", "correct", "hr", "estimate", "iteratively", "make", "agree", "observe", "lr", "image", "under", "determine", "imaging", "model", "-lsb-", "Irani", "Peleg", "1991", "-rsb-", "we", "approach", "we", "extend", "popular", "likelihood", "estimation", "mechanism", "exploit", "rich", "information", "capture", "well", "allow", "inaccuracy", "estimation", "we", "do", "so", "through", "novel", "uncertainty", "processing", "both", "hr", "lr", "domain", "we", "small", "baseline", "only", "transformation", "image", "scene", "feature", "undergo", "different", "camera", "array", "translation", "along", "epipolar", "line", "-lrb-", "perspective", "foreshortening", "-rrb-", "uncertainty", "processing", "hr", "grid", "mention", "earlier", "information", "capture", "each", "hr", "pixel", "location", "conjunction", "camera", "calibration", "information", "-lrb-", "baseline", "lens", "distortion", "-rrb-", "allow", "we", "relate", "each", "hr", "pixel", "LR", "image", "exploit", "rich", "set", "information", "we", "can", "estimate", "warp", "matrix", "-lrb-", "-rrb-", "allow", "we", "project", "hr", "estimate", "onto", "viewpoint", "all", "LR", "camera", "-lrb-", "see", "left", "middle", "column", "Figure", "10", "-rrb-", "note", "warp", "estimate", "-lrb-", "-rrb-", "local", "-lrb-", "per", "pixel", "-rrb-", "apply", "hr", "grid", "location", "use", "locally", "relate", "all", "LR", "camera", "even", "when", "some", "may", "contribute", "directly", "estimate", "initial", "fuse", "pixel", "allow", "we", "correct", "registration", "inaccuracy", "may", "result", "incorrect", "image", "fusion", "additionally", "localized", "warping", "-lrb-", "gradient", "estimation", "-rrb-", "make", "we", "likelihood", "more", "robust", "handle", "practical", "case", "where", "depth", "discontinuity", "may", "exist", "scene", "instead", "we", "restrict", "project", "reference", "hr", "image", "give", "pixel", "position", "-lcb-", "-rcb-", "only", "those", "LR", "camera", "contain", "-lrb-", "-rrb-", "which", "provide", "more", "localized", "fusion", "information", "additionally", "we", "warp", "neighbor", "pixel", "position", "-lcb-", "-rcb-", "camera", "map", "-lrb-", "-rrb-", "use", "respective", "stack", "-lrb-", "camera", "baseline", "optical", "distortion", "information", "-rrb-", "form", "patch", "large", "enough", "blur", "decimate", "thus", "each", "hr", "pixel", "still", "contribute", "gradient", "formation", "from", "multiple", "camera", "some", "stack", "hr", "grid", "position", "may", "still", "empty", "after", "initial", "sr", "fusion", "we", "handle", "sparsely", "populated", "distribution", "visit", "LR", "camera", "-lrb-", "pixel", "position", "inside", "those", "image", "-rrb-", "base", "neighbor", "grid", "position", "stack", "we", "restrict", "ourselves", "relatively", "small", "-lrb-", "-rrb-", "spatial", "neighborhood", "around", "each", "empty", "-lcb-", "-rcb-", "hr", "grid", "uncertainty", "processing", "LR", "grid", "another", "contribution", "we", "sr", "mechanism", "lie", "filter", "likelihood", "gradient", "lr", "domain", "literature", "filter", "against", "imaging", "noise", "outlier", "sometimes", "apply", "likelihood", "term", "improve", "estimation", "quality", "contrast", "we", "work", "deblurr", "usually", "perform", "separately", "decouple", "from", "rest", "SR.", "importantly", "we", "motivation", "development", "section", "only", "address", "impact", "imaging", "noise", "outlier", "likelihood", "gradient", "also", "guard", "against", "spatial", "registration", "error", "reflect", "stack", "implicitly", "we", "work", "instead", "computing", "back-projecting", "current", "hr", "estimate", "gradient", "respect", "only", "LR", "pixel", "position", "-lrb-", "-lcb-", "-rcb-", "Figure", "10", "-rrb-", "indicate", "shift", "matrix", "we", "also", "visit", "neighbor", "LR", "pixel", "around", "those", "position", "since", "most", "case", "we", "parallax", "error", "lie", "within", "support", "single", "LR", "pixel", "we", "visit", "only", "small", "lr", "neighborhood", "refer", "again", "Figure", "10", "each", "decimation", "position", "-lcb-", "-rcb-", "warped", "blur", "hr", "image", "we", "obtain", "lr", "gradient", "-lrb-", "-rrb-", "each", "position", "-lcb-", "-rcb-", "within", "window", "-lrb-", "-rrb-", "center", "-lcb-", "-rcb-", "weight", "-lrb-", "-rrb-", "compute", "from", "set", "pixel", "window", "-lrb-", "-rrb-", "around", "position", "-lcb-", "-rcb-", "correspond", "-lcb-", "-rcb-", "block", "lr", "-lrb-", "-rrb-", "center", "each", "-lcb-", "-rcb-", "where", "noise", "standard", "deviation", "each", "decimation", "position", "-lcb-", "-rcb-", "warped", "blur", "hr", "image", "sum", "weight", "sum", "-lrb-", "-rrb-", "n?u", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "also", "record", "alongside", "filter", "gradient", "from", "equation", "all", "gradient", "value", "-lrb-", "-rrb-", "corresponding", "sum", "weight", "sum", "-lrb-", "-rrb-", "backpropagate", "onto", "grid", "reference", "hr", "image", "upsampling", "blur", "inverse-shifting", "they", "normalization", "multiple", "gradient", "thus", "form", "-lcb-", "-rcb-", "use", "sum", "weight", "sum", "-lrb-", "-rrb-", "obtain", "final", "likelihood", "gradient", "-lcb-", "-rcb-", "use", "neighborhood", "information", "filter", "likelihood", "gradient", "make", "we", "map-based", "sr", "process", "stable", "even", "-lrb-", "up", "lr", "pixel", "-rrb-", "inaccuracy", "registration", "where", "-lrb-", "-rrb-", "edge-preserving", "local", "nonlinear", "operator", "-lrb-", "bilateral", "filter", "-rrb-", "norm", "gradient", "prior", "compute", "add", "likelihood", "gradient", "equation", "apply", "current", "hr", "estimate", "locally-adaptive", "prior", "enforce", "preservation", "edge", "under", "operator", "-lrb-", "-rrb-", "subtle", "yet", "important", "difference", "bilateral", "prior", "introduce", "-lsb-", "Farsiu", "et", "al.", "2006", "-rsb-", "we", "case", "prior", "gradient", "compute", "between", "current", "hr", "estimate", "its", "bilaterally-filtered", "version", "other", "word", "we", "restore", "image", "need", "feature", "property", "bilaterally-filtered", "hr", "image", "datum", "fidelity", "-lrb-", "likelihood", "-rrb-", "term", "itself", "provide", "good", "estimate", "hr", "image", "high", "snr", "region", "statistical", "prior", "help", "estimate", "low", "snr", "region", "better", "ensure", "solution", "stability", "we", "could", "easily", "incorporate", "estimate", "multiple", "channel", "framework", "describe", "inject", "cross-channel", "image", "prior", "however", "due", "computational", "constraint", "we", "first", "perform", "channel", "difference-based", "fusion", "red", "blue", "channel", "respect", "restore", "green", "channel", "follow", "transfer", "weighted", "map-gradient", "from", "green", "channel", "fuse", "red", "blue", "channel", "obtain", "high", "resolution", "RGB", "image", "we", "develop", "camera", "array", "4x4", "sensor", "array", "where", "each", "sensor", "support", "1000", "750", "pixel", "4:3", "aspect", "ratio", "synthesize", "high", "resolution", "image", "output", "we", "processing", "megapixel", "image", "lens", "array", "consist", "type", "lens", "red", "green", "bluewhere", "each", "have", "f-number", "3.1", "diagonal", "field", "view", "56", "focal", "length", "2mm", "overall", "camera", "module", "height", "less", "than", "3.5", "mm", "compare", "height", "anywhere", "between", "5.2", "mm", "6.0", "mm", "legacy", "camera", "module", "least", "count", "-lrb-", "highest", "degree", "accuracy", "measurement", "-rrb-", "disparity", "between", "two", "camera", "function", "system", "noise", "blur", "practice", "we", "have", "find", "lr", "pixel", "put", "limit", "depth", "map", "accuracy", "how", "depth", "resolution", "change", "distance", "since", "disparity", "function", "depth", "we", "let", "u?v", "-lrb-", "-rrb-", "represent", "disparity", "between", "two", "camera", "distance", "u?v", "-lrb-", "-rrb-", "represent", "disparity", "distance", "-lrb-", "where", "-rrb-", "u?v", "-lrb-", "-rrb-", "u?v", "-lrb-", "-rrb-", "constraint", "can", "use", "determine", "how", "estimate", "value", "depth", "change", "increase", "object", "distance", "Figure", "11", "show", "variation", "accuracy", "depth", "estimate", "object", "distance", "obviously", "key", "aspect", "how", "successful", "approach", "be", "reasonable", "alternative", "conventional", "camera", "design", "lie", "how", "effective", "superresolution", "recover", "resolution", "encode", "aliased", "signal", "LR", "image", "array", "under", "all", "lighting", "scene", "condition", "Figure", "show", "we", "system", "behavior", "capture", "high", "frequency", "image", "ability", "camera", "array", "capture", "high", "frequency", "signal", "fundamentally", "define", "lens", "f-number", "-lrb-", "mtf", "-rrb-", "optical", "format", "lens", "pixel", "size", "-lrb-", "pixel", "mtf", "-rrb-", "superresolution", "factor", "Figure", "12", "show", "how", "resolution", "final", "super-resolved", "image", "change", "vary", "value", "parameter", "extrapolate", "from", "current", "reference", "design", "-lrb-", "highlight", "red", "-rrb-", "use", "f3", ".1", "lens", "1000", "750", "1.75", "front-side", "illuminate", "-lrb-", "FSI", "-rrb-", "pixel", "per", "camera", "use", "information", "present", "Figure", "table", "can", "use", "determine", "how", "much", "resolution", "be", "trade", "off", "array", "camera", "comparison", "legacy", "camera", "gain", "depth", "information", "we", "consider", "sensor", "size", "-lrb-", "3264", "2448", "-rrb-", "pixel", "size", "-lrb-", "1.4", "-rrb-", "f-number", "-lrb-", "f2", ".4", "-rrb-", "iphone5", "corresponding", "array", "camera", "similar", "specification", "would", "array", "each", "sub-array", "have", "800", "600", "1.4", "pixel", "f2", ".4", "lens", "achievable", "resolution", "-lrb-", "mtf20", "-rrb-", "comparable", "system", "would", "950", "lw/ph", "-lrb-", "highlight", "blue", "-rrb-", "comparison", "achievable", "resolution", "-lrb-", "mtf20", "-rrb-", "iphone5", "2000lw/ph", "-lrb-", "figure", "-rrb-", "tell", "we", "we", "trade", "resolution", "factor", "depth", "information", "Figure", "13", "we", "also", "illustrate", "effect", "use", "some", "new", "development", "discuss", "section", "4.2", "-lrb-", "namely", "likelihood", "gradient", "formation", "filter", "uncertainty", "processing", "-rrb-", "quality", "restoration", "akin", "perform", "more", "classical", "likelihood", "computation", "basically", "translate", "form", "single", "likelihood", "-lrb-", "filter", "-rrb-", "fusion-populated", "hr", "grid", "position", "where", "we", "use", "median", "filter", "any", "pixel", "stack", "form", "interpolate", "any", "hole", "hr", "grid", "degrade", "image", "quality", "may", "even", "interfere", "correct", "reconstruction", "frequency", "error", "operation", "precede", "map", "processing", "play", "larger", "role", "Figure", "15", "show", "comparison", "noise", "performance", "camera", "array", "against", "iphone5", "100lux", "illumination", "we", "offset", "exposure", "time", "camera", "array", "compensate", "faster", "lens", "iphone5", "figure", "show", "zoom", "out", "region", "relative", "noise", "difference", "between", "two", "camera", "both", "visually", "use", "snr", "metric", "-lrb-", "both", "measure", "ysnr", "about", "39db", "-rrb-", "we", "see", "we", "camera", "array", "perform", "close", "noise", "performance", "iphone5", "must", "point", "out", "however", "smaller", "exposure", "time", "also", "lead", "more", "motion", "blur", "final", "analysis", "adequate", "solution", "one", "can", "only", "address", "corresponding", "faster", "lens", "PiCam", "we", "have", "also", "compare", "we", "camera", "against", "iphone5", "both", "indoors", "outdoors", "Figure", "14", "top", "two", "row", "show", "couple", "example", "indoor", "outdoor", "shot", "last", "row", "show", "example", "post", "capture", "refocus", "application", "virtue", "have", "relatively", "small", "optical", "format", "-lrb-", "-rrb-", "shorter", "focal", "length", "compare", "legacy", "8mp", "camera", "mobile", "device", "hyperfocal", "distance", "we", "imaging", "system", "40cm", "thus", "addition", "all", "object", "from", "20cm", "be", "capture", "focus", "computed", "depth", "map", "allow", "one", "achieve", "degree", "post-capture", "refocus", "provide", "creative", "flexibility", "photographer", "iphone5", "have", "larger", "aperture", "correspondingly", "large", "hyperfocal", "distance", "-lrb-", "100cm", "-rrb-", "top", "row", "image", "iphone5", "focus", "near", "object", "-lrb-", "50cm", "-rrb-", "have", "correspondingly", "short", "depth", "field", "so", "background", "picture", "have", "defocus", "blur", "contrast", "PiCam", "have", "everything", "focus", "background", "appear", "sharper", "than", "do", "iphone5", "second", "point", "consider", "degree", "refocus", "ability", "clearly", "dependent", "resolution", "depth", "map", "thus", "object", "give", "distance", "from", "camera", "may", "refocus", "against", "its", "background", "long", "background", "object", "distance", "larger", "than", "depth", "resolution", "distance", "-lrb-", "see", "Figure", "11", "-rrb-", "finally", "picam", "3d", "capable", "capture", "device", "give", "its", "ability", "compute", "depth", "textured", "surface", "should", "enable", "handheld", "digitization", "device", "capable", "create", "point", "cloud", "lend", "themselves", "subsequent", "meshing", "we", "capture", "object", "-lrb-", "chinese", "dragon", "-rrb-", "show", "Figure", "16", "-lrb-", "-rrb-", "tabletop", "distance", "50cm", "convert", "depth", "map", "from", "object", "point", "cloud", "filter", "depth", "map", "reveal", "just", "high", "confidence", "depth", "value", "total", "87000", "point", "be", "render", "texture", "result", "figure", "show", "Figure", "16", "-lrb-", "-rrb-", "addition", "above", "application", "picam", "architecture", "also", "enable", "view", "synthesis", "from", "any", "position", "array", "also", "front", "behind", "array", "synthesis", "procedure", "consist", "three", "step", "calculate", "displacement", "each", "image", "array", "virtual", "viewpoint", "compute", "depth", "map", "virtual", "viewpoint", "form", "color", "image", "viewpoint", "method", "enable", "one", "showcase", "motion", "parallax", "generate", "stereo", "leave", "right", "view", "be", "able", "generate", "different", "viewpoint", "when", "combine", "real-time", "gyroscope", "reading", "mobile", "device", "provide", "viewer", "sense", "scene", "depth", "rich", "interactive", "experience", "we", "show", "example", "accompany", "video", "we", "approach", "computational", "array", "camera", "have", "show", "robustness", "generate", "high", "resolution", "color", "image", "under", "vary", "imaging", "condition", "addition", "picam", "provide", "significant", "difference", "over", "legacy", "camera", "relax", "z-height", "constraint", "considerably", "resolution", "we", "system", "function", "optical", "format", "f-number", "lens", "pixel", "size", "number", "camera", "array", "depend", "application", "appropriate", "choice", "imaging", "system", "processing", "parameter", "allow", "one", "scale", "resolution", "system", "without", "hit", "z-height", "constraint", "we", "system", "also", "fault", "tolerant", "extent", "since", "failure", "camera", "array", "lead", "graceful", "degradation", "system", "performance", "same", "time", "area", "improvement", "both", "term", "image", "quality", "processing", "speed", "artifact", "from", "incorrect", "depth", "estimate", "along", "contour", "object", "close", "camera", "still", "possible", "depend", "scene", "content", "background", "texture", "correction", "artifact", "while", "focus", "paper", "remain", "important", "area", "continue", "research", "nevertheless", "we", "believe", "continued", "work", "both", "area", "architecture", "have", "potential", "remove", "some", "limitation", "camera", "design", "small", "form", "factor", "device", "enable", "new", "imaging", "application", "privilege", "part", "team", "help", "create", "PiCam", "computational", "camera", "array", "camera", "basis", "Silicon", "Valley", "startup", "Pelican", "Imaging", "Corporation", "found", "author", "Kartik", "Venkataraman", "assistance", "from", "colleague", "Dan", "Lelescu", "Jacques", "Duparr", "Robert", "Mullis", "Bedabrata", "pain", "sensor", "lens", "array", "base", "architectural", "specification", "Pelican", "Imaging", "Corporation", "manufacture", "we", "industry", "partner", "Aptina", "Imaging", "-lsb-", "Cho", "et", "al.", "2013", "-rsb-", "Heptagon", "other", "we", "also", "extend", "thanks", "those", "help", "preparation", "paper", "include", "Herve", "Hornung", "Lisa", "Garvey", "Stephen", "Williams" ],
  "content" : "Our contributions to the post-processing of images from camera arrays include a cost function for parallax detection that integrates across multiple color channels, and a regularized image restoration (superresolution) process that takes into account all the system degradations and adapts to a range of practical imaging conditions. The PiCam camera architecture specifications are presented in Section 3. The design of our sensor array is presented in Section 3.2. To eliminate the color crosstalk that is endemic to all sensors with Bayer filters, each camera in the array is sensitive to a single spectral color and is optically isolated from its neighbors. Eliminating color crosstalk improves color fidelity and reduces the need for aggressive color correction in all lighting conditions. Our lens array design and module integration are presented in Section 3.1. Finally, our approach is unique in that it provides an efficient software imaging solution capable of being implemented on a mobile processor. A key contribution of our approach is also that we compute the scene depth for each pixel, making this not only an imaging system but also potentially a 3D acquisition system. Our approach represents a new paradigm in mobile imaging by providing a depth modality in a small form factor design. The system follows a symbiotic design approach guided both by the needs of image post-processing and the trade-offs in hardware design, ultimately delivering an ultra-thin imaging system that is well-balanced in terms of overall resolving power, signal-to-noise ratio (SNR), dynamic range, spectral fidelity, and depth accuracy. These imagebased rendering techniques provided a way to bridge the gap between synthetically generated scenes and photorealism. This allowed one to record video scenes, which appear frozen in time, but with the ability to view this frozen scene dynamically from a moving viewpoint. The system enabled computing range images using a multi-camera stereo process as well as an intensity image. The work in Wilburn et al. [2005] designed a more general purpose system capable of being extended to large arrays of cameras with the ability to synchronize between the cameras as well as carefully assign their spatial positions. This configuration allowed for significant creative control in capturing very wide fields of view and from which interesting scene collages could be synthesized. These allow one to capture sufficient context that it becomes easy to perceive the overall scene structure. It is also important to acknowledge the works in [Ng et al. 2005; Georgiev et al. 2011] on the plenoptic camera, given that they capture light fields. In contrast, our approach extends the depth of field by shrinking the aperture and the focal length of each lens in the array. In addition, since our approach has all lenses with overlapping fields of view, it offers us the potential for recovering the resolution encoded in the downsampled and aliased images of the array. Post-processing software used an iterative back-projection (IBP) technique to reproduce an image of higher resolution than that captured by any single aperture on the array. As is well known, the ML-solution in noise is unstable, and indeed noise amplification was noted. In addition, the demonstrated resolutions were low compared to today?s expectations for a mobile imaging system. Unlike our system, which is capable of meeting industry expectations for higher resolution (such as 5MP or 8MP), all of these approaches produce low output resolutions and do not offer the full complement of use cases, such as high-resolution stills and 720P/1080P video that are available in the mobile cameras today. We use the term legacy camera to refer to a conventional single aperture design such as cameras in currently available mobile phones. In such cameras, the sensor has a 2x2 Bayer filter pattern over the entire pixel array. Overlaid on top of the sensor is a 4or 5-element lens. A voicecoil motor is included to provide autofocus. In contrast, Figure 3 shows that the PiCam?s focal plane sensor is divided into a number of smaller focal planes, which in turn are overlaid with a lens array. There are several significant changes that are immediately visible: Optical Stack Height: Firstly, the height of the optical stack is reduced by the smaller-aperture lens and a corresponding reduction in the number of lens elements needed. The benefits and problems of scaling a lens (changing its focal length at constant F-number and field of view) on its information capacity are fully explored by [Lohmann 1989; Fischer et al. 2008]. It is shown there that the magnitude of wavefront aberrations scales with the lens size up to the power of 4. In this PiCam instantiation, the filters are on the optical stack. As we will see in Section 3.1, the reduction in spectral bandwidth of each channel enables one to design higher performance lenses. The choice of the color filter pattern is largely decided by the spectral coverage around a reference camera (which defines our rendering viewpoint) in the different camera use modes still capture and video. In video mode, to save on power consumption, only 9 of the 16 cameras are active, while in still capture mode all 16 cameras are activated for the increased resolution requirement. The areas immediately around the foreground object are not visible to all the cameras in the array due to the fact that different cameras are capturing the scene from different viewpoints and the foreground object occludes parts of the background scene. It is therefore important that irrespective of the orientation of the occluded zone around the foreground object, the area is sampled by at least one camera of each spectral color. The chosen filter pattern (shown in Figure 2 (c)) has two overlapping 3 ? 3 sub-groups (the top left and bottom right), wherein the red and blue spectral channels are symmetrically disposed around a reference green camera in the center. Additionally, it also allows for redundancy in that any one of these two sub-groups could be used for generating video. The top right corner blue filter and the bottom left corner red filter complete the uniformity requirement in the 16 camera still image use mode. When the lens is focused at this distance, all objects at distances from half the hyperfocal distance out to infinity will be acceptably sharp. The resulting image for the PiCam, with its significantly shorter focal length, is an ?all in focus? image. Given the above differences, the final imaging performance of the camera array is a function of (a) the resolution of the optics, (b) the optical format of each camera in the array, (c) the number of cameras in the array, (d) the pixel pitch and sensor resolution, and (e) the superresolution factor. Sections 3.1, 3.2, and 3.3 below describe the key features of the lens and sensor array and how the trade-offs impact the final resolution and performance of PiCam. The primary challenge in lens design for array cameras is in preserving sufficient signal at high spatial frequencies to enable effective superresolution from individual low resolution images. As mentioned earlier, each lens in PiCam is designed only for a relatively narrow band of the overall visible spectral band of wavelengths. We performed an analytic study wherein we varied the full width at half maximum (FWHM) of the spectral channel bandwidth over which the lens was designed. Optimal diffraction limited lenses were designed for FWHM?s of 0nm, 30nm, 60nm, and 100nm width respectively. The resulting lenses showed a monotonic decrease in both on-axis and off-axis MTF with increasing FWHM of the spectral channel. A substantial drop of greater than 25% occurs when the FWHM increases from 60nm to 100nm, showing the potential efficiency in MTF achieved when designing lenses for narrow spectral bands. Thus for a given lens blur, correspondingly less achromatization efforts have to be taken, which in turn implies lower material combinations and, therefore, fewer lens elements. Our supplementary materials show the variation in MTF and blur for increasing FWHM. For a diffraction limited optical system, the diameter of the diffraction ring (as characterized by the lens PSF) to the first zero is a function of the wavelength of light (?) and F-number of the lens and defines the optical cut-off frequency. In such a system, the PSF is the square of the Fourier transform of the lens aperture and the MTF is the magnitude of the Fourier transform of this PSF. The monochromatic diffraction limited ? optical MTF is given by M T F optics = ? 2 [arccos(x) ? x 1 ? x 2 ], where x is the normalized spatial frequency defined as x = u , u is the absolute u c spatial frequency, and u c is the diffraction cutoff spatial frequency governed by the wavelength of light and the F-number of the lens as (? ? F ) ?1 . Figure 4 shows the diffraction limited optical MTF and its variation of F-number and optical format respectively. We choose an F3.1 optics design with an optical format equivalent to 1000 ? 750 pixels (that is, 1 inch) as a reference design to illustrate our performance 7 analysis throughout the paper. The figure shows this reference design as the solid blue curve in the graph. Varying the F-number (from F3.1 to F2.6 and F2.0), while holding the optical format at 1000 ? 750 pixels is shown by the two other solid curves in different shades of blue. Finally, we show another curve with an optical format of 800 ? 600 using a 1.4?m pixel and an F2.4 lens. These show a couple of different ways to achieve a higher optical cutoff and MTF. The first option of increasing the optical format increases the cost of XY-size of the camera array, but has the advantage that the optical design is less challenging. The second option of decreasing the F-number of the lens benefits a lower XY-footprint but it would come at the cost of increased lens design complexity. Nonetheless, both of these options are tractable and gives the camera designer a choice based on the various mitigating design factors. As a comparison, we show the MTF of the diffraction limited F2.4 lens for the iPhone5 wherein the larger optical format of the iPhone5 provides a larger space-bandwidth product and permits a much larger amount of information to be transmitted to the focal plane. The gap between the achievable resolution in the large aperture conventional cameras (such as in an iPhone5) and that in a PiCam camera array is what needs to be addressed through a combination of carefully chosen parameters for the optics, sensor, and superresolution software. Much of the camera array functionality is achieved through the use of a novel image sensor architecture. Under certain simplifying assumptions, the system MTF is essentially a product of the optical MTF and geometric pixel MTF, wherein each of the optics and sensor stages lowpass filter the input signal (scene) thereby degrading the response at higher frequencies [Holst 1998]. This requires that it is not just the optics MTF that matters but that the overall system MTF has to be maximized and requires consideration of the pixel architecture and size. Thus, for an image sensor with 1000 ? 750 pixels using a 1.75? pixel, the Nyquist frequency is 286 line-pairs per mm or 750 line-widths per picture height (a line-pair is equivalent to two line-widths). For a legacy camera, very little contrast is desired at spatial frequencies larger than the sensor?s N y, in order to avoid aliasing in the final output image. This is where our camera array differs from legacy camera. Here, it is desirable to have content above N y such that when multiple copies of an aliased signal are present, as is the case in a camera array, it is possible through superresolution to use the information that is inherently present in the aliasing to reconstruct a higher resolution signal. In order to attain a certain minimum MTF in the superresolved high resolution (HR) image, we need to provide sufficient contrast in the aliased low resolution (LR) input images from the camera array, which in turn implies a certain threshold for the lens MTF. These limits and thresholds become evident in the graphs that exemplify the optics and sensor design in Figures 4, 6, and 7. While the pixel size influences the sampling rate and, hence, the aliasing, the structure of the pixel stack itself influences the amount of pixel blur, pixel crosstalk, and the pixel MTF. It is desirable to see what efficiencies can be enabled by exploiting the camera array architecture that is developed here. The top oxide layer separates the microlens layer from The nitride passivation layer and bottom oxide layer provides support and isolation for the metal interconnects that are used to connect the various parts of the sensor. Finally, at the very bottom is the silicon substrate with the active area (photodiode) which collects the photons incident on it. While it is convenient to think of a pixel as a point sample, in reality it is an area sampling construct. The microlens spatially samples light over a given area, focusing it on the active photodiode, and in essence averages the light over the area covered by the microlens. Quite simply, larger the width of the pixel (i.e., spatial box filter), the larger the pixel blur and, hence, a smaller cut-off frequency in frequency domain. In actual implementation, pixel crosstalk reduces the pixel MTF from that defined by its ideal sinc filter. In Figure 5(a) , the angular incoming light is focused onto the pixel photodiode at the bottom of the stack. However, a certain percentage of the light is lost through crosstalk to the neighboring pixel along the common shared boundary of the neighboring pixel stack. It is therefore desirable to reduce the height of the pixel stack to reduce the amount of light lost due to crosstalk. Since the PiCam camera array employs monochromatic cameras wherein each camera in the array is sensitive to just one color of light, it is possible to move the color filter out of the pixel stack and deposit it on the optical stack or the sensor cover-glass instead. The advantage is that the size of the pixel stack is reduced by the removal of the color filter from the pixel stack. From Figure 5(a) and (b) it can be seen that this is almost equivalent to a 30% reduction, thereby reducing crosstalk by a significant amount. In addition, bringing the photodiode closer to the microlens enables the pixel to accept a wider cone of incoming light, thus providing increased design degrees of freedom to the optics design. This reduces light sensitivity at the expense of improving the pixel MTF, but given the significant improvements in sensitivity due to the removal of the color filter we felt that this was a reasonable trade-off. The reference design was done on a front-side illuminated pixel, but similar arguments hold for the next generation backside-illuminated pixels as well. Figure 6 shows the overall system MTF when both the MTF of the diffraction limited optics and the pixel MTF are taken into account. For each of the configurations listed in Figure 4 , we show the corresponding system MTF and it can be seen that the blur induced by the pixel area sampling reduces the system MTF from that of the corresponding optics MTF alone. The sensor array is designed on a single substrate with multiple sub-arrays such that for each image projected through the apertures of the lens array, it provides temporally coherent captures of these sub-images where the relative temporal offset/phase are controllable. This enables features such as single shot HDR where different subimages could have different exposure times, and high-speed video captures through sequential staggering of the capture times of the individual cameras. Given there is a plurality of focal planes simultaneously generating pixel data, it is also necessary to multiplex this output data onto the common output through an industry standard camera serial interface (MIPI CSI2). We developed a container format that enables the interleaving of pixel data from each camera (so as to minimize the on-chip storage requirements) as well as provide the receiver with sufficient information to interpret the received data (for example, which cameras are enabled, relative readout position of each camera, the multiplexing scheme used, the gain and integration time). Two aspects differentiate the PiCam from a conventional camera in terms of system resolution. The first, explained in Section 3.1, was the requirement of sufficient optical MTF above the Nyquist sampling limit of the sensor for superresolution. The second aspect is the superresolution factor itself how much superresolution can  we expect from the signal processing? Perhaps the most relevant study in this respect was that of [Baker and Kanade 2002], who addressed the limits to the resolution improvement one could achieve using linear SR reconstruction. Here, we investigate the effects of aliasing and analyze the superresolution factor while taking into account the amount of aliasing present in signal. Figure 7(a) shows the MTF curves of the low resolution (LR) image using a solid blue line and the corresponding high resolution (HR) superresolved image, using our superresolution algorithm (Section 4.2), using a solid green line. These represent the MTFs for our reference design using an F3.1 optics and a 1000 ? 750, 1.75? sensor. The Nyquist frequency for this is shown by the red line at 750lw/ph. Signals in the LR image above the Nyquist are aliased and appear as signals of lower frequency than the Nyquist and are represented by the dashed blue curve reflected about the red Nyquist line at 750lw/ph. Thus, at the Nyquist frequency all of the LR signal is aliased, and at frequencies below the Nyquist the dashed line represents the percentage of aliased signal present in the LR signal. To determine the maximum resolvable frequency in the LR domain, we choose that frequency where the amount of aliased signal is no more than 20% of the total signal present. This coincides with the frequency of 480lw/ph indicated by the vertical solid blue line in the graph. The lower part of the solid blue vertical line is shaded red to indicate the amount of aliased signal present. This correlates very well visually with the LR image of the ISO12233 chart, which shows the different frequencies imaged by a single camera in the array. We choose a similar threshold of 20% contrast for the MTF of the superresolved HR image to determine the maximum resolvable output HR frequency. The superresolved HR image corresponding to the HR MTF curve is shown in Figure 7(b) and indicates the maximum resolvable frequency to be 1140lw/ph. The ratio of maximum resolvable HR frequency to that of the maximum resolvable input-LR frequency (as determined by the amount of aliasing present in the signal) is taken to be the true superresolution factor. Figure 8 shows the software processing pipeline that is used to synthesize an HR image from the multiple LR images that are generated from the camera array. For purposes of illustration, we choose an aperture in the middle of the array (typically the green camera closest to the array center) as the reference viewpoint, and generate the HR image as seen by that viewpoint. In the description that follows, the green camera corresponding to this viewpoint is denoted as the reference camera. The sections below outline the key aspects of the above stages and highlight how our approach to processing images from the PiCam differs from prior art. Individual cameras in the array are subject to photometric and geometric variations that arise during the manufacturing and module assembly process. The photometric normalization step corrects for transmission differences in individual lenses by capturing a scene of uniform intensity (i.e., a ?flat field?). For each camera in the array, a spatially-varying correction factor is then computed, which reduces variations in photometric responses. Additionally, a pre-calibrated geometric normalization surface G characterizes offsets in geometry between cameras due to differences in lens distortion. The residual distortion vectors in G are later used during parallax and superresolution to account for lens distortion variations.The output of this stage is a photonormalized set of LR images that are input to the parallax detection stage of the pipeline. Parallax Detection: The parallax processing determines pixel correspondences for non-reference images with respect to the reference camera. Our approach builds upon past work done in the area of stereo correspondence [Szeliski 2010]. Because the cameras in the array are coplanar, in a non-reference camera, scene objects will simply shift along an epipolar line in a direction determined by the baseline separation between the cameras. A discrete number of depths D are searched that are distributed in equal increments up to a desired maximum disparity. The number of depths to search is large enough to enable detection to sub-pixel levels of accuracy. This is essential for the fusion and superresolution steps in later stages of the pipeline. To make this determination, at a candidate depth d, the pixels in non-reference cameras that correspond to (i, j) are used to calculate a matching cost. We use the sum-of-absolute-differences (SAD) metric where lower costs indicate higher correspondence between pixels across multiple viewpoints and higher costs indicate that the depth is likely a poor match. The SAD matching cost metric is applied only between pixels from cameras with similar color filters. For the green cameras in the array, a list C G contains cameras that are to be compared against the reference green camera. The list C Rcomb enumerates combinations of red cameras to compare to other red cameras, and a list C Bcomb exists for the same purpose in blue. Combinations of cameras in these lists are compared using the SAD metric and the results are added into the combined matching score below. The even distribution of red and blue cameras around the reference green camera represents another key point of differentiation from prior art such as the TOMBO approach where no special consideration was given to the placement of the red, green, and blue filters [Tanida et al. 2003]. Our approach ensures all pixels captured by the reference green camera are also seen by at least one red and one blue camera. Given these lists, a matching score E is calculated for each candidate depth d for a given pixel (i, j) in the reference camera, using the following equation that sums contributions from red, green, and blue pixel correspondences across multiple cameras. The coordinates (i p , j p ) represent the pixel coordinates in camera p corresponding to the reference pixel location (i, j) at the depth d, and y p (i p , j p ) represents the pixel value at that location in camera p. Similarly, y ref (i, j) represents the intensity of the pixel (i, j) in the reference camera. The camera combinations {r, s} represent individual pairs of cameras to be compared in the latter two summation terms of matching costs. PiCam has an intrinsic advantage over stereo cameras in that with the increase in the number of cameras comes a robustness to  noise in depth searches. Additionally, combining matching scores from multiple orthogonal baselines, as explored in Okutomi and Kanade [1993] previously, can resolve ambiguities in cases that are traditionally problematic for binocular stereo, such as detecting the depth of a feature that is oriented parallel to the baseline. If matching scores from camera pairs with both horizontal and vertical baselines are aggregated into the same matching score, such situations can be resolved because the cameras with vertical baselines will be able to detect the depth of the horizontal features, even if the cameras with horizontal baselines cannot. The depth map provided to super-resolution is the same size as a single, low-resolution array image, and reflects the viewpoint of the reference camera. The depth map contains, for each pixel, the depth value which yields the lowest value of E when aggregating all cameras included in the lists C g , C Rcomb , and C Bcomb . In the next super-resolution step, this depth (parallax) information will be used along with the known pixel correspondences to fuse and restore a higher resolution, full-color version of the reference camera image. Once the LR images are registered to the reference camera, we rely on a superresolution (SR) process to obtain a high-resolution image from the captured low-resolution images. The progression of this process is illustrated in Figure 9 . By design, the PiCam system allows aliased frequencies to be captured in each LR image (see Section 3.3) to serve this process. Our SR approach is designed in a Bayesian framework, and takes full advantage of our knowledge of system parameters. This information is used in the form of an imaging prior, along with the statistical image prior needed for image restoration. The imaging prior includes the sensor analog gain, exposure time, sensor-noise characteristics, optics PSFs and vignetting, geometric distortion, and knowledge of array camera geometry (i.e., baselines). In what follows, we present our superresolution restoration, where the main contributions include: a) an original approach for determining the likelihood-gradient for restoration; b) a new uncertainty processing approach to mitigate the errors in the geometric registration of array images; and c) a statistical prior that regularizes the solution and guides the restoration towards a noise-suppressed HR image. Modeling the Image Formation process: The high resolution image we need to restore is a 2D projection of the 3D scene onto the image plane of a virtual, higher resolution reference camera. Let x be the desired, lexicographically ordered (vectorized), HR image captured on a high resolution grid corresponding to a selected reference LR camera. The images {y p } feature geometric displacements with respect to the reference image, which include optical distortions (scene independent), and parallax disparity (scene dependent). Mathematically, we can define the forward imaging model generating each observed LR image y p as where the shift matrix W p captures the total (geometric distortion and parallax shift) displacement of the image seen from the p th camera, with respect to the reference camera. However, the spatial variation is usually smooth enough to allow us to apply the model in Equation 2 in smaller areas. by the sensor of the high resolution image, while n p is the imaging noise assumed to be i.i.d. Note that, in general, W p and H are not commutative even though H is block-Toeplitz (and thus can be made block-circulant) when spatially localized, as the matrix W p is not necessarily block-Toeplitz. Specifically targeting SR of light field images captured by plenoptic cameras, researchers have recently proposed SR algorithms that exploit specific image capture methods [Wanner and Goldluecke 2012; Georgiev et al. 2011]. To exploit our knowledge of the system characteristics and parameters, we employ a Bayesian restoration approach. More precisely, we formulate a maximum-a-posteriori (MAP) approach, that is derived specifically for our system. To restore the high resolution image x, we iteratively minimize a convex objective function that consists of a likelihood ?(.) and a prior ?(.) term . Here ? and ? denote norms, and F(.) is a statistical prior. The gradient of J(x) with respect to x is computed as Here is the gradient operator. The initial estimate of x is obtained using a fusion SR process. Pixels initially fused at non-integral HR grid positions are interpolated using any interpolation technique (for example, nearest neighbor), at integer HR grid positions. Thus, the outcome of the initial fusion is a stack of a variable number of pixels (including no pixels) at each integer HR grid position. To generate a proper initial HR image estimate, at each HR grid position stacks of pixels are interpolated to generate one pixel value. To keep track of the stack of pixels that are used to initialize each HR pixel, we introduce a data structure S at each integer position  on the fused HR grid to store information about the pixel?s sources in the LR cameras (camera index and pixel coordinates, color channel). Since we determined the geometric registration relating all cameras in the array and the reference camera, the information contained in S also captures the array camera geometry. In fact, for a given reference camera, we can easily map geometric information between cameras in the array, regardless of color channel, using the stacks S and the calibrated baseline information. Thus, S essentially stores a richer geometric information than what is commonly used. One key contribution of our SR process lies in how we exploit this in estimating a new likelihood gradient that allows us to perform superresolution under unconstrained imaging environments. Let us first concentrate on the gradient of the likelihood term, i.e., first term in Equation 4. To compute this gradient a forward transformation model (Equation 2) is first applied to the HR estimate to obtain the simulated LR images y p = DHW p x. We then form the likelihood gradients in the LR domain as g p = DHW p x ? y p . These LR gradients are then back-projected into the current HR estimate x, using the corresponding per-camera backward transformation matrix (DHW p ) T . This process essentially corrects the HR estimate iteratively to make it agree with the observed LR images under a determined imaging model [Irani and Peleg 1991]. In our approach, we extend this popular likelihood estimation mechanism to exploit the rich information captured in S, as well as to allow for inaccuracies in the estimation of W p . We do so through a novel uncertainty processing in both the HR and LR domains. For our small baselines, the only transformation that the images of scene features will undergo in different cameras of the array is a translation along epipolar lines (no perspective foreshortening). Uncertainty processing on the HR grid: As mentioned earlier, information captured in S at each HR pixel location, in conjunction with camera calibration information (baselines and lens distortion), allows us to relate each HR pixel with the LR images. By exploiting this rich set of information, we can estimate the warp matrices (W p ) that allow us to project the HR estimate onto the viewpoint of all the LR cameras (see left and middle columns of Figure 10 ). Note that the warp estimates W p (.) are local (per pixel) and are applied in the HR grid location. Using S to locally relate all LR cameras even when some may not contribute directly to estimating the initial fused pixel allows us to correct for registration inaccuracies that may result in incorrect image fusion. Additionally, such localized warping (and gradient estimation) makes our likelihood more robust to handling practical cases where depth discontinuities may exist in the scene. Instead, we restrict projecting the reference HR image at a given pixel position {s, t} to only those LR cameras that are contained in S(s, t) which provides a more localized fusion information. Additionally, we warp neighboring pixels of position {s, t} to cameras mapped in S(s, t) using their respective stacks S (and camera baseline and optical distortion information) to form patches large enough to blur with H and decimate. Thus, each HR pixel still contributes to the gradient formation from multiple cameras. Some of the stacks on the HR grid positions may still be empty after the initial SR fusion. We handle such sparsely populated S distributions by visiting LR cameras (and pixel positions inside those images) based on the neighboring grid positions? stacks. We restrict ourselves to a relatively small (3 ? 3) spatial neighborhood around each empty {s, t} on the HR grid. Uncertainty processing on the LR grid: Another contribution of our SR mechanism lies in filtering the likelihood gradient in the LR domain. In literature filtering against the imaging noise and outliers is sometimes applied to the likelihood term to improve estimation quality. In contrast to our work, deblurring is usually performed separately, decoupled from the rest of the SR. Importantly, our motivation for the developments in this section is to not only address the impact of imaging noise and outliers on the likelihood gradients, but also to guard against spatial registration errors reflected in the stacks S and implicitly in W p . In our work, instead of computing, and then back-projecting into the current HR estimate the gradients with respect to only the LR pixel positions ({a p , b p } in Figure 10 ) indicated by shift matrices W p , we also visit neighboring LR pixels around those positions. Since in most cases our parallax errors lie within the support of a single LR pixel, we visit only a small 3 ? 3 LR neighborhood. Referring again to Figure 10 , at each decimation position {s p , t p } in the warped and blurred HR images, we obtain an LR gradient g p (s p , t p , m, n) for each position {m, n} within a 3 ? 3 window U (a p , b p ) centered at {a p , b p } in y p . The weight ? p (s p , t p , m, n) is computed from the set of pixels in a 5 ? 5 window B p (a p , b p ) around position {a p , b p } in y p , corresponding to {s p , t p }, and the block B lr p (m, n) centered at each {m, n} in y p as where ? is the noise standard deviation. For each decimation position {s p , t p } in the warped and blurred HR image p, the sum of the weights ? p,sum (s p , t p ) = m,n?U (a,b) ? p (s p , t p , m, n), is also recorded alongside the filtered gradient from Equation 5. All the gradient values g p (s p , t p ) and their corresponding sum of weights ? p,sum (s p , t p ), are backpropagated onto the grid of the reference HR image, by upsampling, blurring, and inverse-shifting them. The normalization of the multiple gradients thus formed at {s, t} uses the sums of weights ? p,sum (s p , t p ), to obtain the final likelihood gradient at {s, t}. The use of such neighborhood information to filter the likelihood gradient makes our MAP-based SR process stable even with (up to 1 LR pixel) inaccuracies in registration. where R(.) is an edge-preserving, local nonlinear operator (the bilateral filter) and the norm is q = 2. The gradient of this prior is computed, and added to the likelihood gradient in Equation. 4, then applied to the current HR estimate. This locally-adaptive prior enforces preservation of edges under the operator R(.). There is a subtle, yet important difference, with the bilateral prior introduced  in [Farsiu et al. 2006]. In our case, the prior gradients are computed between the current HR estimate and its bilaterally-filtered version. In other words, our restored image needs to feature the properties of a bilaterally-filtered HR image. The data fidelity (likelihood) term by itself provides a good estimate of the HR image in high SNR regions, and the statistical prior helps estimate the low SNR regions better, and ensures solution stability. We could easily incorporate estimating multiple channels in the framework described by injecting a cross-channel image prior. However, due to computational constraints, we first perform a channel difference-based fusion of the red and blue channels with respect to the restored green channel. This is followed by a transfer of weighted MAP-gradients from the green channel into the fused red and blue channels to obtain a high resolution RGB image . We developed the camera array as a 4x4 sensor array where each sensor supports 1000 ? 750 pixels in a 4:3 aspect ratio. The synthesized high resolution image output by our processing is an 8 megapixel image. The lens array consists of 3 types of lenses red, green, and bluewhere each has F-number 3.1, a diagonal field of view of 56 ? and a focal length of 2mm. The overall camera module height is less than 3.5mm as compared to a height of anywhere between 5.2mm and 6.0mm for a legacy camera module. The least count (this is the highest degree of accuracy of measurement) of the disparity between two cameras is a function of the system noise and blur and, in practice, we have found this to be 1 of an LR pixel. This puts a limit on the depth 3 map accuracy and how the depth resolution changes with distance. Since disparity is a function of depth, if we let d u?v (z 1 ) represent the disparity between two cameras, u and v, at a distance z 1 and d u?v (z 2 ) represent the disparity at distance z 2 (where z 2 > z 1 ), then d u?v (z 1 ) ? d u?v (z 2 ) ? 1 3 . This constraint can be used to determine how the estimated value of depth changes with increase in object distance. Figure 11 shows this variation of the accuracy of the depth estimate with object distance. Obviously a key aspect of how successful this approach is in being a reasonable alternative to conventional camera design lies in how effective the superresolution is at recovering the resolution encoded in the aliased signals of the LR images in the array, under all lighting and scene conditions. Figure 7 shows our system behavior in capturing high frequency images. The ability of the camera array to capture high frequency signal is fundamentally defined by the lens F-number (MTF), optical format of lens, pixel size (pixel MTF), and the superresolution factor. Figure 12 shows how the resolution of the final super-resolved image changes with varying values of these parameters by extrapolating from the current reference design (highlighted in red) using a F3.1 lens, and 1000 ? 750, 1.75? front-side illuminated (FSI) pixels per camera and using the information presented in Figure 6 . The table can be used to determine how much resolution is being traded off by the array camera in comparison with the legacy camera in gaining depth information. If we consider the sensor size (3264 ? 2448), pixel size (1.4?m), and F-number (F2.4) of the iPhone5, a corresponding array camera of similar specification would be a 4 ? 4 array with each sub-array having 800 ? 600, 1.4?m pixels and an F2.4 lens. The achievable resolution (MTF20) of such a comparable system would be 950 lw/ph (highlighted in blue). In comparison, the achievable resolution (MTF20) of the iPhone5 is 2000lw/ph ( Figure 6 ) that tells us we are trading resolution by a factor of 2 for depth information. In Figure 13 , we also illustrate the effects of not using some of the new developments discussed in Section 4.2 (namely the likelihood gradient formation and filtering, and the uncertainty processing), on the quality of restoration. This is akin to performing a more classical likelihood computation, and it basically translates into forming single likelihoods (no filtering) at fusion-populated HR grid positions, where we used a median filtering of any pixel stacks formed, and interpolating any holes on the HR grid. This degrades image quality, and may even interfere with the correct reconstruction of frequencies, as errors in the operations preceding the MAP processing start playing a larger role. Figure 15 shows a comparison of the noise performance of the camera array against that of the iPhone5 at 100Lux illumination. We offset the exposure time on the camera array to compensate for the faster lens on the iPhone5. The figure shows, in 3 zoomed out regions, the relative noise difference between the two cameras. Both visually and using the SNR metric (both measured YSNR of about 39dB) we see that our camera array performs at close to the noise performance of the iPhone5. It must be pointed out, however, that a smaller exposure time also leads to more motion blur and in the final analysis is not an adequate solution and one that can only be addressed by a corresponding faster lens for the PiCam. We have also compared our camera against iPhone5 both indoors and outdoors. Figure 14 , in the top two rows, shows a couple of examples of an indoor and outdoor shot, and the last row shows an example of a post capture refocus application. By virtue of having a relatively small optical format ( 1 ) and a shorter focal length compared 7 to a legacy 8MP camera in a mobile device, the hyperfocal distance of our imaging system is at 40cm. Thus, in addition to all objects from 20cm to ? being captured in focus, the computed depth map allows one to achieve a degree of post-capture refocus that provides creative flexibility to the photographer. The iPhone5 has a larger aperture and a correspondingly large hyperfocal distance (?100cm). In the top row image, the iPhone5 is focused on a near object (<50cm) and has a correspondingly short depth of field so that the background picture has defocus blur. By contrast, the PiCam has everything in focus and the background appears sharper than it does on the iPhone5. A second point to consider is that the degree of refocus ability is clearly dependent on the resolution of the depth map. Thus, an object at a given distance from the camera may be refocused against its background as long as the background object is at a distance larger than the depth resolution for that distance (see Figure 11 ). Finally, PiCam is a 3D capable capture device given its ability to compute depth of textured surfaces. This should enable a handheld digitization device capable of creating point clouds that lend themselves to subsequent meshing. We captured an object (a Chinese dragon), shown in Figure 16(a) , on a tabletop at a distance of 50cm and converted the depth map from the object to a point cloud by filtering the depth map to reveal just the high confidence depth values. A total of 87000 points were rendered with texture and the resulting figure is shown in Figure 16(b) . In addition to the above applications, the PiCam architecture also enables view synthesis from any position on the array and also in front or behind the array. The synthesis procedure consists of three steps calculating the displacement of each image in the array to the virtual viewpoint, computing the depth map for the virtual viewpoint, and forming the color image at that viewpoint. The method enables one to showcase motion parallax and generate stereo left and right views. Being able to generate different viewpoints, when combined with real-time gyroscope readings on a mobile device, provides the viewer with a sense of scene depth and a rich interactive experience. We show examples in the accompanying video. Our approach to computational array cameras has shown robustness in generating high resolution color images under varying imaging conditions. In addition, PiCam provides a significant difference over the legacy camera in that it relaxes the z-height constraint considerably. The resolution of our system is a function of the optical format and F-number of the lens, the pixel size, and the number of cameras in the array. Depending on the application, an appropriate choice of the imaging system and processing parameters allows one to scale the resolution of the system without hitting the z-height constraint. Our system is also fault tolerant to an extent since the failure of 1 or 2 cameras in the array leads to a graceful degradation of system performance. At the same time, there are areas of improvement both in terms of image quality and processing speed. Artifacts from incorrect depth estimates, along the contours of objects close to the camera, are still possible depending on scene content and background texture. The correction of these artifacts, while not a focus of this paper, remains an important area of continuing research. Nevertheless, we believe that with continued work in both of these areas this architecture has the potential to remove some of the limitations to camera design for small form factor devices, and enable new imaging applications. It is a privilege to be part of the team that helped create the PiCam computational camera array. This camera is the basis of Silicon Valley startup Pelican Imaging Corporation founded by the author Kartik Venkataraman with assistance from colleagues Dan Lelescu, Jacques Duparr?, Robert Mullis, and Bedabrata Pain. The sensor and lens arrays are based on the architectural specification of Pelican Imaging Corporation and manufactured by our industry partners Aptina Imaging [Cho et al. 2013], Heptagon and others. We also extend thanks to those that helped with the preparation of this paper, including Herve Hornung, Lisa Garvey, and Stephen Williams.",
  "resources" : [ ]
}