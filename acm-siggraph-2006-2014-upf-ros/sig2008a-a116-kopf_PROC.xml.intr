{
  "uri" : "sig2008a-a116-kopf_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2008a/a116-kopf_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Deep Photo: Model-Based Photograph Enhancement and Viewing",
    "published" : "2008",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Johannes-Kopf",
      "name" : "Johannes",
      "surname" : "Kopf"
    }, {
      "uri" : "http://drinventor/Boris-Neubert",
      "name" : "Boris",
      "surname" : "Neubert"
    }, {
      "uri" : "http://drinventor/Billy-Chen",
      "name" : "Billy",
      "surname" : "Chen"
    }, {
      "uri" : "http://drinventor/Michael F.-Cohen",
      "name" : "Michael F.",
      "surname" : "Cohen"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Oliver-Deussen",
      "name" : "Oliver",
      "surname" : "Deussen"
    }, {
      "uri" : "http://drinventor/Matthew-Uyttendaele",
      "name" : "Matthew",
      "surname" : "Uyttendaele"
    }, {
      "uri" : "http://drinventor/Dani-Lischinski",
      "name" : "Dani",
      "surname" : "Lischinski"
    } ]
  },
  "bagOfWords" : [ "despite", "increase", "ubiquity", "digital", "photography", "metaphor", "we", "use", "browse", "interact", "we", "photograph", "have", "change", "much", "few", "exception", "we", "still", "treat", "they", "2d", "entity", "whether", "display", "computer", "monitor", "print", "hard", "copy", "well", "understand", "augment", "photograph", "depth", "can", "open", "way", "variety", "new", "exciting", "manipulation", "however", "infer", "depth", "information", "from", "single", "image", "capture", "ordinary", "camera", "still", "longstanding", "unsolved", "problem", "computer", "vision", "luckily", "we", "witness", "great", "increase", "number", "accuracy", "geometric", "model", "world", "include", "terrain", "building", "register", "photograph", "model", "depth", "become", "available", "each", "pixel", "Deep", "Photo", "system", "describe", "paper", "consist", "number", "application", "afford", "newfound", "depth", "value", "well", "many", "other", "type", "information", "typically", "associate", "model", "deep", "Photo", "motivate", "several", "recent", "trend", "now", "reach", "critical", "mass", "first", "trend", "geo-tagged", "photo", "many", "photo", "share", "web", "site", "now", "enable", "user", "manually", "add", "location", "information", "photo", "some", "digital", "camera", "RICOH", "Caplio", "500se", "Nokia", "N95", "feature", "built-in", "GPS", "allow", "automatic", "location", "tagging", "also", "number", "manufacturer", "offer", "small", "gp", "unit", "allow", "photo", "easily", "geo-tag", "software", "synchronize", "gp", "log", "photo", "addition", "location", "tag", "can", "enhance", "digital", "compass", "able", "measure", "orientation", "-lrb-", "tilt", "head", "-rrb-", "camera", "expect", "future", "more", "camera", "have", "functionality", "most", "photograph", "geo-tagged", "second", "trend", "widespread", "availability", "accurate", "digital", "terrain", "model", "well", "detailed", "urban", "model", "thanks", "commercial", "project", "Google", "Earth", "Microsoft?s", "Virtual", "Earth", "both", "quantity", "quality", "model", "rapidly", "increase", "public", "domain", "NASA", "provide", "detailed", "satellite", "imagery", "-lrb-", "e.g.", "landsat", "-lsb-", "NASA", "2008a", "-rsb-", "-rrb-", "elevation", "model", "-lrb-", "e.g.", "Shuttle", "Radar", "Topography", "Mission", "-lsb-", "NASA", "2008b", "-rsb-", "-rrb-", "also", "number", "city", "around", "world", "create", "detailed", "3d", "model", "cityscape", "-lrb-", "e.g.", "Berlin", "3d", "-rrb-", "combination", "geo-tagging", "availability", "fairly", "accurate", "3d", "model", "allow", "many", "photograph", "precisely", "georegister", "we", "envision", "near", "future", "automatic", "georegistration", "available", "online", "service", "thus", "although", "we", "briefly", "describe", "simple", "interactive", "geo-registration", "technique", "we", "currently", "employ", "emphasis", "paper", "application", "enable", "include", "dehazing", "-lrb-", "add", "haze", "-rrb-", "image", "approximate", "change", "lighting", "novel", "view", "synthesis", "expand", "field", "view", "add", "new", "object", "image", "integration", "GIS", "datum", "photo", "browser", "we", "goal", "work", "have", "be", "enable", "application", "single", "outdoor", "image", "take", "casual", "manner", "without", "require", "any", "special", "equipment", "any", "particular", "setup", "thus", "we", "system", "applicable", "large", "body", "exist", "outdoor", "photograph", "so", "long", "we", "know", "rough", "location", "where", "each", "photograph", "take", "we", "choose", "New", "York", "City", "Yosemite", "National", "Park", "two", "many", "location", "around", "world", "which", "detailed", "textured", "model", "already", "available", "we", "demonstrate", "we", "approach", "combine", "number", "photograph", "-lrb-", "obtain", "from", "flickr", "tm", "-rrb-", "model", "should", "note", "while", "model", "we", "use", "fairly", "detail", "still", "far", "cry", "from", "degree", "accuracy", "level", "detail", "one", "would", "need", "order", "use", "model", "directly", "render", "photographic", "image", "thus", "one", "we", "challenge", "work", "have", "be", "understand", "how", "best", "leverage", "3d", "information", "afford", "use", "model", "while", "same", "time", "preserve", "photographic", "quality", "original", "image", "addition", "explore", "application", "list", "above", "paper", "also", "make", "number", "specific", "technical", "contribution", "two", "main", "one", "new", "data-driven", "stable", "dehazing", "procedure", "new", "model-guided", "layered", "depth", "image", "completion", "technique", "novel", "view", "synthesis", "before", "continue", "we", "should", "note", "some", "limitation", "Deep", "Photo", "its", "current", "form", "example", "we", "show", "outdoor", "scene", "we", "count", "available", "model", "describe", "distant", "static", "geometry", "scene", "we", "can", "expect", "have", "access", "geometry", "nearby", "-lrb-", "possibly", "dynamic", "-rrb-", "foreground", "object", "people", "car", "tree", "etc.", "we", "current", "implementation", "foreground", "object", "mat", "out", "before", "combine", "rest", "photograph", "model", "may", "composit", "back", "onto", "photograph", "later", "stage", "so", "some", "image", "user", "must", "spend", "some", "time", "interactive", "matting", "fidelity", "some", "we", "manipulation", "foreground", "may", "reduce", "say", "we", "expect", "kind", "application", "we", "demonstrate", "scale", "Yosemite", "we", "use", "elevation", "datum", "from", "Shuttle", "Radar", "Topography", "Mission", "-lsb-", "NASA", "2008b", "-rsb-", "landsat", "imagery", "-lsb-", "NASA", "2008a", "-rsb-", "datum", "available", "entire", "Earth", "model", "similar", "NYC", "currently", "available", "dozen", "city", "include", "any", "improvement", "automatic", "computer", "vision", "algorithm", "depth", "acquisition", "technology", "we", "system", "touch", "upon", "quite", "few", "distinct", "topic", "computer", "vision", "computer", "graphic", "thus", "comprehensive", "review", "all", "related", "work", "feasible", "due", "space", "constraint", "below", "we", "attempt", "provide", "some", "representative", "reference", "discuss", "detail", "only", "one", "most", "closely", "related", "we", "goal", "technique", "image-based", "modeling", "recent", "year", "much", "work", "have", "be", "do", "image-based", "modeling", "technique", "which", "create", "high", "quality", "3d", "model", "from", "photograph", "one", "example", "pioneering", "Fa", "ade", "system", "-lsb-", "Debevec", "et", "al.", "1996", "-rsb-", "design", "interactive", "modeling", "building", "from", "collection", "photograph", "other", "system", "use", "panoramic", "mosaic", "-lsb-", "Shum", "et", "al.", "1998", "-rsb-", "combine", "image", "range", "datum", "-lsb-", "stamo", "Allen", "2000", "-rsb-", "merge", "ground", "aerial", "view", "-lsb-", "fr?h", "Zakhor", "2003", "-rsb-", "name", "few", "any", "approach", "may", "use", "create", "kind", "textured", "3d", "model", "we", "use", "we", "system", "however", "work", "we", "concern", "creation", "model", "rather", "way", "which", "combination", "single", "photograph", "may", "useful", "casual", "digital", "photographer", "one", "might", "say", "rather", "than", "attempt", "automatically", "manually", "reconstruct", "model", "from", "single", "photo", "we", "exploit", "availability", "digital", "terrain", "urban", "model", "effectively", "replace", "difficult", "3d", "reconstruction/modeling", "process", "much", "simpler", "registration", "process", "recent", "research", "have", "show", "various", "challenging", "task", "image", "completion", "insertion", "object", "photograph", "-lsb-", "Hays", "Efros", "2007", "Lalonde", "et", "al.", "2007", "-rsb-", "can", "greatly", "benefit", "from", "availability", "enormous", "amount", "photograph", "have", "already", "be", "capture", "philosophy", "behind", "we", "work", "somewhat", "similar", "we", "attempt", "leverage", "large", "amount", "textured", "geometric", "model", "have", "already", "be", "create", "unlike", "image", "database", "which", "consist", "mostly", "unrelated", "item", "geometric", "model", "we", "use", "all", "anchor", "world", "surround", "we", "weather", "other", "atmospheric", "phenomenon", "haze", "greatly", "reduce", "visibility", "distant", "region", "image", "outdoor", "scene", "remove", "effect", "haze", "dehazing", "challenging", "problem", "because", "degree", "effect", "each", "pixel", "depend", "depth", "corresponding", "scene", "point", "some", "haze", "removal", "technique", "make", "use", "multiple", "image", "e.g.", "image", "take", "under", "different", "weather", "condition", "-lsb-", "Narasimhan", "Nayar", "2003a", "-rsb-", "different", "polarizer", "orientation", "-lsb-", "Schechner", "et", "al.", "2003", "-rsb-", "since", "we", "interested", "dehaze", "single", "image", "take", "without", "any", "special", "equipment", "method", "suitable", "we", "need", "several", "work", "attempt", "remove", "effect", "haze", "fog", "etc.", "from", "single", "image", "use", "some", "form", "depth", "information", "example", "Oakley", "Satherley", "-lsb-", "1998", "-rsb-", "dehaze", "aerial", "imagery", "use", "estimate", "terrain", "model", "however", "method", "involve", "estimate", "large", "number", "parameter", "quality", "report", "result", "unlikely", "satisfy", "today?s", "digital", "photography", "enthusiast", "Narasimhan", "Nayar", "-lsb-", "2003b", "-rsb-", "dehaze", "single", "image", "base", "rough", "depth", "approximation", "provide", "user", "derive", "from", "satellite", "orthophoto", "very", "latest", "dehazing", "method", "-lsb-", "fattal", "2008", "Tan", "2008", "-rsb-", "able", "dehaze", "single", "image", "make", "various", "assumption", "about", "color", "scene", "we", "work", "differ", "from", "previous", "single", "image", "dehaze", "method", "leverage", "availability", "more", "accurate", "3d", "model", "use", "novel", "data-driven", "dehaze", "procedure", "result", "we", "method", "capable", "effective", "stable", "high-quality", "contrast", "restoration", "even", "extremely", "distant", "region", "novel", "view", "synthesis", "have", "be", "long", "recognize", "add", "depth", "information", "photograph", "provide", "means", "alter", "viewpoint", "classic", "tour", "Picture", "system", "-lsb-", "Horry", "et", "al.", "1997", "-rsb-", "demonstrate", "fitting", "simple", "mesh", "scene", "sometimes", "enough", "enable", "compelling", "3d", "navigation", "experience", "subsequent", "papers", "Kang", "-lsb-", "1998", "-rsb-", "Criminisi", "et", "al.", "-lsb-", "2000", "-rsb-", "oh", "et", "al.", "-lsb-", "2001", "-rsb-", "Zhang", "et", "al.", "-lsb-", "2002", "-rsb-", "extend", "provide", "more", "sophisticated", "user-guided", "3d", "modelling", "technique", "more", "recently", "Hoiem", "et", "al.", "-lsb-", "2005", "-rsb-", "use", "machine", "learn", "technique", "order", "construct", "simple", "pop-up", "3d", "model", "completely", "automatically", "from", "single", "photograph", "system", "despite", "simplicity", "model", "3d", "experience", "can", "quite", "compelling", "work", "we", "use", "already", "available", "3d", "model", "order", "add", "depth", "photograph", "we", "present", "new", "model-guided", "image", "completion", "technique", "enable", "we", "expand", "field", "view", "perform", "high-quality", "novel", "view", "synthesis", "relighting", "number", "sophisticated", "relight", "system", "have", "be", "propose", "various", "researcher", "over", "year", "-lrb-", "e.g.", "-lsb-", "Yu", "Malik", "1998", "Yu", "et", "al.", "1999", "Loscos", "et", "al.", "2000", "Debevec", "et", "al.", "2000", "-rsb-", "-rrb-", "typically", "system", "make", "use", "highly", "accurate", "geometric", "model", "and/or", "collection", "photograph", "often", "take", "under", "different", "lighting", "condition", "give", "input", "often", "able", "predict", "appearance", "scene", "under", "novel", "lighting", "condition", "very", "high", "degree", "accuracy", "realism", "another", "alternative", "use", "time-lapse", "video", "sequence", "-lsb-", "Sunkavalli", "et", "al.", "2007", "-rsb-", "we", "case", "we", "assume", "availability", "geometric", "model", "have", "just", "one", "photograph", "work", "furthermore", "although", "model", "might", "detail", "typically", "quite", "far", "from", "perfect", "match", "photograph", "example", "tree", "cast", "shadow", "nearby", "building", "typically", "absent", "from", "we", "model", "thus", "we", "can", "hope", "correctly", "recover", "reflectance", "each", "pixel", "photograph", "which", "necessary", "order", "perform", "physically", "accurate", "relighting", "therefore", "work", "we", "propose", "very", "simple", "relighting", "approximation", "which", "nevertheless", "able", "produce", "fairly", "compelling", "result", "Photo", "browse", "also", "related", "Photo", "Tourism", "system", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "which", "enable", "browse", "explore", "large", "collection", "photograph", "certain", "location", "use", "3d", "interface", "browse", "experience", "we", "provide", "very", "different", "moreover", "contrast", "Photo", "Tourism", "we", "system", "require", "only", "single", "geo-tagged", "photograph", "make", "applicable", "even", "location", "without", "many", "available", "photo", "Photo", "Tourism", "system", "also", "demonstrate", "transfer", "annotation", "from", "one", "register", "photograph", "another", "Deep", "Photo", "photograph", "register", "model", "world", "make", "possible", "tap", "much", "richer", "source", "information", "work", "geo-referenced", "image", "once", "photo", "register", "geo-referenced", "datum", "map", "3d", "model", "plethora", "information", "become", "available", "example", "Cho", "-lsb-", "Cho", "2007", "-rsb-", "note", "absolute", "geo-location", "can", "assign", "individual", "pixel", "gi", "annotation", "building", "street", "name", "may", "project", "onto", "image", "plane", "deep", "Photo", "support", "similar", "labeling", "well", "several", "additional", "visualization", "contrast", "cho?s", "system", "do", "so", "dynamically", "context", "interactive", "photo", "browse", "application", "furthermore", "discuss", "earlier", "also", "enable", "variety", "other", "application", "addition", "enhance", "photo", "location", "also", "useful", "organize", "visualize", "photo", "collection", "system", "develop", "Toyama", "et", "al.", "-lsb-", "2003", "-rsb-", "enable", "user", "browse", "large", "collection", "geo-referenced", "photo", "2d", "map", "map", "serve", "both", "visualization", "device", "well", "way", "specify", "spatial", "query", "i.e.", "all", "photo", "within", "region", "contrast", "DeepPhoto", "focus", "enhance", "browse", "single", "photograph", "two", "system", "actually", "complementary", "one", "focus", "organize", "large", "photo", "collection", "other", "enhance", "view", "single", "photograph" ],
  "content" : "Despite the increasing ubiquity of digital photography, the metaphors we use to browse and interact with our photographs have not changed much. With few exceptions, we still treat them as 2D entities, whether they are displayed on a computer monitor or printed as a hard copy. It is well understood that augmenting a photograph with depth can open the way for a variety of new exciting manipulations. However, inferring the depth information from a single image that was captured with an ordinary camera is still a longstanding unsolved problem in computer vision. Luckily, we are witnessing a great increase in the number and the accuracy of geometric models of the world, including terrain and buildings. By registering photographs to these models, depth becomes available at each pixel. The Deep Photo system described in this paper, consists of a number of applications afforded by these newfound depth values, as well as the many other types of information that are typically associated with such models. Deep Photo is motivated by several recent trends now reaching critical mass. The first trend is that of geo-tagged photos. Many photo sharing web sites now enable users to manually add location information to photos. Some digital cameras, such as the RICOH Caplio 500SE and the Nokia N95, feature a built-in GPS, allowing automatic location tagging. Also, a number of manufacturers offer small GPS units that allow photos to be easily geo-tagged by software that synchronizes the GPS log with the photos. In addition, location tags can be enhanced by digital compasses that are able to measure the orientation (tilt and heading) of the camera. It is expected that, in the future, more cameras will have such functionality, and that most photographs will be geo-tagged. The second trend is the widespread availability of accurate digital terrain models, as well as detailed urban models. Thanks to commercial projects, such as Google Earth and Microsoft?s Virtual Earth, both the quantity and the quality of such models is rapidly increasing. In the public domain, NASA provides detailed satellite imagery (e.g., Landsat [NASA 2008a]) and elevation models (e.g., Shuttle Radar Topography Mission [NASA 2008b]). Also, a number of cities around the world are creating detailed 3D models of their cityscape (e.g., Berlin 3D). The combination of geo-tagging and the availability of fairly accurate 3D models allows many photographs to be precisely georegistered. We envision that in the near future automatic georegistration will be available as an online service. Thus, although we briefly describe the simple interactive geo-registration technique that we currently employ, the emphasis of this paper is on the applications that it enables, including: ? dehazing (or adding haze to) images, ? approximating changes in lighting, ? novel view synthesis, ? expanding the field of view, ? adding new objects into the image, ? integration of GIS data into the photo browser. Our goal in this work has been to enable these applications for single outdoor images, taken in a casual manner without requiring any special equipment or any particular setup. Thus, our system is applicable to a large body of existing outdoor photographs, so long as we know the rough location where each photograph was taken. We chose New York City and Yosemite National Park as two of the many locations around the world, for which detailed textured models are already available 1 . We demonstrate our approach by combining a number of photographs (obtained from flickr TM ) with these models. It should be noted that while the models that we use are fairly detailed, they are still a far cry from the degree of accuracy and the level of detail one would need in order to use these models directly to render photographic images. Thus, one of our challenges in this work has been to understand how to best leverage the 3D information afforded by the use of these models, while at the same time preserving the photographic qualities of the original image. In addition to exploring the applications listed above, this paper also makes a number of specific technical contributions. The two main ones are a new data-driven stable dehazing procedure, and a new model-guided layered depth image completion technique for novel view synthesis. Before continuing, we should note some of the limitations of Deep Photo in its current form. The examples we show are of outdoor scenes. We count on the available models to describe the distant static geometry of the scene, but we cannot expect to have access to the geometry of nearby (and possibly dynamic) foreground objects, such as people, cars, trees, etc. In our current implementation such foreground objects are matted out before combining the rest of the photograph with a model, and may be composited back onto the photograph at a later stage. So, for some images, the user must spend some time on interactive matting, and the fidelity of some of our manipulations in the foreground may be reduced. That said, we expect the kinds of applications we demonstrate will scale to 1 For Yosemite, we use elevation data from the Shuttle Radar Topography Mission [NASA 2008b] with Landsat imagery [NASA 2008a]. Such data is available for the entire Earth. Models similar to that of NYC are currently available for dozens of cities. include any improvements in automatic computer vision algorithms and depth acquisition technologies. Our system touches upon quite a few distinct topics in computer vision and computer graphics; thus, a comprehensive review of all related work is not feasible due to space constraints. Below, we attempt to provide some representative references, and discuss in detail only the ones most closely related to our goals and techniques. Image-based modeling. In recent years, much work has been done on image-based modeling techniques, which create high quality 3D models from photographs. One example is the pioneering Fa  ?ade system [Debevec et al. 1996], designed for interactive modeling of buildings from collections of photographs. Other systems use panoramic mosaics [Shum et al. 1998], combine images with range data [Stamos and Allen 2000], or merge ground and aerial views [Fr?h and Zakhor 2003], to name a few. Any of these approaches may be used to create the kinds of textured 3D models that we use in our system; however, in this work we are not concerned with the creation of such models, but rather with the ways in which their combination with a single photograph may be useful for the casual digital photographer. One might say that rather than attempting to automatically or manually reconstruct the model from a single photo, we exploit the availability of digital terrain and urban models, effectively replacing the difficult 3D reconstruction/modeling process by a much simpler registration process. Recent research has shown that various challenging tasks, such as image completion and insertion of objects into photographs [Hays and Efros 2007; Lalonde et al. 2007] can greatly benefit from the availability of the enormous amounts of photographs that had already been captured. The philosophy behind our work is somewhat similar: we attempt to leverage the large amount of textured geometric models that have already been created. But unlike image databases, which consist mostly of unrelated items, the geometric models we use are all anchored to the world that surrounds us. Weather and other atmospheric phenomena, such as haze, greatly reduce the visibility of distant regions in images of outdoor scenes. Removing the effect of haze, or dehazing, is a challenging problem, because the degree of this effect at each pixel depends on the depth of the corresponding scene point. Some haze removal techniques make use of multiple images; e.g., images taken under different weather conditions [Narasimhan and Nayar 2003a], or with different polarizer orientations [Schechner et al. 2003]. Since we are interested in dehazing single images, taken without any special equipment, such methods are not suitable for our needs. There are several works that attempt to remove the effects of haze, fog, etc., from a single image using some form of depth information. For example, Oakley and Satherley [1998] dehaze aerial imagery using estimated terrain models. However, their method involves estimating a large number of parameters, and the quality of the reported results is unlikely to satisfy today?s digital photography enthusiasts. Narasimhan and Nayar [2003b] dehaze single images based on a rough depth approximation provided by the user, or derived from satellite orthophotos. The very latest dehazing methods [Fattal 2008; Tan 2008] are able to dehaze single images by making various assumptions about the colors in the scene. Our work differs from these previous single image dehazing methods in that it leverages the availability of more accurate 3D models, and uses a novel data-driven dehazing procedure. As a result, our method is capable of effective, stable high-quality contrast restoration even of extremely distant regions. Novel view synthesis. It has been long recognized that adding depth information to photographs provides the means to alter the viewpoint. The classic ?Tour Into the Picture? system [Horry et al. 1997] demonstrates that fitting a simple mesh to the scene is sometimes enough to enable a compelling 3D navigation experience. Subsequent papers, Kang [1998], Criminisi et al. [2000], Oh et al. [2001], Zhang et al. [2002], extend this by providing more sophisticated, user-guided 3D modelling techniques. More recently Hoiem et al. [2005] use machine learning techniques in order to construct a simple ?pop-up? 3D model, completely automatically from a single photograph. In these systems, despite the simplicity of the models, the 3D experience can be quite compelling. In this work, we use already available 3D models in order to add depth to photographs. We present a new model-guided image completion technique that enables us to expand the field of view and to perform high-quality novel view synthesis. Relighting. A number of sophisticated relighting systems have been proposed by various researchers over the years (e.g., [Yu and Malik 1998; Yu et al. 1999; Loscos et al. 2000; Debevec et al. 2000]). Typically, such systems make use of a highly accurate geometric model, and/or a collection of photographs, often taken under different lighting conditions. Given this input they are often able to predict the appearance of a scene under novel lighting conditions with a very high degree of accuracy and realism. Another alternative to use a time-lapse video sequence [Sunkavalli et al. 2007]. In our case, we assume the availability of a geometric model, but have just one photograph to work with. Furthermore, although the model might be detailed, it is typically quite far from a perfect match to the photograph. For example, a tree casting a shadow on a nearby building will typically be absent from our model. Thus, we cannot hope to correctly recover the reflectance at each pixel of the photograph, which is necessary in order to perform physically accurate relighting. Therefore, in this work we propose a very simple relighting approximation, which is nevertheless able to produce fairly compelling results. Photo browsing. Also related is the ?Photo Tourism? system [Snavely et al. 2006], which enables browsing and exploring large collections of photographs of a certain location using a 3D interface. But, the browsing experience that we provide is very different. Moreover, in contrast to ?Photo Tourism?, our system requires only a single geo-tagged photograph, making it applicable even to locations without many available photos. The ?Photo Tourism? system also demonstrates the transfer of annotations from one registered photograph to another. In Deep Photo, photographs are registered to a model of the world, making it possible to tap into a much richer source of information. Working with geo-referenced images. Once a photo is registered to geo-referenced data such as maps and 3D models, a plethora of information becomes available. For example, Cho [Cho 2007] notes that absolute geo-locations can be assigned to individual pixels and that GIS annotations, such as building and street names, may be projected onto the image plane. Deep Photo supports similar labeling, as well as several additional visualizations, but in contrast to Cho?s system, it does so dynamically, in the context of an interactive photo browsing application. Furthermore, as discussed earlier, it also enables a variety of other applications. In addition to enhancing photos, location is also useful in organizing and visualizing photo collections. The system developed by Toyama et al. [2003] enables a user to browse large collections of geo-referenced photos on a 2D map. The map serves as both a visualization device, as well as a way to specify spatial queries, i.e., all photos within a region. In contrast, DeepPhoto focuses on enhancing and browsing of a single photograph; the two systems are actually complementary, one focusing on organizing large photo collections, and the other on enhancing and viewing single photographs.",
  "resources" : [ ]
}