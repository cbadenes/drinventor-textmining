{
  "uri" : "sig2014a-a200-wu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014a/a200-wu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Real-time Shading-based Refinement for Consumer Depth Cameras",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shahram-Izadi",
      "name" : "Shahram",
      "surname" : "Izadi"
    } ]
  },
  "bagOfWords" : [ "we", "test", "we", "real-time", "enhancement", "software", "datum", "from", "PrimeSense", "Carmine", "1.09", "short", "Range", "-lrb-", "RGB", "re", "1280", "1024", "depth", "re", "640", "480", "framerate", "12", "fp", "-rrb-", "Kinect", "one", "-lrb-", "rgb", "re", "1920", "1080", "depth", "re", "512", "424", "frame", "rate", "30", "fp", "-rrb-", "well", "Asus", "Xtion", "pro", "-lrb-", "RGB", "re", "640", "480", "depth", "re", "640", "480", "framerate", "30", "fp", "-rrb-", "camera", "since", "video", "RGB", "datum", "frame", "synchronize", "Kinect", "one", "camera", "need", "move", "slowly", "order", "prevent", "artifact", "we", "approach", "run", "real-time", "rate", "excess", "30", "fp", "both", "static", "dynamic", "scene", "all", "rgb-d", "sensor", "significant", "enhancement", "detail", "compare", "raw", "depth", "datum", "achieve", "see", "Fig.", "12", "fig.", "supplemental", "video", "document", "which", "show", "screen", "capture", "visualization", "reconstruction", "before", "after", "refinement", "total", "qualitative", "test", "be", "do", "scene", "see", "Tab", "we", "always", "enable", "prior", "term", "lighting", "estimation", "set", "10", "we", "set", "empirically", "find", "weight", "depth", "refinement", "follow", "400", "10", "100", "all", "static", "scene", "100", "10", "all", "dynamic", "scene", "please", "refer", "Tab", "detail", "sequence", "timing", "individual", "step", "measure", "Intel", "Core", "i7-3770", "cpu", "3.4", "ghz", "-lrb-", "16gb", "Ram", "-rrb-", "Nvidia", "Geforce", "GTX", "780", "list", "preprocessing", "step", "include", "depth-to-color", "alignment", "filter", "depth", "color", "resampling", "image", "foreground", "segmentation", "those", "result", "gauss-newton", "optimizer", "run", "follow", "parameter", "hierarchy", "level", "10", "outer", "iteration", "pcg", "iteration", "-lrb-", "coarse-to-fine", "-rrb-", "we", "enable", "temporal", "smoothness", "prior", "term", "capture", "static", "scene", "i.e.", "vase", "sequence", "Lucy", "sequence", "Socrates", "sequence", "require", "icp", "alignment", "add", "3.5", "m", "three", "sequence", "total", "computation", "time", "yield", "effective", "frame", "rate", "between", "15", "fp", "full", "hd", "93", "fp", "SVGA", "quantitative", "evaluation", "we", "quantitatively", "evaluate", "accuracy", "we", "method", "two", "synthetic", "sequence", "400", "frame", "long", "we", "use", "ground", "truth", "detailed", "performance-captured", "face", "geometry", "-lsb-", "valgaert", "et", "al.", "2012", "-rsb-", "ground", "truth", "lighting", "from", "St.", "Peter?s", "basilica", "-lsb-", "Debevec", "1998", "-rsb-", "render", "two", "rgb-d", "sequence", "first", "one", "-lrb-", "coa", "-rrb-", "albedo", "uniform", "second", "one", "-lrb-", "da", "-rrb-", "we", "use", "dense", "albedo", "map", "obtain", "from", "one", "capture", "face", "image", "synthesize", "depth", "map", "sequence", "we", "first", "obtain", "quantize", "depth", "map", "from", "stereo", "result", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "add", "gaussian", "noise", "mimic", "noise", "from", "depth", "sensor", "we", "compare", "we", "method", "space-time", "multi-lateral", "rgbd", "filter", "method", "-lsb-", "Richardt", "et", "al.", "2012", "-rsb-", "-lrb-", "stfilt", "-rrb-", "reconstruction", "coa", "da", "use", "single-frame", "shade", "base", "refinement", "algorithm", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "-lrb-", "sbrol", "-rrb-", "error", "metric", "we", "employ", "average", "pixel-wise", "euclidean", "distance", "mm", "per", "frame", "-lrb-", "-rrb-", "well", "average", "angular", "difference", "normal", "degree", "-lrb-", "-rrb-", "distance", "normal", "error", "average", "over", "all", "frame", "summarize", "Tab", "compare", "stfilt", "we", "method", "produce", "result", "much", "lower", "distance", "normal", "error", "obtain", "metrically", "faithful", "reconstruction", "oppose", "only", "plausible", "result", "-lrb-", "see", "Fig.", "-rrb-", "comparison", "more", "involved", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "we", "result", "exhibit", "comparable", "distance", "error", "we", "real-time", "capability", "come", "price", "slightly", "higher", "error", "reconstruct", "normal", "orientation", "respective", "error", "curve", "over", "time", "both", "DA", "-lrb-", "Fig.", "-rrb-", "coa", "-lrb-", "additional", "material", "-rrb-", "further", "confirm", "above", "conclusion", "qualitative", "comparison", "we", "also", "compare", "we", "method", "stfilt", "real-world", "datum", "-lrb-", "talk", "sequence", "see", "Tab", "-rrb-", "use", "same", "hardware", "previously", "describe", "we", "approach", "only", "have", "runtime", "advantage", "-lrb-", "55.8", "fp", "against", "8.5", "fp", "-rrb-", "also", "produce", "much", "more", "detailed", "result", "-lrb-", "see", "Fig.", "10", "video", "-rrb-", "quantitative", "evaluation", "we", "quantitatively", "evaluate", "accuracy", "we", "method", "two", "synthetic", "sequence", "400", "frame", "long", "we", "use", "ground", "truth", "detailed", "performance-captured", "face", "geometry", "-lsb-", "valgaert", "et", "al.", "2012", "-rsb-", "ground", "truth", "lighting", "from", "St.", "Peter?s", "basilica", "-lsb-", "Debevec", "1998", "-rsb-", "render", "two", "rgb-d", "sequence", "first", "one", "-lrb-", "coa", "-rrb-", "albedo", "uniform", "second", "one", "-lrb-", "da", "-rrb-", "we", "use", "dense", "albedo", "map", "obtain", "from", "one", "capture", "face", "image", "synthesize", "depth", "map", "sequence", "we", "first", "obtain", "quantize", "depth", "map", "from", "stereo", "result", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "add", "gaussian", "noise", "mimic", "noise", "from", "depth", "sensor", "we", "compare", "we", "method", "space-time", "multi-lateral", "rgbd", "filter", "method", "-lsb-", "Richardt", "et", "al.", "2012", "-rsb-", "-lrb-", "stfilt", "-rrb-", "reconstruction", "coa", "da", "use", "single-frame", "shade", "base", "refinement", "algorithm", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "-lrb-", "sbrol", "-rrb-", "error", "metric", "we", "employ", "average", "pixel-wise", "euclidean", "distance", "mm", "per", "frame", "-lrb-", "-rrb-", "well", "average", "angular", "difference", "normal", "degree", "-lrb-", "-rrb-", "distance", "normal", "error", "average", "over", "all", "frame", "summarize", "Tab", "compare", "stfilt", "we", "method", "produce", "result", "much", "lower", "distance", "normal", "error", "obtain", "metrically", "faithful", "reconstruction", "oppose", "only", "plausible", "result", "-lrb-", "see", "Fig.", "-rrb-", "comparison", "more", "involved", "offline", "method", "-lsb-", "Valgaerts", "et", "al.", "2012", "-rsb-", "we", "result", "exhibit", "comparable", "distance", "error", "we", "real-time", "capability", "come", "price", "slightly", "higher", "error", "reconstruct", "normal", "orientation", "respective", "error", "curve", "over", "time", "both", "DA", "-lrb-", "Fig.", "-rrb-", "coa", "-lrb-", "additional", "material", "-rrb-", "further", "confirm", "above", "conclusion", "qualitative", "comparison", "we", "also", "compare", "we", "method", "stfilt", "real-world", "datum", "-lrb-", "talk", "sequence", "see", "Tab", "-rrb-", "use", "same", "hardware", "previously", "describe", "we", "approach", "only", "have", "runtime", "advantage", "-lrb-", "55.8", "fp", "against", "8.5", "fp", "-rrb-", "also", "produce", "much", "more", "detailed", "result", "-lrb-", "see", "Fig.", "10", "video", "-rrb-", "real-time", "3d", "reconstruction", "we", "use", "we", "algorithm", "together", "real-time", "voxel-hashing-based", "hand-held", "scanning", "approach", "depth", "camera", "propose", "-lsb-", "nie?ner", "et", "al.", "2013", "-rsb-", "we", "drastically", "enhance", "depth", "map", "quality", "full", "3d", "model", "more", "detail", "can", "reconstruct", "-lrb-", "voxel", "size", "0.5", "mm", "-rrb-", "see", "fig.", "Fig.", "11", "deformable", "3d", "reconstruction", "we", "also", "integrate", "we", "algorithm", "real-time", "deformable", "tracking", "approach", "-lsb-", "zollh?fer", "et", "al.", "2014a", "-rsb-", "which", "lead", "improve", "space-time", "coherent", "reconstruction", "non-rigidly", "deform", "template", "see", "Fig.", "13", "we", "approach", "enable", "leap", "forward", "real-time", "scene", "reconstruction", "depth", "camera", "still", "subject", "several", "well-known", "shape", "from", "shade", "limitation", "instance", "texture-copy", "artifact", "introduce", "high-frequency", "albedo", "change", "cause", "problem", "both", "online", "offline", "method", "-lsb-", "Richardt", "et", "al.", "2012", "Han", "et", "al.", "2013", "Yu", "et", "al.", "2013", "-rsb-", "we", "adaptive", "refinement", "-lrb-", "sect", "4.2", "-rrb-", "efficient", "mitigate", "visual", "presence", "artifact", "however", "we", "still", "can", "completely", "prevent", "they", "instance", "see", "fig.", "Fig.", "12", "-lrb-", "top", "left", "-rrb-", "generally", "we", "believe", "underconstrained", "nature", "problem", "inspire", "future", "research", "direction", "contrast", "some", "offline", "method", "we", "real-time", "constraint", "allow", "only", "simplify", "light", "transport", "model", "we", "initial", "constant", "albedo", "assumption", "may", "exacerbate", "texture", "copying", "general", "scene", "however", "we", "result", "show", "practice", "very", "faithful", "surface", "reconstruction", "spatially-varying", "albedo", "feasible", "due", "second", "order", "spherical", "harmonic", "representation", "non-diffuse", "surface", "still", "challenge", "we", "method", "addition", "we", "able", "improve", "depth", "map", "around", "silhouette", "since", "normal", "undefined", "we", "further", "assume", "one-bounce", "local", "illumination", "ignore", "lighting", "visibility", "which", "may", "lead", "error", "some", "case", "example", "hard", "shadow", "may", "result", "artificial", "detail", "around", "boundary", "interesting", "future", "direction", "would", "incorporation", "screen-space", "ambient", "occlusion", "term", "account", "local", "visibility", "we", "present", "first", "method", "real-time", "shading-based", "refinement", "rgb-d", "datum", "capture", "commodity", "depth", "camera", "general", "uncontrolled", "scene", "enable", "new", "real-time", "inverse", "rendering", "framework", "approximate", "time-varying", "incident", "lighting", "well", "albedo", "scene", "algorithm", "refine", "raw", "depth", "camera", "optimize", "complex", "non-linear", "energy", "use", "new", "highly", "parallel", "Gauss-Newton", "solver", "GPU", "result", "superior", "previous", "online", "depth", "map", "enhancement", "algorithm", "par", "offline", "shape-from-shading", "approach", "we", "experiment", "further", "show", "approach", "enable", "new", "level", "accuracy", "handheld", "3d", "scanning", "well", "deformable", "surface", "tracking" ],
  "content" : "We tested our real-time enhancement software on data from a PrimeSense Carmine 1.09 Short Range (RGB res. 1280 ? 1024, depth res 640 ? 480, framerate 12 fps), a Kinect One (RGB res 1920 ? 1080, depth res. 512 ? 424, frame rate 30 fps), as well as an Asus Xtion Pro (RGB res. 640 ? 480, depth res 640 ? 480, framerate 30 fps) camera. Since video and RGB data are not frame synchronized in the Kinect One, the camera needs to be moved slowly in order to prevent artifacts. Our approach runs at real-time rates in excess of 30 fps. On both static and dynamic scenes and for all RGB-D sensors, a significant enhancement of detail compared to the raw depth data was achieved, see Fig. 12 , Fig. 1 , and the supplemental video and document, which show screen captured visualizations of the reconstructions before and after refinement. In total, qualitative tests were done on 9 scenes, see Tab. 1. We always enable the prior term in lighting estimation by setting ? l = 10. We set the empirically found weights for depth refinement as follows: w g = 1, w s = 400, w p = 10, w r = 100 for all static scenes; w g = 1, w s = 100, w p = 10, w r = 0 for all dynamic scenes. Please refer to Tab. 1 for details of the sequences and timings of the individual steps measured on an Intel Core i7-3770 CPU with 3.4GHz (16GB Ram) and an Nvidia Geforce GTX 780. The listed preprocessing steps include: depth-to-color alignment, filtering of depth and color, resampling of images, and foreground segmentation. For those results, the Gauss-Newton optimizer ran with the following parameters: 3 hierarchy levels, N e = 10, 8, 6 outer iterations, and K = 5, 5, 5 PCG iterations (coarse-to-fine). We enable the temporal smoothness prior term for capturing static scenes, i.e., the Vase sequence, Lucy sequence and Socrates sequence. The required ICP alignment adds 3.5 ms for these three sequences to the total computation time. This yields effective frame rates between 15 fps at full HD and 93 fps at SVGA. Quantitative Evaluation We quantitatively evaluate the accuracy of our method on two synthetic sequences that are 400 frames long. We use ground truth, detailed performance-captured face geometry [Valgaerts et al. 2012], and the ground truth lighting from St. Peter?s Basilica [Debevec 1998] and render two RGB-D sequences. In the first one (CoA) the albedo is uniform, and in the second one (DA) we use a dense albedo map obtained from one of the captured face images. To synthesize the depth map sequences, we first obtain a quantized depth map from the stereo results of [Valgaerts et al. 2012], and then add Gaussian noise to mimic the noise from a depth sensor. We compare our method with the space-time multi-lateral RGBD filtering method of [Richardt et al. 2012] (STFilt), and with reconstructions of CoA and DA using the single-frame shading based refinement algorithm in the offline method of [Valgaerts et al. 2012](SBRol). As an error metric, we employ the average pixel-wise Euclidean distance in mm per frame (d e ), as well as the average angular difference of normals in degrees (d n ). The distance and normal errors averaged over all frames are summarized in Tab. 2. Compared to STFilt, our method produces results with much lower distance and normal errors, as it obtains metrically faithful reconstructions as opposed to only plausible results (see Fig. 8 ). In comparison to the more involved offline method by [Valgaerts et al. 2012], our results exhibit comparable distance error, but our real-time capability comes at the price of a slightly higher error in reconstructed normal orientation. The respective error curves over time on both DA ( Fig. 9 ) and CoA (additional material) further confirm the above conclusions. Qualitative Comparison We also compared our method with STFilt on real-world data (talking sequence, see Tab. 1). Using the same hardware as previously described, our approach not only has a runtime advantage (55.8 fps against 8.5 fps), but also produces much more detailed results (see Fig. 10 and video). Quantitative Evaluation We quantitatively evaluate the accuracy of our method on two synthetic sequences that are 400 frames long. We use ground truth, detailed performance-captured face geometry [Valgaerts et al. 2012], and the ground truth lighting from St. Peter?s Basilica [Debevec 1998] and render two RGB-D sequences. In the first one (CoA) the albedo is uniform, and in the second one (DA) we use a dense albedo map obtained from one of the captured face images. To synthesize the depth map sequences, we first obtain a quantized depth map from the stereo results of [Valgaerts et al. 2012], and then add Gaussian noise to mimic the noise from a depth sensor. We compare our method with the space-time multi-lateral RGBD filtering method of [Richardt et al. 2012] (STFilt), and with reconstructions of CoA and DA using the single-frame shading based refinement algorithm in the offline method of [Valgaerts et al. 2012](SBRol). As an error metric, we employ the average pixel-wise Euclidean distance in mm per frame (d e ), as well as the average angular difference of normals in degrees (d n ). The distance and normal errors averaged over all frames are summarized in Tab. 2. Compared to STFilt, our method produces results with much lower distance and normal errors, as it obtains metrically faithful reconstructions as opposed to only plausible results (see Fig. 8 ). In comparison to the more involved offline method by [Valgaerts et al. 2012], our results exhibit comparable distance error, but our real-time capability comes at the price of a slightly higher error in reconstructed normal orientation. The respective error curves over time on both DA ( Fig. 9 ) and CoA (additional material) further confirm the above conclusions. Qualitative Comparison We also compared our method with STFilt on real-world data (talking sequence, see Tab. 1). Using the same hardware as previously described, our approach not only has a runtime advantage (55.8 fps against 8.5 fps), but also produces much more detailed results (see Fig. 10 and video). Real-time 3D reconstruction We used our algorithm together with the real-time voxel-hashing-based hand-held scanning approach for depth cameras proposed by [Nie?ner et al. 2013]. With our drastically enhanced depth map quality, full 3D models with more detail can be reconstructed (voxel size of 0.5mm); see Fig. 1 and Fig. 11 . Deformable 3D reconstruction We also integrated our algorithm into the real-time deformable tracking approach by [Zollh?fer et al. 2014a], which leads to improved space-time coherent reconstructions of a non-rigidly deforming template, see Fig. 13 . Our approach enables a leap forward in real-time scene reconstruction with depth cameras, but is still subject to several well-known shape from shading limitations. For instance, texture-copy artifacts are introduced by high-frequency albedo changes, causing problems for both online and offline methods [Richardt et al. 2012; Han et al. 2013; Yu et al. 2013]. Our adaptive refinement (Sect. 4.2) is efficient to mitigate the visual presence of these artifacts. However, we still cannot completely prevent them; for instance see Fig. 4 and Fig. 12 (top left). Generally, we believe that the underconstrained nature of this problem will inspire future research directions. In contrast to some offline methods, our real-time constraint allows only for a simplified light transport model. That is, our initial constant albedo assumption may exacerbate texture copying on general scenes; however, our results show that in practice, very faithful surface reconstructions with spatially-varying albedo are feasible. Due to the second order spherical harmonics representation, non-diffuse surfaces are still challenging for our method. In addition, we are not able to improve depth maps around silhouettes since the normal is undefined. We further assume a one-bounce local illumination and ignore lighting visibility, which may lead to errors in some cases. For example, hard shadows may result in artificial detail around their boundaries. An interesting future direction would be the incorporation of a screen-space ambient occlusion term to account for local visibility. We presented the first method for real-time shading-based refinement of RGB-D data captured with commodity depth cameras in general uncontrolled scenes. This is enabled by a new real-time inverse rendering framework that approximates time-varying incident lighting as well as albedo in the scene. The algorithm then refines the raw depth of the camera by optimizing a complex non-linear energy using a new highly parallel Gauss-Newton solver on the GPU. The results are superior to previous online depth map enhancement algorithms, and on par with offline shape-from-shading approaches. Our experiments further show that the approach enables a new level of accuracy in handheld 3D scanning as well as deformable surface tracking.",
  "resources" : [ ]
}