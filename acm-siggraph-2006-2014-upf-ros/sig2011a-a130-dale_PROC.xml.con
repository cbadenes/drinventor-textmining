{
  "uri" : "sig2011a-a130-dale_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011a/a130-dale_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Video Face Replacement",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Mahmoud-Afifi",
      "name" : "Mahmoud",
      "surname" : "Afifi"
    }, {
      "uri" : "http://drinventor/Khaled F.-Hussain",
      "name" : "Khaled F.",
      "surname" : "Hussain"
    }, {
      "uri" : "http://drinventor/Hosny M.-Ibrahim",
      "name" : "Hosny M.",
      "surname" : "Ibrahim"
    }, {
      "uri" : "http://drinventor/Nagwa M.-Omar",
      "name" : "Nagwa M.",
      "surname" : "Omar"
    } ]
  },
  "bagOfWords" : [ "result", "we", "show", "result", "number", "different", "subject", "capture", "condition", "replacement", "scenario", "fig.", "show", "multi-take", "video", "montage", "example", "both", "shot", "outdoors", "handheld", "camera", "fig.", "show", "dub", "result", "translation", "scenario", "where", "source", "target", "depict", "same", "subject", "speaking", "different", "language", "source", "capture", "studio", "setting", "target", "capture", "outdoors", "Figs.", "show", "replacement", "result", "different", "source", "target", "subject", "notably", "different", "performance", "fig.", "10", "show", "retargeting", "result", "different", "subject", "where", "target", "use", "audiovisual", "guide", "source", "retime", "match", "target", "user", "interaction", "although", "majority", "we", "system", "automatic", "some", "user", "interaction", "require", "include", "place", "marker", "FaceGen", "adjust", "marker", "track", "initialization", "specify", "initial", "blending", "mask", "interaction", "FaceGen", "require", "2-3", "minute", "per", "subject", "track", "initialization", "perform", "less", "than", "minute", "all", "video", "use", "we", "result", "amount", "interaction", "here", "depend", "accuracy", "automatic", "face", "detection", "degree", "which", "subject?s", "expression", "viseme", "differ", "from", "closed-mouth", "neutral", "finally", "specify", "mask", "blend", "first", "frame", "every", "example", "take", "between", "30", "seconds", "minute", "any", "give", "result", "total", "interaction", "time", "therefore", "order", "few", "minute", "which", "significantly", "less", "than", "what", "would", "require", "use", "exist", "video", "compositing", "method", "comparison", "Vlasic", "et", "al.", "-lsb-", "2005", "-rsb-", "we", "reprocess", "original", "scan", "datum", "-lsb-", "Vlasic", "et", "al.", "2005", "-rsb-", "place", "correspondence", "face", "mesh", "cover", "full", "face", "include", "jaw", "do", "two", "reason", "first", "original", "model", "only", "cover", "interior", "face", "restricted", "we", "scenario", "where", "timing", "source", "target?s", "mouth", "motion", "must", "match", "exactly", "while", "case", "multi-take", "montage", "some", "dub", "scenario", "when", "speech", "same", "both", "source", "target", "video", "present", "problem", "other", "situation", "when", "motion", "target", "jaw", "source", "mouth", "do", "match", "situation", "change", "language", "during", "dub", "arbitrary", "face", "replacement", "full", "face", "model", "necessary", "so", "source?s", "jaw", "can", "also", "transfer", "-lrb-", "fig.", "-rrb-", "second", "we", "experience", "use", "original", "interior-only", "face", "model", "confirm", "earlier", "psychological", "study", "have", "conclude", "face", "shape", "one", "stronger", "cue", "identity", "when", "source", "target", "subject", "differ", "replace", "interior", "face", "always", "sufficient", "convey", "identity", "source", "subject", "particularly", "when", "source", "target", "face", "shape", "differ", "significantly", "Vlasic", "et", "al.", "face", "texture", "can", "come", "from", "either", "source", "target", "morphable", "model", "parameter", "can", "mixture", "source", "target", "when", "target", "texture", "use", "puppetry", "application", "blend", "warped", "texture", "relatively", "easy", "however", "expressiveness", "result", "stem", "exclusively", "from", "morphable", "model", "which", "limited", "lack", "detail", "nuance", "real", "facial", "performance", "video", "other", "hand", "take", "face", "texture", "from", "source", "make", "task", "blend", "far", "more", "difficult", "can", "see", "Fig.", "na?ve", "blending", "source", "face", "texture", "target", "use", "Vlasic", "et", "al.", "produce", "bleeding", "flicker", "artifact", "mitigate", "we", "seam", "finding", "blending", "method", "user", "study", "quantitatively", "objectively", "evaluate", "we", "system", "we", "run", "user", "study", "use", "Amazon?s", "mechanical", "Turk", "we", "test", "set", "consist", "24", "video", "10", "unmodified", "video", "10", "video", "replace", "face", "four", "additional", "video", "design", "verify", "subject", "be", "watch", "video", "simply", "click", "random", "response", "all", "video", "be", "present", "640", "360", "pixel", "five", "seconds", "disappear", "from", "page", "prevent", "subject", "from", "analyze", "final", "frame", "subject", "be", "inform", "video", "view", "either", "capture", "directly", "video", "camera", "manipulate", "computer", "program", "be", "ask", "respond", "statement", "video", "capture", "directly", "video", "camera", "choose", "response", "from", "five-point", "Likert", "scale", "strongly", "agree", "-lrb-", "-rrb-", "agree", "-lrb-", "-rrb-", "neither", "agree", "nor", "disagree", "-lrb-", "-rrb-", "disagree", "-lrb-", "-rrb-", "strongly", "disagree", "-lrb-", "-rrb-", "we", "collect", "40", "distinct", "opinion", "per", "video", "pay", "subject", "0.04", "per", "opinion", "per", "video", "additional", "four", "video", "begin", "similar", "footage", "rest", "instruct", "subject", "click", "specific", "response", "e.g.", "agree", "verify", "be", "pay", "attention", "subject", "who", "do", "respond", "instruct", "video", "be", "discard", "from", "study", "approximately", "20", "opinion", "per", "video", "remain", "after", "remove", "user", "average", "response", "face-replaced", "video", "4.1", "indicate", "subject", "believe", "video", "be", "capture", "directly", "camera", "be", "manipulate", "computer", "program", "average", "response", "authentic", "video", "4.3", "indicate", "slightly", "stronger", "belief", "video", "be", "capture", "camera", "none", "face-replaced", "video", "have", "median", "score", "below", "three", "video", "have", "median", "score", "result", "indicate", "we", "method", "can", "produce", "convincing", "video", "look", "similar", "those", "come", "directly", "from", "camera", "Limitations", "we", "approach", "without", "limitation", "-lrb-", "fig.", "-rrb-", "track", "base", "optical", "flow", "which", "require", "lighting", "change", "slowly", "over", "face", "high", "frequency", "lighting", "hard", "shadow", "must", "avoid", "ensure", "good", "tracking", "additionally", "method", "assume", "orthographic", "camera", "while", "estimation", "parameter", "more", "sophisticated", "camera", "model", "possible", "we", "use", "simple", "model", "shoot", "we", "input", "video", "longer", "focal", "length", "better", "approximate", "orthographic", "projection", "finally", "track", "often", "degrade", "beyond", "range", "pose", "outside", "45", "from", "frontal", "even", "successful", "tracking", "geometric", "fit", "can", "cause", "artifact", "final", "result", "example", "fit", "sometimes", "insufficient", "large", "pose", "difference", "between", "source", "target", "particularly", "noticeable", "nose", "area", "when", "example", "head", "significantly", "tilted", "downward", "cause", "nose", "distort", "slightly", "pose", "also", "constrain", "sufficiently", "similar", "between", "source", "target", "prevent", "occluded", "region", "source", "face", "from", "appear", "pose-transformed", "target", "frame", "case", "where", "we", "have", "control", "over", "source", "acquisition", "source", "subject", "can", "capture", "frontal", "pose", "we", "do", "here", "pose", "similar", "target", "both", "ensure", "occluded", "region", "however", "when", "exist", "footage", "use", "source", "necessary", "ensure", "compatible", "pose", "between", "source", "target", "issue", "could", "alleviate", "automatic", "user-assisted", "inpainting", "derive", "miss", "texture", "from", "spatially", "temporally", "adjacent", "pixel", "video", "sequence", "all", "example", "show", "here", "source", "target", "pair", "same", "gender", "approximate", "age", "thus", "roughly", "similar", "proportion", "any", "difference", "face", "shape", "can", "account", "single", "global", "scale", "ensure", "source", "face", "cover", "target", "vastly", "different", "face", "shape", "e.g.", "child", "adult", "may", "sufficient", "however", "plausible", "add", "2d", "warping", "step", "similar", "use", "-lsb-", "Jain", "et", "al.", "2010", "-rsb-", "warp", "target", "face", "nearby", "background", "match", "source", "before", "blend", "Lighting", "must", "also", "similar", "between", "source", "target", "multitake", "montage", "scenario", "where", "source", "target", "typically", "cap", "-lrb-", "-rrb-", "when", "tracking", "fail", "source", "content", "replacement", "distort", "see", "here", "after", "alignment", "-lrb-", "-rrb-", "significant", "difference", "lighting", "between", "source", "target", "lead", "unrealistic", "blended", "result", "where", "lighting", "right", "darker", "source", "face", "target", "environment", "ture", "close", "succession", "same", "setting", "condition", "trivially", "meet", "likewise", "when", "either", "source", "target", "capture", "studio", "setting", "full", "control", "over", "lighting", "setup", "condition", "can", "also", "meet", "same", "effort", "require", "plausible", "green", "screening", "however", "matching", "can", "difficult", "novice", "may", "impossible", "source", "target", "from", "exist", "footage", "finally", "seam", "finding", "blending", "can", "fail", "difficult", "input", "example", "when", "hair", "fall", "along", "forehead", "may", "seam", "generate", "natural", "blend", "between", "source", "target", "strong", "difference", "illuminations", "lead", "bleed", "artifact", "because", "sometimes", "possible", "seam", "avoid", "region", "fig.", "show", "some", "example", "where", "limitation", "manifest", "final", "result", "we", "have", "present", "system", "produce", "face", "replacement", "video", "require", "only", "single-camera", "video", "minimal", "user", "input", "robust", "under", "significant", "difference", "between", "source", "target", "we", "have", "show", "user", "study", "result", "generate", "method", "perceive", "realistic", "we", "method", "useful", "variety", "situation", "include", "multi-take", "montage", "dub", "retargeting", "face", "replacement", "future", "improvement", "inpaint", "occlusion", "during", "large", "pose", "variation", "2d", "background", "warping", "vastly", "different", "face", "shape", "lighting", "transfer", "between", "source", "target", "make", "approach", "applicable", "even", "broader", "range", "scenario" ],
  "content" : "Results We show results for a number of different subjects, capture conditions, and replacement scenarios. Fig. 6 shows multi-take video montage examples, both shot outdoors with a handheld camera. Fig. 7 shows dubbing results of a translation scenario, where the source and target depict the same subject speaking in different languages, with source captured in a studio setting and target captured outdoors. Figs. 9 shows a replacement result with different source and target subjects and notably different performances. Fig. 10 shows a retargeting result with different subjects, where the target was used as an audiovisual guide and the source retimed to match the target. User interaction Although the majority of our system is automatic, some user interaction is required. This includes placing markers in FaceGen, adjusting markers for tracking initialization, and specifying the initial blending mask. Interaction in FaceGen required 2-3 minutes per subject. Tracking initialization was performed in less than a minute for all videos used in our results; the amount of interaction here depends on the accuracy of the automatic face detection and the degree to which the subject?s expression and viseme differ from closed-mouth neutral. Finally, specifying the mask for blending in the first frame of every example took between 30 seconds and 1 minute. For any given result, total interaction time is therefore on the order of a few minutes, which is significantly less than what would be required using existing video compositing methods. Comparison with Vlasic et al. [2005] We reprocessed the original scan data [Vlasic et al. 2005] to place it into correspondence with a face mesh that covers the full face, including the jaw. This was done for two reasons. First, the original model only covered the interior of the face; this restricted us to scenarios where the timing of the source and target?s mouth motion must match exactly. While this is the case for multi-take montage and some dubbing scenarios when the speech is the same in both source and target videos, it presents a problem for other situations when the motion of the target jaw and source mouth do not match. For these situations ? changing the language during dubbing or in arbitrary face replacements ? a full face model is necessary so that the source?s jaw can also be transferred ( Fig. 8 a). Second, our experience using the original interior-only face model confirmed earlier psychological studies that had concluded that face shape is one of the stronger cues for identity. When source and target subjects differ, replacing the interior of the face was not always sufficient to convey the identity of the source subject, particularly when source and target face shapes differ significantly. In Vlasic et al., face texture can come from either the source or the target, and morphable model parameters can be a mixture of source and target. When the target texture is used, as in their puppetry application, blending the warped texture is relatively easy. However, the expressiveness of the result stems exclusively from the morphable model, which is limited and lacks the detail and nuances of real facial performances in video. On the other hand, taking face texture from the source makes the task of blending far more difficult; as can be seen in Fig. 5 , the na?ve blending of source face texture into the target used in Vlasic et al. produces bleeding and flickering artifacts that are mitigated with our seam finding and blending method. User study To quantitatively and objectively evaluate our system, we ran a user study using Amazon?s Mechanical Turk. Our test set consisted of 24 videos: 10 unmodified videos, 10 videos with replaced faces, and four additional videos designed to verify that the subjects were watching the videos and not simply clicking on random responses. All videos were presented at 640 ? 360 pixels for five seconds and then disappeared from the page to prevent the subject from analyzing the final frame. The subjects were informed that the video they viewed was either ?captured directly by a video camera? or ?manipulated by a computer program. ? They were asked to respond to the statement ?This video was captured directly by a video camera? by choosing a response from a five-point Likert scale: strongly agree (5), agree (4), neither agree nor disagree (3), disagree (2), or strongly disagree (1). We collected 40 distinct opinions per video and paid the subjects $0.04 per opinion per video. The additional four videos began with similar footage as the rest but then instructed the subjects to click a specific response, e.g., ?agree?, to verify that they were paying attention. Subjects who did not respond as instructed to these videos were discarded from the study. Approximately 20 opinions per video remained after removing these users. The average response for the face-replaced videos was 4.1, indicating that the subjects believed the videos were captured directly by a camera and were not manipulated by a computer program. The average response for the authentic videos was 4.3, indicating a slightly stronger belief that the videos were captured by a camera. None of the face-replaced videos had a median score below 4 and three of the videos had a median score of 5. These results indicate that our method can produce convincing videos that look similar to those coming directly from a camera. Limitations Our approach is not without limitations ( Fig. 8 ). Tracking is based on optical flow, which requires that the lighting change slowly over the face. High frequency lighting, such as hard shadows, must be avoided to ensure good tracking. Additionally, the method assumes an orthographic camera; while estimation of parameters of a more sophisticated camera model is possible, we use the simple model and shot our input videos with longer focal lengths that better approximate an orthographic projection. Finally, tracking often degrades beyond the range of poses outside ?45 o from frontal. Even with successful tracking, the geometric fit can cause artifacts in the final result. For example, the fit is sometimes insufficient for the large pose differences between source and target. This is particularly noticeable in the nose area when, for example, the head is significantly tilted downwards, causing the nose to distort slightly. Pose is also constrained to be sufficiently similar between source and target to prevent occluded regions in the source face from appearing in the pose-transformed target frame. For cases where we have control over source acquisition, the source subject can be captured in a frontal pose as we do here, or in a pose similar to the target, both ensuring no occluded regions. However when existing footage is used as the source, it is necessary to ensure compatible pose between source and target. This issue could be alleviated by automatic or user-assisted inpainting that derives the missing texture from spatially and temporally adjacent pixels in the video sequence. In all examples shown here, source / target pairs are of the same gender and approximate age and thus of roughly similar proportions. Any difference in face shape can be accounted for by a single global scale to ensure the source face covers the target. For vastly different face shape, e.g., a child and adult, this may not be sufficient. However it is plausible to add a 2D warping step, similar to that used in [Jain et al. 2010], that warps the target face and nearby background to match the source before blending. Lighting must also be similar between source and target. For multitake montage scenarios, where source and target are typically cap- (b) When the tracking fails, the source content for replacement is distorted, seen here after alignment. (c) Significant differences in lighting between source and target lead to an unrealistic blended result, where the lighting on the right is darker on the source face but not in the target environment. tured in close succession in the same setting, this condition is trivially met. Likewise, when either the source or target is captured in a studio setting, with full control over the lighting setup, this condition can also be met with the same efforts required for plausible green screening. However such matching can be difficult for novices or may be impossible if the source and target are from existing footage. Finally, seam finding and blending can fail for difficult inputs. For example, when hair falls along the forehead, there may be no seam that generates a natural blend between source and target. Strong differences in illuminations will lead to bleeding artifacts because it sometimes is not possible for the seam to avoid such regions. Fig. 8 shows some examples where these limitations are manifested in the final result. We have presented a system for producing face replacements in video that requires only single-camera video and minimal user input and is robust under significant differences between source and  target. We have shown with a user study that results generated with this method are perceived as realistic. Our method is useful in a variety of situations, including multi-take montage, dubbing, retargeting, and face replacement. Future improvements such as inpainting for occlusions during large pose variations, 2D background warping for vastly different face shapes, and lighting transfer between source and target will make this approach applicable to an even broader range of scenarios.",
  "resources" : [ ]
}