{
  "uri" : "sig2014a-a232-liu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014a/a232-liu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Fast Burst Images Denoising",
    "published" : null,
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ ]
  },
  "bagOfWords" : [ "paper", "present", "fast", "denoising", "method", "produce", "clean", "image", "from", "burst", "noisy", "image", "we", "propose", "fast", "noise", "reduction", "method", "produce", "clean", "image", "from", "burst", "image", "high", "speed", "we", "method", "enable", "introduce", "three", "accelerate", "step", "first", "step", "we", "use", "lightweight", "parametric", "motion", "representation", "homog", "raphy", "flow", "model", "motion", "cause", "camera", "movement", "since", "estimate", "homography", "flow", "only", "require", "spare", "feature", "matching", "step", "both", "efficient", "robust", "noise", "select", "consistent", "pixel", "use", "we", "temporal", "pixel", "fusion", "-lrb-", "third", "step", "-rrb-", "average", "we", "have", "evaluate", "we", "algorithm", "variety", "real", "datum", "presence", "moderate", "strong", "noise", "we", "algorithm", "perform", "par", "state-of-the-art", "multi-image", "denoising", "method", "-lrb-", "e.g.", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-rrb-", "noisebrush", "-lsb-", "Chen", "et", "al.", "2009", "-rsb-", "provide", "interactive", "way", "further", "quality", "improvement", "Patch", "matching", "more", "robust", "noise", "have", "be", "widely", "use", "multiple", "image", "processing", "-lsb-", "Tico", "2008", "Buades", "et", "al.", "2009", "Zhang", "et", "al.", "2009", "Maggioni", "et", "al.", "2013", "Sen", "et", "al.", "2012", "Kalantari", "et", "al.", "2013", "-rsb-", "recent", "work", "-lsb-", "Grundmann", "et", "al.", "2012", "Liu", "et", "al.", "2013", "-rsb-", "demonstrate", "success", "use", "spatially-variant", "homography", "camera", "motion", "we", "be", "inspire", "idea", "extend", "image", "denoising", "we", "use", "pyramid-based", "pixel", "fusion", "method", "improve", "result", "quality", "figure", "we", "algorithm", "pipeline", "we", "estimate", "homography", "flow", "-lrb-", "section", "3.1", "-rrb-", "represent", "motion", "-lrb-", "camera", "-rrb-", "between", "reference", "frame", "any", "other", "frame", "finally", "we", "apply", "pixel", "fusion", "-lrb-", "section", "3.3", "-rrb-", "aggregate", "consistent", "pixel", "all", "scale", "produce", "final", "result", "pyramid", "homography", "graph", "we", "represent", "motion", "between", "two", "frame", "through", "pyramid", "homography", "graph", "show", "Figure", "-lrb-", "-rrb-", "next", "we", "introduce", "coarse-to-fine", "optimization", "robustly", "obtain", "accurate", "result", "optimization", "we", "perform", "level-by-level", "optimization", "start", "from", "global", "homography", "coarsest", "level", "-lrb-", "-rrb-", "let", "homography", "node", "level", "-lcb-", "-rcb-", "its", "neighbor", "homography", "same", "level", "we", "estimate", "-lcb-", "-rcb-", "minimize", "follow", "energy", "function", "-lcb-", "-rcb-", "arg", "min", "-lcb-", "-rcb-", "-lrb-", "-rrb-", "where", "-lrb-", "default", "0.1", "-rrb-", "control", "amount", "spatial", "regularization", "enforce", "second", "smoothness", "term", "first", "datum", "term", "best", "-lrb-", "-rrb-", "select", "from", "two", "candidate", "one", "its", "parent", "homography", "level", "other", "its", "own", "estimate", "homography", "-lrb-", "use", "all", "match", "feature", "within", "grid", "node", "-rrb-", "we", "implementation", "we", "pick", "feature", "number", "grid", "insufficient", "-lrb-", "-rrb-", "rigidness", "-lsb-", "Hartley", "Zisserman", "2003", "-rsb-", "too", "weak", "otherwise", "we", "choose", "best", "model", "which", "have", "lower", "matching", "error", "all", "feature", "within", "grid", "because", "objective", "function", "-lrb-", "-rrb-", "quadratic", "we", "can", "obtain", "global", "optimum", "Jacobi", "solver", "-lsb-", "bronshtein", "Semendyayev", "1997", "-rsb-", "form", "we", "motion", "model", "similar", "mesh-based", "homography", "-lsb-", "Liu", "et", "al.", "2013", "-rsb-", "we", "coarse-to-fine", "optimization", "more", "efficient", "500", "feature", "we", "method", "take", "m", "per", "frame", "while", "mesh-based", "homography", "require", "50", "m", "per", "frame", "homography", "flow", "parameter", "empirically", "set", "fix", "all", "experiment", "show", "Figure", "-lrb-", "-rrb-", "we", "compute", "translation", "vector", "map", "each", "pixel", "from", "one", "frame", "another", "frame", "accord", "homography", "graph", "finally", "estimate", "homography", "flow", "scale", "accordingly", "round", "off", "use", "other", "scale", "algorithm", "validation", "we", "evaluate", "global", "homography", "optical", "flow", "patch", "match", "we", "homography", "flow", "set", "burst", "image", "we", "ask", "four", "different", "subject", "capture", "20", "set", "clean", "burst", "image", "-lrb-", "low", "iso", "under", "good", "lighting", "condition", "-rrb-", "we", "add", "gaussian", "noise", "different", "standard", "deviation", "-lrb-", "from", "20", "60", "-rrb-", "synthesize", "100", "set", "noisy", "burst", "image", "register", "noisy", "image", "we", "compare", "six", "algorithm", "global", "homography", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "global", "homography", "optical", "flow", "patch", "match", "-lrb-", "exhaustive", "search", "-rrb-", "global", "homography", "patch", "match", "we", "homography", "flow", "we", "compute", "psnr", "measure", "difference", "between", "registered", "clean", "image", "pair", "Figure", "-lrb-", "-rrb-", "show", "result", "from", "result", "we", "can", "observe", "two", "optical", "flow", "base", "method", "perform", "well", "when", "noise", "level", "small", "-lrb-", "20", "-rrb-", "when", "noise", "level", "increase", "degrade", "more", "quickly", "than", "other", "global", "homography", "can", "improve", "patch", "match", "optical", "flow", "we", "believe", "reason", "coarse-to-fine", "mechanism", "optical", "flow", "have", "already", "handle", "global", "motion", "we", "homography", "flow", "consistently", "best", "all", "noise", "level", "Figure", "-lrb-", "-rrb-", "further", "show", "run", "time", "method", "various", "image", "size", "since", "global", "homography", "we", "homography", "flow", "only", "rely", "sparse", "feature", "detection", "matching", "both", "outperform", "patch", "match", "-lrb-", "exhausive", "search", "even", "randomize", "search", "-lsb-", "Barnes", "et", "al.", "2009", "-rsb-", "-rrb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "speed", "only", "small", "margin", "between", "two", "run", "time", "curve", "-lrb-", "global", "homography", "we", "homography", "flow", "-rrb-", "which", "indicate", "we", "pyramid", "optimization", "very", "efficient", "efficient", "implementation", "bottleneck", "step", "sparse", "feature", "extraction", "matching", "we", "implementation", "we", "work", "coarse", "scale", "-lrb-", "-rrb-", "pyramid", "luminance", "channel", "efficiency", "robustness", "-lrb-", "noise", "-rrb-", "specially", "we", "use", "Harris", "corner", "detection", "-lsb-", "Harris", "Stephens", "1988", "-rsb-", "128-bit", "brief", "descriptor", "-lsb-", "calonder", "et", "al.", "2010", "-rsb-", "which", "can", "achieve", "real-time", "performance", "even", "mobile", "phone", "we", "reject", "incorrectly", "match", "feature", "use", "local", "ransac", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "addition", "we", "estimate", "homography", "flow", "each", "non-overlapped", "block", "-lrb-", "pixel", "-rrb-", "instead", "per", "pixel", "all", "pixel", "each", "block", "share", "same", "translation", "vector", "which", "compute", "map", "block", "center", "between", "two", "frame", "approximation", "only", "slightly", "scarify", "quality", "-lrb-", "nearly", "0.01", "db", "psnr", "-rrb-", "accelerate", "pixel", "map", "2.5", "time", "consistent", "pixel", "handle", "scene", "motion", "we", "borrow", "simple", "idea", "from", "hdr", "deghosting", "-lsb-", "Granados", "et", "al.", "2013", "-rsb-", "avoid", "complex", "motion", "tracking", "two", "method", "which", "separately", "satisfy", "one", "two", "goal", "other", "median-based", "we", "collect", "pixel", "consistent", "median", "-lrb-", "below", "same", "threshold", "-rrb-", "all", "pixel", "profile", "reference", "method", "can", "guarantee", "ghost-free", "result", "pixel", "dynamic", "object", "we", "may", "get", "insufficient", "sample", "denoising", "-lrb-", "show", "Figure", "-lrb-", "-rrb-", "-rrb-", "median", "method", "collect", "more", "consistent", "pixel", "might", "lead", "ghost", "because", "median", "may", "happen", "color", "move", "object", "-lrb-", "show", "Figure", "-lrb-", "-rrb-", "-rrb-", "combine", "strategy", "we", "propose", "simple", "combination", "strategy", "both", "method", "each", "pixel", "-lrb-", "reference", "frame", "-rrb-", "we", "compute", "two", "set", "consistent", "pixel", "-lcb-", "-rcb-", "separately", "median", "reference", "method", "-lrb-", "-rrb-", "record", "frame", "index", "consistent", "pixel", "every", "pixel", "reference", "frame", "belong", "we", "take", "union", "final", "result", "because", "both", "method", "agree", "otherwise", "we", "choose", "median", "result", "reliable", "-lrb-", "i.e.", "size", "more", "than", "half", "frame", "number", "-rrb-", "choose", "reference", "result", "unreliable", "further", "reduce", "chance", "ghost", "-lrb-", "enforce", "spatial", "coherence", "-rrb-", "we", "do", "measure", "reliability", "pixel", "pixel", "instead", "we", "find", "all", "connectedcomponent", "undecided", "pixel", "-lrb-", "where", "reference", "frame", "do", "belong", "-rrb-", "determine", "reliability", "each", "connect", "component", "whole", "majority", "voting", "after", "combination", "we", "obtain", "set", "consistent", "pixel", "all", "pixel", "further", "make", "combination", "seamless", "we", "run", "morphological", "-lrb-", "majority", "-rrb-", "filter", "-lsb-", "Gonzalez", "Woods", "2007", "-rsb-", "stack", "index", "consistent", "pixel", "frame-by-frame", "Figure", "-lrb-", "a-b", "-rrb-", "show", "two", "real", "example", "map", "which", "record", "number", "consistent", "pixel", "every", "pixel", "location", "efficient", "implementation", "consistent", "pixel", "also", "select", "coarse", "scale", "-lrb-", "i.e.", "-rrb-", "pyramid", "purpose", "enable", "fast", "computation", "detect", "motion", "relatively", "clean", "scale", "approximation", "could", "achieve", "98.7", "outlier", "detection", "rate", "operate", "original", "scale", "expense", "half", "time", "other", "scale", "just", "reuse", "index", "computed", "consistent", "pixel", "upsampling", "downsampling", "both", "median", "reference", "base", "method", "we", "use", "patch", "-lrb-", "-rrb-", "difference", "instead", "single", "pixel", "difference", "use", "threshold", "10", "we", "use", "integral", "image", "-lsb-", "Viola", "Jones", "2001", "-rsb-", "compute", "patch", "difference", "more", "efficiently", "give", "consistent", "pixel", "each", "pixel", "all", "scale", "we", "fuse", "all", "they", "temporal", "multi-scale", "fusion", "we", "keep", "we", "fusion", "simple", "possible", "while", "be", "able", "significantly", "denoise", "temporal", "fusion", "suppose", "-lcb-", "-rcb-", "consistent", "pixel", "pixel", "location", "-lrb-", "certain", "scale", "-rrb-", "where", "pixel", "color", "from", "tth", "frame", "we", "compute", "fusion", "result", "linear", "minimum", "mean", "square", "error", "-lrb-", "lmmse", "-rrb-", "estimator", "which", "widely", "use", "previous", "work", "-lrb-", "e.g.", "-lsb-", "Zhang", "Wu", "2005", "-rsb-", "-rrb-", "optimal", "unbiased", "denoising", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "mean", "all", "consistent", "pixel", "-lcb-", "-rcb-", "variance", "true", "pixel", "approximate", "max", "-lrb-", "-rrb-", "standard", "deviation", "-lcb-", "-rcb-", "noise", "LMMSE", "estimator", "can", "help", "we", "adaptively", "handle", "outlier", "some", "severely", "misalign", "pixel", "occur", "discontinuity", "depths", "-lrb-", "we", "homography", "flow", "more", "suitable", "spatially", "smooth", "depth", "variation", "-rrb-", "subtle", "residual", "move", "pixel", "fine", "scale", "-lrb-", "we", "scene", "motion", "detection", "perform", "coarse", "scale", "-rrb-", "wold", "make", "variance", "-lcb-", "-rcb-", "much", "larger", "than", "noise", "variance", "thus", "we", "fusion", "result", "would", "-lrb-", "denoising", "-rrb-", "otherwise", "result", "have", "value", "close", "mean", "-lcb-", "-rcb-", "temporal", "fusion", "run", "independently", "every", "scale", "next", "we", "describe", "how", "aggregate", "result", "all", "scale", "multi-scale", "fusion", "we", "aggregate", "result", "from", "top", "bottom", "point-wise", "manner", "let", "temporal", "fusion", "result", "two", "adjacent", "scale", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "bilinear", "upscale", "operator", "m/n", "adaptive", "fusion", "weight", "larger", "means", "we", "have", "more", "consistent", "pixel", "current", "scale", "we", "should", "trust", "current", "estimation", "more", "otherwise", "we", "should", "borrow", "more", "from", "parent", "scale", "here", "we", "replace", "temporal", "fusion", "result", "equation", "-lrb-", "-rrb-", "very", "fast", "filter", "spatial", "domain", "tex", "-lrb-", "-rrb-", "-lrb-", "tex", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "filter", "operator", "-lrb-", "-rrb-", "directional", "spatial", "pixel", "fusion", "we", "find", "all", "spatially", "consistent", "pixel", "along", "most", "probable", "since", "we", "detect", "motion", "coarser", "relatively", "clean", "scale", "we", "can", "use", "constant", "threshold", "instead", "adaptive", "threshold", "which", "may", "require", "complex", "noise", "modeling", "we", "find", "work", "well", "we", "experiment", "lmmse", "estimator", "describe", "equation", "-lrb-", "-rrb-", "use", "pixel", "efficiency", "we", "only", "apply", "spatial", "fusion", "texture", "pixel", "-lrb-", "tex", "0.01", "-rrb-", "efficiency", "we", "estimate", "compute", "standard", "deviation", "pixel", "difference", "between", "median", "image", "reference", "image", "within", "flat", "-lrb-", "non-textured", "-rrb-", "area", "area", "median", "image", "-lrb-", "generate", "coarse", "scale", "-rrb-", "good", "approximation", "clean", "version", "reference", "image", "real", "noise", "we", "use", "approach", "similar", "-lsb-", "Liu", "et", "al.", "2008", "-rsb-", "divide", "illuminance", "10", "discrete", "bin", "estimate", "corresponding", "each", "bin", "Extension", "patch", "idea", "combine", "temporal", "multiscale", "fusion", "can", "also", "extend", "patch", "level", "better", "denoising", "quality", "algorithm", "validation", "we", "perform", "quantitative", "evaluation", "synthetic", "datum", "set", "ground-truth", "clean", "image", "come", "from", "68", "image", "bsd300", "-lsb-", "Martin", "et", "al.", "2001", "-rsb-", "we", "add", "gaussian", "noise", "various", "noise", "level", "-lrb-", "20", "60", "-rrb-", "note", "we", "synthetic", "datum", "set", "ignore", "many", "key", "factor", "real", "datum", "parallax", "non-gaussian", "noise", "non-rigid", "object", "motion", "ignore", "key", "factor", "may", "lead", "incomplete", "conclusion", "however", "we", "still", "provide", "preliminary", "evaluation", "here", "reference", "help", "we", "gain", "better", "understanding", "Figure", "-lrb-", "-rrb-", "show", "average", "psnr", "average", "-lrb-", "baseline", "-rrb-", "optical", "flow", "median", "filter", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "we", "method", "-lrb-", "pixel", "fusion", "-rrb-", "we", "method", "-lrb-", "patch", "fusion", "-rrb-", "we", "apply", "same", "global", "homography", "all", "method", "better", "result", "overall", "we", "method", "-lrb-", "pixel", "fusion", "-rrb-", "perform", "comparably", "vbm3d", "bm4d", "-lrb-", "two", "state-of-the-art", "denoising", "method", "-rrb-", "better", "than", "average", "optical", "flow", "all", "noise", "level", "when", "noise", "level", "increase", "we", "method", "perform", "slightly", "worse", "than", "bm4d", "vbm3d", "drop", "more", "quickly", "than", "ours", "evaluation", "partially", "demonstrate", "true", "power", "we", "method", "presence", "real", "camera", "motion", "registration", "error", "keep", "mind", "we", "method", "2-3", "order", "magnitude", "faster", "than", "vbm3d", "optical", "flow", "bm4d", "Figure", "-lrb-", "-rrb-", "show", "run", "time", "method", "different", "image", "size", "addition", "we", "can", "achieve", "best", "result", "extend", "we", "fusion", "patch", "level", "compare", "two", "patch-based", "method", "-lrb-", "vbm3d", "bm4d", "-rrb-", "we", "patch-based", "fusion", "still", "efficient", "-lrb-", "1-2", "order", "magnitude", "faster", "-rrb-", "Figure", "also", "demonstrate", "patch", "perform", "better", "both", "temporal", "multi-scale", "fusion", "than", "pixel", "more", "interestingly", "multiscale", "fusion", "complementary", "temporal", "pixel", "fusion", "play", "important", "role", "we", "pixel-based", "method", "help", "greatly", "reduce", "gap", "between", "we", "pixel-based", "method", "we", "patch-based", "method", "we", "acquire", "20", "set", "burst", "image", "various", "content", "camera", "include", "mobile", "phone", "dslr", "camera", "compact", "camera", "each", "burst", "contain", "10", "shot", "all", "we", "result", "generate", "set", "fix", "parameter", "pixel-based", "fusion", "all", "original", "sequence", "more", "result", "provide", "we", "webpage", "we", "compare", "we", "method", "three", "point-wise", "method", "-lrb-", "spatialtemporal", "filter", "-lsb-", "Bennett", "McMillan", "2005", "-rsb-", "lucky", "imaging", "-lsb-", "Joshi", "Cohen", "2010", "-rsb-", "optical", "flow", "-lsb-", "Liu", "2009", "-rsb-", "temporally", "median", "filter", "-rrb-", "two", "state-of-the-art", "patch-based", "method", "-lrb-", "vbm3d", "-lsb-", "Dabov", "et", "al.", "2007a", "-rsb-", "bm4d", "-lsb-", "Maggioni", "et", "al.", "2013", "-rsb-", "-rrb-", "former", "two", "base", "we", "own", "implementation", "optical", "flow", "latter", "two", "from", "author", "all", "method", "we", "apply", "same", "global", "homography", "estimate", "we", "method", "help", "they", "obtain", "more", "reliable", "correspondence", "since", "some", "algorithm", "require", "known", "noise", "variance", "we", "try", "all", "possible", "noise", "level", "choose", "result", "best", "visual", "quality", "through", "balanced", "tradeoff", "between", "detail", "recovery", "noise", "reduction", "static", "scene", "example", "Figure", "capture", "HTC", "802d", "Android", "phone", "motion", "mainly", "cause", "camera", "movement", "we", "can", "see", "spatial-temporal", "filter", "vbm3d", "bm4d", "still", "leave", "certain", "noise", "flat", "region", "-lrb-", "e.g.", "sky", "area", "-rrb-", "building", "structure", "-lrb-", "Figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "well", "restore", "vbm3d", "bm4d", "filter", "patch", "matching", "have", "risk", "find", "mismatched", "correspondence", "which", "eventually", "lead", "undesired", "result", "overall", "result", "optical", "flow", "lucky", "imaging", "ours", "comparable", "we", "result", "be", "slightly", "cleaner", "portrait", "small", "motion", "typical", "scenario", "take", "portrait", "photo", "dark", "lighting", "example", "Figure", "record", "JVC", "gc-px10", "camera", "spatial-temporal", "filter", "lucky", "imaging", "optical", "flow", "method", "produce", "staircase", "artifact", "around", "eye", "-lrb-", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "because", "small", "non-rigid", "motion", "subject", "vbm3d", "bm4d", "do", "have", "issue", "blur", "fine", "detail", "scarf", "-lrb-", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "we", "result", "achieve", "best", "both", "kind", "region", "complex", "scene", "motion", "Figure", "10", "Figure", "11", "show", "two", "case", "complex", "dynamic", "scene", "first", "example", "capture", "Cannon", "EOS", "500d", "iso", "6400", "its", "noise", "level", "relatively", "low", "later", "example", "obtain", "Nokia", "lumia920", "we", "can", "see", "from", "both", "example", "lucky", "imaging", "optical", "flow", "-lrb-", "median", "filter", "-rrb-", "base", "result", "contain", "noticeable", "ghost", "while", "vbm3d", "result", "over-smoothed", "bm4d", "better", "than", "former", "two", "method", "still", "leave", "certain", "amount", "chrominance", "noise", "pattern", "background", "we", "solution", "we", "can", "automatically", "choose", "pixel", "color", "consistent", "reference", "frame", "dynamic", "region", "collect", "more", "consistent", "pixel", "static", "region", "-lrb-", "e.g.", "cloth", "door", "-rrb-", "result", "we", "result", "strike", "best", "balance", "among", "remove", "noise", "reconstruct", "fine", "detail", "avoid", "ghost", "handle", "motion", "blur", "during", "capture", "some", "individual", "frame", "-lrb-", "e.g.", "10", "30", "total", "frame", "-rrb-", "may", "blurry", "due", "sudden", "camera", "shake", "object", "motion", "Figure", "12", "show", "example", "capture", "iPhone", "5", "we", "examine", "what", "would", "happen", "blurry", "frame", "be", "select", "reference", "frame", "know", "we", "separately", "select", "frame", "-lrb-", "sharp", "-rrb-", "frame", "-lrb-", "blurry", "-rrb-", "reference", "frame", "generate", "two", "result", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "show", "we", "method", "insensitive", "selection", "because", "we", "method", "capable", "find", "consistent", "pixel", "from", "majority", "frame", "similar", "method", "propose", "video", "deblurring", "-lsb-", "Cho", "et", "al.", "2012", "-rsb-", "which", "find", "similar", "patch", "from", "sharp", "frame", "deblurr", "handle", "extreme", "low", "light", "Figure", "13", "sequence", "capture", "iPhone", "4", "under", "extreme", "low-light", "condition", "since", "input", "extremely", "dark", "we", "preprocess", "input", "boost", "brightness", "-lrb-", "apply", "shadow/highlight", "adjustment", "-rrb-", "while", "noise", "after", "boost", "very", "strong", "we", "method", "still", "manage", "produce", "clean", "image", "fine", "detail", "-lrb-", "thin", "wire", "air", "steel", "tower", "left", "-rrb-", "handle", "large", "occlusion", "Figure", "14", "show", "sequence", "large", "fast", "move", "foreground", "-lrb-", "person", "-rrb-", "consistent", "pixel", "map", "figure", "reveal", "how", "mechanism", "we", "algorithm", "can", "reliably", "deal", "-lrb-", "fast", "-rrb-", "large", "occlusion", "we", "run", "we", "method", "Intel", "i5", "3.2", "GHZ", "machine", "16g", "RAM", "we", "unoptimized", "C++", "implementation", "-lrb-", "single", "core", "sse", "simd", "acceleration", "-rrb-", "take", "920", "m", "average", "process", "10", "frame", "5mpixel", "image", "specifically", "we", "method", "take", "82m", "177m", "51m", "30m", "253m", "328m", "build", "pyramid", "extract", "match", "sparse", "feature", "estimate", "homography", "flow", "select", "consistent", "pixel", "run", "temporal", "fusion", "execute", "multi-scale", "fusion", "we", "prototype", "smartphone", "-lrb-", "Nokia", "Lumia", "920", "-rrb-", "cost", "about", "4.7", "seconds", "average", "without", "use", "multi-core", "neon", "instruction", "GPU", "acceleration", "we", "solution", "mainly", "base", "point-wise", "operation", "we", "expect", "can", "significantly", "accelerate", "Table", "further", "demonstrate", "processing", "time", "different", "method", "same", "machine", "figure", "10", "11", "image", "burst", "we", "expect", "camera", "motion", "from", "hand", "shake", "small/moderate", "motion", "main", "subject", "-lrb-", "-rrb-", "we", "method", "design", "handle", "dramatic", "motion", "-lrb-", "e.g.", "sport", "-rrb-", "denoi", "ing", "general", "video", "when", "motion", "between", "two", "frame", "can", "well", "represent", "we", "homography", "flow", "scene", "transition", "fast", "camera", "panning", "even", "non-rigid", "deformation", "-lrb-", "e.g.", "water", "wave", "motion", "flag", "wave", "-rrb-", "we", "approach", "break", "besides", "two", "case", "which", "we", "consistent", "pixel", "selection", "may", "fail", "first", "case", "when", "motion", "blur", "cause", "dynamic", "object", "appear", "majority", "frame", "-lrb-", "more", "than", "half", "all", "frame", "-rrb-", "dynamic", "region", "we", "pixel", "selection", "would", "automatically", "choose", "reference-based", "strategy", "reference", "frame", "contain", "blur", "we", "result", "would", "retain", "blur", "effect", "sharp", "frame", "choose", "reference", "issue", "can", "avoid", "Figure", "15", "-lrb-", "-rrb-", "show", "example", "therefore", "we", "need", "better", "strategy", "select", "reference", "frame", "besides", "fast", "move", "object", "would", "automatically", "remove", "median-based", "strategy", "aggressive", "denoising", "avoid", "issue", "we", "may", "provide", "another", "option", "allow", "user", "choose", "reference", "frame", "constrain", "reference", "region", "pixel", "selection", "second", "case", "when", "different", "move", "object", "background", "may", "have", "similar", "color", "same", "pixel", "location", "we", "pixel", "selection", "algorithm", "rely", "per-pixel", "color", "difference", "which", "too", "weak", "distinguish", "object", "Figure", "15", "-lrb-", "-rrb-", "show", "example", "different", "move", "person", "have", "very", "similar", "color", "region", "ambiguous", "region", "-lrb-", "indicate", "highlight", "box", "-rrb-", "occur", "majority", "frame", "-lrb-", "i.e.", "more", "than", "half", "all", "frame", "-rrb-", "finally", "lead", "ghost", "because", "we", "selection", "wrongly", "regard", "background", "issue", "also", "appear", "ghost-free", "hdr", "reconstruction", "-lsb-", "Granados", "et", "al.", "2013", "-rsb-", "which", "require", "interaction", "exclude", "ambiguous", "region", "despite", "above", "issue", "we", "believe", "we", "highly", "efficient", "solution", "practical", "enough", "deploy", "improve", "photo", "experience", "user", "broad", "range", "lighting", "condition", "we", "thank", "all", "reviewer", "helpful", "discussion", "Steve", "Lin", "Jiangyu", "Liu", "help", "proofread", "input", "burst", "image", "we", "result", "-lrb-", "-rrb-", "motion", "blur", "dynamic", "object", "-lrb-", "majority", "frame", "-rrb-", "-lrb-", "-rrb-", "ambiguous", "region", "-lrb-", "similar", "color", "-rrb-", "from", "different", "move", "object" ],
  "content" : "This paper presents a fast denoising method that produces a clean image from a burst of noisy images. We propose a fast noise reduction method that produces a clean image from a burst of images. The high speed of our method is enabled by introducing three accelerating steps. In the first step, we use a lightweight, parametric motion representation homog- raphy flow to model the motion caused by camera movements. Since estimating homography flow only requires spare feature matching, this step is both efficient and robust to noise. These selected consistent pixels are used in our temporal pixel fusion (in the third step) by averaging. We have evaluated our algorithm on a variety of real data. In the presence of moderate or strong noise, our algorithm performs on par with state-of-the-art multi-image denoising methods (e.g., VBM3D [Dabov et al. 2007a], BM4D [Maggioni et al. 2013]). NoiseBrush [Chen et al. 2009] provided a interactive way for further quality improvement. Patch matching is more robust to noise and has been widely used in multiple image processing [Tico 2008; Buades et al. 2009; Zhang et al. 2009; Maggioni et al. 2013; Sen et al. 2012; Kalantari et al. 2013]. Recent work [Grundmann et al. 2012; Liu et al. 2013] demonstrated the success of using a spatially-variant homography for the camera motion. We were inspired by this idea and extend it for image denoising. We use a pyramid-based pixel fusion method to improve the result quality. Figure 2 is our algorithm pipeline. Then, we estimate homography flow (in Section 3.1) to represent the motion (by camera) between the reference frame and any of the other frames. Finally, we apply a pixel fusion (in Section 3.3) to aggregate consistent pixels at all scales for producing the final result. Pyramid homography graph. We represent the motion between two frames through a pyramid homography graph, as shown in Figure 3 (a). Next, we introduce a coarse-to-fine optimization to robustly obtain accurate results. Optimization. We perform a level-by-level optimization starting from a global homography at the coarsest level (l = 0). Let H i l be the homography of node i at level l , and {H j l } be its 4 neighboring homographies at the same level. We estimate {H i l } by minimizing the following energy function:  2 2 { H ? i l } = arg min H i l ? R i l + ? H i l ? H j l , {H i l } i j (1) where ? (by default, ? = 0.1) controls the amount of spatial regularization enforced by the second smoothness term. In the first data term, R i l = best( H ? i l?1 , F i l ) is selected from two candidates: one is its parent homography H ? i l?1 at the level l ? 1, the other is its own estimated homography F i l (using all matched features within the grid of node i). In our implementation, we pick H ? i l?1 if the feature number in the grid of i is insufficient (< 8) or the rigidness [Hartley and Zisserman 2003] of F i l is too weak 2 ; otherwise, we choose the best model which has lower matching errors of all features within the grid. Because the objective function (1) is quadratic, we can obtain the global optimum by a Jacobi solver [Bronshtein and Semendyayev 1997]. The form of our motion model is similar to a mesh-based homography [Liu et al. 2013]. But our coarse-to-fine optimization is more efficient. For 500 features, our method takes 5 ms per frame while the mesh-based homography requires 50 ms per frame. Homography flow. These parameters are empirically set and fixed in all experiments. As shown in Figure 3 (b), we compute the translation vector by mapping each pixel from one frame to another frame according to the homography graph. Finally, the estimated homography flows are scaled accordingly and rounded off for use at other scales. Algorithm validation. We evaluate global homography, optical flow, patch match, and our homography flow on a set of burst images. We asked four different subjects to capture 20 sets of clean burst images (with low ISO, under good lighting conditions). Then we added Gaussian noise with different standard deviations (from 20 to 60) to synthesize 100 sets of noisy burst images. To register these noisy images, we compare six algorithms: global homography, optical flow [Liu 2009], global homography + optical flow, patch match (exhaustive search), global homography + patch match, and our homography flow. We compute the PSNR to measure the difference between registered clean image pairs. Figure 4 (b) shows the results. From the results, we can observe that two optical flow based methods perform well when noise level is small (? = 20). But when the noise level increases, they degrade more quickly than others; global homography can improve patch match but not optical flow. We believe the reason is that the coarse-to-fine mechanism in the optical flow has already handled the global motion. Our homography flow is consistently the best at all noise levels. Figure 4 (c) further shows the running time of these methods on various image sizes. Since global homography and our homography flow only rely on sparse feature detection and matching, they both outperform patch match (exhausive search or even randomized search [Barnes et al. 2009]) and optical flow [Liu 2009] in speed. There is only a small margin between the two running time curves (of global homography and our homography flow), which indicates that our pyramid optimization is very efficient. Efficient implementation. The bottleneck in this step is sparse features extraction and matching. In our implementation, we work on the coarse scale (s = 1) in the pyramids of luminance channel for efficiency and robustness (to noise). Specially, we use the Harris corner detection [Harris and Stephens 1988] and 128-bit BRIEF descriptor [Calonder et al. 2010], which can achieve real-time performance even on a mobile phone. We reject incorrectly matched features using local RANSAC [Grundmann et al. 2012]. In addition, we estimate homography flow in each non-overlapped block (8 ? 8 pixels) instead of per pixel. All pixels in each block share the same translation vector, which is computed by mapping the block center between two frames. The approximation only slightly scarifies the quality (by nearly 0.01dB in PSNR), but accelerates pixels mapping by 2.5 times. Consistent pixels. To handle scene motion, we borrow a simple idea from HDR deghosting [Granados et al. 2013] to avoid complex motion tracking. There are two methods which separately satisfy one of two goals. The other is median-based: we collect pixels consistent to the median (below the same threshold ? ) of all pixels on the profile. The reference method can guarantee a ghost-free result. But for a pixel on a dynamic object, we may get insufficient samples for denoising (shown on Figure 5 (c)). The median method collects more consistent pixels but might lead to ghosting because the median may happen to be a color on the moving object (shown on Figure 5 (d)). Combined strategy. We propose a simple combination strategy of both methods: for each pixel (on the reference frame), we compute two sets of consistent pixels {M, R} separately by the median and the reference methods. M (or R) records the frame indices of consistent pixels for every pixel. If the reference frame belongs to M, we take the union of M and R as the final result because both methods agree. Otherwise, we choose the median result if it is reliable (i.e., the size of M is more than half of the frame number), and choose the reference result if it is unreliable. To further reduce the chance of ghosting (by enforcing spatial coherence), we do not measure reliability pixel by pixel. Instead, we find all connectedcomponents of undecided pixels (where the reference frame does not belong to M), and then determine the reliability of each connected component as a whole, by majority voting. After the combination, we obtain the sets of consistent pixels for all pixels. To further make the combination seamless, we run a 3 ? 3 morphological (majority) filter [Gonzalez and Woods 2007] on the stack of the indices of consistent pixels, frame-by-frame. Figure 5 (a-b) shows two real examples and the maps which record the number of consistent pixels at every pixel location. Efficient implementation. The consistent pixels are also selected at the coarse scale (i.e., s = 1) in the pyramids for the purpose of enabling fast computation and detecting motion at a relatively clean scale. The approximation could achieve 98.7% outlier detection rate as operating at the original scale at the expense of half the time. Other scales just reuse the indices of computed consistent pixels by upsampling or downsampling. In both median and reference based methods, we use patch (5 ? 5) difference instead of single pixel difference and use the threshold 3 ? = 10. We use integral image [Viola and Jones 2001] to compute patch differences more efficiently. Given consistent pixels for each pixel at all scales, we fuse all of them in a temporal and multi-scale fusion. We keep our fusion as simple as possible, while being able to significantly denoise. Temporal fusion. Suppose {x t } are consistent pixels at a pixel location (at a certain scale), where x t is pixel color from the tth frame. We compute the fusion result x by a linear minimum mean squared error (LMMSE) estimator, which is widely used in previous work (e.g., [Zhang and Wu 2005]) for optimal unbiased denoising: x = u + ? c 2 ? + c 2 ? 2 (x t ? u) , (2) where u is the mean of all consistent pixels {x t }. The variance ? c 2 of true pixels is approximated by max(0, ? t 2 ? ? 2 ). ? t and ? are the standard deviation of {x t } and noise. The LMMSE estimator can help us adaptively handle outliers. Some severely misaligned pixels occurring at discontinuities of depths (our homography flow is more suitable for spatially smooth depth variations) or subtle residual moving pixels at a fine scale (our scene motion detection is performed at a coarse scale) wold make the variance of {x t } much larger than the noise variance. Thus, our fusion result would be x ? x t (no denoising). Otherwise, the result x has a value close to the mean u of {x t }. The temporal fusion runs independently at every scale. Next, we describe how to aggregate results in all scales. Multi-scale fusion. We aggregate the results from top to bottom, in a point-wise manner. Let x s and x s?1 be temporal fusion results at two adjacent scales s and s ? 1. x s = ? ? x s + (1 ? ?) ? (x s?1 ) ?, (3)\n        where ? is a bilinear upscale operator. ? = m/N is an adaptive fusion weight. A larger ? means we have more consistent pixels at the current scale and we should trust current estimation more; otherwise, we should borrow more from the parent scale. Here, we replace the temporal fusion result x s in Equation (3) with a very fast filtering in the spatial domain:\n        x s = p tex ? f (x s ) + (1 ? p tex ) ? (x s?1 ) ?, (4)\n        where the filtering operator f (x s ) is a directional spatial pixel fusion. We find all spatially consistent pixels along the most probable\n        3 Since we detect motion at a coarser and relatively clean scale, we can use a constant threshold instead of an adaptive threshold, which may require complex noise modeling. We find that it works well in our experiments. Then a LMMSE estimator described in Equation (2) is used on these pixels. For efficiency, we only apply the spatial fusion on texture pixels (p tex > 0.01). For efficiency, we estimate ? by computing the standard deviation of pixels differences between the median image and the reference image within the flat (non-textured) areas. On these areas, the median image (generated in the coarse scale) is a good approximation to a clean version of the reference image. For real noise, we use an approach similar to [Liu et al. 2008] to divide the illuminance into 10 discrete bins and estimate the corresponding ? for each bin. Extension to patch. The idea of combining temporal and multiscale fusion can also be extended to the patch level for better denoising quality. Algorithm validation. We perform a quantitative evaluation on a synthetic data set. The ground-truth clean images come from 68 images of BSD300 [Martin et al. 2001]. Then, we add Gaussian noise with various noise levels (? = 20 ? 60). Note that our synthetic data set ignores many key factors in real data: parallax, non-Gaussian noise, and non-rigid object motion. Ignoring these key factors may lead to incomplete conclusions. However, we still provide a preliminary evaluation here for reference and to help us gain a better understanding. Figure 6(a) shows the average PSNRs of: average (baseline), optical flow + median filtering, VBM3D [Dabov et al. 2007a], BM4D [Maggioni et al. 2013], our method (with pixel fusion), and our method (with patch fusion). We applied the same global homography to all methods for better results. Overall, our method (with pixel fusion) performs comparably to VBM3D and BM4D (two state-of-the-art denoising methods) and better than averaging and optical flow at all noise levels. When the noise level increases, our method performs slightly worse than BM4D, but VBM3D drops more quickly than ours. This evaluation partially demonstrates the true power of our method in the presence of real camera motion and registration error. Keep in mind that our method is 2-3 orders of magnitude faster than VBM3D, optical flow, and BM4D. Figure 6(b) shows the running time of these methods on different image sizes. In addition, we can achieve the best results by extending our fusion to the patch level. Compared with two patch-based methods (VBM3D and BM4D), our patch-based fusion is still efficient (1-2 orders of magnitude faster). Figure 7 also demonstrates that patches perform better in both temporal and multi-scale fusion than pixels. More interestingly, multiscale fusion as complementary to temporal pixel fusion plays an important role in our pixel-based method. It helps greatly reduce the gap between our pixel-based method and our patch-based method. We acquired 20 sets of burst images on various content with 5 cameras, including 3 mobile phones, 1 DSLR camera, and 1 compact camera. Each burst contains 10 shots. All our results are generated with a set of fixed parameters and by pixel-based fusion. All original sequences and more results are provided on our webpage 4 . We compare our method with three point-wise methods (spatialtemporal filtering [Bennett and McMillan 2005], lucky imaging [Joshi and Cohen 2010], and optical flow [Liu 2009] + temporally median filtering), and two state-of-the-art patch-based methods (VBM3D [Dabov et al. 2007a] and BM4D [Maggioni et al. 2013]). The former two are based our own implementations, and optical flow and the latter two are from the authors. For all methods, we apply the same global homography estimated by our method to help them to obtain more reliable correspondences. Since some algorithms require a known noise variance, we try all possible noise levels and choose the result with the best visual quality through a balanced tradeoff between detail recovery and noise reduction. Static scene. The example in Figure 8 was captured by a HTC 802d Android phone. The motion is mainly caused by camera movement. As we can see, spatial-temporal filtering, VBM3D, and BM4D still leave certain noises in flat regions (e.g., sky area). The building structures (in Figure 8(b) (f)) are not well restored by VBM3D and BM4D. filter or patch matching has the risk to find mismatched correspondences which will eventually lead to undesired results. Overall, the results by optical flow, lucky imaging, and ours are comparable, with our result being slightly cleaner. Portrait with small motion. This is a typical scenario for taking a portrait photo in dark lighting. The example in Figure 9 was recorded by a JVC GC-PX10 camera. Spatial-temporal filtering, lucky imaging, and the optical flow method produced ?staircase? artifacts around the eyes ( Figure 9 (b)(c)(d)). This is because of small non-rigid motion of the subject. VBM3D and BM4D do not have this issue but blurred fine details on scarves ( Figure 9 (e)(f)). Our result achieves the best on both kinds of regions. Complex scene motion. Figure 10 and Figure 11 show two cases of complex dynamic scenes. The first example was captured by a Cannon EOS 500D with ISO 6400. Its noise level is relatively low. The later example was obtained by a Nokia Lumia920. As we can see from both examples, lucky imaging and optical flow (+ median filtering) based results contain noticeable ghosting while the VBM3D results are over-smoothed. BM4D is better than the former two methods but still leaves a certain amount of chrominance noise patterns on the background. In our solution, we can automatically choose pixel colors consistent to the reference frame on the dynamic regions and collect more consistent pixels on the static regions (e.g., cloth and door). As a result, our result strikes the best balance among removing noise, reconstructing fine details, and avoiding ghosting. Handling motion blur. During the capture, some individual frames (e.g., 10% ? 30% of the total frames) may be blurry due to sudden camera shake or object motion. Figure 12 shows such an example captured with an iPhone 5S. We examined what would happen if a blurry frame were selected as the reference frame. To know this, we separately select frame 4 (sharp) and frame 5 (blurry) as the reference frame and generate two results. Figure 12 (b) and (d) show that our method is insensitive to the selection. This is because our method is capable of finding consistent pixels from the the majority of frames. A similar method was proposed in video deblurring [Cho et al. 2012], which found similar patches from sharp frames for deblurring. Handling extreme low light. Figure 13 is a sequence captured by an iPhone 4S under an extreme low-light condition. Since the inputs are extremely dark, we preprocess the inputs by boosting the brightness (applying a shadow/highlight adjustment). While the noise after the boosting is very strong, our method still managed to produce a clean image with fine details (thin wires in the air and steel tower on the left). Handling large occlusions. Figure 14 shows a sequence with a large, fast moving foreground (person). The consistent pixel map in the figure reveals how the mechanisms in our algorithm can reliably deal with (fast) large occlusion. We run our method on an Intel i5 3.2GHZ machine with 16G RAM. Our unoptimized C++ implementation (single core, no SSE SIMD acceleration) takes 920 ms on average to process 10 frames of 5Mpixel image. Specifically, our method takes 82ms, 177ms, 51ms, 30ms, 253ms, 328ms to build pyramid, extract and match sparse features, estimate homography flow, select consistent pixels, run temporal fusion, and execute multi-scale fusion. Our prototype on a smartphone (Nokia Lumia 920) costs about 4.7 seconds on average, without using multi-core or NEON instructions or GPU acceleration. As our solution is mainly based on point-wise operations, we expect it can be significantly accelerated. Table 1 further demonstrates the processing time of different methods on the same machine for Figure 8 , 9, 10, and 11. In an image burst, we expect camera motion from hand shake and small/moderate motion of the main subject(s). Our method is not designed for handling dramatic motion (e.g., in sports), or denois- ing a general video. When the motion between two frames cannot be well represented by our homography flow, such as scene transition or fast camera panning, or even non-rigid deformation (e.g., water wave motion, flag waving), our approach will break. Besides, there are two cases in which our consistent pixels selection may fail. The first case is when motion blur caused by dynamic objects appears on a majority of frames (more than half of all frames). On the dynamic regions, our pixel selection would automatically choose the reference-based strategy. If the reference frame contains blur, our result would retain the blur effect. But if a sharp frame is chosen as the reference, the issue can be avoided. Figure 15 (a) shows such an example. Therefore, we need a better strategy for selecting the reference frame. Besides, fast moving objects would be automatically removed by the median-based strategy for aggressive denoising. To avoid this issue, we may provide another option that allows users to choose the reference frame and constrain the reference region for pixels selection. The second case is when different moving objects and the background may have similar colors in the same pixel locations. Our pixels selection algorithm relies on per-pixel color difference, which is too weak to distinguish such objects. Figure 15 (b) shows an example. Different moving persons have very similar color regions and such ambiguous regions (indicated by highlight box) occur in a majority of frames (i.e., more than half of all frames). Finally, it will lead to ghosting because our selection wrongly regard it as the background. This issue also appears in ghost-free HDR reconstruction [Granados et al. 2013], which requires interactions to exclude these ambiguous regions. Despite the above issues, we believe that our highly efficient solution is practical enough to be deployed for improving the photo experience of users in a broad range of lighting conditions. We thank all the reviewers for their helpful discussions, Steve Lin and Jiangyu Liu for their help in proofreading. input burst images our results (a) motion blurs on dynamic objects (on a majority of frames). (b) ambiguous regions (with similar colors) from different moving objects.",
  "resources" : [ ]
}