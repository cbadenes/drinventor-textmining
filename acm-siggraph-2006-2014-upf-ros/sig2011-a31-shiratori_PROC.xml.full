{
  "uri" : "sig2011-a31-shiratori_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2011/a31-shiratori_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Motion Capture from Body-Mounted Cameras",
    "published" : "2011",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Takaaki-Shiratori",
      "name" : "Takaaki",
      "surname" : "Shiratori"
    }, {
      "uri" : "http://drinventor/Hyun Soo-Park",
      "name" : "Hyun Soo",
      "surname" : "Park"
    }, {
      "uri" : "http://drinventor/Leonid-Sigal",
      "name" : "Leonid",
      "surname" : "Sigal"
    }, {
      "uri" : "http://drinventor/Yaser-Sheikh",
      "name" : "Yaser",
      "surname" : "Sheikh"
    }, {
      "uri" : "http://drinventor/Jessica K.-Hodgins",
      "name" : "Jessica K.",
      "surname" : "Hodgins"
    } ]
  },
  "bagOfWords" : [ "a191cd8b521ea1f52d8164b0dec08ed4f6c174e4478e1e7f5ee9a1b889d9e17c", "p24", "10.1145", "1964921.1964926", "name", "identification", "possible", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "Takaaki", "Shiratori", "Hyun", "Soo", "Park", "Leonid", "Sigal", "Yaser", "Sheikh", "Jessica", "K.", "Hodgins", "??", "Disney", "Research", "Pittsburgh", "Carnegie", "Mellon", "University", "-lrb-", "-rrb-", "body-mounted", "camera", "-lrb-", "-rrb-", "skeletal", "motion", "3d", "structure", "figure", "capture", "both", "relative", "global", "motion", "natural", "environment", "use", "camera", "mount", "body", "Motion", "capture", "technology", "generally", "require", "recording", "perform", "laboratory", "closed", "stage", "set", "control", "lighting", "restriction", "preclude", "capture", "motion", "require", "outdoor", "setting", "traversal", "large", "area", "paper", "we", "present", "theory", "practice", "use", "body-mounted", "camera", "reconstruct", "motion", "subject", "outward-looking", "camera", "attach", "limb", "subject", "joint", "angle", "root", "pose", "estimate", "through", "non-linear", "optimization", "optimization", "objective", "function", "incorporate", "term", "image", "matching", "error", "temporal", "continuity", "motion", "structure-from-motion", "use", "estimate", "skeleton", "structure", "provide", "initialization", "non-linear", "optimization", "procedure", "global", "motion", "estimate", "drift", "control", "match", "capture", "set", "video", "reference", "imagery", "we", "show", "result", "setting", "where", "capture", "would", "difficult", "impossible", "traditional", "motion", "capture", "system", "include", "walk", "outside", "swing", "monkey", "bar", "quality", "motion", "reconstruction", "evaluate", "compare", "we", "result", "against", "motion", "capture", "datum", "produce", "commercially", "available", "optical", "system", "cr", "category", "i.", "3.7", "-lsb-", "Computer", "Graphics", "-rsb-", "three", "Dimensional", "Graphics", "realism?animation", "Keywords", "Motion", "capture", "structure-from-motion", "articulate", "motion", "wearable", "camera", "Links", "dl", "pdf", "EB", "IDEO", "-lcb-", "shiratorus", "lsigal", "-rcb-", "@disneyresearch", "com", "-lcb-", "hyunsoop", "yaser", "jkh", "-rcb-", "@cs", "cmu.edu", "ACM", "Reference", "Format", "Shiratori", "T.", "Park", "H.", "Sigal", "L.", "Sheikh", "Y.", "Hodgins", "J.", "2011", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "ACM", "Trans", "graph", "30", "Article", "31", "-lrb-", "July", "2011", "-rrb-", "10", "page", "dous", "10.1145", "1964921.1964926", "http://doi.acm.org/10.1145/1964921.1964926", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2011", "ACM", "0730-0301/2011", "07-art31", "10.00", "DOI", "10.1145", "1964921.1964926", "http://doi.acm.org/10.1145/1964921.1964926", "-lrb-", "-rrb-", "rendered", "actor", "introduction", "Motion", "capture", "have", "be", "use", "provide", "much", "character", "motion", "several", "recent", "theatrical", "release", "Avatar", "motion", "capture", "use", "animate", "character", "ride", "direhorse", "fly", "back", "mountain", "banshee", "-lsb-", "Duncan", "2010", "-rsb-", "capture", "realistic", "motion", "scene", "actor", "ride", "horse", "robotic", "mockup", "expansive", "motion", "capture", "studio", "require", "large", "number", "camera", "Coverage", "lighting", "problem", "often", "prevent", "director", "from", "capture", "motion", "natural", "setting", "other", "large", "environment", "inertial", "system", "one", "describe", "Vlasic", "colleague", "-lsb-", "2007", "-rsb-", "allow", "capture", "occur", "outdoor", "space", "design", "recover", "only", "relative", "motion", "joint", "global", "root", "motion", "paper", "we", "present", "wearable", "system", "outward-looking", "camera", "allow", "reconstruction", "relative", "global", "motion", "actor", "outside", "laboratory", "closed", "stage", "camera", "can", "mount", "casual", "clothing", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "easily", "mount", "remove", "use", "velcro", "attachment", "lightweight", "enough", "allow", "unimpeded", "movement", "structurefrom-motion", "-lrb-", "sfm", "-rrb-", "use", "estimate", "pose", "camera", "throughout", "capture", "estimate", "camera", "movement", "from", "range-of-motion", "sequence", "use", "automatically", "build", "skeleton", "use", "co-occur", "transformation", "limb", "connect", "each", "joint", "reconstruct", "camera", "skeleton", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "use", "initialization", "overall", "optimization", "compute", "root", "position", "orientation", "joint", "angle", "while", "minimize", "image", "match", "error", "Reference", "imagery", "capture", "area", "leverage", "reduce", "drift", "we", "render", "motion", "skin", "character", "apply", "recover", "skeletal", "motion", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "estimate", "camera", "pose", "global", "relative", "motion", "actor", "can", "capture", "outdoors", "under", "wide", "variety", "lighting", "condition", "extended", "indoor", "region", "without", "any", "additional", "equipment", "we", "also", "avoid", "some", "miss", "datum", "problem", "introduce", "occlusion", "between", "marker", "camera", "traditional", "optical", "motion", "capture", "because", "we", "system", "any", "visually", "distinctive", "feature", "world", "can", "serve", "marker", "traditional", "system", "by-product", "capture", "process", "sparse", "3d", "structure", "scene", "structure", "useful", "guide", "define", "ground", "geometry", "first", "sketch", "scene", "3d", "animator", "director", "we", "evaluate", "we", "approach", "against", "motion", "capture", "datum", "generate", "Vicon", "optical", "motion", "capture", "system", "report", "mean", "joint", "position", "error", "1.76", "cm", "mean", "joint", "angle", "error", "3.01", "full", "range-of-motion", "sequence", "use", "skeleton", "estimation", "we", "result", "demonstrate", "system", "can", "reconstruct", "action", "difficult", "capture", "traditional", "motion", "capture", "system", "include", "outdoor", "activity", "direct", "sunlight", "activity", "occlude", "near", "proximal", "structure", "extend", "indoor", "activity", "we", "prototype", "first", "we", "knowledge", "employ", "camera", "sensor", "motion", "capture", "measure", "environment", "estimate", "motion", "set", "camera", "relate", "underlie", "articulate", "structure", "current", "camera", "inexpensive", "have", "form", "factor", "rival", "inertial", "measurement", "unit", "-lrb-", "imus", "-rrb-", "already", "embed", "everyday", "handheld", "device", "we", "approach", "continue", "benefit", "from", "consumer", "trend", "drive", "camera", "become", "cheaper", "smaller", "faster", "more", "pervasive", "give", "expect", "continuation", "technological", "trend", "we", "believe", "system", "one", "propose", "here", "become", "viable", "alternative", "traditional", "motion", "capture", "technology", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "31:2", "T.", "Shiratori", "et", "al.", "related", "work", "variety", "motion", "capture", "technology", "currently", "available", "both", "commercially", "prototype", "advantage", "disadvantage", "different", "design", "discuss", "several", "survey", "-lrb-", "e.g.", "-lsb-", "Welch", "Foxlin", "2002", "Moeslund", "et", "al.", "2006", "-rsb-", "-rrb-", "Motion", "capture", "system", "can", "classify", "outside-in", "-lsb-", "Welch", "Foxlin", "2002", "-rsb-", "rely", "sensor", "mount", "environment", "passive", "any", "marker", "body", "definition", "requirement", "restrict", "use", "laboratory", "environment", "closed", "stage", "setting", "because", "capture", "space", "have", "instrument", "sensor", "inside-out", "system", "-lsb-", "Welch", "Foxlin", "2002", "-rsb-", "rely", "sensor", "body", "recover", "3d", "pose", "portability", "allow", "use", "both", "indoor", "outdoor", "environment", "we", "approach", "fall", "latter", "category", "here", "we", "review", "most", "relevant", "method", "system", "Optical", "motion", "capture", "system", "-lsb-", "woltring", "1974", "-rsb-", "among", "most", "widely", "use", "industry", "today", "commercial", "system", "available", "from", "Vicon", "-lrb-", "www.vicon.com", "-rrb-", "qualisy", "-lrb-", "www.qualisys.com", "-rrb-", "among", "other", "Optical", "motion", "capture", "system", "use", "set", "specialize", "high-resolution", "video", "camera", "track", "retro-reflective", "marker", "light-emitting", "diode", "-lrb-", "led", "-rrb-", "place", "key", "point", "body", "Triangulation", "use", "recover", "3d", "position", "marker", "space", "3d", "marker", "position", "turn", "use", "fit", "skeletal", "model", "observe", "motion", "system", "popular", "due", "accuracy", "major", "disadvantage", "cost", "portability", "intrusiveness", "Optical", "system", "require", "indoor", "setup", "typically", "cost", "between", "ten", "hundred", "thousand", "dollar", "use", "photosensor", "explore", "raskar", "colleague", "-lsb-", "2007", "-rsb-", "propose", "system", "rely", "measure", "spatio-temporal", "light", "modulation", "produce", "multiple", "led", "transmitter", "emit", "gray", "code", "pattern", "receiver", "module", "equip", "infrared", "rgb", "photosensor", "be", "task", "decoding", "-lrb-", "demultiplexing", "-rrb-", "observe", "pattern", "do", "so", "directly", "produce", "3d", "spatial", "location", "-lrb-", "side", "effect", "measure", "incident", "light", "scene", "light", "matching", "-rrb-", "while", "system", "inspirational", "we", "utilize", "simplify", "photosensor", "camera", "wear", "body", "fundamentally", "different", "from", "we", "approach", "because", "require", "transmitter", "environment", "alleviate", "intrusive", "characteristic", "marker-based", "motion", "capture", "system", "marker-less", "motion", "capture", "technology", "have", "be", "develop", "number", "researcher", "-lsb-", "Cheung", "et", "al.", "2003", "Deutscher", "Reid", "2005", "Moeslund", "et", "al.", "2006", "Hasler", "et", "al.", "2009", "-rsb-", "marker-less", "method", "most", "often", "use", "regular", "video", "camera", "simple", "-lrb-", "e.g.", "chromakey", "-rrb-", "background", "reconstruct", "voxel", "representation", "body", "over", "time", "fit", "skeletal", "model", "voxel", "representation", "similar", "paradigm", "use", "system", "develop", "Organic", "Motion", "-lrb-", "www.organicmotion.com", "-rrb-", "recent", "study", "-lsb-", "Corazza", "et", "al.", "2006", "Corazza", "et", "al.", "2010", "-rsb-", "suggest", "sufficient", "number", "camera", "favorable", "imaging", "condition", "accuracy", "marker-less", "method", "can", "rival", "traditional", "optical", "motion", "capture", "Hasler", "colleague", "-lsb-", "2009", "-rsb-", "introduce", "approach", "capture", "motion", "actor", "outdoor", "environment", "from", "multiple", "inward-looking", "move", "camera", "method", "use", "audio", "synchronize", "camera", "fit", "3d", "scan", "actor", "silhouette", "estimate", "each", "move", "camera", "markerless", "method", "require", "image", "segmentation", "3d", "scan", "actor", "most", "direct", "approach", "measure", "human", "motion", "through", "use", "wearable", "electro-mechanical", "system", "e.g.", "gypsy", "-lrb-", "www.animazoo.com", "-rrb-", "system", "consist", "exoskeleton", "suit", "embedded", "lightweight", "rod", "articulate", "performer?s", "bone", "potentiometer", "joint", "measure", "angular", "rotation", "rod", "convert", "joint", "angle", "use", "kinematic", "model", "system", "while", "capable", "directly", "measure", "motion", "subject", "intrusive", "uncomfortable", "wear", "recently", "have", "be", "number", "self-contained", "wearable", "experimental", "system", "develop", "base", "variety", "sensor", "technology", "-lrb-", "e.g.", "-lsb-", "Schwarz", "et", "al.", "2010", "Zhang", "et", "al.", "2009", "-rsb-", "-rrb-", "include", "ultrasound", "imus", "tri-axial", "accelerometer", "inertial", "motion", "capture", "system", "-lrb-", "e.g.", "Xsens", "MVN", "www.xsens.com", "-rrb-", "measure", "rotation", "body", "part", "world", "use", "accelerometer", "gyroscope", "system", "portable", "can", "take", "outside", "however", "only", "able", "measure", "orientation", "body", "part", "motion", "body", "world", "multiple", "sensor", "can", "combine", "alleviate", "drift", "example", "Vlasic", "colleague", "-lsb-", "2007", "-rsb-", "add", "ultrasonic", "sensor", "imus", "Alternatives", "battle", "drift", "include", "data-driven", "approach", "base", "motion", "capture", "datum", "stabilize", "accelerometer", "estimate", "-lsb-", "Slyper", "Hodgins", "2008", "Xie", "et", "al.", "2008", "Kelly", "et", "al.", "2010", "Tautges", "et", "al.", "2011", "-rsb-", "we", "system", "camera-based", "therefore", "rely", "rich", "datum", "detailed", "view", "environment", "we", "use", "image", "from", "camera", "along", "estimate", "3d", "geometry", "environment", "recover", "3d", "limb", "position", "orientation", "world", "over", "time", "thus", "we", "build", "substantial", "prior", "work", "sfm", "-lsb-", "Hartley", "Zisserman", "2004", "Pollefeys", "et", "al.", "2004", "Snavely", "et", "al.", "2006", "-rsb-", "visual", "simultaneous", "localization", "mapping", "-lrb-", "slam", "-rrb-", "-lsb-", "Welch", "et", "al.", "1999", "Davison", "et", "al.", "2007", "Klein", "Murray", "2007", "-rsb-", "approach", "have", "be", "use", "estimate", "motion", "move", "platform", "-lsb-", "Ballan", "et", "al.", "2010", "N?ster", "et", "al.", "2006", "-rsb-", "even", "human", "-lsb-", "Oskiper", "et", "al.", "2007", "Zhu", "et", "al.", "2007", "Zhu", "et", "al.", "2008", "-rsb-", "however", "recover", "only", "independent", "ego-motion", "individual", "camera", "platform", "we", "work", "first", "reconstruct", "3d", "motion", "set", "camera", "relate", "underlie", "articulate", "structure", "-lrb-", "-rrb-", "front", "-lrb-", "-rrb-", "side", "-lrb-", "-rrb-", "Skeleton", "Figure", "setting", "camera", "from", "-lrb-", "-rrb-", "front", "view", "-lrb-", "-rrb-", "side", "view", "-lrb-", "-rrb-", "illustration", "skeleton", "body-mounted", "camera", "Blue", "camera", "mount", "body", "orange", "camera", "use", "virtual", "camera", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "31:3", "SfM", "Figure", "we", "system", "take", "video", "datum", "capture", "body-mounted", "camera", "output", "reconstruction", "human", "motion", "motion", "body", "estimate", "use", "sfm", "individual", "camera", "initial", "guess", "optimize", "reprojection", "error", "3d", "structure", "while", "enforce", "underlie", "articulate", "relationship", "between", "camera", "smoothness", "motion", "across", "time", "hardware", "Setup", "one", "camera", "attach", "each", "body", "segment", "use", "velcro", "strapon", "mount", "show", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "three", "camera", "attach", "waist", "root", "pose", "estimation", "two", "camera", "attach", "torso", "camera", "synchronize", "use", "standard", "audio", "calibration", "signal", "subject", "perform", "range-of-motion", "trial", "skeleton", "estimation", "perform", "desire", "activity", "capture", "video", "datum", "download", "from", "camera", "after", "capture", "processing", "we", "system", "produce", "skeleton", "actor", "root", "position", "orientation", "joint", "angle", "across", "time", "also", "3d", "structure", "scene", "by-product", "we", "use", "16", "more", "commercially", "available", "wide-angle", "-lrb-", "170", "field", "view", "-rrb-", "sport", "action", "camera", "call", "HD", "Hero", "from", "GoPro", "-lrb-", "www.goprocamera.com", "-rrb-", "cost", "250", "dollar", "per", "camera", "make", "we", "entire", "setup", "approximately", "5,000", "dollar", "camera", "lightweight", "94", "have", "small", "form", "factor", "-lrb-", "42", "mm", "60", "mm", "30", "mm", "-rrb-", "hd", "hero", "camera", "equip", "CMOS", "sensor", "capable", "variety", "resolution/frame", "rate", "setting", "we", "record", "720p", "-lrb-", "1280", "720", "-rrb-", "resolution", "60", "frame", "per", "second", "camera", "some", "body", "segment", "often", "occluded", "limb", "-lrb-", "e.g.", "waist", "torso", "-rrb-", "we", "use", "additional", "camera", "provide", "robustness", "create", "wider", "aggregate", "field", "view", "all", "camera", "calibrate", "advance", "use", "fisheye", "lens", "distortion", "model", "-lsb-", "devernay", "faugeras", "2000", "-rsb-", "provide", "estimate", "focal", "length", "principal", "point", "distortion", "coefficient", "lens", "focal", "length", "fix", "camera", "estimate", "need", "compute", "only", "once", "re-usable", "across", "capture", "reconstruct", "human", "Motion", "conventional", "sfm", "can", "provide", "visually", "feasible", "estimate", "3d", "structure", "camera", "pose", "estimate", "often", "sufficiently", "accurate", "capture", "human", "motion", "order", "compute", "we", "use", "clapper", "board", "produce", "loud", "clap", "beginning", "end", "each", "trial", "we", "find", "peak", "audio", "signal", "result", "movie", "file", "from", "all", "camera", "look", "most", "consistent", "duration", "between", "peak", "use", "simple", "form", "clustering", "exhaustive", "search", "-lsb-", "Hasler", "et", "al.", "2009", "-rsb-", "appropriate", "estimate", "human", "motion", "across", "time", "we", "SfM", "solution", "consider", "articulation", "body-mounted", "camera", "underlie", "skeleton", "actor", "fit", "they", "image", "measurement", "-lcb-", "-rcb-", "argmin", "where", "time-series", "datum", "root", "position", "joint", "angle", "respectively", "account", "reprojection", "error", "3d", "reconstruction", "measure", "image", "feature", "location", "use", "skeleton", "constraint", "camera", "consider", "smoothness", "result", "motion", "weight", "control", "influence", "smoothness", "constraint", "equation", "-lrb-", "-rrb-", "highly", "non-linear", "optimization", "equation", "require", "good", "initial", "estimate", "camera", "pose", "skeleton", "we", "develop", "pipeline", "show", "Figure", "after", "datum", "capture", "we", "reconstruct", "3d", "structure", "scene", "from", "reference", "image", "use", "SfM", "while", "step", "optional", "principle", "substantially", "reduce", "drift", "reconstructed", "motion", "we", "choose", "perform", "all", "we", "capture", "3d", "structure", "use", "reconstruct", "body-mounted", "camera", "pose", "across", "time", "-lrb-", "section", "4.1", "-rrb-", "new", "skeleton", "require", "subject", "ask", "perform", "standard", "range-of-motion", "exercise", "beginning", "capture", "session", "skeleton", "automatically", "generate", "-lrb-", "see", "appendix", "-rrb-", "use", "reconstruct", "whole", "body", "pose", "from", "camera", "-lrb-", "section", "4.2", "-rrb-", "user", "can", "optionally", "refine", "skeleton", "change", "pose", "camera", "respect", "joint", "through", "graphical", "user", "interface", "finally", "motion", "refine", "use", "image-based", "non-linear", "optimization", "incorporate", "temporal", "smoothing", "-lrb-", "section", "4.3", "-rrb-", "4.1", "initialize", "Camera", "pose", "use", "SfM", "Direct", "incremental", "sfm", "from", "body-mounted", "camera", "yield", "precise", "3d", "reconstruction", "locally", "suffer", "from", "global", "drift", "when", "capture", "area", "large", "3d", "structure", "far", "from", "camera", "location", "-lsb-", "Hartley", "Zisserman", "2004", "-rsb-", "avoid", "problem", "we", "record", "reference", "image", "capture", "area", "reconstruct", "3d", "structure", "use", "image", "use", "3d", "structure", "corresponding", "2d", "measurement", "from", "body-mounted", "camera", "camera", "pose", "can", "reconstruct", "we", "call", "process", "absolute", "camera", "registration", "significant", "difference", "view", "between", "reference", "image", "record", "video", "some", "camera", "may", "reconstruct", "we", "handle", "situation", "add", "new", "structure", "point", "newly", "register", "camera", "rerun", "camera", "registration", "we", "call", "iterative", "process", "relative", "camera", "registration", "repeat", "process", "until", "most", "camera", "reconstruct", "3d", "reconstruction", "Reference", "Images", "from", "reference", "image", "we", "extract", "Scale-Invariant", "Feature", "Transform", "-lrb-", "SIFT", "-rrb-", "key", "point", "-lsb-", "Lowe", "2004", "-rsb-", "find", "correspondence", "between", "pair", "image", "use", "approximate", "nearest", "neighbor", "search", "-lsb-", "muja", "Lowe", "2009", "-rsb-", "fundamental", "matrix", "estimation", "base", "ransac", "-lsb-", "fischler", "Bolles", "1981", "-rsb-", "enable", "we", "obtain", "geometrically", "consistent", "match", "estimate", "extrinsic", "parameter", "camera", "we", "choose", "initial", "pair", "image", "have", "significant", "number", "match", "can", "account", "homography", "from", "those", "match", "we", "estimate", "relative", "camera", "orientation", "translation", "extract", "essential", "matrix", "triangulate", "location", "match", "feature", "point", "3d", "use", "direct", "Linear", "Transform", "algorithm", "-lsb-", "Hartley", "Zisserman", "2004", "-rsb-", "follow", "two-image", "bundle", "adjustment", "-lsb-", "Lourakis", "Argyros", "2009", "-rsb-", "we", "incrementally", "add", "image", "have", "greatest", "number", "inlier", "3d-2d", "correspondence", "among", "remain", "image", "from", "correspondence", "we", "reconstruct", "camera", "pose", "use", "perspective-n-point", "-lrb-", "pnp", "-rrb-", "algorithm", "-lsb-", "Lepetit", "et", "al.", "2009", "-rsb-", "inside", "ransac", "procedure", "once", "extrinsic", "parameter", "new", "camera", "reconstruct", "2d-2d", "correspondence", "between", "reconstructed", "image", "newly", "add", "image", "reconstruct", "3d", "accuracy", "we", "exclude", "3d", "point", "follow", "criterion", "any", "point", "have", "high", "reprojection", "error", "-lrb-", "pixel", "-rrb-", "any", "point", "when", "angle", "subtend", "ray", "use", "triangulation", "small", "-lrb-", "-rrb-", "once", "structure", "have", "be", "update", "sparse", "bundle", "adjustment", "run", "refine", "entire", "model", "process", "continue", "until", "most", "reference", "image", "register", "absolute", "Camera", "Registration", "after", "3d", "structure", "reconstruct", "from", "reference", "image", "use", "estimate", "bodymounted", "camera", "pose", "process", "register", "image", "from", "body-mounted", "camera", "similar", "add", "new", "reference", "image", "SfM", "process", "use", "ransac", "pnp", "we", "find", "best", "extrinsic", "camera", "parameter", "produce", "less", "than", "pixel", "reprojection", "error", "when", "number", "inlier", "3d-2d", "correspondence", "sufficient", "-lrb-", "50", "-rrb-", "once", "camera", "parameter", "estimate", "new", "3d", "point", "triangulate", "use", "2d-2d", "correspondence", "between", "newly", "register", "image", "previously", "register", "image", "reduce", "computational", "cost", "keypoint", "matching", "new", "3d", "point", "we", "ignore", "camera", "pair", "whose", "optical", "axis", "have", "more", "than", "90", "orientation", "difference", "criterion", "add", "new", "point", "same", "those", "use", "SfM", "process", "bundle", "adjustment", "refine", "newly", "register", "camera", "pose", "3d", "structure", "3d", "structure", "obtain", "from", "reference", "image", "fix", "during", "optimization", "so", "structure", "can", "act", "anchor", "avoid", "drift", "relative", "Camera", "Registration", "reconstruction", "from", "absolute", "camera", "registration", "may", "sparse", "particularly", "when", "view", "angle", "reference", "image", "different", "from", "those", "image", "from", "body-mounted", "camera", "increase", "density", "reconstruction", "body-mounted", "camera", "pose", "we", "find", "match", "between", "image", "from", "absolute-registered", "camera", "image", "from", "unregistered", "camera", "because", "unregistered", "camera", "close", "absolute-registered", "camera", "viewpoint", "similar", "process", "enable", "we", "reconstruct", "pose", "remain", "camera", "relative", "camera", "registration", "process", "iterate", "until", "camera", "registration", "satisfactory", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "show", "result", "absolute", "relative", "camera", "registration", "while", "absolute", "camera", "registration", "produce", "gap", "fifth", "iteration", "relative", "camera", "registration", "fill", "most", "gap", "homography", "Unregistered", "Cameras", "after", "iterative", "camera", "registration", "may", "still", "unregistered", "camera", "particular", "window", "time", "situation", "occur", "example", "when", "actor", "perform", "fast", "motion", "run", "image", "blurry", "deal", "remain", "unregistered", "camera", "we", "estimate", "relative", "camera", "orientation", "between", "consecutive", "frame", "use", "homography", "when", "camera", "center", "two", "image", "coincide", "relative", "orientation", "can", "estimate", "from", "homography", "here", "we", "assume", "camera", "center", "difference", "between", "two", "consecutive", "frame", "small", "enough", "neglect", "compare", "distance", "between", "3d", "point", "camera", "center", "we", "extract", "2d-2d", "match", "base", "sift", "keypoint", "descriptor", "robustly", "find", "consistent", "homography", "use", "ransac", "once", "homography", "estimate", "relative", "orientation", "can", "obtain", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "31:4", "T.", "Shiratori", "et", "al.", "-lrb-", "-rrb-", "3d", "structure", "-lrb-", "-rrb-", "after", "absolute", "camera", "registration", "figure", "-lrb-", "-rrb-", "Reference", "structure", "reconstruction", "-lrb-", "-rrb-", "after", "absolute", "camera", "registration", "-lrb-", "-rrb-", "after", "fifth", "iteration", "relative", "camera", "registration", "add", "point", "from", "absolute", "relative", "camera", "registration", "process", "allow", "we", "make", "camera", "registration", "denser", "-lrb-", "-rrb-", "after", "fifth", "iteration", "relative", "camera", "registration", "hk", "where", "intrinsic", "parameter", "matrix", "avoid", "drift", "cause", "one-way", "camera", "orientation", "estimation", "homography", "we", "take", "average", "forward", "backward", "interpolation", "camera", "position", "also", "need", "linear", "interpolation", "position", "between", "registered", "camera", "use", "interpolation", "provide", "initialization", "joint", "angle", "root", "position", "inlier", "2d-2d", "correspondence", "use", "homography", "computation", "use", "image", "measurement", "subsequent", "optimization", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "31:5", "4.2", "mapping", "camera", "Skeleton", "beginning", "capture", "actor", "ask", "perform", "predefined", "range-of-motion", "exercise", "which", "he", "exercise", "each", "joint", "through", "its", "full", "range", "motion", "we", "extract", "underlie", "skeleton", "structure", "from", "image", "record", "during", "range-of-motion", "performance", "common", "commercial", "motion", "capture", "system", "like", "Vicon", "we", "use", "predefined", "kinematic", "structure", "one", "more", "camera", "associate", "each", "link", "kinematic", "structure", "show", "figure", "-lrb-", "-rrb-", "root", "skeleton", "have", "six", "degree", "freedom", "-lrb-", "dof", "-rrb-", "joint", "have", "three", "dof", "we", "apply", "method", "o?brien", "colleague", "-lsb-", "2000", "-rsb-", "estimate", "skeleton", "3d", "spatial", "relationship", "each", "camera", "kinematic", "structure", "-lrb-", "see", "appendix", "-rrb-", "we", "do", "currently", "consider", "biomechanical", "constraint", "forward", "kinematic", "from", "Camera", "pose", "skeleton", "provide", "range-of-motion", "exercise", "parameterize", "root", "position", "root", "orientation", "joint", "angle", "root", "position", "orientation", "take", "coincident", "root", "camera", "hence", "give", "skeleton", "we", "can", "obtain", "pose", "each", "time", "instant", "apply", "waist", "camera", "pose", "root", "segment", "directly", "apply", "relative", "orientation", "between", "pair", "camera", "along", "kinematic", "chain", "joint", "note", "position", "camera", "pose", "use", "except", "waist", "camera", "equation", "-lrb-", "-rrb-", "consider", "skeleton", "hard", "constraint", "refinement", "forward", "kinematic", "enable", "we", "maintain", "constraint", "estimate", "camera", "position", "respect", "skeleton", "euclidean", "transformation", "from", "joint", "coordinate", "system", "world", "coordinate", "system", "define", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "orientation", "corresponding", "camera", "position", "joint", "respectively", "therefore", "position", "its", "child", "joint", "+1", "-lrb-", "-rrb-", "compute", "+1", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "vector", "from", "parent", "joint", "child", "joint", "formulation", "allow", "we", "estimate", "hierarchical", "joint", "position", "recursively", "similarly", "camera", "position", "world", "coordinate", "system", "can", "re-estimate", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "camera", "center", "attach", "j-th", "joint", "time", "W.", "Virtual", "Cameras", "Robust", "Limb", "pose", "estimation", "estimate", "body-attached", "camera", "pose", "from", "SfM", "while", "body", "move", "sometimes", "difficult", "because", "motion", "blur", "roll", "shutter", "effect", "occlusion", "limb", "lack", "texture", "background", "-lrb-", "e.g.", "sky", "-rrb-", "under", "condition", "camera", "pose", "can", "reconstruct", "very", "noisy", "when", "camera", "pose", "mis-estimated", "result", "motion", "skeleton", "incorrect", "alleviate", "problem", "we", "attach", "multiple", "camera", "limb", "estimate", "limb", "motion", "from", "virtual", "camera", "which", "take", "robust", "average", "those", "camera", "-lrb-", "estimate", "use", "sfm", "-rrb-", "we", "use", "virtual", "camera", "where", "occlusion", "occur", "frequently", "where", "precise", "estimation", "essential", "-lrb-", "e.g.", "root", "-rrb-", "where", "camera", "registration", "difficult", "-lrb-", "e.g.", "chest", "account", "non-rigidity", "shin", "deal", "inhomogeneous", "representation", "p.", "fast", "motion", "impact", "result", "imaging", "artifact", "-rrb-", "virtual", "camera", "reduce", "occlusion", "problem", "significantly", "allow", "skeletal", "motion", "reconstruct", "robustly", "virtual", "camera", "pose", "can", "estimate", "from", "motion", "over", "time", "here", "we", "assume", "three", "physical", "camera", "tightly", "connect", "single", "limb", "one", "camera", "e.g.", "select", "reference", "camera", "show", "Figure", "-lrb-", "-rrb-", "average", "relative", "transform", "from", "other", "two", "camera", "reference", "camera", "can", "estimate", "across", "time", "-lrb-", "-rrb-", "Computing", "virtual", "camera", "pose", "-lrb-", "-rrb-", "parameterize", "physical", "average", "transform", "camera", "virtual", "camera", "figure", "illustration", "virtual", "camera", "-lrb-", "blue", "-rrb-", "create", "from", "physical", "camera", "-lrb-", "orange", "-rrb-", "virtual", "camera", "use", "combine", "information", "from", "multiple", "body-attached", "camera", "additional", "accuracy", "-lrb-", "-rrb-", "average", "relative", "transform", "between", "camera", "estimate", "across", "time", "virtual", "camera", "pose", "estimate", "apply", "average", "relative", "transform", "physical", "camera", "each", "time", "instant", "-lrb-", "-rrb-", "physical", "camera", "parameterize", "virtual", "camera", "use", "average", "relative", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "function", "take", "average", "transform", "once", "average", "transform", "estimate", "inverse", "average", "transform", "use", "transform", "from", "virtual", "camera", "physical", "camera", "i.e.", "virtual", "camera", "pose", "can", "obtain", "again", "take", "average", "transform", "now", "transform", "from", "virtual", "camera", "each", "physical", "camera", "know", "which", "imply", "all", "physical", "camera", "can", "parameterize", "virtual", "camera", "pose", "-lrb-", "Figure", "-lrb-", "-rrb-", "-rrb-", "parameterization", "use", "when", "reprojection", "error", "compute", "subsequent", "optimization", "4.3", "estimate", "Body", "pose", "global", "optimization", "final", "step", "optimize", "body", "pose", "minimize", "objective", "function", "equation", "-lrb-", "-rrb-", "conceptually", "optimization", "seek", "find", "body", "pose", "skeleton", "over", "time", "temporally", "smooth", "result", "low", "spatial", "error", "between", "project", "3d", "structure", "through", "estimate", "camera", "actual", "observe", "structure", "image", "initial", "guess", "body", "pose", "set", "register", "camera", "pose", "homography", "levenberg-marquardt", "method", "apply", "refine", "pose", "consider", "all", "pose", "over", "time", "optimization", "computationally", "expensive", "instead", "we", "use", "short", "time", "window", "sequentially", "optimize", "pose", "shift", "window", "camera", "rigidly", "connect", "average", "relative", "transform", "exactly", "same", "relative", "transform", "each", "time", "instant", "reprojection", "error", "term", "refine", "whole", "body", "pose", "base", "reconstruct", "3d", "structure", "image", "measurement", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "31:6", "T.", "Shiratori", "et", "al.", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "camera", "projection", "function", "-lrb-", "-rrb-", "function", "apply", "homography", "between", "consecutive", "image", "image", "measurement", "index", "camera", "time", "3d", "point", "2d", "measurement", "homography", "respectively", "location", "3d", "structure", "point", "world", "coordinate", "system", "corresponding", "2d", "measurement", "2d", "measurement", "after", "lens", "distortion", "correction", "homography", "first", "term", "consider", "reprojection", "error", "3d", "point", "2d", "measurement", "register", "camera", "minimization", "different", "from", "typical", "bundle", "adjustment", "SfM", "camera", "pose", "constrain", "skeleton", "use", "projection", "matrix", "-lrb-", "-rrb-", "camera", "associate", "j-th", "joint", "projection", "function", "represent", ":1", "-lrb-", "-rrb-", ":2", "-lrb-", "-rrb-", "-lrb-", "-rrb-", ":3", "-lrb-", "-rrb-", ":3", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "distort", "reprojected", "position", "use", "fisheye", "lens", "distortion", "parameter", "j-th", "camera", "i-th", "row", "projection", "matrix", "second", "term", "camera", "can", "register", "through", "absolute", "relative", "registration", "rotation", "matrix", "homography", "equation", "-lrb-", "-rrb-", "parameterize", "joint", "angle", "inlier", "2d-2d", "correspondence", "detect", "ransacbased", "homography", "estimation", "use", "image", "measurement", "Smoothness", "term", "can", "also", "consider", "obtain", "smooth", "motion", "difference", "root", "position", "joint", "angle", "between", "consecutive", "frame", "minimize", "10", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "11", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "term", "effective", "particularly", "when", "camera", "pose", "estimate", "from", "absolute", "relative", "registration", "contain", "undesirable", "jitter", "result", "section", "we", "evaluate", "we", "system", "quantitatively", "use", "conventional", "motion", "capture", "system", "ground", "truth", "show", "additional", "result", "collect", "out", "door", "5.1", "quantitative", "evaluation", "First", "we", "evaluate", "effect", "global", "optimization", "step", "Figure", "-lrb-", "-rrb-", "show", "comparison", "between", "camera", "center", "estimate", "Vicon", "marker", "we", "reconstruction", "before", "global", "optimization", "after", "camera", "center", "be", "adjust", "base", "estimate", "skeleton", "Figure", "-lrb-", "-rrb-", "show", "reduction", "error", "after", "global", "optimization", "smoothness", "term", "Figure", "-lrb-", "-rrb-", "compare", "joint", "angle", "trajectory", "obtain", "we", "system", "measurement", "from", "Vicon", "motion", "capture", "system", "top", "row", "show", "joint", "angle", "trajectory", "upper", "body", "bottom", "row", "show", "joint", "angle", "trajectory", "lower", "body", "joint", "angle", "illustrate", "figure", "angle", "axis-angle", "representation", "normalize", "angle", "first", "frame", "capture", "session", "mean", "median", "error", "3.0093", "1.8076", "respectively", "minimum", "maximum", "error", "0.038", "9.52", "respectively", "standard", "deviation", "2.1891", "because", "error", "parent", "joint", "angle", "propagate", "child", "joint", "joint", "angle", "error", "may", "sufficient", "characterize", "error", "overall", "system", "therefore", "we", "also", "evaluate", "error", "joint", "position", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "error", "do", "propagate", "significantly", "because", "optimization", "equation", "-lrb-", "-rrb-", "find", "solution", "all", "camera", "satisfy", "image", "measurement", "mean", "median", "position", "error", "1.76", "cm", "1.42", "cm", "respectively", "minimum", "maximum", "error", "0.053", "cm", "12.24", "cm", "respectively", "standard", "deviation", "1.26", "cm", "method", "comparison", "we", "now", "describe", "how", "we", "obtain", "quantitative", "comparison", "we", "system", "produce", "camera", "pose", "SfM", "space", "while", "motion", "capture", "system", "output", "3d", "marker", "position", "motion", "capture", "space", "compare", "two", "different", "reconstruction", "we", "need", "compute", "follow", "transform", "between", "two", "space", "we", "attach", "three", "marker", "each", "camera", "several", "marker", "static", "object", "collect", "image", "from", "camera", "corresponding", "marker", "position", "from", "motion", "capture", "system", "subject", "move", "use", "3d", "position", "static", "marker", "motion", "capture", "space", "corresponding", "image", "measurement", "specify", "manually", "camera", "center", "position", "orientation", "motion", "capture", "space", "be", "estimate", "thus", "we", "could", "convert", "three", "marker", "position", "motion", "capture", "datum", "camera", "pose", "recover", "similarity", "transform", "from", "SfM", "space", "motion", "capture", "space", "we", "estimate", "scale", "from", "distance", "between", "camera", "center", "pair", "both", "space", "we", "estimate", "translation", "orientation", "from", "SfM", "space", "motion", "capture", "space", "apply", "iterative", "closest", "point", "algorithm", "two", "set", "camera", "center", "parameter", "be", "use", "similarity", "transform", "after", "non-linear", "refinement", "300", "300", "-lrb-", "mm", "-rrb-", "100", "200", "-lrb-", "mm", "-rrb-", "100", "200", "100", "100", "2000Â 4000Â 6000Â 8000", "2000Â 4000Â 6000Â 8000", "before", "global", "opt", "after", "global", "opt", "-lrb-", "mm", "-rrb-", "200", "Vicon", "system", "-lrb-", "mm", "-rrb-", "200", "Vicon", "system", "400", "400", "2000Â 4000Â 6000Â 8000", "2000Â 4000Â 6000Â 8000", "1800", "1800", "-lrb-", "mm", "-rrb-", "1400", "1600", "-lrb-", "mm", "-rrb-", "1400", "1600", "1200", "1200", "2000Â 4000Â 6000Â 8000", "2000Â 4000Â 6000Â 8000", "frame", "-lrb-", "60", "hz", "-rrb-", "frame", "-lrb-", "60", "hz", "-rrb-", "-lrb-", "-rrb-", "before", "global", "optimization", "-lrb-", "-rrb-", "after", "global", "optimization", "figure", "quantitative", "comparison", "estimate", "camera", "center", "those", "obtain", "use", "motion", "capture", "system", "-lrb-", "-rrb-", "sfm", "produce", "noisy", "reconstruction", "camera", "pose", "-lrb-", "-rrb-", "nonlinear", "optimization", "smoothness", "term", "result", "more", "accurate", "estimation", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "31:7", "root", "orientation", "25", "spine", "joint", "30", "20", "we", "system", "15", "25", "Vicon", "system", "10", "20", "500", "15", "hip", "joint", "40", "10", "30", "20", "10", "500", "1000", "Frame", "-lrb-", "60", "hz", "-rrb-", "500", "-lrb-", "-rrb-", "Joint", "angle", "comparison", "spine", "joint", "300Â 200Â 100", "root", "position", "100", "500", "500Â 1000Â 300Â 200", "100", "100", "500", "1000", "500", "200Â 400Â 600Â 800", "1000Â 1200Â 1400Â 1200", "1000", "1000", "we", "system", "800", "Vicon", "system", "50Â 500Â 110000", "1000", "hip", "joint", "400", "1000", "200Â 400Â 600Â 800", "1000Â 1200Â 200Â 2000", "500Â 1000Â 200Â 1000", "200", "400", "500", "1000", "200Â 400Â 600Â 800", "1000Â 1200Â 1000", "frame", "-lrb-", "60", "hz", "-rrb-", "500", "500", "1000", "-lrb-", "-rrb-", "joint", "position", "comparison", "figure", "comparison", "-lrb-", "-rrb-", "joint", "angle", "trajectory", "-lrb-", "-rrb-", "joint", "position", "trajectory", "motion", "measure", "Vicon", "motion", "capture", "system", "5.2", "outdoor", "experiment", "major", "benefit", "we", "system", "portable", "selfcontained", "allow", "prolonged", "capture", "outdoor", "environment", "illustrate", "benefit", "we", "capture", "two", "sequence", "local", "playground", "result", "illustrate", "figure", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "also", "test", "ability", "capture", "fast", "motion", "run", "motion", "street", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "top", "row", "show", "photo", "subject", "perform", "motion", "bottom", "row", "illustrate", "pose", "skin", "character", "use", "joint", "angle", "estimate", "we", "system", "some", "motion", "be", "quite", "dynamic", "we", "observe", "faster", "than", "2.4", "m/s", "instantaneous", "velocity", "camera", "swing", "run", "sequence", "though", "motion", "result", "image", "blur", "roll", "shutter", "effect", "we", "be", "able", "properly", "reconstruct", "sequence", "Figure", "show", "reconstructed", "long", "walking", "motion", "along", "wind", "path", "uneven", "terrain", "subject", "traverse", "considerable", "distance", "far", "greater", "than", "what", "would", "possible", "traditional", "indoor", "motion", "capture", "setup", "we", "superimpose", "sparse", "3d", "structure", "manually", "match", "view", "angle", "photo", "take", "during", "capture", "reference", "sparse", "structure", "provide", "context", "motion", "show", "path", "along", "which", "subject", "have", "walk", "discussion", "we", "introduce", "novel", "system", "capture", "human", "motion", "both", "indoor", "outdoor", "environment", "we", "system", "consist", "16", "more", "consumer", "video", "camera", "attach", "body", "segment", "we", "estimate", "motion", "respect", "world", "geometry", "through", "SfM", "algorithm", "we", "relate", "refine", "camera", "skeletal", "motion", "through", "non-linear", "optimization", "procedure", "we", "system", "have", "number", "advantage", "over", "traditional", "optical", "imu-based", "system", "because", "-lrb-", "-rrb-", "require", "instrumentation", "environment", "can", "easily", "take", "outside", "-lrb-", "ii", "-rrb-", "stable", "do", "suffer", "from", "drift", "-lrb-", "iii", "-rrb-", "provide", "sparse", "3d", "reconstruction", "world", "contextual", "replay", "scene", "creation", "principal", "cause", "failure", "we", "system", "motion", "blur", "automatic", "white", "balancing", "roll", "shutter", "effect", "motion", "scene", "low", "light", "cropped-frame", "format", "find", "many", "commercially", "available", "camera", "can", "introduce", "motion", "blur", "camera", "move", "quickly", "blur", "make", "difficult", "estimate", "correspondence", "across", "frame", "Automatic", "white", "balancing", "which", "can", "disable", "many", "commercial", "camera", "include", "ours", "also", "make", "finding", "correspondence", "challenge", "when", "lighting", "condition", "change", "rapidly", "most", "cmo", "chip", "employ", "rolling", "shutter", "become", "noticeable", "high", "impact", "motion", "substantial", "motion", "scene", "may", "occur", "example", "when", "record", "forest", "windy", "day", "also", "likely", "present", "challenge", "violate", "intrinsic", "assumption", "make", "SfM", "despite", "limitation", "however", "we", "illustrate", "we", "system", "capable", "capture", "everyday", "motion", "outdoors", "extended", "time", "without", "noticeable", "drift", "occlusion", "other", "body", "part", "can", "cause", "error", "motion", "estimation", "practice", "three", "mitigation", "strategy", "use", "first", "camera", "carefully", "place", "body", "minimize", "probabil", "150", "shoulder", "joint", "elbow", "joint", "80", "100", "60", "40", "50", "20", "1000", "500", "1000", "500", "1000", "knee", "joint", "ankle", "joint", "80", "60", "40", "20", "1000", "50", "40", "30", "20", "10", "500", "1000", "500", "1000", "400", "200", "200", "200", "400", "1500", "1000", "shoulder", "joint", "elbow", "joint", "hand", "600Â 800Â 400Â 600", "400Â 200Â 200Â 500", "1000", "500", "1000", "500Â 1000Â 100Â 200", "100", "300", "200Â 500Â 1000", "500", "1000", "500Â 1000Â 400Â 1600", "1200", "800", "-400", "500", "1000", "500", "1000", "500", "1000", "500", "1000", "knee", "joint", "ankle", "joint", "Foot", "600Â 600Â 200Â 200", "500", "1000", "500", "1000", "500", "1000", "1000", "1000", "500", "1000", "500", "500", "1000", "-1000", "500", "1000", "500", "1000", "500", "500", "1000", "600Â 600Â 400Â 400", "200", "200", "500", "1000", "500", "1000", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "31:8", "T.", "Shiratori", "et", "al.", "-lrb-", "-rrb-", "swing", "monkey", "bar", "-lrb-", "-rrb-", "swinging", "swing", "-lrb-", "-rrb-", "run", "street", "figure", "three", "capture", "outside", "laboratory", "environment", "-lrb-", "-rrb-", "swing", "monkey", "bar", "-lrb-", "-rrb-", "swing", "-lrb-", "-rrb-", "run", "street", "top", "row", "illustrate", "recording", "from", "reference", "camcorder", "camera", "performance", "approximately", "match", "time", "render", "result", "below", "we", "able", "reconstruct", "motion", "even", "though", "quite", "dynamic", "ity", "self-occlusion", "from", "body", "part", "instance", "camera", "thigh", "shin", "place", "look", "outward", "right", "side", "leg", "second", "body", "part", "likely", "occluded", "pelvis", "we", "place", "multiple", "camera", "redundancy", "allow", "we", "estimate", "motion", "even", "when", "some", "camera", "experience", "self-occlusion", "finally", "ransac", "provide", "robustness", "case", "minor", "occlusion", "we", "system", "require", "significant", "computation", "power", "compare", "other", "motion", "capture", "system", "bulk", "processing", "time", "involve", "sift", "keypoint", "detection/matching", "minute", "capture", "step", "may", "require", "day", "processing", "all", "camera", "after", "match", "each", "sequence", "require", "approximately", "10", "hour", "10", "iteration", "absolute", "relative", "camera", "registration", "final", "optimization", "can", "take", "up", "hour", "however", "process", "highly", "parallelizable", "GPU", "should", "very", "effective", "speed", "up", "computation", "consumer", "demand", "continue", "push", "camera", "price", "lower", "quality", "higher", "motion", "capture", "use", "body-mounted", "camera", "may", "become", "setup", "choice", "outdoor", "capture", "smaller", "camera", "reduce", "motion", "camera", "relative", "its", "limb", "also", "we", "camera", "produce", "approximately", "1000", "SIFT", "match", "approximately", "300", "inlier", "per", "image", "permit", "other", "attachment", "technology", "camera", "already", "small", "enough", "embedded", "invisibly", "clothing", "city-scale", "3d", "geometrical", "model", "also", "start", "emerge", "-lsb-", "Agarwal", "et", "al.", "2009", "-rsb-", "faster", "structure-from-motion", "implementation", "-lsb-", "Frahm", "et", "al.", "2010", "-rsb-", "introduce", "large", "scale", "model", "can", "directly", "utilize", "we", "system", "contextualize", "long-term", "motion", "compose", "motion", "multiple", "people", "single", "geometrically", "coherent", "environment", "acknowledgement", "we", "would", "like", "thank", "Takeo", "Kanade", "Irfan", "Essa", "Srinivasa", "Narasimhan", "useful", "discussion", "project", "we", "would", "also", "like", "thank", "Moshe", "Mahler", "Valeria", "Reznitskaya", "Matthew", "Kaemmerer", "help", "modeling", "rendering", "Justin", "Macey", "he", "help", "record", "motion", "capture", "datum", "reference", "garwal", "S.", "navely", "N.", "imon", "i.", "eitz", "S.", "M.", "ZELISKI", "R.", "2009", "building", "Rome", "day", "72", "79", "Proc", "International", "Conference", "Computer", "Vision", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "Motion", "Capture", "from", "Body-Mounted", "Cameras", "31:9", "Figure", "Subject", "walk", "along", "long", "wind", "path", "leave", "photo", "scene", "middle", "two", "reconstruct", "walk", "motion", "sparse", "3d", "structure", "right", "photo", "manually", "superimpose", "reconstructed", "scene", "red", "curve", "represent", "trajectory", "subject", "ALLAN", "L.", "uwein", "J.", "ROSTOW", "G.", "olletey", "M.", "2010", "unstructured", "video-based", "rendering", "interactive", "exploration", "casually", "capture", "video", "ACM", "transaction", "Graphics", "29", "heung", "G.", "K.", "AKER", "S.", "ANADE", "T.", "2003", "shapefrom-silhouette", "articulated", "object", "its", "use", "human", "body", "kinematic", "estimation", "motion", "capture", "Proc", "IEEE", "Computer", "Society", "Conference", "Computer", "Vision", "Pattern", "recognition", "77", "84", "orazza", "S.", "UNDERMANN", "L.", "HAUDHARI", "A.", "EMAT", "TIO", "T.", "OBELLI", "C.", "NDRIACCHI", "T.", "2006", "markerless", "motion", "capture", "system", "study", "musculoskeletal", "biomechanic", "visual", "hull", "simulated", "annealing", "approach", "annals", "Biomedical", "Engineering", "34", "1019", "1029", "orazza", "S.", "AMBARETTO", "E.", "UNDERMANN", "L.", "DRIACCHI", "T.", "2010", "Automatic", "generation", "subject-specific", "model", "accurate", "markerless", "motion", "capture", "biomechanical", "application", "IEEE", "transaction", "Biomedical", "Engineering", "57", "806", "812", "AVISON", "a.", "eid", "i.", "olton", "N.", "TASSE", "O.", "2007", "MonoSLAM", "real-time", "single", "camera", "slam", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "29", "1052", "1067", "eutscher", "J.", "eid", "i.", "2005", "articulate", "body", "motion", "capture", "stochastic", "search", "International", "Journal", "Computer", "Vision", "61", "185", "205", "evernay", "F.", "augera", "O.", "2000", "straight", "line", "have", "straight", "machine", "Vision", "application", "13", "14", "24", "UNCAN", "J.", "2010", "Avatar", "cinefex", "120", "-lrb-", "January", "-rrb-", "68", "146", "ischler", "m.", "olle", "R.", "1981", "Random", "sample", "consensus", "paradigm", "model", "fitting", "application", "image", "analysis", "automate", "cartography", "Communications", "ACM", "24", "381", "395", "rahm", "J.-M.", "EORGEL", "P.", "ALLUP", "D.", "OHNSON", "T.", "AGURAM", "R.", "C.", "EN", "Y.-H.", "UNN", "E.", "LIPP", "B.", "AZEBNIK", "S.", "ollefey", "M.", "2010", "building", "Rome", "cloudless", "day", "Proc", "european", "conference", "computer", "Vision", "368", "381", "artley", "R.", "I.", "isserman", "a.", "2004", "multiple", "View", "Geometry", "Computer", "Vision", "Cambridge", "University", "Press", "asler", "N.", "OSENHAHN", "B.", "HORM", "AHLEN", "T.", "M.", "all", "J.", "EIDEL", "h.-p", "2009", "markerless", "motion", "capture", "unsynchronized", "move", "camera", "Proc", "IEEE", "Computer", "Society", "Conference", "Computer", "Vision", "Pattern", "Recogni", "tion", "224", "231", "ELLY", "P.", "ONAIRE", "C.", "O.", "o?c", "onnor", "N.", "E.", "2010", "human", "motion", "reconstruction", "use", "wearable", "accelerometer", "Proc", "ACM", "SIGGRAPH", "Eurographics", "Symposium", "Computer", "Animation", "-lrb-", "Poster", "-rrb-", "LEIN", "G.", "urray", "D.", "2007", "parallel", "tracking", "mapping", "small", "ar", "workspace", "Proc", "ieee", "ACM", "International", "Symposium", "Mixed", "augmented", "reality", "225", "234", "epetit", "V.", "oreno", "oguer", "F.", "ua", "P.", "2009", "EPnP", "accurate", "-lrb-", "-rrb-", "solution", "pnp", "problem", "International", "Journal", "Computer", "Vision", "81", "155", "166", "ouraki", "M.", "A.", "RGYROS", "A.", "2009", "sba", "software", "package", "generic", "sparse", "bundle", "adjustment", "ACM", "transaction", "mathematical", "Software", "36", "30", "owe", "D.", "2004", "distinctive", "image", "feature", "from", "scale-invariant", "key", "point", "International", "Journal", "Computer", "Vision", "60", "91", "110", "oeslund", "T.", "B.", "ILTON", "a.", "UGER", "V.", "2006", "survey", "advance", "vision-based", "human", "motion", "capture", "analysis", "computer", "Vision", "image", "understand", "104", "90", "126", "uja", "m.", "owe", "D.", "G.", "2009", "fast", "approximate", "nearest", "neighbor", "automatic", "algorithm", "configuration", "Proc", "ternational", "conference", "computer", "Vision", "Theory", "Appli", "cation", "331", "340", "ster", "D.", "ARODITSKY", "O.", "ERGEN", "J.", "2006", "visual", "odometry", "ground", "vehicle", "application", "Journal", "Field", "Robotics", "23", "20", "O?B", "RIEN", "J.", "F.", "ODENHEIMER", "R.", "E.", "ROSTOW", "G.", "J.", "odgin", "J.", "K.", "2000", "Automatic", "joint", "parameter", "estimation", "from", "magnetic", "motion", "capture", "datum", "Proc", "Graphics", "Interface", "53", "60", "skiper", "T.", "HU", "Z.", "amarasekera", "S.", "UMAR", "R.", "2007", "visual", "odometry", "system", "use", "multiple", "stereo", "camera", "inertial", "measurement", "unit", "Proc", "IEEE", "Computer", "Society", "Conference", "Computer", "Vision", "Pattern", "recognition", "ollefey", "M.", "OOL", "L.", "V.", "ERGAUWEN", "M.", "ERBIEST", "F.", "ORNELIS", "K.", "op", "J.", "OCH", "R.", "2004", "visual", "modeling", "hand-held", "camera", "International", "Journal", "Computer", "Vision", "59", "207", "232", "askar", "R.", "II", "H.", "de", "ecker", "B.", "ASHIMOTO", "Y.", "UM", "met", "J.", "oore", "D.", "HAO", "Y.", "ESTHUES", "J.", "IETZ", "P.", "nami", "M.", "AYAR", "S.", "ARNWELL", "J.", "OLAND", "M.", "EKAERT", "P.", "RANZOI", "V.", "run", "E.", "2007", "Prakash", "lighting-aware", "motion", "capture", "use", "photosense", "marker", "multiplexed", "illuminator", "ACM", "transaction", "Graphics", "26", "chwarz", "L.", "A.", "ateus", "D.", "avab", "N.", "2010", "multipleactivity", "human", "body", "tracking", "unconstrained", "environment", "Proc", "International", "Conference", "Articulated", "Motion", "Deformable", "Objects", "192", "202", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011", "31:10", "T.", "Shiratori", "et", "al.", "lyper", "R.", "odgin", "J.", "K.", "2008", "Action", "capture", "accelerometer", "Proc", "ACM", "SIGGRAPH", "Eurographics", "Symposium", "Computer", "Animation", "navely", "N.", "eitz", "S.", "M.", "ZELISKI", "R.", "2006", "Photo", "tourism", "explore", "photo", "collection", "3d", "ACM", "transaction", "Graphics", "25", "835", "846", "autge", "J.", "inke", "a.", "UGER", "B.", "AUMANN", "J.", "BER", "A.", "ELTEN", "T.", "ULLER", "M.", "eidel", "h.-p.", "berhardt", "B.", "2011", "Motion", "reconstruction", "use", "sparse", "accelerometer", "datum", "ACM", "transaction", "Graphics", "30", "lasic", "D.", "DELSBERGER", "R.", "ANNUCCI", "G.", "ARNWELL", "J.", "ROSS", "M.", "atusik", "W.", "opovus", "J.", "2007", "Practical", "motion", "capture", "everyday", "surroundings", "ACM", "transaction", "Graphics", "26", "35", "ELCH", "G.", "oxlin", "E.", "2002", "Motion", "tracking", "silver", "bullet", "respectable", "arsenal", "IEEE", "Computer", "Graphics", "application", "22", "24", "38", "ELCH", "G.", "ISHOP", "G.", "ICCI", "L.", "RUMBACK", "S.", "ELLER", "K.", "oluccus", "D.", "1999", "HiBall", "tracker", "Highperformance", "wide-area", "tracking", "virtual", "augmented", "environment", "Proc", "ACM", "Symposium", "Virtual", "reality", "Software", "Technology", "10", "OLTRING", "H.", "1974", "New", "possibility", "human", "motion", "study", "real-time", "light", "spot", "position", "measurement", "Biotelemetry", "ie", "L.", "UMAR", "M.", "ao", "Y.", "RACANIN", "D.", "UEK", "F.", "2008", "data-driven", "motion", "estimation", "low-cost", "sensor", "Proc", "International", "Conference", "Visual", "Information", "Engineering", "hang", "Z.", "Z.", "HEN", "J.", "J.-K", "2009", "ubiquitous", "human", "body", "motion", "capture", "use", "micro-sensor", "Proc", "IEEE", "International", "Conference", "Pervasive", "Computing", "Communications", "hu", "Z.", "SKIPER", "T.", "amarasekera", "S.", "awhney", "H.", "UMAR", "R.", "2007", "ten-fold", "improvement", "visual", "odometry", "use", "landmark", "matching", "Proc", "International", "Conference", "Computer", "Vision", "hu", "Z.", "SKIPER", "T.", "amarasekera", "S.", "UMAR", "R.", "AWHNEY", "H.", "2008", "real-time", "global", "localization", "prebuilt", "visual", "landmark", "database", "Proc", "IEEE", "Computer", "Society", "Conference", "Computer", "Vision", "Pattern", "recognition", "appendix", "estimate", "Skeleton", "from", "Rangeof-Motion", "Joints", "point", "connect", "parent", "child", "limb", "limb", "associate", "parent", "camera", "child", "camera", "while", "joint", "position", "world", "coordinate", "system", "change", "over", "time", "joint", "position", "parent", "child", "camera", "coordinate", "system", "constant", "-lsb-", "o?brien", "et", "al.", "2000", "-rsb-", "-lrb-", "Figure", "10", "-lrb-", "-rrb-", "-rrb-", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "euclidean", "transformation", "matrix", "from", "parent", "child", "camera", "coordinate", "system", "world", "coordinate", "system", "respectively", "equation", "-lrb-", "12", "-rrb-", "follow", "13", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "Rotation", "axis", "-lrb-", "-rrb-", "Joint", "associate", "camera", "-lrb-", "-rrb-", "knee", "joint", "figure", "10", "-lrb-", "-rrb-", "skeleton", "parameterize", "parent", "child", "camera", "pose", "use", "local", "coordinate", "vector", "+1", "-lrb-", "-rrb-", "one-dof", "joint", "produce", "family", "solution", "joint", "position", "lie", "axis", "rotation", "assume", "rest", "pose", "fully", "extend", "extremity", "where", "both", "limb", "coincident", "joint", "co-linear", "joint", "position", "can", "regularize", "thus", "collect", "equation", "-lrb-", "13", "-rrb-", "j-th", "joint", "across", "time", "provide", "homogeneous", "equation", "14", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "...", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "two", "three-dof", "ball", "joint", "right", "null", "vector", "obtain", "singular", "value", "decomposition", "-lrb-", "svd", "-rrb-", "solution", "can", "also", "compute", "similar", "way", "obtain", "skeleton", "whole", "body", "-lrb-", "+1", "-rrb-", "th", "joint", "position", "from", "parent", "joint", "corresponding", "camera", "coordinate", "system", "compute", "each", "limb", "15", "+1", "where", "inhomogeneous", "coordinate", "joint", "coordinate", "system", "additional", "constraint", "knee", "knee", "one-dof", "hinge", "joint", "equation", "-lrb-", "14", "-rrb-", "become", "undetermined", "system", "two", "null", "vector", "can", "obtain", "from", "knee", "joint", "position", "thigh", "camera", "coordinate", "system", "linear", "combination", "null", "vector", "-lrb-", "figure", "10", "-lrb-", "-rrb-", "-rrb-", "16", "where", "matrix", "consist", "two", "null", "vector", "2d", "coefficient", "vector", "null", "vector", "determine", "we", "consider", "collinearity", "constraint", "cause", "straight", "knee", "rest", "pose", "collinearity", "constraint", "represent", "17", "where", "hip", "ankle", "joint", "position", "-lsb-", "-rsb-", "skew-symmetric", "representation", "vector", "cross", "product", "collinearlity", "constraint", "enable", "unique", "solution", "knee", "joint", "position", "ACM", "transaction", "Graphics", "Vol", "30", "no.", "Article", "31", "publication", "date", "July", "2011" ],
  "content" : "\n  \n    a191cd8b521ea1f52d8164b0dec08ed4f6c174e4478e1e7f5ee9a1b889d9e17c\n    p24\n    10.1145/1964921.1964926\n    Name identification was not possible. \n  \n  \n    \n      \n        Motion Capture from Body-Mounted Cameras\n      \n      Takaaki Shiratori ? Hyun Soo Park ? Leonid Sigal ? Yaser Sheikh ? Jessica K. Hodgins ?? ? Disney Research, Pittsburgh ? Carnegie Mellon University\n      \n        \n        \n      \n      (a) Body-mounted cameras (b) Skeletal motion and 3D structure\n      \n        Figure 1: Capturing both relative and global motion in natural environments using cameras mounted on the body.\n      \n      Motion capture technology generally requires that recordings be performed in a laboratory or closed stage setting with controlled lighting. This restriction precludes the capture of motions that require an outdoor setting or the traversal of large areas. In this paper, we present the theory and practice of using body-mounted cameras to reconstruct the motion of a subject. Outward-looking cameras are attached to the limbs of the subject, and the joint angles and root pose are estimated through non-linear optimization. The optimization objective function incorporates terms for image matching error and temporal continuity of motion. Structure-from-motion is used to estimate the skeleton structure and to provide initialization for the non-linear optimization procedure. Global motion is estimated and drift is controlled by matching the captured set of videos to reference imagery. We show results in settings where capture would be difficult or impossible with traditional motion capture systems, including walking outside and swinging on monkey bars. The quality of the motion reconstruction is evaluated by comparing our results against motion capture data produced by a commercially available optical system. CR Categories: I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism?Animation; Keywords: Motion capture, structure-from-motion, articulated motion, wearable cameras Links: DL PDF W EB V IDEO\n      \n        \n        \n        \n        \n      \n      ? {shiratori, lsigal}@disneyresearch.com ? {hyunsoop, yaser, jkh}@cs.cmu.edu ACM Reference Format Shiratori, T., Park, H., Sigal, L., Sheikh, Y., Hodgins, J. 2011. Motion Capture from Body-Mounted Cameras. ACM Trans. Graph. 30, 4, Article 31 (July 2011), 10 pages. DOI = 10.1145/1964921.1964926 http://doi.acm.org/10.1145/1964921.1964926. Copyright Notice Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2011 ACM 0730-0301/2011/07-ART31 $10.00 DOI 10.1145/1964921.1964926 http://doi.acm.org/10.1145/1964921.1964926\n      \n        \n      \n      (c) Rendered actor\n    \n    \n      \n        1 Introduction\n      \n      Motion capture has been used to provide much of the character motion in several recent theatrical releases. In Avatar, motion capture was used to animate characters riding on direhorses and flying on the back of mountain banshees [Duncan 2010]. To capture realistic motion for such scenes, the actors rode horses and robotic mockups in an expansive motion capture studio requiring a large number of cameras. Coverage and lighting problems often prevent directors from capturing motion in natural settings or in other large environments. Inertial systems, such as the one described by Vlasic and colleagues [2007], allow capture to occur in outdoor spaces but are designed to recover only the relative motion of the joints, not the global root motion. In this paper, we present a wearable system of outward-looking cameras that allow the reconstruction of the relative and the global motion of an actor outside of a laboratory or closed stage. The cameras can be mounted on casual clothing ( Figure 1(a) ), are easily mounted and removed using Velcro attachments, and are lightweight enough to allow unimpeded movement. Structurefrom-motion (SfM) is used to estimate the pose of the cameras throughout the capture. The estimated camera movements from a range-of-motion sequence are used to automatically build a skeleton using co-occurring transformations of the limbs connecting each joint. The reconstructed cameras and skeleton ( Figure 1(b) ) are used as an initialization for an overall optimization to compute the root position, orientation, and joint angles while minimizing the image matching error. Reference imagery of the capture area is leveraged to reduce drift. We render the motion of a skinned character by applying the recovered skeletal motion ( Figure 1(c) ). By estimating the camera poses, the global and relative motion of an actor can be captured outdoors under a wide variety of lighting conditions or in extended indoor regions without any additional equipment. We also avoid some of the missing data problems introduced by occlusions between the markers and cameras in traditional optical motion capture, because, in our system, any visually distinctive feature in the world can serve as a marker in the traditional systems. A by-product of the capture process is a sparse 3D structure of the scene. This structure is useful as a guide for defining the ground geometry and as a first sketch of the scene for 3D animators and directors. We evaluate our approach against motion capture data generated by a Vicon optical motion capture system and report a mean joint position error of 1.76 cm and a mean joint angle error of 3.01 ? on the full range-of-motion sequence used for skeleton estimation. Our results demonstrate that the system can reconstruct actions that are difficult to capture with traditional motion capture systems, including outdoor activities in direct sunlight, activities that are occluded by near by proximal structures, and extended indoor activities. Our prototype is the first, to our knowledge, to employ camera sensors for motion capture by measuring the environment and to estimate the motion of a set of cameras that are related by an underlying articulated structure. Current cameras are inexpensive, have form factors that rival inertial measurement units (IMUs), and are already embedded in everyday handheld devices. Our approach will continue to benefit from consumer trends that are driving cameras to become cheaper, smaller, faster, and more pervasive. Given the expected continuation of these technological trends, we believe that systems such as the one proposed here, will become viable alternatives to traditional motion capture technologies.\n      ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n      31:2\n      ?\n      T. Shiratori et al.\n      \n        2 Related work\n        There are a variety of motion capture technologies currently available both commercially and as prototypes. The advantages and disadvantages of the different designs are discussed in several surveys (e.g., [Welch and Foxlin 2002; Moeslund et al. 2006]). Motion capture systems can be classified as outside-in [Welch and Foxlin 2002], in that they rely on sensors mounted in the environment and passive, if any, markers on the body. By definition, this requirement restricts their use to laboratory environments or closed stage settings, because the capture space has to be instrumented with the sensors. Inside-out systems [Welch and Foxlin 2002] rely on sensors on the body to recover the 3D pose. This portability allows their use in both indoor and outdoor environments. Our approach falls into the latter category. Here, we review the most relevant methods and systems. Optical motion capture systems [Woltring 1974] are among the most widely used in the industry today; commercial systems are available from Vicon (www.vicon.com) and Qualisys (www.qualisys.com), among others. Optical motion capture systems use a set of specialized high-resolution video cameras to track retro-reflective markers or light-emitting diodes (LEDs) placed at key points on the body. Triangulation is used to recover the 3D position of these markers in space, and the 3D marker positions, in turn, are used to fit a skeletal model to the observed motion. These systems are popular due to their accuracy; their major disadvantages are cost, portability, and intrusiveness. Optical systems require indoor setups that typically cost between tens and hundreds of thousands of dollars. The use of photosensors was explored by Raskar and colleagues [2007]. Their proposed system relied on measuring the spatio-temporal light modulations produced by multiple LED transmitters that emitted gray coded patterns. The receiver modules, equipped with infrared and RGB photosensors, were tasked with decoding (demultiplexing) the observed patterns and, in doing so, directly producing the 3D spatial location (and as a side effect measuring incident light for scene light matching). While their system was inspirational for us in that it utilized a simplified photosensor as a ?camera? worn on the body, it is fundamentally different from our approach, because it requires transmitters in the environment. To alleviate the intrusive characteristics of marker-based motion capture systems, marker-less motion capture technologies have been developed by a number of researchers [Cheung et al. 2003;  Deutscher and Reid 2005; Moeslund et al. 2006; Hasler et al. 2009]. Marker-less methods most often use regular video cameras with simple (e.g., chromakey) backgrounds to reconstruct a voxel representation of the body over time and then fit a skeletal model to the voxel representations. A similar paradigm is used by the system developed by Organic Motion (www.organicmotion.com). Recent studies [Corazza et al. 2006; Corazza et al. 2010] suggest that with a sufficient number of cameras and favorable imaging conditions, the accuracy of marker-less methods can rival that of traditional optical motion capture. Hasler and colleagues [2009] introduced an approach to capture the motion of an actor in outdoor environments from multiple inward-looking moving cameras. The method uses audio to synchronize the cameras and fits a 3D scan of the actor to silhouettes estimated in each of the moving cameras. The markerless methods require image segmentation, or a 3D scan of the actor. The most direct approach to measuring human motion is through the use of a wearable electro-mechanical system; e.g., Gypsy (www.animazoo.com). Such systems consist of an exoskeleton suit with embedded lightweight rods that articulate with the performer?s bones. Potentiometers at the joints measure the angular rotation of the rods, and are converted to joint angles using a kinematic model. Such systems, while capable of directly measuring the motion of the subject, are intrusive and uncomfortable to wear. Recently, there have been a number of self-contained, wearable experimental systems developed based on a variety of sensor technologies (e.g., [Schwarz et al. 2010; Zhang et al. 2009]), including ultrasound, IMUs, and tri-axial accelerometers. Inertial motion capture systems (e.g., Xsens MVN, www.xsens.com) measure the rotation of body parts in the world using accelerometers and gyroscopes. These systems are portable and can be taken outside; however, they are only able to measure the orientation of body parts, not the motion of the body in the world. Multiple sensors can be combined to alleviate drift. For example, Vlasic and colleagues [2007] added ultrasonic sensors to IMUs. Alternatives for battling drift include data-driven approaches based on motion capture data to stabilize accelerometer estimates [Slyper and Hodgins 2008; Xie et al. 2008; Kelly et al. 2010; Tautges et al. 2011]. Our system is camera-based and therefore relies on the rich data in a detailed view of the environment. We use the images from the cameras along with the estimated 3D geometry of the environment to recover the 3D limb positions and orientations in the world over time. Thus, we build on substantial prior work in SfM [Hartley and Zisserman 2004; Pollefeys et al. 2004; Snavely et al. 2006] and visual Simultaneous Localization and Mapping (SLAM) [Welch et al. 1999; Davison et al. 2007; Klein and Murray 2007]. These approaches have been used for estimating the motion of moving platforms [Ballan et al. 2010; N?ster et al. 2006] and even humans [Oskiper et al. 2007; Zhu et al. 2007; Zhu et al. 2008]. However, they recovered only the independent ego-motion of individual camera platforms. Our work is the first to reconstruct the 3D motion of a set of cameras related by an underlying articulated structure.\n        \n          \n        \n        (a) Front (b) Side (c) Skeleton\n        \n          Figure 2: Settings of cameras from (a) front view and (b) side view. (c) Illustration of skeleton and body-mounted cameras. Blue: cameras mounted on the body, and orange: cameras used as virtual cameras.\n        \n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n        Motion Capture from Body-Mounted Cameras\n        ?\n        31:3\n        SfM\n        \n          \n          \n          Figure 3: Our system takes video data captured by body-mounted cameras and outputs the reconstruction of the human motion. The motion of the body is estimated by using SfM on individual cameras as an initial guess and optimizing the reprojection errors of the 3D structure while enforcing the underlying articulated relationships between cameras and the smoothness of motion across time.\n        \n      \n      \n        3 Hardware Setup\n        One camera is attached to each body segment using a Velcro strapon mount, as shown in Figures 2(a) and (b). Three cameras are attached to the waist for root pose estimation, and two cameras are attached to the torso. The cameras are synchronized using a standard audio calibration signal 1 . The subject performs a range-of-motion trial for skeleton estimation and then performs the desired activity for capture. The video data are downloaded from the cameras after the capture for processing. Our system produces the skeleton of the actor, root position, and orientation and joint angles across time and also the 3D structure of the scene as a by-product. We use 16 or more commercially available wide-angle (170 ? field of view) sport action cameras called HD Hero from GoPro (www.goprocamera.com) at a cost of 250 dollars per camera; making our entire setup approximately 5,000 dollars. The cameras are lightweight at 94 g and have a small form factor (42 mm ? 60 mm ? 30 mm). HD Hero cameras are equipped with a CMOS sensor and are capable of a variety of resolution/frame rate settings; we record at 720p (1280 ? 720) resolution at 60 frames per second. If cameras on some body segments are often occluded by limbs (e.g., waist, torso), we use additional cameras to provide robustness by creating a wider aggregate field of view. All the cameras are calibrated in advance using a fisheye lens distortion model [Devernay and Faugeras 2000] to provide estimates of focal length, principal point, and the distortion coefficient. As the lens and focal length are fixed for the cameras, these estimates need to be computed only once and are re-usable across captures.\n      \n      \n        4 Reconstructing Human Motion\n        Conventional SfM can provide visually feasible estimates of 3D structure and camera pose, but these estimates are often not sufficiently accurate for capturing human motion. In order to compute\n        1 We use a clapper board to produce a loud clap at the beginning and end of each trial. We find peaks in the audio signal of resulting movie files from all the cameras and look for the most consistent duration between peaks using a simple form of clustering and exhaustive search [Hasler et al. 2009].\n        appropriate estimates of human motion across time, our SfM solution considers the articulation of body-mounted cameras with the underlying skeleton of the actor and fits them to image measurements:\n        \n          1\n          {O ? , A ? } = argmin E r + ? O E O + ? A E A , O,A\n        \n        where O and A are the time-series data of the root position and the joint angles, respectively. E r accounts for reprojection errors of the 3D reconstruction with measured image feature locations using the skeleton constraint for the cameras. E O and E A consider the smoothness of resulting motion. ? O and ? A are weights that control the influence of the smoothness constraints. Equation (1) is highly non-linear and optimization of the equation requires good initial estimates of camera poses and skeleton. We develop the pipeline shown in Figure 3 . After the data are captured, we reconstruct the 3D structure of the scene from reference images using SfM. While this step is optional in principle, it substantially reduces the drift in the reconstructed motions, and we chose to perform it for all our captures. The 3D structure is used to reconstruct body-mounted camera poses across time (Section 4.1). If a new skeleton is required, the subject is asked to perform a standard range-of-motion exercise at the beginning of the capture session. The skeleton is automatically generated (see Appendix) and is used to reconstruct whole body poses from the cameras (Section 4.2). The user can optionally refine the skeleton by changing the pose of the camera with respect to the joint through a graphical user interface. Finally, the motion is refined using an image-based non-linear optimization that incorporates temporal smoothing (Section 4.3).\n        \n          4.1 Initializing Camera Poses Using SfM\n          Direct incremental SfM from body-mounted cameras yields precise 3D reconstruction locally but suffers from global drift when the capture area is large and 3D structure is far from the camera locations [Hartley and Zisserman 2004]. To avoid this problem, we record reference images of the capture area, and reconstruct the 3D structure using the images. Using this 3D structure and corresponding 2D measurements from a body-mounted camera, the camera pose can be reconstructed. We call this process absolute camera registration. If there are significant differences in view between the reference images and the recorded videos, some cameras may not be reconstructed. We handle this situation by adding new structure points with newly registered cameras, and rerun the camera registration. We call this iterative process relative camera registration, and repeat the process until most of cameras are reconstructed. 3D Reconstruction of Reference Images: From the reference images, we extract Scale-Invariant Feature Transform (SIFT) key points [Lowe 2004] and find correspondences between a pair of images using an approximate nearest neighbor search [Muja and Lowe 2009]. The fundamental matrix estimation based on RANSAC [Fischler and Bolles 1981] enables us to obtain geometrically consistent matches. To estimate the extrinsic parameters of the cameras, we choose an initial pair of images that has a significant number of matches that cannot be accounted for by a homography. From those matches, we estimate the relative camera orientation and translation extracted by the essential matrix and triangulate the location of the matched feature points in 3D using the Direct Linear Transform algorithm [Hartley and Zisserman 2004], followed by a two-image bundle adjustment [Lourakis and Argyros 2009]. We incrementally add an image that has the greatest number of inlier 3D-2D correspondences, among the remaining images. From these correspondences, we reconstruct the camera pose using a Perspective-n-Point (PnP) algorithm [Lepetit et al. 2009] inside a RANSAC procedure. Once the extrinsic parameters for the new camera are reconstructed, 2D-2D correspondences between reconstructed images and the newly added image are reconstructed in 3D. For accuracy, we exclude 3D points with the following criteria: any point that has high reprojection error (>1 pixel) and any point when the angle subtended by the rays used for triangulation is small (<2 ? ). Once the structure has been updated, a sparse bundle adjustment is run to refine the entire model. This process continues until most of the reference images are registered. Absolute Camera Registration: After the 3D structure is reconstructed from the reference images, it is used to estimate the bodymounted camera poses. The process of registering images from body-mounted cameras is similar to that of adding a new reference image in the SfM process. Using RANSAC with PnP, we find the best extrinsic camera parameters that produce less than 1 pixel reprojection error when the number of inlier 3D-2D correspondences is sufficient (>50). Once the camera parameters are estimated, new 3D points are triangulated using 2D-2D correspondences between the newly registered image and the previously registered images. To reduce the computational cost of keypoint matching for new 3D points, we ignore camera pairs whose optical axes have more than 90 ? orientation difference. The criteria for adding a new point are the same as those used in the SfM process. The bundle adjustment refines newly registered camera poses and the 3D structure. The 3D structure obtained from the reference images is fixed during the optimization so that the structure can act as an anchor to avoid drift. Relative Camera Registration: The reconstruction from the absolute camera registration may be sparse, particularly when the viewing angles of the reference images are different from those of images from the body-mounted cameras. To increase the density of the reconstruction for the body-mounted camera poses, we find matches between the images from the absolute-registered camera and the images from the unregistered cameras. Because the unregistered cameras are close to the absolute-registered cameras, the viewpoints are similar. This process enables us to reconstruct the poses of the remaining cameras. The relative camera registration processes are iterated until camera registration is satisfactory. Figures 4(b) and 4(c) show the results of absolute and relative camera registration. While the absolute camera registration produces gaps, the fifth iteration of the relative camera registration fills most of the gaps. Homographies for Unregistered Cameras: After the iterative camera registration, there may still be unregistered cameras for particular windows of time. This situation occurs, for example, when an actor performs a fast motion such as running and the images are blurry. To deal with the remaining unregistered cameras, we estimate relative camera orientation between consecutive frames C 1 and C 2 using a homography. When the camera centers of two images coincide, the relative orientation can be estimated from the homographies. Here, we assume that the camera center difference between two consecutive frames is small enough to neglect, compared to the distance between the 3D points and the camera centers. We extract 2D-2D matches based on the SIFT keypoint descriptors and robustly find the consistent homography using RANSAC. Once the homography H is estimated, the relative orientation, C 2 R C 1 , can be obtained by\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n          31:4\n          ?\n          T. Shiratori et al.\n          \n            \n            \n          \n          (a) 3D structure (b) After absolute camera registration\n          \n            Figure 4: (a) Reference structure reconstruction, (b) after absolute camera registration, and (c) after the fifth iteration of relative camera registration. Adding points from absolute and relative camera registration processes allows us to make the camera registration denser.\n          \n          \n            \n          \n          (c) After fifth iteration of relative camera registration\n          \n            2\n            C 2 R C 1 = K C ?1 2 HK C 1 ,\n          \n          where K is an intrinsic parameter matrix. To avoid drift caused by one-way camera orientation estimation with homographies, we take an average of forward and backward interpolation. If camera positions are also needed, linear interpolation of the positions between registered cameras is used. This interpolation provides the initialization of joint angles and root positions. The inlier 2D-2D correspondences used for the homography computation are used as image measurements in the subsequent optimization.\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n          Motion Capture from Body-Mounted Cameras\n          ?\n          31:5\n        \n        \n          4.2 Mapping Cameras to a Skeleton\n          At the beginning of the capture, the actor is asked to perform a predefined range-of-motion exercise, in which he exercises each joint through its full range of motion. We extract the underlying skeleton structure from the images recorded during the range-of-motion performance. As is common with commercial motion capture systems like Vicon, we use a predefined kinematic structure. One or more cameras are associated with each link in the kinematic structure, as shown in Figure 2(c) . The root of the skeleton has six degrees of freedom (DOFs), and the joints have three DOFs. We apply the method of O?Brien and colleagues [2000] to estimate the skeleton and the 3D spatial relationship of each camera to the kinematic structure (see Appendix). We do not currently consider biomechanical constraints. Forward Kinematics from Camera Poses: The skeleton provided by the range-of-motion exercise is parameterized by the root position, root orientation and joint angles. The root position and orientation are taken to be coincident with the root camera. Hence given the skeleton, we can obtain a pose for each time instant by applying the waist camera pose to the root segment directly and applying the relative orientations between pairs of cameras, along the kinematic chain, to the joints. Note that positions of the camera poses are not used except for the waist cameras. Equation (1) considers the skeleton as a hard constraint for refinement. Forward kinematics enables us to maintain this constraint by estimating camera positions with respect to the skeleton. The Euclidean transformation from the joint coordinate system, J , to the world coordinate system, W, is defined as\n          \n            3\n            W T J (t) = W R J (t) W p j (t) , 0 1\n          \n          where W R J and W p j are the orientation of the corresponding camera and the position of the joint in W, respectively 2 . Therefore, the position of its child joint, W p j+1 (t), in W is computed as\n          \n            4\n            W W J p j+1 (t) = T J (t) q,\n          \n          where J q is a vector from the parent joint to the child joint in J . This formulation allows us to estimate the hierarchical joint position, recursively. Similarly, the camera position in the world coordinate system can be re-estimated as\n          \n            5\n            W C j (t) = W T J (t) ? J p j , 1\n          \n          where W C j (t) is a camera center attached to the j-th joint at time t in W. Virtual Cameras for Robust Limb Pose Estimation: Estimating body-attached camera poses from SfM while the body is moving is sometimes difficult because of motion blur, the rolling shutter effect, occlusion by limbs, and lack of texture in the background (e.g., sky). Under such conditions, the camera poses cannot be reconstructed or are very noisy. When camera poses are mis-estimated, the resulting motion of the skeleton is incorrect. To alleviate this problem, we attach multiple cameras to the limb and estimate the limb motion from a virtual camera, which takes a robust average of those cameras (estimated using SfM). We use a virtual camera where occlusion occurs frequently, where a precise estimation is essential (e.g., for the root), or where camera registration is difficult (e.g., for the chest to account for non-rigidity, or shin to deal with 2 p is an inhomogeneous representation of p.  fast motion and impacts that result in imaging artifacts). The virtual camera reduces the occlusion problem significantly and allows skeletal motion to be reconstructed robustly. The virtual camera poses can be estimated from motion over time. Here, we assume that there are three physical cameras, C 1 , C 2 , and C 3 , tightly connected to a single limb. One camera, e.g., C 1 , is selected as a reference camera. As shown in Figure 5(a) , the average relative transforms from the other two cameras to the reference camera 3 , C 1 T  ? C 2 and C 1 T  ? C 3 , can be estimated across time,\n          \n            \n            \n            \n            \n            \n            \n          \n          (a) Computing a virtual camera pose (b) Parameterizing physical with average transform cameras with a virtual camera\n          \n            Figure 5: Illustration of a virtual camera (blue) created from physical cameras (orange). The virtual cameras are used to combine information from multiple body-attached cameras for additional accuracy. (a) Average relative transforms between the cameras are estimated across time, and the virtual camera pose is estimated by applying the average relative transforms to the physical cameras at each time instant. (b) The physical cameras are parameterized by the virtual camera using the average relative transforms.\n          \n          \n            6\n            C 1 T  ? C 2 = f a C 1 T C 2 (1), C 1 T C 2 (2), ? ? ? , C 1 T C 2 (T ) ,\n          \n          where f a (?) is a function that takes an average of the transforms. Once the average transform is estimated, the inverse of the average transform is used as a transform from the virtual camera, V, to the physical cameras, i.e.,\n          \n            7\n            C 2 T V = C 1 T  ? C ?1 2 , C 3 T V = C 1 T  ? C ?1 3 .\n          \n          Then, the virtual camera pose can be obtained by again taking an average of the transforms for C 1 , C 2 , and C 3 . Now the transforms from the virtual camera to each physical camera are known, which implies all physical cameras can be parameterized by the virtual camera pose ( Figure 5(b) ). This parameterization will be used when reprojection errors are computed in the subsequent optimization.\n        \n        \n          4.3 Estimating Body Poses with Global Optimization O A\n          The final step is to optimize body poses and by minimizing the objective function in Equation (1). Conceptually, the optimization seeks to find body poses of the skeleton, over time, that are temporally smooth and result in low spatial error between the projected 3D structure, through the estimated cameras, and the actual observed structure in the images. The initial guess of the body pose is set with the registered camera poses and homographies, and the Levenberg-Marquardt method is applied to refine the poses. Considering all poses over time in the optimization is computationally expensive. Instead, we use a short time window and sequentially optimize the poses by shifting the window. 3 If the cameras are rigidly connected, the average relative transforms are exactly the same as the relative transform at each time instant.  Reprojection Error Term: E r refines whole body poses based on the reconstructed 3D structure and image measurements:\n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n          31:6\n          ?\n          T. Shiratori et al.\n          \n            8\n            E r = P j (X p , t, O, A) ? x j,t,p ? 2 j,t,p + H j (t, A, x j,t,h ) ? x j,t?1,h ? 2 , j,t,h\n          \n          where P (?) is a camera projection function, H(?) is a function to apply a homography between consecutive images to an image measurement, and j, t, p, and h are indices of cameras, time, 3D points and 2D measurements for homographies, respectively. X is the location of the 3D structure point in the world coordinate system, and x is the corresponding 2D measurement. x is a 2D measurement after lens distortion correction for the homography. The first term considers the reprojection errors of the 3D points with the 2D measurements for the registered cameras. This minimization is different from typical bundle adjustment of SfM in that the camera poses are constrained by the skeleton. Using the projection matrix, P j (t), of the camera associated with the j-th joint, the projection function P j is represented as\n          \n            9\n            P j:1 (t)X p P j:2 (t)X p P j (X p , t, O, A) = L j , , P j:3 (t)X p P j:3 (t)X p\n          \n          where L j (?) distorts the reprojected position using the fisheye lens distortion parameter of the j-th camera, and P j:i is the i-th row of the projection matrix P j . The second term is for the cameras that cannot be registered through the absolute and relative registration. The rotation matrices of the homographies in Equation (2) are parameterized with the joint angles. The inlier 2D-2D correspondences detected in the RANSACbased homography estimation are used as image measurements. Smoothness Terms: E O and E A can be also considered to obtain smooth motion. The differences of the root positions and joint angles between consecutive frames are minimized as\n          \n            10\n            E O = O(t) ? O(t ? 1) 2 ? , t\n          \n          \n            11\n            E A = A(t) ? A(t ? 1) 2 ? . t\n          \n          These terms are effective, particularly when the camera poses estimated from the absolute and relative registration contain undesirable jitter.\n        \n      \n      \n        5 Results\n        In this section, we evaluate our system quantitatively using a conventional motion capture system as ground truth, and show additional results collected out of doors.\n        \n          5.1 Quantitative Evaluation\n          First, we evaluate the effect of the global optimization step. Figure 6(a) shows the comparison between the camera centers estimated by the Vicon markers and our reconstruction before the global optimization but after the camera centers were adjusted based on the estimated skeleton. Figure 6(b) shows the reduction in error after global optimization with the smoothness terms. Figure 7(a) compares the joint angle trajectories obtained by our system with the measurements from the Vicon motion capture system. The top row shows the joint angle trajectories of the upper body and the bottom row shows the joint angle trajectories of the lower body. The joint angles illustrated in the figure are the angle of the axis-angle representation normalized by the angle of the first frame in the capture session. The mean and median errors are 3.0093 ? and 1.8076 ? , respectively, and the minimum and the maximum errors are 0.038 ? and 9.52 ? , respectively. The standard deviation is 2.1891 ? . Because the error of a parent joint angle propagates to a child joint, the joint angle errors may not be sufficient to characterize the error of the overall system. Therefore, we also evaluate the errors of the joint positions ( Figure 7(b) ). The error does not propagate significantly, because the optimization of Equation (1) finds a solution such that all cameras satisfy the image measurements. The mean and median position errors are 1.76 cm and 1.42 cm, respectively, and the minimum and the maximum errors are 0.053 cm and 12.24 cm, respectively. The standard deviation is 1.26 cm. Method of Comparison: We now describe how we obtained these quantitative comparisons. Our system produces camera poses in the SfM space, while the motion capture system outputs 3D marker positions in the motion capture space. To compare the two different reconstructions, we needed to compute the following transforms between the two spaces. We attached three markers on each of the cameras and several markers on static objects and collected images from the cameras and the corresponding marker positions from the motion capture system as the subject moved. Using the 3D positions of the static markers in the motion capture space and the corresponding image measurements specified manually, the camera center positions and orientations in the motion capture space were estimated. Thus we could convert the three marker positions in the motion capture data to the camera poses. To recover the similarity transform from the SfM space to the motion capture space, we estimated a scale from the distances between the camera center pairs in both of the spaces. Then, we estimated translation and orientation from the SfM space to the motion capture space by applying the iterative closest point algorithm to the two sets of the camera centers. The parameters were used for the similarity transform after non-linear refinement.\n          300 300 (mm) 100 200 (mm) 100 200 X 0 X 0 ?100 ?100 0 2000 4000 6000 8000 0 2000 4000 6000 8000 0 Before global opt. 0 After global opt. (mm) ?200 Vicon system (mm) ?200 Vicon system Y ?400 Y ?400 0 2000 4000 6000 8000 0 2000 4000 6000 8000 1800 1800 (mm) 1400 1600 (mm) 1400 1600 Z 1200 Z 1200 0 2000 4000 6000 8000 0 2000 4000 6000 8000 Frame (60 Hz) Frame (60 Hz) (a) Before global optimization (b) After global optimization\n          \n            Figure 6: Quantitative comparison of estimated camera centers with those obtained using a motion capture system. (a) SfM produces a noisy reconstruction of the camera poses, and (b) the nonlinear optimization with the smoothness terms results in a more accurate estimation.\n          \n          ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n          Motion Capture from Body-Mounted Cameras\n          ?\n          31:7\n          Root orientation 25 Spine joint 30 20 Our system 15 25 Vicon system 10 20 5 0 0 500 15 Hip joint 40 10 30 5 20 0 10 0 500 1000 Frame (60 Hz) 0 0 500 (a) Joint angle comparison Spine joint 300 200 100 Root position 0 ?100 500 0 500 1000 300 200 0 100 0 100 0 500 1000 ?500 0 200 400 600 800 1000 1200 1400 1200 1000 1000 Our system 800 Vicon system 0 0 50 500 110000 1000 0 Hip joint 400 1000 0 200 400 600 800 1000 1200 200 2000 0 500 1000 200 1000 0 ?200 ?400 0 0 500 1000 0 200 400 600 800 1000 1200 1000 Frame (60 Hz) 500 0 500 1000 (b) Joint position comparison\n          \n            Figure 7: Comparison of (a) joint angle trajectories and (b) joint position trajectories with the motion measured by the Vicon motion capture system.\n          \n        \n        \n          5.2 Outdoor experiments\n          The major benefit of our system is that it is portable and selfcontained, allowing prolonged captures in outdoor environments. To illustrate these benefits we captured two sequences in the local playground; the results are illustrated in Figures 8(a) and 8(b). We also tested the ability to capture fast motions with a running motion on a street ( Figure 8(c) ). The top rows show the photos of the subject performing the motions, and the bottom rows illustrate a posed skinned character using the joint angles estimated by our system. Some of these motions were quite dynamic and we observed faster than 2.4 m/s instantaneous velocity of a camera in the swing and running sequences. Though these motions resulted in image blur, and the rolling shutter effect, we were able to properly reconstruct the sequences. Figure 9 shows a reconstructed long walking motion along the winding path on an uneven terrain. The subject traversed a considerable distance that is far greater than what would be possible in a traditional indoor motion capture setup. We superimposed the sparse 3D structure and manually matched the viewing angle to a photo taken during the capture for reference. The sparse structure provides the context for the motion by showing the path along which the subject has walked.\n        \n      \n      \n        6 Discussion\n        We introduce a novel system for capturing human motion in both indoor and outdoor environments. Our system consists of 16 or  more consumer video cameras attached to body segments. We estimate their motion with respect to the world geometry through a SfM algorithm. We then relate and refine camera and skeletal motion through a non-linear optimization procedure. Our system has a number of advantages over traditional optical and IMU-based systems, because it: (i) requires no instrumentation of the environment and can easily be taken outside, (ii) is stable and does not suffer from drift, and (iii) provides sparse 3D reconstruction of the world for contextual replay or scene creation. The principal causes of failure for our system are motion blur, automatic white balancing, rolling shutter effects, and motion in the scene. Low light and the cropped-frame formats found in many commercially available cameras can introduce motion blur as the camera moves quickly. The blur makes it difficult to estimate correspondences across frames. Automatic white balancing, which cannot be disabled on many commercial cameras including ours, also makes finding correspondences challenging when lighting conditions are changing rapidly. Most CMOS chips employ a rolling shutter that becomes noticeable in high impact motions. Substantial motion in the scene, that may occur, for example, when recording in a forest on a windy day, are also likely to present challenges as they violate the intrinsic assumptions made by SfM. Despite these limitations, however, as we illustrate, our system is capable of capturing everyday motions outdoors for extended periods of time and without noticeable drift. Occlusion by other body parts can cause errors in motion estimation. In practice, three mitigation strategies are used. First, the cameras are carefully placed on the body to minimize the probabil-\n        150\n        Shoulder joint Elbow joint 80\n        100\n        60 40\n        50\n        20\n        1000\n        0 0\n        0 500 1000 0 500 1000 Knee joint Ankle joint 80\n        60\n        40\n        20\n        1000\n        50 40 30 20 10 0 0\n        0 500 1000 0 500 1000\n        400\n        200\n        0 200 0 ?200 ?400 0 1500\n        1000 0\n        Shoulder joint Elbow joint Hand 600 800 400 600 400 200 200 500 1000 0 500 1000 0 500 1000 100 200 ?100 0 ?300 ?200 500 1000 0 500 1000 0 500 1000 400 1600 1200 0 800 -400 500 1000 0 500 1000 0 500 1000\n        500\n        0 0 1000\n        Knee joint Ankle joint Foot 600 600 200 200 0 0 500 1000 0 500 1000 0 500 1000\n        0\n        1000 0 1000\n        500 1000\n        0 0 ?500 ?500 ?1000 -1000 0 500 1000 0 500 1000\n        500\n        0 0\n        500 1000\n        600 600 400 400 200 200 0 500 1000 0 500 1000\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n        31:8\n        ?\n        T. Shiratori et al.\n        \n          \n        \n        (a) Swinging on a monkey bar\n        \n          \n        \n        (b) Swinging on a swing\n        \n          \n        \n        (c) Running on a street\n        \n          Figure 8: Three captures outside of the laboratory environment: (a) Swinging on a monkey bar, (b) on a swing and (c) running on a street. Top rows illustrate recordings from a reference camcorder camera of the performance, approximately matched in time to the rendered results below. We are able to reconstruct these motions even though they are quite dynamic.\n        \n        ity of self-occlusion from body parts. For instance, the cameras on the thighs and shins are placed looking outward on the right side of the leg. Second, for body parts that are likely to be occluded such as the pelvis, we place multiple cameras. This redundancy allows us to estimate motion even when some cameras experience self-occlusion. Finally, RANSAC provides robustness in the case of minor occlusions. Our system requires significant computation power compared to other motion capture systems. The bulk of processing time involves SIFT keypoint detection/matching. For a minute of capture, this step may require a day of processing for all cameras 4 . After matching, each sequence requires approximately 10 hours for 10 iterations of absolute and relative camera registration. The final optimization can take up to 4 hours. However, this process is highly parallelizable and a GPU should be very effective in speeding up this computation. As consumer demand continues to push camera prices lower and quality higher, motion capture using body-mounted cameras may become the setup of choice for outdoor capture. Smaller cameras will reduce the motion of the camera relative to its limb and also 4 Our cameras produce approximately 1000 SIFT matches and approximately 300 inliers per image.  permit other attachment technologies. Cameras are already small enough to be embedded invisibly in clothing. City-scale 3D geometrical models are also starting to emerge [Agarwal et al. 2009] as faster structure-from-motion implementations [Frahm et al. 2010] are introduced. Such large scale models can be directly utilized in our system to contextualize long-term motions and compose motions of multiple people in a single geometrically coherent environment.\n      \n      \n        Acknowledgements\n        We would like to thank Takeo Kanade, Irfan Essa, and Srinivasa Narasimhan for useful discussions on this project. We would also like to thank Moshe Mahler, Valeria Reznitskaya, and Matthew Kaemmerer for their help in modeling and rendering, and Justin Macey for his help in recording the motion capture data.\n      \n      \n        References\n        \n          A GARWAL , S., S NAVELY , N., S IMON , I., S EITZ , S. M., AND S ZELISKI , R. 2009. Building Rome in a day. In 72?79.\n        \n      \n      \n        Proc. International Conference on Computer Vision,\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n        Motion Capture from Body-Mounted Cameras\n        ?\n        31:9\n        \n          \n          Figure 9: Subject walking along a long winding path. Left: a photo of the scene, middle two: the reconstructed walking motion and sparse 3D structure, and right: the photo is manually superimposed to the reconstructed scene. The red curve represents the trajectory of the subject.\n        \n        B ALLAN , L., P UWEIN , J., B ROSTOW , G., AND P OLLETEYS , M. 2010. Unstructured video-based rendering: Interactive exploration of casually captured videos. ACM Transactions on Graphics 29, 4. C HEUNG , G. K., B AKER , S., AND K ANADE , T. 2003. Shapefrom-silhouette of articulated objects and its use for human body kinematics estimation and motion capture. In Proc. IEEE\n      \n      \n        Computer Society Conference on Computer Vision and Pattern\n        Recognition, 77?84. C ORAZZA , S., M UNDERMANN  ? , L., C HAUDHARI , A., D EMAT TIO , T., C OBELLI , C., AND A NDRIACCHI , T. 2006. A markerless motion capture system to study musculoskeletal biomechanics: Visual hull and simulated annealing approach. Annals of Biomedical Engineering 34, 6, 1019?1029. C ORAZZA , S., G AMBARETTO , E., M UNDERMANN  ? , L., AND A N DRIACCHI , T. 2010. Automatic generation of a subject-specific model for accurate markerless motion capture and biomechanical applications. IEEE Transactions on Biomedical Engineering 57, 4, 806?812. D AVISON , A., R EID , I., M OLTON , N., AND S TASSE , O. 2007. MonoSLAM: Real-time single camera SLAM. IEEE Transactions on Pattern Analysis and Machine Intelligence 29, 6, 1052? 1067. D EUTSCHER , J., AND R EID , I. 2005. Articulated body motion capture by stochastic search. International Journal of Computer Vision 61, 2, 185?205. D EVERNAY , F., AND F AUGERAS , O. 2000. Straight lines have to be straight. Machine Vision and Applications 13, 1, 14?24. D UNCAN , J. 2010. Avatar. Cinefex 120 (January), 68?146. F ISCHLER , M., AND B OLLES , R. 1981. Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM 24, 6, 381?395. F RAHM , J.-M., G EORGEL , P., G ALLUP , D., J OHNSON , T., R AGURAM , R., W U , C., J EN , Y.-H., D UNN , E., C LIPP , B., L AZEBNIK , S., AND P OLLEFEYS , M. 2010. Building Rome on a cloudless day. In Proc. European Conference on Computer Vision, 368?381. H ARTLEY , R. I., AND Z ISSERMAN , A. 2004. Multiple View Geometry in Computer Vision. Cambridge University Press. H ASLER , N., R OSENHAHN , B., T HORM AHLEN  ? , T., W AND , M., G ALL , J., AND S EIDEL , H.-P. 2009. Markerless motion capture with unsynchronized moving cameras. In Proc. IEEE Computer\n      \n      \n        Society Conference on Computer Vision and Pattern Recogni-\n        tion, 224?231. K ELLY , P., C ONAIRE , C. O.,  ? AND O?C ONNOR , N. E. 2010. Human motion reconstruction using wearable accelerometers. In\n      \n      \n        Proc. ACM SIGGRAPH / Eurographics Symposium on Computer Animation (Poster).\n        K LEIN , G., AND M URRAY , D. 2007. Parallel tracking and mapping for small AR workspaces. In Proc. IEEE and ACM International Symposium on Mixed and Augmented Reality, 225?234. L EPETIT , V., M ORENO -N OGUER , F., AND F UA , P. 2009. EPnP: An accurate O(n) solution to the PnP problem. International Journal of Computer Vision 81, 2, 155?166. L OURAKIS , M. A., AND A RGYROS , A. 2009. SBA: A software package for generic sparse bundle adjustment. ACM Transactions on Mathematical Software 36, 1, 1?30. L OWE , D. 2004. Distinctive image features from scale-invariant key points. International Journal of Computer Vision 60, 2, 91? 110. M OESLUND , T. B., H ILTON , A., AND K R UGER  ? , V. 2006. A survey of advances in vision-based human motion capture and analysis. Computer Vision and Image Understanding 104, 90? 126. M UJA , M., AND L OWE , D. G. 2009. Fast approximate nearest neighbors with automatic algorithm configuration. In Proc. In-\n      \n      \n        ternational Conference on Computer Vision Theory and Appli-\n        cation, 331?340. N ? STER , D., N ARODITSKY , O., AND B ERGEN , J. 2006. Visual odometry for ground vehicle applications. Journal of Field Robotics 23, 1, 3?20. O?B RIEN , J. F., B ODENHEIMER , R. E., B ROSTOW , G. J., AND H ODGINS , J. K. 2000. Automatic joint parameter estimation from magnetic motion capture data. In Proc. Graphics Interface, 53?60. O SKIPER , T., Z HU , Z., S AMARASEKERA , S., AND K UMAR , R. 2007. Visual odometry system using multiple stereo cameras and inertial measurement unit. In Proc. IEEE Computer Society\n      \n      \n        Conference on Computer Vision and Pattern Recognition.\n        P OLLEFEYS , M., G OOL , L. V., V ERGAUWEN , M., V ERBIEST , F., C ORNELIS , K., T OPS , J., AND K OCH , R. 2004. Visual modeling with a hand-held camera. International Journal of Computer Vision 59, 3, 207?232. R ASKAR , R., N II , H., DE D ECKER , B., H ASHIMOTO , Y., S UM MET , J., M OORE , D., Z HAO , Y., W ESTHUES , J., D IETZ , P., I NAMI , M., N AYAR , S., B ARNWELL , J., N OLAND , M., B EKAERT , P., B RANZOI , V., AND B RUNS , E. 2007. Prakash: Lighting-aware motion capture using photosensing markers and multiplexed illuminators. ACM Transactions on Graphics 26, 3. S CHWARZ , L. A., M ATEUS , D., AND N AVAB , N. 2010. Multipleactivity human body tracking in unconstrained environments. In\n      \n      \n        Proc. International Conference on Articulated Motion and Deformable Objects,\n        192?202.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n        31:10\n        ?\n        T. Shiratori et al.\n        S LYPER , R., AND H ODGINS , J. K. 2008. Action capture with accelerometers. In\n      \n      \n        Proc. ACM SIGGRAPH / Eurographics Symposium on Computer Animation.\n        S NAVELY , N., S EITZ , S. M., AND S ZELISKI , R. 2006. Photo tourism: Exploring photo collections in 3D. ACM Transactions on Graphics 25, 3, 835?846. T AUTGES , J., Z INKE , A., K R UGER  ? , B., B AUMANN , J., W E BER , A., H ELTEN , T., M ULLER  ? , M., S EIDEL , H.-P., AND E BERHARDT , B. 2011. Motion reconstruction using sparse accelerometer data. ACM Transactions on Graphics 30, 3. V LASIC , D., A DELSBERGER , R., V ANNUCCI , G., B ARNWELL , J., G ROSS , M., M ATUSIK , W., AND P OPOVI ? , J. 2007. Practical motion capture in everyday surroundings. ACM Transactions on Graphics 26, 3, 35. W ELCH , G., AND F OXLIN , E. 2002. Motion tracking: No silver bullet, but a respectable arsenal. IEEE Computer Graphics and Applications 22, 6, 24?38. W ELCH , G., B ISHOP , G., V ICCI , L., B RUMBACK , S., K ELLER , K., AND C OLUCCI , D. 1999. The HiBall tracker: Highperformance wide-area tracking for virtual and augmented environments. In Proc. ACM Symposium on Virtual Reality Software and Technology, 1?10. W OLTRING , H. 1974. New possibilities for human motion studies by real-time light spot position measurement. Biotelemetry 1, 3. X IE , L., K UMAR , M., C AO , Y., G RACANIN , D., AND Q UEK , F. 2008. Data-driven motion estimation with low-cost sensors. In\n      \n      \n        Proc. International Conference on Visual Information Engineering.\n        Z HANG , Z., W U , Z., C HEN , J., AND W U , J.-K. 2009. Ubiquitous human body motion capture using micro-sensors. In Proc. IEEE\n      \n      \n        International Conference on Pervasive Computing and Communications.\n        Z HU , Z., O SKIPER , T., S AMARASEKERA , S., S AWHNEY , H., AND K UMAR , R. 2007. Ten-fold improvement in visual odometry using landmark matching. In Proc. International Conference\n      \n      \n        on Computer Vision.\n        Z HU , Z., O SKIPER , T., S AMARASEKERA , S., K UMAR , R., AND S AWHNEY , H. 2008. Real-time global localization with a prebuilt visual landmark database. In Proc. IEEE Computer Society\n      \n      \n        Conference on Computer Vision and Pattern Recognition.\n      \n      \n        Appendix: Estimating Skeleton from Rangeof-Motion\n        Joints are a point that connects the parent and child limbs, and these limbs are associated to the parent camera P and the child camera C. While the joint positions in the world coordinate system, W p j , change over time, the joint positions in the parent and child camera coordinate systems, P p j and C p j , are constant [O?Brien et al. 2000] ( Figure 10(a) ):\n        \n          12\n          W W P W C p j (t) = T P (t) p j = T C (t) p j ,\n        \n        where W T P and W T C are 4?4 Euclidean transformation matrices from the parent and child camera coordinate systems to the world coordinate system, respectively. Equation (12) follows that\n        \n          13\n          P W ?1 W C p j = T P (t) T C (t) p j P C = T C (t) p j .\n        \n        Rotation axis (a) Joint and associated cameras (b) Knee joint\n        \n          Figure 10: (a) The skeleton is parameterized by parent and child camera poses, P and C using local coordinate vectors, C p j and\n        \n        C p j+1 . (b) One-DOF joints produce a family of solutions for a joint position that lie on the axis of rotation. By assuming the rest pose is a fully extended extremity, where both limbs coincident at the joint are co-linear, joint position can be regularized. Thus, collecting Equation (13) for the j-th joint across time provides the homogeneous equation for C p j ,\n        \n          14\n          ? P T C (t 1 ) ? P T C (t 2 ) ? ? P T C (t 1 ) ? P T C (t 3 ) ? C C ? ? ? . . . ? ? ? p j = ?T p j = 0. P T C (t 1 ) ? P T C (t T )\n        \n        For two or three-DOF ball joints, the right null vector of ?T obtained with singular value decomposition (SVD) is a solution of C p j . P p j can be also computed in a similar way. To obtain the skeleton for the whole body, the (j+1)-th joint position from the parent joint in the corresponding camera coordinate system, J q, is computed for each limb as\n        \n          15\n          J q = J p j+1 ? J p j , 1\n        \n        where p is an inhomogeneous coordinate of p, and J is the joint coordinate system. Additional Constraint on Knees: Knees are one-DOF hinge joints, and Equation (14) becomes an undetermined system: two null vectors can be obtained from ?T, and the knee joint position in the thigh camera coordinate system, C T p K , is a linear combination of the null vectors ( Figure 10(b) ):\n        \n          16\n          C T p K = V K c,\n        \n        where V K is a matrix consisting of the two null vectors of ?T and c is a 2D coefficient vector for the null vectors. To determine c, we consider the collinearity constraint caused by straight knees in the rest pose. This collinearity constraint is represented as\n        \n          17\n          W p H ? W p A ? W p K ? W p A = 0,\n        \n        where W p H and W p A are the hip and ankle joint positions, and [?] ? is the skew-symmetric representation for vector cross product. The collinearlity constraint enables a unique solution of the knee joint positions.\n        ACM Transactions on Graphics, Vol. 30, No. 4, Article 31, Publication date: July 2011.\n      \n    \n  ",
  "resources" : [ ]
}