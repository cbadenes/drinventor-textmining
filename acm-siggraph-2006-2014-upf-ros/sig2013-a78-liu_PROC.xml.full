{
  "uri" : "sig2013-a78-liu_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2013/a78-liu_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Bundled Camera Paths for Video Stabilization",
    "published" : "2013",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Shuaicheng-Liu",
      "name" : "Shuaicheng",
      "surname" : "Liu"
    }, {
      "uri" : "http://drinventor/Lu-Yuan",
      "name" : "Lu",
      "surname" : "Yuan"
    }, {
      "uri" : "http://drinventor/Ping-Tan",
      "name" : "Ping",
      "surname" : "Tan"
    }, {
      "uri" : "http://drinventor/Jian Sun-null",
      "name" : "Jian Sun",
      "surname" : null
    } ]
  },
  "bagOfWords" : [ "a64539a83c37a9b4748793bf763bbed1f6879625915cf3d86f6025a3f65431b3", "p2q", "10.1145", "2461912.2461995", "name", "identification", "possible", "bundle", "Camera", "Paths", "Video", "Stabilization", "Shuaicheng", "Liu", "Lu", "Yuan", "National", "University", "Singapore", "-lrb-", "-rrb-", "single", "global", "path", "figure", "comparison", "between", "traditional", "2d", "stabilization", "-lrb-", "single", "global", "camera", "path", "-rrb-", "we", "bundle", "camera", "path", "stabilization", "we", "plot", "camera", "trajectory", "-lrb-", "visualize", "y-axis", "translation", "over", "time", "-rrb-", "show", "original", "path", "-lrb-", "red", "-rrb-", "smooth", "path", "-lrb-", "blue", "-rrb-", "both", "method", "we", "bundle", "path", "rely", "2d", "mesh-based", "motion", "representation", "smooth", "space-time", "we", "present", "novel", "video", "stabilization", "method", "which", "model", "camera", "motion", "bundle", "-lrb-", "multiple", "-rrb-", "camera", "path", "propose", "model", "base", "mesh-based", "spatially-variant", "motion", "representation", "adaptive", "space-time", "path", "optimization", "we", "motion", "representation", "allow", "we", "fundamentally", "handle", "parallax", "roll", "shutter", "effect", "while", "do", "require", "long", "feature", "trajectory", "sparse", "3d", "reconstruction", "we", "introduce", "as-similaras-possible", "idea", "make", "motion", "estimation", "more", "robust", "we", "space-time", "path", "smoothing", "adaptively", "adjust", "smoothness", "strength", "consider", "discontinuity", "crop", "size", "geometrical", "distortion", "unify", "optimization", "framework", "evaluation", "large", "variety", "consumer", "video", "demonstrate", "merit", "we", "method", "cr", "category", "i.", "4.3", "-lsb-", "image", "processing", "computer", "Vision", "-rsb-", "enhancement?registration", "keyword", "video", "stabilization", "image", "warping", "camera", "path", "Links", "dl", "pdf", "introduction", "video", "capture", "hand-held", "device", "-lrb-", "e.g.", "cell-phone", "portable", "camcorder", "-rrb-", "often", "appear", "remarkably", "shaky", "undirected", "Digital", "video", "stabilization", "improve", "video", "quality", "remove", "unwanted", "camera", "motion", "great", "practical", "importance", "because", "device", "-lrb-", "mobile", "phone", "tablet", "camcorder", "-rrb-", "capable", "capture", "video", "have", "become", "widespread", "online", "sharing", "so", "ubiquitous", "prior", "video", "stabilization", "method", "synthesize", "new", "stabilize", "video", "estimate", "smooth", "2d", "camera", "motion", "-lsb-", "Matsushita", "et", "al.", "2006", "Grundmann", "et", "al.", "2011", "-rsb-", "3d", "camera", "motion", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2012", "-rsb-", "general", "2d", "method", "more", "robust", "faster", "because", "only", "estimate", "linear", "transformation", "-lrb-", "affine", "homography", "-rrb-", "between", "consecutive", "frame", "2d", "linear", "motion", "model", "too", "weak", "fundamentally", "handle", "parallax", "cause", "non-trivial", "depth", "variation", "scene", "contrary", "3d", "method", "can", "deal", "parallax", "principle", "generate", "strongly", "stabilize", "result", "however", "motion", "model", "estimation", "less", "robust", "various", "degeneration", "feature", "tracking", "failure", "motion", "blur", "camera", "zooming", "rapid", "rotation", "briefly", "2d", "method", "more", "robust", "may", "sacrifice", "quality", "-lrb-", "e.g.", "introduce", "unpleasant", "geometrical", "distortion", "produce", "less", "stabilize", "output", "-rrb-", "while", "3d", "method", "can", "achieve", "high-quality", "result", "more", "fragile", "some", "recent", "method", "-lsb-", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "-rsb-", "have", "successfully", "combine", "advantage", "two", "kind", "method", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "apply", "low-rank", "subspace", "constraint", "2d", "feature", "trajectory", "which", "effective", "simplification", "3d", "reconstruction", "Goldstein", "Fattal", "-lsb-", "2012", "-rsb-", "avoid", "3d", "reconstruction", "exploit", "epipolar", "transfer", "technique", "method", "relax", "requirement", "from", "3d", "reconstruction", "2d", "long", "feature", "tracking", "nevertheless", "require", "long", "feature", "tracking", "-lrb-", "typically", "over", "20", "frame", "-rrb-", "make", "difficult", "handle", "more", "challenging", "case", "-lrb-", "e.g.", "rapid", "motion", "fast", "scene", "transition", "large", "occlusion", "-rrb-", "consumer", "video", "paper", "aim", "same", "goal", "robust", "high-quality", "result", "from", "opposite", "direction", "we", "propose", "more", "powerful", "2d", "camera", "motion", "model", "specifically", "we", "present", "bundle", "camera", "path", "model", "which", "maintain", "multiple", "spatially-variant", "camera", "path", "other", "word", "each", "different", "location", "video", "have", "its", "own", "camera", "path", "flexible", "model", "allow", "we", "fundamentally", "deal", "nonlinear", "motion", "cause", "parallax", "roll", "shutter", "effect", "-lsb-", "Liang", "et", "al.", "2008", "Baker", "et", "al.", "2010", "Grundmann", "et", "al.", "2012", "-rsb-", "same", "time", "model", "enjoy", "robustness", "simplicity", "2d", "method", "because", "only", "require", "feature", "correspondence", "between", "two", "consecutive", "frame", "we", "bundle", "camera", "path", "model", "build", "two", "novel", "component", "warping-based", "motion", "representation", "-lrb-", "estimation", "-rrb-", "adaptive", "space-time", "path", "smoothing", "first", "component", "represent", "motion", "between", "two", "consecutive", "frame", "mesh-based", "spatially-variant", "homography", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "as-similar-aspossible", "regularization", "constraint", "-lsb-", "Igarashi", "et", "al.", "2005", "Schaefer", "et", "al.", "2006", "-rsb-", "constraint", "critical", "because", "estimate", "model", "high", "degree", "freedom", "usually", "risky", "case", "insufficient", "feature", "large", "occlusion", "best", "we", "knowledge", "first", "work", "employ", "mesh-based", "as-similar-aspossible", "regularization", "spatially-variant", "motion", "estimation", "video", "stabilization", "Notice", "as-similar-as-possible", "warp", "use", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "-rsb-", "video", "stabilization", "we", "directly", "use", "mesh", "vertex", "motion", "model", "itself", "intermediate", "representation", "use", "3d", "reconstruction", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "subspace", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "base", "propose", "motion", "representation", "we", "construct", "bundle", "camera", "path", "each", "which", "concatenation", "local", "homography", "same", "grid", "cell", "over", "time", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "we", "second", "component", "smooth", "all", "bundle", "camera", "path", "whole", "maintain", "both", "spatial", "temporal", "coherence", "furthermore", "avoid", "excessive", "cropping/geometrical", "distortion", "approximate", "cinematography", "favor", "path", "we", "adopt", "discontinuity-preserving", "idea", "similar", "bilateral", "filter", "-lsb-", "Tomasi", "Manduchi", "1998", "-rsb-", "adaptively", "control", "strength", "smoothing", "quantitative", "evaluation", "we", "provide", "comprehensive", "dataset", "-lrb-", "include", "both", "public", "example", "we", "own", "video", "clip", "different", "kind", "motion", "-rrb-", "we", "show", "we", "new", "2d", "method", "comparable", "outperform", "other", "competitive", "2d", "3d", "method", "ACM", "Reference", "Format", "Liu", "S.", "Yuan", "L.", "Tan", "P.", "Sun", "J.", "2013", "bundle", "Camera", "Paths", "Video", "Stabilization", "ACM", "Trans", "graph", "32", "Article", "78", "-lrb-", "July", "2013", "-rrb-", "10", "page", "dous", "10.1145", "2461912.2461995", "http://doi.acm.org/10.1145/2461912.2461995", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "all", "part", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "commercial", "advantage", "copy", "bear", "notice", "full", "citation", "fus", "rst", "page", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "require", "prior", "specific", "permission", "and/or", "fee", "request", "permission", "from", "permissions@acm.org", "copyright", "ACM", "0730-0301/13", "07-art78", "15.00", "DOI", "http://doi.acm.org/10.1145/2461912.2461995", "ping", "Tan", "Jian", "Sun", "Microsoft", "Research", "Asia", "-lrb-", "-rrb-", "we", "bundle", "path", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "78:2", "S.", "Liu", "et", "al.", "related", "work", "2d", "method", "estimate", "2d", "transformation", "between", "consecutive", "video", "frame", "smooth", "they", "over", "time", "generate", "steady", "video", "most", "previously", "develop", "method", "apply", "affine", "homography", "model", "focus", "design", "smoothing", "algorithm", "earlier", "work", "-lsb-", "Morimoto", "Chellappa", "1998", "Matsushita", "et", "al.", "2006", "-rsb-", "apply", "low-pass", "filter", "individual", "model", "parameter", "some", "method", "assume", "prior", "motion", "model", "polynomial", "curve", "-lsb-", "Chen", "et", "al.", "2008", "-rsb-", "desire", "camera", "trajectory", "Gleicher", "Liu", "-lsb-", "2007", "-rsb-", "divide", "original", "camera", "trajectory", "multiple", "segment", "subsequent", "individual", "smoothing", "more", "recently", "Grundmann", "et", "al.", "-lsb-", "2011", "-rsb-", "gracefully", "apply", "norm", "optimization", "generate", "camera", "path", "consist", "constant", "linear", "parabolic", "motion", "which", "follow", "cinematography", "rule", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "further", "adopt", "homography-array-based", "motion", "model", "deal", "roll", "shutter", "effect", "two", "technique", "have", "be", "integrate", "Google", "YouTube", "robust", "follow", "cinematography", "rule", "perform", "well", "many", "consumer", "video", "we", "method", "belong", "category", "we", "use", "spatially-variant", "model", "represent", "motion", "between", "video", "frame", "design", "appropriate", "smoothing", "technique", "model", "3d", "method", "often", "rely", "robust", "feature", "tracking", "stabilization", "Beuhler", "et", "al.", "-lsb-", "2001", "-rsb-", "perform", "stabilization", "projective", "3d", "reconstruction", "scene", "from", "uncalibrated", "camera", "Liu", "et", "al.", "-lsb-", "2009", "-rsb-", "develop", "first", "successful", "3d", "video", "stabilization", "system", "first", "introduce", "content-preserving", "warp", "stabilization", "since", "3d", "reconstruction", "difficult", "recent", "method", "directly", "smooth", "trajectory", "track", "feature", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "smooth", "some", "basis", "trajectory", "-lrb-", "preferably", "longer", "than", "50", "frame", "-rrb-", "subspace", "form", "feature", "track", "method", "achieve", "similar", "quality", "3d", "reconstruction-based", "method", "while", "reduce", "require", "ment", "from", "3d", "reconstruction", "long", "feature", "tracking", "have", "be", "transfer", "Adobe", "after", "effect", "feature", "call", "Warp", "Stabilizer", "Goldstein", "Fattal", "-lsb-", "2012", "-rsb-", "utilize", "epipolar", "transfer", "technique", "avoid", "fragile", "3d", "reconstruction", "technique", "also", "alleviate", "strain", "long", "feature", "track", "still", "require", "moderate", "feature", "track", "length", "-lrb-", "typically", "over", "20", "frame", "-rrb-", "feature", "track", "smoothing", "also", "use", "light-field", "camera", "video", "stabilization", "work", "-lsb-", "Smith", "et", "al.", "2009", "-rsb-", "address", "occlusion", "issue", "Lee", "et", "al.", "-lsb-", "2009", "-rsb-", "introduce", "feature", "pruning", "choose", "robust", "feature", "trajectory", "smoothing", "nearly", "all", "method", "involve", "feature", "tracking", "face", "common", "obstacle", "many", "consumer", "video", "obtain", "long", "feature", "track", "fragile", "due", "occlusion", "motion", "blur", "rapid", "camera", "motion", "we", "method", "do", "encounter", "issue", "since", "only", "compute", "relative", "motion", "between", "consecutive", "frame", "Motion", "Estimation", "compute", "transition", "between", "two", "image", "view", "overlap", "Optical", "flow", "algorithm", "-lsb-", "Lucas", "Kanade", "1981", "-rsb-", "model", "transition", "individual", "displacement", "vector", "every", "pixel", "when", "parallax", "transition", "can", "represent", "elegantly", "global", "homography", "transformation", "-lsb-", "Hartley", "Zisserman", "2003", "-rsb-", "local", "alignment", "-lsb-", "shum", "Szeliski", "2000", "-rsb-", "dual-homography", "model", "-lsb-", "Gao", "et", "al.", "2011", "-rsb-", "can", "reduce", "alignment", "error", "cause", "parallax", "Szeliski", "Shum", "-lsb-", "1996", "-rsb-", "represent", "motion", "use", "mixture", "spline", "model", "spatially", "variant", "spatial", "support", "facilitate", "registration", "Lin", "et", "al.", "-lsb-", "2011", "-rsb-", "estimate", "smoothly", "vary", "affine", "field", "align", "image", "large", "viewpoint", "change", "model", "can", "potentially", "use", "video", "stabilization", "however", "its", "current", "motion", "estimation", "technique", "slow", "-lrb-", "may", "take", "minute", "process", "720p", "frame", "-rrb-", "we", "motion", "model", "essentially", "mesh-based", "spatially-variant", "homography", "model", "inspire", "recent", "image", "warping", "technique", "-lsb-", "Igarashi", "et", "al.", "2005", "Schaefer", "et", "al.", "2006", "Liu", "et", "al.", "2009", "-rsb-", "we", "extend", "as-similar-as-possible", "idea", "from", "image", "synthesis", "motion", "estimation", "apply", "video", "stabilization", "very", "efficient", "estimate", "we", "motion", "model", "-lrb-", "may", "take", "only", "50", "millisecond", "process", "720p", "frame", "-rrb-", "Rolling", "Shutter", "removal", "estimate", "correct", "inter-row", "motion", "cause", "row-parallel", "readout", "i.e.", "electronic", "rolling", "shutter", "-lsb-", "Nakamura", "2005", "-rsb-", "mainly", "CMOS", "sensor", "prior", "work", "design", "different", "parametric", "inter-row", "motion", "model", "include", "per-row", "translation", "model", "-lsb-", "Liang", "et", "al.", "2008", "Baker", "et", "al.", "2010", "-rsb-", "3d", "rotation", "model", "-lsb-", "forss?n", "Ringaby", "2010", "-rsb-", "recently", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "calibration-free", "homography", "mixture", "model", "which", "show", "significant", "improvement", "Karpenko", "et", "al.", "-lsb-", "2011", "-rsb-", "use", "dedicate", "hardware", "gyroscope", "mobile", "device", "correct", "rolling", "shutter", "effect", "real-time", "similar", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "method", "correct", "roll", "shutter", "effect", "without", "any", "prior", "calibration", "we", "warping-based", "model", "naturally", "handle", "rolling", "shutter", "effect", "special", "kind", "spatially", "variant", "motion", "so", "we", "do", "need", "separate", "rolling", "shutter", "correction", "step", "we", "stabilization", "Bundled", "Camera", "Paths", "section", "we", "introduce", "we", "warping-based", "motion", "model", "bundle", "camera", "path", "3.1", "warping-based", "Motion", "Model", "we", "propose", "use", "image", "warping", "model", "represent", "motion", "between", "consecutive", "video", "frame", "which", "provide", "stronger", "modeling", "power", "than", "conventional", "single", "2d", "linear", "transformation", "we", "adopt", "warping", "model", "-lsb-", "Igarashi", "et", "al.", "2005", "Liu", "et", "al.", "2009", "-rsb-", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "Bundled", "Camera", "Paths", "Video", "Stabilization", "78:3", "-lrb-", "-rrb-", "frame", "frame", "+1", "figure", "-lrb-", "-rrb-", "parameterization", "motion", "between", "two", "frame", "regular", "grid", "mesh", "where", "pair", "match", "feature", "-lrb-", "-rrb-", "should", "represent", "same", "bilinear", "interpolation", "four", "enclose", "vertex", "-lrb-", "-rrb-", "as-similar-as-possible", "term", "require", "each", "triangle", "follow", "similarity", "transformation", "though", "more", "general", "model", "moving-least-square", "-lsb-", "Schaefer", "et", "al.", "2006", "-rsb-", "parameterized", "optical", "flow", "-lsb-", "Nir", "et", "al.", "2008", "-rsb-", "might", "use", "Model", "each", "frame", "we", "define", "uniform", "grid", "mesh", "illustrate", "figure", "motion", "represent", "-lrb-", "unknown", "-rrb-", "warping", "grid", "mesh", "register", "two", "frame", "-lrb-", "fact", "corresponding", "feature", "point", "-rrb-", "we", "require", "match", "feature", "-lrb-", "e.g.", "figure", "-rrb-", "share", "same", "bilinear", "interpolation", "four", "corner", "enclose", "grid", "cell", "after", "warp", "i-th", "grid", "cell", "warping", "from", "frame", "frame", "introduce", "homography", "-lrb-", "-rrb-", "which", "can", "determine", "from", "motion", "four", "enclose", "vertex", "thus", "warping-based", "motion", "model", "actually", "set", "spatially-variant", "homography", "2d", "grid", "note", "highly", "flexible", "model", "able", "handle", "parallax", "between", "global", "homography", "per-pixel", "optical", "flow", "however", "estimate", "model", "high", "degree", "freedom", "very", "risky", "because", "we", "may", "have", "sufficient", "feature", "-lrb-", "due", "textureless", "region", "occlusion", "-rrb-", "every", "cell", "regularization", "address", "challenge", "we", "propose", "impose", "shape-preserving", "-lrb-", "i.e.", "as-similar-as-possible", "-lsb-", "Igarashi", "et", "al.", "2005", "-rsb-", "-rrb-", "constraint", "combination", "shape-preserving", "mesh", "representation", "together", "provide", "two", "kind", "regularization", "-rrb-", "each", "cell", "fitted", "homography", "should", "bias", "toward", "reduce", "similarity", "-lrb-", "rigid", "-rrb-", "transformation", "-rrb-", "intrinsic", "connection", "mesh", "-lrb-", "two", "neighbor", "mesh", "cell", "share", "two", "vertex", "-rrb-", "enforce", "first-order", "continuity", "constraint", "can", "help", "propagate", "fill", "information", "from", "region", "sufficient", "feature", "other", "region", "finally", "we", "estimate", "motion", "minimize", "two", "energy", "term", "datum", "term", "match", "feature", "shape-preserving", "term", "enforce", "regularization", "3.2", "Model", "Estimation", "we", "first", "describe", "we", "basic", "method", "follow", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "later", "extend", "better", "robustness", "next", "subsection", "Data", "term", "show", "figure", "suppose", "-lcb-", "-rcb-", "p-th", "match", "feature", "pair", "from", "frame", "frame", "feature", "can", "represent", "2d", "bilinear", "interpolation", "four", "vertex", "-lsb-", "-rsb-", "enclose", "grid", "cell", "where", "-lsb-", "-rsb-", "interpolation", "weight", "sum", "we", "expect", "corresponding", "feature", "can", "represent", "same", "weight", "warped", "grid", "vertex", "-lsb-", "-rsb-", "therefore", "datum", "term", "define", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "comparison", "motion", "estimation", "without", "shape-preserving", "term", "here", "contain", "all", "warped", "grid", "vertex", "solve", "determine", "warping", "grid", "shape-preserving", "term", "we", "use", "same", "shape-preserving", "term", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "involve", "all", "vertex", "-lrb-", "-rrb-", "sr", "90", "-lrb-", "-rrb-", "90", "-lrb-", "-rrb-", "where", "known", "scalar", "compute", "from", "initial", "mesh", "shape-preserving", "term", "require", "triangle", "neighbor", "vertex", "follow", "similarity", "transformation", "linearly", "combine", "two", "term", "form", "we", "final", "energy", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "important", "weight", "control", "amount", "regularization", "we", "discuss", "how", "adaptively", "determine", "later", "since", "energy", "-lrb-", "-rrb-", "quadratic", "warped", "mesh", "can", "easily", "solve", "sparse", "linear", "system", "solver", "estimate", "homography", "after", "have", "new", "mesh", "we", "can", "estimate", "each", "local", "homography", "-lrb-", "-rrb-", "grid", "cell", "frame", "solve", "linear", "equation", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "four", "vertex", "before", "after", "warping", "Figure", "show", "warped", "mesh", "grid", "accord", "estimated", "motion", "leave", "right", "result", "without", "shapepreserving", "term", "clear", "regularization", "term", "help", "maintain", "smooth", "vary", "mesh", "representation", "3.3", "robust", "estimation", "we", "further", "generalize", "we", "motion", "estimation", "make", "more", "robust", "outlier", "rejection", "we", "reject", "incorrectly", "match", "feature", "two", "scale", "coarse", "scale", "-lrb-", "whole", "image", "-rrb-", "we", "apply", "ransac", "algorithm", "-lsb-", "fischler", "Bolles", "1981", "-rsb-", "fit", "global", "homography", "-lrb-", "-rrb-", "discard", "feature", "relatively", "large", "threshold", "fitting", "error", "-lrb-", "image", "width", "-rrb-", "fine", "scale", "-lrb-", "sub-image", "-rrb-", "we", "apply", "ransac", "again", "reject", "feature", "relatively", "small", "threshold", "-lrb-", "image", "width", "-rrb-", "pre-warping", "facilitate", "warping", "estimation", "we", "use", "global", "homography", "-lrb-", "-rrb-", "bring", "match", "feature", "closer", "we", "solve", "warping", "estimate", "residual", "motion", "which", "generate", "homography", "-lrb-", "-rrb-", "each", "grid", "cell", "final", "homography", "-lrb-", "-rrb-", "simply", "compute", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "note", "coarse-to-fine", "strategy", "have", "be", "use", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "image", "synthesis", "proven", "effective", "motion", "estimation", "literature", "-lsb-", "Brox", "et", "al.", "2004", "-rsb-", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "78:4", "S.", "Liu", "et", "al.", "0.8", "0.8", "0.6", "0.6", "0.4", "0.6", "0.9", "1.2", "1.5", "1.8", "2.1", "2.4", "2.7", "0.4", "0.3", "0.6", "0.9", "1.2", "1.5", "1.8", "2.1", "2.4", "2.7", "figure", "we", "method", "automatically", "choose", "appropriate", "different", "scene", "-lrb-", "-rrb-", "scene", "free", "occlusion", "-lrb-", "-rrb-", "scene", "severe", "occlusion", "adaptive", "regularization", "good", "regularization", "should", "adaptive", "image", "content", "example", "reliable", "feature", "uniformly", "distribute", "over", "whole", "image", "we", "should", "trust", "datum", "term", "more", "use", "smaller", "weight", "equation", "weaker", "regularization", "when", "occlusion", "insufficient", "feature", "we", "prefer", "stronger", "regularization", "datum", "term", "less", "reliable", "implement", "strategy", "we", "adaptively", "set", "per", "frame", "base", "two", "error", "fitting", "error", "smoothness", "error", "fitting", "error", "average", "residual", "feature", "match", "under", "estimate", "homography", "i.e.", "where", "homography", "cell", "contain", "number", "feature", "pair", "smoothness", "error", "measure", "similarity", "-lrb-", "distance", "-rrb-", "between", "neighbor", "local", "homography", "??", "where", "consist", "neighboring", "cell", "i.", "here", "homography", "matrix", "normalize", "so", "sum", "all", "its", "element", "one", "we", "empirically", "set", "0.01", "since", "make", "scale", "similar", "most", "example", "we", "define", "combined", "error", "we", "equally", "discretize", "10", "value", "between", "0.3", "we", "perform", "model", "estimation", "use", "every", "discretize", "value", "select", "model", "minimum", "error", "show", "Figure", "-lrb-", "-rrb-", "simple", "scene", "smooth", "depth", "variation", "neighbor", "cell", "tend", "have", "similar", "homography", "so", "we", "choose", "small", "-lrb-", "0.9", "-rrb-", "better", "minimize", "datum", "error", "contrary", "scene", "large", "occlusion", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "neighbor", "local", "homography", "less", "similar", "smoothness", "error", "can", "significantly", "reduce", "increase", "so", "we", "system", "automatically", "choose", "large", "-lrb-", "3.0", "-rrb-", "ensure", "consistent", "local", "motion", "finally", "we", "show", "example", "Figure", "verify", "strength", "regularization", "we", "method", "example", "we", "compare", "two", "mesh", "estimate", "use", "all", "feature", "subset", "feature", "two", "similar", "result", "indicate", "we", "method", "can", "robustly", "deal", "region", "insufficient", "feature", "3.4", "Bundled", "Camera", "Paths", "estimate", "local", "homography", "we", "can", "define", "bundle", "spatially-variant", "camera", "path", "whole", "video", "let", "-lrb-", "-rrb-", "Figure", "leave", "estimate", "warping", "mesh", "from", "all", "feature", "point", "right", "we", "exclude", "all", "feature", "orange", "box", "when", "estimate", "warping", "model", "similar", "mesh", "can", "obtain", "despite", "lack", "feature", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "-lrb-", "-rrb-", "bundle", "camera", "path", "-lrb-", "-rrb-", "Relationships", "among", "original", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "smooth", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "transformation", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "camera", "pose", "grid", "cell", "frame", "can", "write", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lcb-", "-lrb-", "-rrb-", "...", "-lrb-", "-rrb-", "-rcb-", "estimate", "local", "homography", "same", "grid", "cell", "show", "Figure", "-lrb-", "-rrb-", "we", "call", "spatially-variant", "path", "bundle", "camera", "path", "next", "section", "we", "describe", "how", "we", "smoothen", "bundle", "path", "video", "stabilization", "path", "optimization", "we", "first", "describe", "we", "smoothing", "method", "single", "camera", "path", "extend", "bundle", "camera", "path", "4.1", "optimize", "single", "path", "good", "camera", "path", "smoothing", "should", "consider", "multiple", "compete", "factor", "remove", "jitters", "avoid", "excessive", "crop", "minimize", "various", "geometrical", "distortion", "-lrb-", "shearing/skewing", "wobble", "-rrb-", "reach", "desire", "balance", "we", "propose", "optimization-based", "framework", "take", "all", "factor", "account", "Formulation", "give", "original", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "we", "seek", "optimize", "path", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "minimize", "follow", "function", "-lrb-", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "-rrb-", "-lrb-", "-rrb-", "??", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "where", "neighborhood", "frame", "other", "term", "datum", "term", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "enforce", "new", "camera", "path", "close", "original", "one", "reduce", "crop", "distortion", "smoothness", "term", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "stabilize", "path", "weight", "-lrb-", "-rrb-", "preserve", "motion", "discontinuity", "under", "fast", "panning/rotation", "scene", "transition", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "Bundled", "Camera", "Paths", "Video", "Stabilization", "78:5", "original", "original", "adaptive", "adaptive", "camera", "path", "-lrb-", "adaptive", "-rrb-", "camera", "path", "-lrb-", "adaptive", "-rrb-", "output", "frame", "-lrb-", "adaptive", "weight", "-rrb-", "figure", "comparison", "without", "adaptive", "weight", "-lrb-", "-rrb-", "video", "rapid", "camera", "panning", "camera", "path", "top", "plot", "x-translation", "over", "time", "parameter", "balance", "above", "two", "term", "since", "equation", "quadratic", "we", "can", "solve", "any", "linear", "system", "solver", "here", "we", "use", "jacobi-based", "iterative", "solver", "-lsb-", "bronshtein", "Semendyayev", "1997", "-rsb-", "-lrb-", "+1", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "??", "where", "??", "iteration", "index", "initialization", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "once", "we", "obtain", "optimize", "path", "we", "compute", "warping", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "warp", "original", "video", "frame", "stabilize", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "discontinuity-preserving", "adaptive", "weight", "important", "preserve", "motion", "discontinuity", "we", "follow", "idea", "bilateral", "filter", "-lsb-", "tomasus", "Manduchi", "1998", "-rsb-", "design", "two", "gaussian", "function", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-rrb-", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "give", "larger", "weight", "nearby", "frame", "-lrb-", "-rrb-", "measure", "change", "two", "camera", "pose", "we", "use", "large", "kernel", "ensure", "successful", "suppression", "both", "highfrequency", "jitters", "-lrb-", "e.g.", "handshake", "-rrb-", "low-frequency", "bounce", "-lrb-", "e.g.", "walk", "-rrb-", "we", "implementation", "we", "set", "60", "neighbor", "frame", "standard", "deviation", "-lrb-", "-rrb-", "10", "contrast", "previous", "low-pass", "filter", "base", "method", "-lsb-", "Matsushita", "et", "al.", "2006", "-rsb-", "typically", "need", "smaller", "amount", "support", "-lrb-", "e.g.", "10", "frame", "-rrb-", "avoid", "aggressive", "crop", "distortion", "small", "kernel", "often", "insufficient", "suppress", "low", "frequency", "bounce", "reason", "why", "we", "can", "use", "larger", "kernel", "lie", "-lrb-", "-rrb-", "video", "stabilization", "rapid", "camera", "motion", "-lrb-", "e.g", "cause", "fast", "panning", "scene", "transition", "-rrb-", "inappropriate", "amount", "smoothing", "may", "lead", "excessive", "crop", "show", "figure", "case", "camera", "pan", "quickly", "na?ve", "gaussian", "smoothing", "-lrb-", "second", "row", "-rrb-", "cause", "camera", "path", "significantly", "deviate", "from", "its", "original", "path", "indicate", "dash", "line", "left", "plot", "top", "corresponding", "frame", "show", "second", "row", "require", "large", "crop", "we", "adaptive", "term", "-lrb-", "-rrb-", "preserve", "sudden", "camera", "motion", "certain", "degree", "result", "from", "we", "adaptive", "smoothing", "-lrb-", "bottom", "row", "-rrb-", "produce", "much", "less", "crop", "measure", "camera", "motion", "we", "use", "change", "translation", "component", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "extract", "from", "camera", "pose", "-lrb-", "-rrb-", "namely", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "frame", "translation", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "can", "describe", "most", "camera", "motion", "practice", "except", "in-plane", "rotation", "scale", "around", "principal", "axis", "crop", "distortion", "control", "above", "adaptive", "term", "can", "give", "we", "certain", "amount", "ability", "control", "crop", "distortion", "however", "user", "may", "want", "have", "strict", "control", "crop", "ratio", "distortion", "principle", "we", "could", "formulate", "constrain", "optimization", "address", "issue", "may", "too", "complex", "solve", "reproduce", "work", "we", "resort", "simple", "effective", "method", "adaptively", "adjust", "parameter", "each", "frame", "we", "first", "run", "optimization", "global", "fix", "-lrb-", "empirically", "set", "-rrb-", "check", "crop", "ratio", "distortion", "every", "frame", "any", "frame", "do", "satisfy", "user", "requirement", "-lrb-", "crop", "ratio", "distortion", "smaller", "than", "pre-defined", "threshold", "-rrb-", "we", "decrease", "its", "parameter", "step", "-lrb-", "1/10", "-rrb-", "re-run", "optimization", "note", "accord", "equation", "smaller", "make", "optimize", "path", "closer", "original", "one", "which", "have", "less", "crop", "distortion", "procedure", "iterate", "until", "all", "frame", "satisfy", "requirement", "we", "measure", "crop", "ratio", "distortion", "from", "warping", "transform", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "anisotropic", "scaling", "-lrb-", "-rrb-", "measure", "distortion", "can", "compute", "ratio", "two", "largest", "eigenvalue", "affine", "part", "-lrb-", "-rrb-", "-lsb-", "Hartley", "Zisserman", "2003", "-rsb-", "we", "use", "-lrb-", "-rrb-", "compute", "overlap", "area", "original", "video", "frame", "stabilize", "frame", "crop", "ratio", "ratio", "area", "original", "frame", "area", "we", "experiment", "we", "require", "crop", "ratio", "larger", "than", "0.8", "distortion", "score", "larger", "than", "0.95", "all", "example", "principle", "we", "can", "further", "measure", "perspective", "distortion", "two", "perspective", "component", "-lrb-", "-rrb-", "we", "empirically", "find", "always", "too", "small", "when", "compare", "affine", "component", "do", "include", "they", "4.2", "optimize", "bundle", "path", "we", "motion", "model", "generate", "bundle", "camera", "path", "path", "optimize", "independently", "neighbor", "path", "could", "less", "consistent", "which", "may", "generate", "distortion", "final", "render", "video", "hence", "we", "do", "space-time", "optimization", "all", "path", "minimize", "follow", "objective", "function", "-lrb-", "-lcb-", "-lrb-", "-rrb-", "-rcb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "j?n", "-lrb-", "-rrb-", "where", "-lrb-", "-rrb-", "include", "eight", "neighbor", "grid", "cell", "first", "term", "objective", "function", "equation", "each", "single", "path", "second", "term", "enforce", "smoothness", "between", "neighbor", "path", "optimization", "also", "quadratic", "optimum", "result", "can", "obtain", "solve", "large", "sparse", "linear", "system", "again", "we", "solution", "update", "jacobi-based", "iteration", "-lsb-", "bronshtein", "Semendyayev", "1997", "-rsb-", "-lrb-", "+1", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "2p", "-lrb-", "-rrb-", "-rrb-", "??", "j?n", "-lrb-", "-rrb-", "where", "2n", "-lrb-", "-rrb-", "??", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "78:6", "S.", "Liu", "et", "al.", "we", "typically", "iterate", "20", "time", "optimize", "camera", "path", "during", "optimization", "motion-adaptive", "term", "-lrb-", "-rrb-", "evaluate", "individual", "cell", "since", "different", "cell", "have", "different", "motion", "comparison", "determine", "from", "global", "path", "-lrb-", "generate", "concatenate", "pre-warping", "global", "homography", "-rrb-", "because", "control", "overall", "crop", "distortion", "we", "use", "optimize", "camera", "path", "all", "cell", "result", "synthesis", "after", "path", "optimization", "we", "compute", "warp", "matrix", "-lrb-", "-rrb-", "each", "cell", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "we", "apply", "-lrb-", "-rrb-", "warp", "i-th", "cell", "t-th", "frame", "generate", "final", "output", "video", "usually", "apply", "-lrb-", "-rrb-", "directly", "generate", "good", "result", "because", "we", "motion", "estimation", "ensure", "first", "order", "smoothness", "original", "path", "furthermore", "bundle", "optimization", "equation", "require", "nearby", "optimize", "path", "similar", "thus", "smoothness", "naturally", "satisfy", "-lrb-", "-rrb-", "most", "time", "sometimes", "slight", "distortion", "-lrb-", "e.g.", "seam", "about", "1-pixel", "width", "-rrb-", "which", "case", "we", "perform", "bilinear", "interpolation", "fix", "they", "4.3", "correct", "Rolling", "Shutter", "Effects", "we", "bundle", "path", "model", "can", "naturally", "handle", "roll", "shutter", "effect", "without", "pre-calibration", "principle", "we", "method", "similar", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "system", "do", "roll", "shutter", "correction", "while", "simultaneously", "stabilize", "video", "shaky", "video", "rolling", "shutter", "cause", "spatially", "variant", "high", "frequency", "jitters", "when", "smooth", "camera", "path", "we", "simultaneously", "rectify", "roll", "shutter", "effect", "other", "jitters", "cause", "camera", "shake", "result", "we", "run", "we", "method", "Intel", "i7", "3.2", "GHZ", "Quad-Core", "machine", "8g", "RAM", "we", "extract", "400-600", "surf", "feature", "-lsb-", "Bay", "et", "al.", "2008", "-rsb-", "per", "frame", "motion", "estimation", "we", "always", "divide", "video", "frame", "16", "16", "cell", "video", "1280", "720", "resolution", "we", "unoptimized", "system", "take", "392", "millisecond", "process", "frame", "-lrb-", "around", "2.5", "fp", "-rrb-", "specifically", "we", "spend", "300m", "50m", "12m", "30m", "extract", "feature", "estimate", "motion", "optimize", "camera", "path", "render", "final", "result", "all", "original", "result", "video", "provide", "we", "webpage", "5.1", "Algorithm", "Validation", "we", "first", "verify", "effectiveness", "different", "component", "propose", "approach", "global", "path", "vs.", "bundle", "path", "example", "Figure", "result", "accord", "global", "path", "have", "remain", "jitters", "some", "image", "region", "because", "parallax", "make", "global", "homography", "motion", "model", "invalid", "therefore", "some", "image", "region", "can", "stabilize", "very", "well", "we", "bundle", "path", "can", "handle", "kind", "typical", "situation", "please", "refer", "we", "accompany", "video", "visual", "comparison", "spatially-variant", "homography", "vs.", "Homography", "Mixture", "Grundmann", "et", "al.", "-lsb-", "2012", "-rsb-", "propose", "homography", "mixture", "model", "roll", "shutter", "correction", "divide", "video", "frame", "1d", "array", "horizontal", "block", "use", "gaussian", "mixture", "homography", "each", "block", "model", "beyond", "single", "2d", "transformation", "able", "partially", "handle", "parallax", "http://www.ece.nus.edu.sg/stfpage/eletp/projects/stabilization/stabili", "zationsig13.html", "-lrb-", "-rrb-", "original", "video", "frame", "-lrb-", "-rrb-", "YouTube", "result", "-lrb-", "-rrb-", "we", "result", "-lrb-", "-rrb-", "homography", "mixture", "-lrb-", "we", "implementation", "-rrb-", "figure", "comparison", "homography", "mixture", "model", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "-lrb-", "-rrb-", "sample", "frame", "original", "video", "-lrb-", "-rrb-", "output", "frame", "produce", "YouTube", "Stabilizer", "-lrb-", "-rrb-", "result", "produce", "we", "method", "-lrb-", "-rrb-", "result", "produce", "use", "we", "implementation", "homography", "mixture", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "-lrb-", "same", "bundle", "path", "smoothing", "-rrb-", "compare", "we", "2d", "mesh-based", "spatially-variant", "homography", "model", "have", "two", "limitation", "-rrb-", "do", "address", "horizontal", "depth", "variation", "-rrb-", "use", "weaker", "feature", "point", "-lrb-", "which", "apply", "lower", "threshold", "level", "feature", "detection", "-rrb-", "simple", "gaussian", "mixture", "regularization", "weaker", "feature", "point", "may", "result", "larger", "fitting", "error", "ability", "use", "simple", "gaussian", "smoothing", "limit", "Figure", "show", "comparison", "two", "model", "example", "scene", "have", "horizontal", "depth", "variation", "sky", "region", "lack", "feature", "point", "Figure", "-lrb-", "-rrb-", "result", "use", "YouTube", "Stabilizer", "-lrb-", "integrate", "Homography", "Mixture", "feature", "-rrb-", "we", "can", "observe", "severe", "geometrical", "distortion", "further", "verify", "we", "observation", "we", "replace", "we", "spatially-variant", "model", "homography", "mixture", "model", "-lrb-", "we", "implementation", "-rrb-", "we", "framework", "generate", "result", "Figure", "-lrb-", "-rrb-", "where", "we", "observe", "similar", "distortion", "comparison", "we", "warping-based", "motion", "estimation", "can", "fundamentally", "handle", "depth", "variation", "-lrb-", "limit", "vertical", "direction", "-rrb-", "we", "result", "-lrb-", "figure", "-lrb-", "-rrb-", "-rrb-", "do", "suffer", "from", "distortion", "please", "also", "see", "comparison", "accompany", "video", "Rolling", "Shutter", "Handling", "Figure", "compare", "we", "method", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "two", "example", "video", "from", "paper", "we", "model", "account", "frame", "distortion", "skew", "-lrb-", "leave", "example", "-rrb-", "local", "wobble", "-lrb-", "right", "example", "-rrb-", "more", "example", "include", "supplementary", "video", "which", "show", "we", "achieve", "similar", "result", "correct", "rolling", "shutter", "distortion", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "5.2", "quantitative", "evaluation", "quantitatively", "evaluate", "measure", "result", "from", "different", "aspect", "we", "define", "three", "objective", "metric", "crop", "distortion", "we", "first", "two", "metric", "measure", "crop", "ratio", "global", "distortion", "we", "first", "fit", "global", "homography", "each", "frame", "between", "input", "video", "output", "video", "we", "compute", "crop", "ratio", "distortion", "each", "frame", "crop", "ratio", "can", "directly", "compute", "from", "scale", "component", "homography", "one", "global", "crop", "ratio", "whole", "sequence", "each", "frame", "provide", "estimation", "we", "average", "estimation", "all", "frame", "final", "metric", "distortion", "compute", "define", "section", "4.1", "because", "any", "distortion", "single", "frame", "destroy", "perfection", "whole", "result", "we", "choose", "minimum", "across", "whole", "sequence", "final", "metric", "worst-case", "metric", "allow", "we", "easily", "see", "whether", "whole", "result", "video", "completely", "successful", "good", "result", "both", "metric", "should", "close", "stability", "third", "metric", "measure", "stability", "result", "Designing", "good", "metric", "non-trivial", "because", "hard", "compare", "two", "different", "video", "we", "suggest", "empirically", "good", "metric", "use", "frequency", "analysis", "estimate", "2d", "motion", "from", "video", "we", "basic", "assumption", "more", "energy", "contain", "low", "frequency", "part", "motion", "more", "stable", "video", "computationally", "we", "estimate", "we", "bundle", "camera", "path", "approximate", "true", "motion", "-lrb-", "optical", "flow", "-rrb-", "video", "we", "do", "smooth", "out", "anything", "after", "estimation", "we", "extract", "translation", "rotation", "component", "from", "each", "path", "each", "component", "1d", "temporal", "signal", "finally", "we", "evaluate", "energy", "percentage", "low", "frequency", "component", "-lrb-", "expect", "dc", "component", "-rrb-", "1d", "signal", "measure", "stability", "specifically", "we", "take", "few", "lowest", "-lrb-", "empirically", "set", "from", "2nd", "6th", "-rrb-", "frequency", "calculate", "energy", "percentage", "over", "full", "frequency", "-lrb-", "exclude", "dc", "component", "-rrb-", "similar", "distortion", "we", "take", "smallest", "measurement", "among", "translation", "rotation", "final", "metric", "good", "result", "metric", "should", "approach", "here", "well", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "Bundled", "Camera", "Paths", "Video", "Stabilization", "78:7", "input", "frame", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "result", "Figure", "two", "rolling", "shutter", "removal", "example", "use", "we", "method", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "result", "par", "from", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "please", "see", "video", "full", "comparison", "5.4", "comparison", "State-of-the-Art", "Systems", "5.3", "comparison", "publicly", "available", "result", "purpose", "comparison", "test", "whether", "we", "result", "comparable", "-lrb-", "better", "than", "-rrb-", "previous", "successful", "result", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "Grundmann", "et", "al.", "2011", "-rsb-", "we", "collect", "eleven", "test", "video", "from", "papers", "-lrb-", "thumbnail", "Figure", "10", "-rrb-", "compare", "we", "result", "publish", "result", "-lrb-", "all", "from", "author", "project", "webpage", "-rrb-", "overall", "all", "method", "generate", "similar", "stability", "both", "subjectively", "quantitatively", "-lrb-", "Figure", "10", "-rrb-", "example", "while", "we", "result", "slightly", "better", "some", "video", "term", "crop", "ratio", "distortion", "video", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "3d", "stabilization", "-lsb-", "Liu", "et", "al.", "2009", "-rsb-", "achieve", "best", "stability", "distortion", "score", "suggest", "3d", "method", "first", "choice", "-lrb-", "term", "stability", "distortion", "error", "-rrb-", "when", "3d", "motion", "can", "successfully", "estimate", "although", "we", "result", "slightly", "worse", "stability", "visual", "difference", "quite", "small", "-lrb-", "please", "verify", "from", "supplementary", "video", "-rrb-", "furthermore", "aggressive", "smoothing", "3d", "method", "sometimes", "lead", "output", "fov", "too", "small", "demonstrate", "crop", "score", "we", "method", "manage", "provide", "good", "trade-off", "video", "-lrb-", "5-9", "-rrb-", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "-lsb-", "Goldstein", "Fattal", "2012", "-rsb-", "we", "method", "achieve", "similar", "stability", "while", "we", "method", "slightly", "better", "crop", "distortion", "video", "-lrb-", "10-11", "-rrb-", "we", "method", "outperform", "l1optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "stability", "-lrb-", "slightly", "-rrb-", "crop", "ratio", "distortion", "score", "Figure", "11", "highlight", "most", "challenging", "video", "-lrb-", "10", "-rrb-", "dataset", "Liu", "et", "al.", "-lsb-", "2011", "-rsb-", "refer", "example", "failure", "case", "because", "single", "subspace", "can", "account", "feature", "trajectory", "both", "face", "background", "result", "have", "visible", "distortion", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "produce", "better", "result", "example", "video", "result", "we", "still", "observe", "large", "temporal", "distortion", "background", "region", "-lrb-", "see", "we", "accompany", "video", "-rrb-", "comparison", "we", "method", "can", "successfully", "handle", "example", "-lrb-", "achieve", "best", "term", "all", "three", "metric", "-rrb-", "because", "warping-based", "motion", "model", "can", "represent", "complicated", "motion", "due", "publicly", "available", "implementation", "previous", "work", "we", "compare", "we", "system", "two", "well-known", "commercial", "system", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "after", "Effects", "CS6", "YouTube", "Stabilizer", "base", "combination", "norm", "path", "optimization", "-lsb-", "Grundmann", "et", "al.", "2011", "-rsb-", "homography", "mixture", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "Warp", "Stabilizer", "Adobe", "after", "Effects", "largely", "base", "subspace", "stabilization", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "we", "understand", "commercial", "product", "often", "different", "from", "give", "research", "system", "we", "believe", "two", "system", "represent", "essential", "element", "research", "conduct", "field", "comparison", "make", "sense", "examine", "strength", "weakness", "robustness", "-lrb-", "various", "video", "use", "set", "fix", "parameter", "-rrb-", "we", "system", "Dataset", "we", "assemble", "comprehensive", "dataset", "174", "short", "video", "-lrb-", "10", "60", "seconds", "-rrb-", "from", "previous", "publication", "internet", "we", "own", "capture", "know", "strength", "weakness", "method", "different", "situation", "we", "roughly", "divide", "we", "datum", "category", "base", "camera", "motion", "scene", "type", "-lrb-", "-rrb-", "simple", "-lrb-", "II", "-rrb-", "quick", "rotation", "-lrb-", "III", "-rrb-", "zoom", "-lrb-", "iv", "-rrb-", "large", "parallax", "-lrb-", "-rrb-", "driving", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "running", "better", "measure", "stability", "background", "motion", "-lrb-", "cause", "camera", "shake", "-rrb-", "we", "use", "manual", "foreground", "mask", "exclude", "foreground", "motion", "YouTube", "Stabilizer", "parameter-free", "online", "tool", "Warp", "Stabilizer", "interactive", "system", "user", "might", "carefully", "tune", "few", "parameter", "here", "we", "wish", "examine", "its", "robustness", "automatic", "tool", "fix", "its", "parameter", "we", "use", "example", "video", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "decide", "best", "parameter", "finally", "we", "choose", "default", "parameter", "-lrb-", "smoothness", "50", "smooth", "Motion", "Subspace", "Warp", "-rrb-", "produce", "result", "quantitative", "comparison", "each", "category", "we", "compute", "average", "metric", "standard", "deviation", "three", "system", "-lrb-", "figure", "12", "-lrb-", "-rrb-", "-rrb-", "we", "discuss", "result", "regard", "each", "system", "detail", "below", "all", "three", "system", "perform", "well", "category", "-lrb-", "-rrb-", "simple", "since", "category", "contain", "video", "relatively", "smooth", "camera", "motion", "mild", "depth", "variation", "though", "we", "method", "have", "minor", "advantage", "user", "can", "safely", "choose", "any", "three", "get", "desire", "result", "among", "remain", "category", "we", "want", "highlight", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "three", "system", "achieve", "similar", "stability", "while", "we", "system", "clearly", "better", "term", "distortion", "we", "show", "two", "example", "Figure", "12", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "visual", "comparison", "we", "system", "YouTube", "Stabilizer", "example", "show", "limitation", "1d", "array", "homography", "mixture", "can", "model", "depth", "change", "horizontal", "direction", "Warp", "Stabilizer", "also", "generate", "some", "shearing/skewing", "artifact", "some", "video", "frame", "though", "principle", "3d", "method", "should", "able", "handle", "parallax", "Figure", "12", "-lrb-", "-rrb-", "show", "example", "-lrb-", "please", "note", "shearing", "bookshelf", "-rrb-", "probably", "due", "subspace", "analysis", "failure", "cause", "occlusion", "we", "method", "succeed", "all", "example", "comparison", "category", "clearly", "demonstrate", "advantage", "we", "warping-based", "motion", "model", "deal", "large", "parallax", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "which", "challenging", "case", "method", "require", "long", "feature", "tracking", "Warp", "Stabilizer", "often", "generate", "significant", "crop", "Figure", "12", "-lrb-", "-rrb-", "example", "alleviate", "problem", "we", "try", "interactively", "tune", "its", "smoothing", "parameter", "when", "apply", "weaker", "smoothing", "however", "we", "find", "its", "result", "become", "shaky", "comparison", "we", "method", "generate", "stable", "result", "much", "less", "crop", "category", "-lrb-", "v?vii", "-rrb-", "three", "system", "generate", "similar", "stability", "level", "-lrb-", "Warp", "Stabilizer", "slightly", "better", "category", "vii", "-rrb-", "while", "we", "sy", "tem", "consistently", "better", "respect", "either", "crop", "ratio", "distortion", "control", "we", "notice", "we", "method", "generate", "relatively", "smaller", "standard", "deviation", "three", "metric", "all", "category", "suggest", "we", "method", "generate", "more", "consistent", "result", "from", "various", "input", "user", "study", "we", "further", "conduct", "user", "study", "40", "participant", "evaluate", "compare", "we", "method", "YouTube", "Stabilizer", "Warp", "Stabilizer", "Adobe", "AfterEffects", "CS6", "every", "participant", "require", "evaluate", "result", "28", "different", "input", "video", "-lrb-", "randomly", "sample", "from", "we", "dataset", "-rrb-", "which", "video", "each", "category", "mention", "above", "-lrb-", "video", "prepare", "way", "two", "they", "compare", "we", "result", "YouTube", "Stabilizer", "other", "two", "Warp", "Stabilizer", "-rrb-", "user", "study", "we", "use", "scheme", "forced", "two-alternative", "choice", "every", "participant", "ask", "pick", "better", "one", "between", "result", "we", "method", "YouTube", "Stabilizer", "between", "result", "we", "method", "Warp", "Stabilizer", "video", "display", "subject", "random", "order", "subject", "unaware", "video", "category", "neither", "do", "know", "which", "technique", "use", "produce", "stabilize", "result", "Figure", "13", "-lrb-", "-rrb-", "show", "interface", "user", "study", "original", "video", "display", "top", "two", "stabilize", "one", "show", "side-by-side", "below", "user", "can", "simultaneously", "play", "input", "video", "both", "two", "result", "better", "examine", "difference", "video", "can", "play", "back", "forth", "pause", "certain", "frame", "help", "user", "carefully", "make", "decision", "user", "can", "also", "play", "each", "video", "individually", "examine", "quality", "without", "other", "distraction", "we", "ask", "user", "disregard", "difference", "aspect", "ratio", "sharpness", "since", "each", "one", "may", "undergo", "different", "video", "codec", "further", "post-processing", "which", "make", "uniform", "treatment", "difficult", "user", "study", "result", "show", "13", "-lrb-", "-rrb-", "each", "category", "we", "show", "average", "percentage", "user", "preference", "general", "majority", "all", "user", "show", "significant", "preference", "towards", "we", "result", "when", "compare", "any", "other", "two", "system", "respectively", "particular", "participant", "prefer", "overall", "quality", "we", "result", "category", "-lrb-", "iv", "-rrb-", "large", "parallax", "over", "YouTube", "Stabilizer", "-lrb-", "72", "vs.", "28", "-rrb-", "Warp", "Stabilizer", "-lrb-", "69", "vs.", "31", "-rrb-", "result", "consistent", "we", "metric", "evaluation", "category", "-lrb-", "ii?iii", "-rrb-", "contain", "quick", "rotation", "zooming", "user", "show", "strong", "bias", "preference", "toward", "we", "result", "over", "Warp", "Stabilizer", "-lrb-", "93", "vs.", "rotation", "input", "frame", "-lsb-", "Grundmann", "et", "al.", "2012", "-rsb-", "we", "result", "input", "-lsb-", "Liu", "et", "al.", "2011", "-rsb-", "we", "result", "figure", "11", "comparison", "failure", "case", "prior", "method", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "78:8", "S.", "Liu", "et", "al.", "0.9", "crop", "0.8", "0.7", "0.6", "0.5", "0.9", "distortion", "0.7", "0.8", "0.6", "0.5", "0.9", "stability", "0.7", "0.8", "0.6", "0.5", "Figure", "10", "quantitative", "comparison", "exist", "stabilization", "technique", "publicly", "available", "datum", "10", "11", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "Bundled", "Camera", "Paths", "Video", "Stabilization", "78:9", "iv", "-lrb-", "-rrb-", "simple", "-lrb-", "ii", "-rrb-", "quick", "rotation", "-lrb-", "iii", "-rrb-", "zoom", "large", "parallax", "-lrb-", "-rrb-", "drive", "0.9", "0.9", "0.9", "0.9", "0.9", "0.8", "0.8", "0.8", "0.8", "0.8", "-lrb-", "-rrb-", "0.7", "0.6", "0.7", "0.6", "0.6", "0.7", "0.6", "0.7", "0.7", "0.6", "0.5", "0.5", "0.5", "0.5", "0.5", "0.4", "0.4", "0.4", "0.4", "0.4", "0.3", "0.3", "0.3", "0.3", "0.3", "Google", "YouTube", "Adobe", "after", "effect", "cs6", "-lrb-", "Warp", "Stabilizer", "-rrb-", "we", "system", "crop", "ratio", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "input", "video", "YouTube", "result", "we", "result", "figure", "12", "comparison", "two", "popular", "system", "YouTube", "Stabilizer", "Adobe", "after", "effect", "Warp", "Stabilizer", "top", "quantitative", "comparison", "three", "metric", "crop", "-lrb-", "-rrb-", "distortion", "-lrb-", "-rrb-", "stability", "-lrb-", "-rrb-", "bottom", "some", "sample", "video", "frame", "visual", "comparison", "-lrb-", "-rrb-", "simple", "-lrb-", "ii", "-rrb-", "quick", "rotation", "100", "90", "80", "70", "60", "-lrb-", "-rrb-", "50", "40", "30", "20", "10", "figure", "13", "-lrb-", "-rrb-", "pair-wise", "comparison", "interface", "user", "study", "-lrb-", "-rrb-", "user", "study", "result", "compare", "we", "method", "two", "popular", "system", "YouTube", "Stabilizer", "Adobe", "after", "effect", "Warp", "Stabilizer", "83", "vs.", "17", "zoom", "-rrb-", "possibly", "due", "significant", "crop", "result", "Warp", "Stabilizer", "category", "-lrb-", "v?vii", "-rrb-", "more", "participant", "prefer", "we", "result", "other", "two", "system", "although", "three", "system", "generate", "similar", "stability", "level", "accord", "we", "stability", "metric", "likely", "because", "superior", "distortion", "crop", "control", "we", "method", "category", "-lrb-", "-rrb-", "simple", "user", "express", "similar", "preference", "toward", "three", "result", "after", "user", "study", "we", "also", "ask", "all", "participant", "articulate", "criterion", "feedback", "we", "conclude", "main", "criterion", "unacceptable", "video", "-rrb-", "video", "get", "smaller", "field", "view", "even", "contain", "frame", "visible", "empty", "-lrb-", "black", "-rrb-", "area", "-rrb-", "video", "present", "structure", "distortion", "individual", "frame", "-rrb-", "motion", "some", "video", "frame", "vibrate", "oscillate", "-rrb-", "scene", "transition", "look", "abrupt", "smooth", "video", "from", "criterion", "we", "propose", "metric", "can", "partially", "relate", "human", "preference", "both", "quantitative", "evaluation", "user", "study", "result", "consistently", "indicate", "we", "system", "perform", "better", "than", "other", "two", "system", "5.5", "Limitations", "Discussion", "we", "find", "when", "3d", "reconstruction", "successful", "3d", "method", "often", "generate", "best", "result", "however", "we", "system", "more", "robust", "we", "do", "require", "feature", "tracking", "produce", "comparable", "only", "slightly", "worse", "result", "interesting", "note", "we", "adaptive", "path", "optimization", "can", "also", "apply", "path", "smoothing", "3d", "method", "-lsb-", "Liu", "et", "al.", "2009", "Liu", "et", "al.", "2011", "Goldstein", "Fattal", "2012", "-rsb-", "which", "often", "use", "low-pass", "filter", "-lrb-", "gaussian", "smoothing", "-rrb-", "curve", "fitting", "path", "planning", "comparison", "we", "adaptive", "camera", "path", "smoothing", "technique", "can", "automatically", "adjust", "smoothness", "strength", "consider", "discontinuity", "distortion", "we", "show", "example", "video", "we", "project", "webpage", "case", "where", "warping-based", "motion", "model", "fail", "handle", "severe", "occlusion", "dis-occlusion", "especially", "when", "combine", "roll", "shutter", "effect", "Figure", "14", "show", "two", "example", "we", "warping-based", "motion", "model", "choose", "large", "enforce", "strong", "coherence", "between", "grid", "cell", "way", "we", "can", "minimize", "geometrical", "distortion", "same", "time", "we", "sacrifice", "motion", "accuracy", "eventually", "stability", "result", "general", "we", "find", "geometrical", "distortion", "more", "disruptive", "than", "some", "slight", "remain", "jitters", "we", "path", "optimization", "do", "strictly", "follow", "cinematography", "rule", "which", "may", "desirable", "certain", "application", "we", "discontinuity-preservation", "optimization", "produce", "visually", "please", "result", "most", "example", "necessary", "we", "could", "apply", "strategy", "-lsb-", "gleicher", "Liu", "2007", "-rsb-", "post-process", "solve", "problem", "we", "also", "do", "deal", "motion", "blur", "sometimes", "stabilize", "result", "contain", "visible", "blur", "artifact", "problem", "can", "address", "recent", "work", "-lsb-", "Cho", "et", "al.", "2012", "-rsb-", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "run", "0.9", "0.9", "0.8", "0.8", "0.7", "0.7", "0.6", "0.6", "0.5", "0.5", "0.4", "0.4", "0.3", "0.3", "distortion", "stability", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "iii", "-rrb-", "zoom", "-lrb-", "iv", "-rrb-", "large", "parallax", "-lrb-", "-rrb-", "driving", "-lrb-", "VI", "-rrb-", "crowd", "-lrb-", "vii", "-rrb-", "run", "effect", "cs6", "-lrb-", "Warp", "Ours", "conclusion", "we", "have", "present", "new", "2d", "video", "stabilization", "method", "bundle", "camera", "path", "model", "propose", "method", "can", "simultaneously", "generate", "comparable", "result", "3d", "method", "while", "keep", "merit", "2d", "method", "use", "image", "warping", "technique", "motion", "representation", "interesting", "finding", "paper", "future", "we", "would", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013", "78:10", "S.", "Liu", "et", "al.", "-lrb-", "-rrb-", "figure", "14", "two", "failure", "case", "left", "due", "severe", "occlusion", "together", "roll", "shutter", "effect", "right", "cause", "crowd", "extend", "kind", "representation", "other", "video-based", "application", "acknowledgement", "we", "thank", "all", "reviewer", "helpful", "discussion", "Kaimo", "Lin", "other", "subject", "help", "user", "study", "Nathan", "Holdstein", "Jiangyu", "Liu", "help", "proofread", "work", "also", "partially", "support", "Singapore", "project", "r-263-000-620-112", "reference", "aker", "S.", "ENNETT", "E.", "P.", "ang", "S.", "B.", "ZELISKI", "R.", "2010", "remove", "rolling", "shutter", "wobble", "Proc", "cvpr", "ay", "H.", "SS", "a.", "uytelaar", "t.", "ool", "L.", "2008", "speeded-up", "robust", "feature", "-lrb-", "surf", "-rrb-", "Comput", "Vis", "image", "underst", "110", "346", "359", "ronshtein", "i.", "N.", "EMENDYAYEV", "K.", "A.", "1997", "handbook", "mathematics", "Springer-Verlag", "New", "York", "NY", "USA", "rox", "T.", "RUHN", "a.", "apenberg", "N.", "EICKERT", "J.", "2004", "high", "accuracy", "optical", "flow", "estimation", "base", "theory", "warping", "Proc", "eccv", "uehler", "C.", "OSSE", "M.", "ILLAN", "L.", "2001", "nonmetric", "image-based", "rendering", "video", "stabilization", "Proc", "cvpr", "hen", "b.-y.", "ee", "k.-y.", "uang", "w.-t.", "j.-s", "2008", "capture", "intention-based", "full-frame", "video", "stabilization", "Computer", "Graphics", "Forum", "27", "1805", "1814", "ho", "S.", "ang", "J.", "ee", "S.", "2012", "Video", "deblurr", "hand-held", "camera", "use", "patch-based", "synthesis", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "31", "ischler", "M.", "A.", "OLLES", "R.", "C.", "1981", "Random", "sample", "consensus", "paradigm", "model", "fitting", "application", "image", "analysis", "automate", "cartography", "Commun", "ACM", "24", "381", "395", "orss", "p.-e.", "ingaby", "E.", "2010", "rectify", "rolling", "shutter", "video", "from", "hand-held", "device", "cvpr", "ao", "J.", "IM", "S.", "J.", "ROWN", "M.", "S.", "2011", "construct", "image", "panorama", "use", "dual-homography", "warping", "Proc", "cvpr", "leicher", "M.", "L.", "iu", "F.", "2007", "re-cinematography", "improving", "camera", "dynamics", "casual", "video", "Proc", "ACM", "Multimedia", "oldstein", "a.", "attal", "R.", "2012", "Video", "stabilization", "use", "epipolar", "geometry", "ACM", "Trans", "graph", "-lrb-", "tog", "-rrb-", "31", "126:1", "126:10", "rundmann", "M.", "WATRA", "V.", "ssa", "i.", "2011", "autodirect", "video", "stabilization", "robust", "l1", "optimal", "camera", "path", "Proc", "cvpr", "rundmann", "M.", "WATRA", "V.", "ASTRO", "D.", "ssa", "i.", "2012", "calibration-free", "rolling", "shutter", "removal", "Proc", "iccp", "artley", "R.", "isserman", "a.", "2003", "multiple", "View", "Geometry", "Computer", "Vision", "ed", "Cambridge", "University", "Press", "New", "York", "NY", "USA", "garashi", "T.", "OSCOVICH", "T.", "UGHES", "J.", "F.", "2005", "asrigid-as-possible", "shape", "manipulation", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "24", "1134", "1141", "ARPENKO", "A.", "ACOBS", "D.", "AEK", "J.", "EVOY", "M.", "2011", "Digital", "video", "stabilization", "roll", "shutter", "correction", "use", "gyroscope", "Stanford", "CS", "Tech", "Report", "ee", "k.-y.", "huang", "y.-y.", "hen", "b.-y.", "uhyoung", "M.", "2009", "Video", "stabilization", "use", "robust", "feature", "trajectory", "Proc", "iccv", "iang", "c.-k.", "hang", "l.-w.", "hen", "H.", "H.", "2008", "analysis", "compensation", "roll", "shutter", "effect", "IEEE", "Trans", "image", "processing", "W.-Y.", "IU", "S.", "atsushita", "Y.", "T.-T.", "heong", "l.-f", "2011", "smoothly", "vary", "affine", "stitching", "iu", "F.", "LEICHER", "M.", "H.", "garwalum", "a.", "2009", "content-preserving", "warp", "3d", "video", "stabilization", "Proc", "cvpr", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "28", "iu", "F.", "LEICHER", "M.", "ang", "J.", "H.", "garwalum", "a.", "2011", "Subspace", "video", "stabilization", "ACM", "Trans", "graph", "30", "iu", "S.", "ang", "Y.", "uan", "L.", "J.", "P.", "UN", "J.", "2012", "Video", "stabilization", "depth", "camera", "Proc", "cvpr", "uca", "B.", "D.", "ANADE", "T.", "1981", "iterative", "image", "registration", "technique", "application", "stereo", "vision", "Proc", "International", "Joint", "Conference", "Artificial", "Intelligence", "-lrb-", "IJCAI", "-rrb-", "674", "679", "atsushita", "Y.", "FEK", "E.", "W.", "ang", "X.", "HUM", "H.Y.", "2006", "full-frame", "video", "stabilization", "motion", "inpainting", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "28", "1150", "1163", "orimoto", "C.", "hellappa", "R.", "1998", "evaluation", "image", "stabilization", "algorithm", "Proc", "IEEE", "International", "Conference", "Acoustics", "speech", "signal", "processing", "2789", "2792", "akamura", "J.", "2005", "image", "sensor", "signal", "processing", "Digital", "still", "Cameras", "crc", "Press", "Inc.", "ir", "T.", "RUCKSTEIN", "A.", "M.", "IMMEL", "R.", "2008", "overparameterize", "variational", "optical", "flow", "int", "J.", "Comput", "Vision", "-lrb-", "IJCV", "-rrb-", "76", "205", "216", "chaefer", "S.", "hail", "T.", "ARREN", "J.", "2006", "image", "deformation", "use", "move", "least", "square", "ACM", "Trans", "graph", "-lrb-", "Proc", "SIGGRAPH", "-rrb-", "25", "533", "540", "hum", "h.-y.", "zeliskus", "R.", "2000", "construction", "panoramic", "image", "mosaic", "global", "local", "alignment", "int", "J.", "Comput", "Vision", "-lrb-", "IJCV", "-rrb-", "36", "101", "130", "mith", "B.", "M.", "HANG", "L.", "H.", "garwalum", "a.", "2009", "light", "field", "video", "stabilization", "Proc", "iccv", "zeliskus", "R.", "1996", "Motion", "estimation", "quadtree", "spline", "IEEE", "Trans", "pattern", "Anal", "Mach", "Intell", "18", "12", "1199", "1210", "omasus", "C.", "anduchus", "R.", "1998", "bilateral", "filter", "gray", "color", "image", "Proc", "iccv", "839", "846", "ACM", "transaction", "Graphics", "Vol", "32", "no.", "Article", "78", "publication", "date", "July", "2013" ],
  "content" : "\n  \n    a64539a83c37a9b4748793bf763bbed1f6879625915cf3d86f6025a3f65431b3\n    p2q\n    10.1145/2461912.2461995\n    Name identification was not possible. \n  \n  \n    \n      \n        Bundled Camera Paths for Video Stabilization\n      \n      Shuaicheng Liu ? Lu Yuan ? ? National University of Singapore\n      \n        \n      \n      (a) a single global path\n      \n        Figure 1: Comparison between traditional 2D stabilization (a single global camera path) and our bundled camera paths stabilization. We plot the camera trajectories (visualized by the y-axis translation over time) and show the original path (red) and the smoothed path (blue) for both methods. Our bundled paths rely on a 2D mesh-based motion representation, and are smoothed in space-time.\n      \n      We present a novel video stabilization method which models camera motion with a bundle of (multiple) camera paths. The proposed model is based on a mesh-based, spatially-variant motion representation and an adaptive, space-time path optimization. Our motion representation allows us to fundamentally handle parallax and rolling shutter effects while it does not require long feature trajectories or sparse 3D reconstruction. We introduce the ?as-similaras-possible? idea to make motion estimation more robust. Our space-time path smoothing adaptively adjusts smoothness strength by considering discontinuities, cropping size and geometrical distortion in a unified optimization framework. The evaluation on a large variety of consumer videos demonstrates the merits of our method. CR Categories: I.4.3 [Image Processing and Computer Vision]: Enhancement?Registration Keywords: video stabilization, image warping, camera paths\n      Links: DL PDF\n    \n    \n      \n        1 Introduction\n      \n      A video captured with a hand-held device (e.g., a cell-phone or a portable camcorder) often appears remarkably shaky and undirected. Digital video stabilization improves the video quality by removing unwanted camera motion. It is of great practical importance because the devices (mobile phones, tablets, camcorders) capable  of capturing video have become widespread and online sharing is so ubiquitous. Prior video stabilization methods synthesized a new stabilized video by estimating and smoothing 2D camera motion [Matsushita et al. 2006; Grundmann et al. 2011] or 3D camera motion [Liu et al. 2009; Liu et al. 2012]. In general, 2D methods are more robust and faster because they only estimate a linear transformation (affine or homography) between consecutive frames. But the 2D linear motion model is too weak to fundamentally handle the parallax caused by non-trivial depth variation in the scene. On the contrary, the 3D methods can deal with the parallax in principle and generate strongly stabilized results. However, their motion model estimation is less robust to various degenerations such as feature tracking failure, motion blur, camera zooming, and rapid rotation. Briefly, 2D methods are more robust but may sacrifice quality (e.g., introducing unpleasant geometrical distortion or producing less stabilized output), while 3D methods can achieve high-quality results but are more fragile. Some recent methods [Liu et al. 2011; Goldstein and Fattal 2012] have successfully combined the advantages of these two kinds of methods. Liu et al. [2011] applied a low-rank, subspace constraint on 2D feature trajectories, which is an effective simplification of 3D reconstruction. Goldstein and Fattal [2012] avoided 3D reconstruction by exploiting the ?epipolar transfer? technique. These methods relax the requirement from 3D reconstruction to 2D long feature tracking. Nevertheless, requiring long feature tracking (typically over 20 frames) makes it difficult to handle more challenging cases (e.g., rapid motion, fast scene transition, large occlusion) in the consumer videos. This paper aims at the same goal of robust high-quality result but from an opposite direction: we propose a more powerful 2D camera motion model. Specifically, we present bundled camera paths model which maintains multiple, spatially-variant camera paths. In other words, each different location in the video has its own camera path. This flexible model allows us to fundamentally deal with nonlinear motion caused by parallax and rolling shutter effects [Liang et al. 2008; Baker et al. 2010; Grundmann et al. 2012]. At the same time, the model enjoys the robustness and simplicity of 2D methods, because it only requires feature correspondences between two consecutive frames. Our bundled camera paths model is built on two novel components: a warping-based motion representation (and estimation), and an adaptive space-time path smoothing. The first component represents the motion between two consecutive frames by mesh-based, spatially-variant homographies ( Figure 1(b) ) with a ?as-similar-aspossible? regularization constraint [Igarashi et al. 2005; Schaefer et al. 2006]. This constraint is critical because estimating a model with such a high degree of freedom is usually risky in the cases of insufficient features or large occlusions. To the best of our knowledge, this is the first work to employ the mesh-based ?as-similar-aspossible? regularization for spatially-variant motion estimation in video stabilization. Notice that the ?as-similar-as-possible? warping was used in [Liu et al. 2009; Liu et al. 2011] for video stabilization. But we directly use the mesh vertices as the motion model itself. No intermediate representation is used, such as 3D reconstruction [Liu et al. 2009] or subspace [Liu et al. 2011]. Based on the proposed motion representation, we construct a bundle of camera paths, each of which is the concatenation of local homographies at the same grid cell over time ( Figure 1(b) ). Our second component smooths all bundled camera paths as a whole to maintain both spatial and temporal coherences. Furthermore, to avoid excessive cropping/geometrical distortion and approximate cinematography favored path, we adopt a discontinuity-preserving idea similar to bilateral filtering [Tomasi and Manduchi 1998] to adaptively control the strength of smoothing. For a quantitative evaluation, we provide a comprehensive dataset (including both public examples and our own video clips of different kinds of motions). We show that our new 2D method is comparable to or outperforms other competitive 2D or 3D methods.\n      ACM Reference Format Liu, S., Yuan, L., Tan, P., Sun, J. 2013. Bundled Camera Paths for Video Stabilization. ACM Trans. Graph. 32, 4, Article 78 (July 2013), 10 pages. DOI = 10.1145/2461912.2461995 http://doi.acm.org/10.1145/2461912.2461995. Copyright Notice Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the fi rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org . Copyright ? ACM 0730-0301/13/07-ART78 $15.00. DOI: http://doi.acm.org/10.1145/2461912.2461995\n      Ping Tan ? Jian Sun ? ? Microsoft Research Asia\n      (b) our bundled paths\n      ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n      78:2\n      ?\n      S. Liu et al.\n      \n        2 Related Work\n        2D Methods estimate 2D transformations between consecutive video frames and smooth them over time to generate a steady video. Most previously developed methods apply an affine or homography model, and focus on the design of the smoothing algorithm. Earlier works [Morimoto and Chellappa 1998; Matsushita et al. 2006] apply low-pass filters to individual model parameters. Some methods assume prior motion models such as polynomial curves [Chen et al. 2008] for desired camera trajectories. Gleicher and Liu [2007] divide the original camera trajectory into multiple segments for subsequent individual smoothing. More recently, Grundmann et al. [2011] gracefully apply L 1 -norm optimization to generate a camera path consisting of constant, linear and parabolic motions, which follow cinematography rules. Grundmann et al. [2012] further adopt a homography-array-based motion model to deal with rolling shutter effects. These two techniques have been integrated into Google YouTube. It is robust, follows cinematography rules, and performs well on many consumer videos. Our method belongs to this category. But we use a spatially-variant model to represent the motion between video frames and design an appropriate smoothing technique for this model. 3D Methods often rely on robust feature tracking for stabilization. Beuhler et al. [2001] perform stabilization with a projective 3D reconstruction of the scene from an uncalibrated camera. Liu et al. [2009] develop the first successful 3D video stabilization system and are the first to introduce ?content-preserving? warping for stabilization. Since 3D reconstruction is difficult, recent methods directly smooth the trajectories of tracked features. Liu et al. [2011] smooth some basis trajectories (preferably longer than 50 frames) of the subspace formed by the feature tracks. This method achieves similar quality to 3D reconstruction-based methods, while reducing the require ment from 3D reconstruction to long feature tracking. It has been transferred to Adobe After Effects as a feature called ?Warp Stabilizer?. Goldstein and Fattal [2012] utilize an ?epipolar transfer? technique to avoid the fragile 3D reconstruction. This technique also alleviates the strain on long feature tracks. But it still requires moderate feature track length (typically over 20 frames). Feature track smoothing is also used in light-field camera video stabilization work [Smith et al. 2009]. To address the occlusion issue, Lee et al. [2009] introduce feature pruning to choose robust feature trajectories for smoothing. Nearly all methods involving feature tracking face a common obstacle ? in many consumer videos obtaining long feature tracks is fragile due to occlusion, motion blur or rapid camera motion. Our method does not encounter this issue since it only computes relative motion between consecutive frames. Motion Estimation computes the transition between two images with view overlap. Optical flow algorithms [Lucas and Kanade 1981] model this transition by individual displacement vectors at every pixel. When there is no parallax, this transition can be represented elegantly by a global homography transformation [Hartley and Zisserman 2003]. Local alignment [Shum and Szeliski 2000] or a dual-homography model [Gao et al. 2011] can reduce alignment error caused by parallax. Szeliski and Shum [1996] represent motion using a mixture of spline models with spatially variant spatial support to facilitate registration. Lin et al. [2011] estimate a smoothly varying affine field to align images of large viewpoint changes. This model can be potentially used for video stabilization. However, its current motion estimation technique is slow (may take 8 minutes to process a 720p frame). Our motion model is essentially a mesh-based, spatially-variant homography model, inspired by recent image warping techniques [Igarashi et al. 2005; Schaefer et al. 2006; Liu et al. 2009]. We extend the ?as-similar-as-possible? idea from image synthesis to motion estimation, and apply it to video stabilization. It is very efficient to estimate our motion model (may take only 50 milliseconds to process a 720p frame). Rolling Shutter Removal estimates and corrects inter-row motion caused by the row-parallel readout, i.e., electronic rolling shutter [Nakamura 2005] mainly in CMOS sensors. Prior works design different parametric inter-row motion models, including a per-row translation model [Liang et al. 2008; Baker et al. 2010] and 3D rotation model [Forss?n and Ringaby 2010]. Recently, Grundmann et al. [2012] proposed a calibration-free homography mixture model, which shows significant improvement. Karpenko et al. [2011] use dedicated hardware ? the gyroscope on mobile devices, to correct the rolling shutter effects in real-time. Similar to [Grundmann et al. 2012], our method corrects rolling shutter effects without any prior calibration. Our warping-based model naturally handles the rolling shutter effects as a special kind of spatially variant motion. So we do not need a separate rolling shutter correction step in our stabilization.\n      \n      \n        3 Bundled Camera Paths\n        In this section, we introduce our warping-based motion model and bundled camera paths.\n        \n          3.1 Warping-based Motion Model\n          We propose using an image warping model to represent the motion between consecutive video frames, which provides stronger modeling power than conventional single, 2D linear transformations. We adopt the warping model in [Igarashi et al. 2005; Liu et al. 2009],\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          Bundled Camera Paths for Video Stabilization\n          ?\n          78:3\n          \n            a\n            v 1 p v 2 p v ? 1 p v ? 2 p v ? v 4 p p v 3 p ? 4 p p v ? 3 p (b) v ? 1 v ? 0 frame t frame t+1\n          \n          \n            Figure 2: (a) Parameterization of the motion between two frames by a regular grid mesh, where a pair of matched features (p, p) should be represented by the same bilinear interpolation of their four enclosing vertices. (b) The as-similar-as-possible term requires each triangle v, v 0 , v 1 to follow a similarity transformation.\n          \n          though more general models such as ?moving-least-square? [Schaefer et al. 2006] or parameterized optical flow [Nir et al. 2008] might be used.  Model At each frame, we define a uniform grid mesh as illustrated in Figure 2 . The motion is represented by an (unknown) warping of the grid mesh to register two frames (in fact, their corresponding feature points). We require matched features (e.g., p and p in Figure 2 ) to share the same bilinear interpolation of the four corners of the enclosing grid cell after warping. At the i-th grid cell, the warping from frame t to frame t + 1 introduces a homography F i (t), which can be determined from the motion of the four enclosing vertices. Thus, the warping-based motion model is actually a set of spatially-variant homographies on a 2D grid. Note that this highly flexible model is able to handle parallax. It is between global homography and per-pixel optical flow. However, estimating a model with such a high degree of freedom is very risky because we may not have sufficient features (due to textureless regions or occlusions) in every cell. Regularization To address this challenge, we propose imposing a shape-preserving (i.e., ?as-similar-as-possible? [Igarashi et al. 2005]) constraint. The combination of the shape-preserving and mesh representation together provides two kinds of regularizations: 1) for each cell, the fitted homography should be biased toward a reduced similarity (or rigid) transformation; 2) the intrinsic connection of the mesh (two neighboring mesh cells share two vertices) enforces a first-order continuity constraint. They can help to propagate or fill in information from regions with sufficient features to other regions. Finally, we estimate the motion by minimizing two energy terms: a data term for matching features, and a shape-preserving term for enforcing regularization.\n        \n        \n          3.2 Model Estimation\n          We first describe our basic method by following [Liu et al. 2009], and later extend it for better robustness in the next subsection. Data term As shown in Figure 2 , suppose {p, p} is the p-th matched feature pair from frame t to frame t + 1. The feature p can be represented by a 2D bilinear interpolation of the four vertices V p = [v p 1 , v p 2 , v p 3 , v p 4 ] of the enclosing grid cell: p = V p w p , where w p = [w p 1 , w p 2 , w p 3 , w p 4 ] are interpolation weights that sum to 1. We expect that the corresponding feature p can be represented by the same weights of the warped grid vertices V ? p = [ v p 1 , v p 2 , v p 3 , v p 4 ]. Therefore the data term is defined as\n          E d ( V ? ) = || V ? p w p ? p|| 2 . (1) p\n          \n            \n            Figure 3: Comparison of motion estimation with and without the shape-preserving term.\n          \n          Here V ? contains all the warped grid vertices. Solving V ? determines the warping of the grid. Shape-preserving term We use the same shape-preserving term as [Liu et al. 2009] involving all vertices in V ? ,\n          E s ( V ? ) = v ? v 1 ? sR 90 ( v 0 ? v 1 ) 2 , R 90 = ?1 0 0 1 , (2) v\n          where s = v ? v 1 / v 0 ? v 1 is a known scalar computed from the initial mesh. This shape-preserving term requires the triangle of neighboring vertices v, v 0 , v 1 to follow a similarity transformation. Linearly combining two terms forms our final energy E( V ? ):\n          E( V ? ) = E d ( V ? ) + ?E s ( V ? ), (3)\n          where ? is an important weight to control the amount of regularization. We will discuss how to adaptively determine it later. Since the energy E( V ? ) is quadratic, the warped mesh V ? can be easily solved by a sparse linear system solver. Estimating homographies After having a new mesh, we can estimate each local homography F i (t) in the grid cell i of frame t by solving a linear equation:\n          V ? i = F i (t)V i , (4)\n          where V i and V ? i are the four vertices before and after the warping. Figure 3 shows the warped mesh grid according to the estimated motion. Left and right are the results with and without the shapepreserving term. It is clear that the regularization term helps maintain a smooth varying mesh representation.\n        \n        \n          3.3 Robust Estimation\n          We further generalize our motion estimation to make it more robust. Outlier rejection We reject incorrectly matched features at two scales. At the coarse scale (the whole image), we apply RANSAC algorithm [Fischler and Bolles 1981] to fit a global homography F  ? (t) and discard features by a relatively large threshold on fitting error (6% image width). At the fine scale (4 ? 4 sub-images), we apply RANSAC again to reject features by a relatively small threshold (2% image width). Pre-warping To facilitate the warping estimation, we use global homography F  ? (t) to bring matching features closer. We then solve the warping to estimate the residual motion, which generates a homography F i (t) at each grid cell. The final homography F i (t) is simply computed as F i (t) ? F  ? (t). Note that this coarse-to-fine strategy has been used in [Liu et al. 2009] for image synthesis and proven effective in motion estimation literature [Brox et al. 2004].\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          78:4\n          ?\n          S. Liu et al.\n          \n            \n          \n          1 1 0.8 0.8 0.6 0.6 0.4 0. 3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3 0.4 0.3 0.6 0.9 1.2 1.5 1.8 2.1 2.4 2.7 3\n          \n            Figure 4: Our method automatically chooses an appropriate ? for\n          \n          different scenes: (a) a scene free of occlusion; (b) a scene with severe occlusion.\n          Adaptive regularization A good regularization should be adaptive to image content. For example, if reliable features are uniformly distributed over the whole image, we should trust the data term more and use a smaller weight ? in Equation 3 for a weaker regularization. But when there is occlusion or insufficient features, we prefer stronger regularization as the data term is less reliable. To implement this strategy, we adaptively set ? per frame, based on two errors: fitting error e h and smoothness error e s .  The fitting error e h is the average residual of the feature matching under the estimated homographies, i.e., e h = n 1 p F p ? p ? p 2 , where F p is the homography in the cell containing p, and n is the number of feature pairs. The smoothness error e s measures the similarity (L 2 distance) between neighboring local homographies by e s = ? j?? i F i ? F j 2 , where ? i consists of the neighboring cells of i. Here, the homography matrix is normalized so that sum of all its elements is one. We empirically set ? = 0.01, since it makes the scale of e h and e s similar on most of the examples. Then we define the combined error as e = e h + e s . We equally discretize ? into 10 values between 0.3 and 3. We perform the model estimation using every discretized value and select the model with minimum error e. As shown in Figure 4(a) , for simple scenes with smooth depth variation, neighboring cells tend to have similar homographies. So we choose a small ?(=0.9) to better minimize the data error. On the contrary, for scenes with large occlusion ( Figure 4(b) ), neighboring local homographies are less similar. The smoothness error can be significantly reduced by increasing ?. So our system will automatically choose a large ?(=3.0) to ensure consistent local motion. Finally, we show an example in Figure 5 to verify the strength of the regularization of our method. In this example, we compare two meshes estimated using all features and a subset of features. Two similar results indicate our method can robustly deal with regions of insufficient features.\n        \n        \n          3.4 Bundled Camera Paths\n          With estimated local homographies, we can define a bundle of spatially-variant camera paths for the whole video. Let C i (t) be\n          \n            \n            Figure 5: Left: the estimated warping mesh from all feature points. Right: we exclude all the features in the orange box when estimating the warping model. A similar mesh can be obtained despite the lack of features.\n          \n          \n            a\n            (b) F ( t\n          \n          F i ( t )\n          \n            \n            \n            \n            \n          \n          B ( t ) B ( t + 1)\n          F j ( t )\n          \n            \n            \n            \n            \n            Figure 6: (a) Bundled camera paths.\n          \n          (b) Relationships among original path {C(t)}, smoothed path {P (t)}, and transformations {B(t)} the camera pose of the grid cell i at frame t. It can be written as: C i (t) = C i (t ? 1)F i (t ? 1), ? C i (t) = F i (0)F i (1) ? ? ? F i (t ? 1),\n          where {F i (0), ..., F i (t ? 1)} are estimated local homographies at the same grid cell i, as shown in Figure 6 (a). We call these spatially-variant paths as ?bundled camera paths?. In the next section, we describe how we smoothen these bundled paths for video stabilization.\n        \n      \n      \n        4 Path Optimization\n        We first describe our smoothing method for a single camera path, and extend it to a bundle of camera paths.\n        \n          4.1 Optimizing a Single Path\n          A good camera path smoothing should consider multiple competing factors: removing jitters, avoiding excessive cropping, and minimizing various geometrical distortions (shearing/skewing, wobble). To reach a desired balance, we propose an optimization-based framework taking all factors into account.  Formulation Given an original path C = {C(t)}, we seek an optimized path P = {P (t)} by minimizing the following function:\n          O ({P (t)}) =\n          \n            C\n            P ? C(t) 2 + ? t r?? t ? t,r ? P (t) ? P (r) 2 ,\n          \n          t (5) where ? t are the neighborhood at frame t. The other terms are: ? data term P (t) ? C(t) 2 enforcing the new camera path to be close to the original one to reduce cropping and distortion; ? smoothness term P (t) ? P (r) 2 stabilizing the path; ? weight ? t,r (C) to preserve motion discontinuities under fast panning/rotation or scene transition;\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          Bundled Camera Paths for Video Stabilization\n          ?\n          78:5\n          original original no adaptive adaptive camera path (no adaptive) camera path (with adaptive)\n          \n            \n          \n          output frames (no adaptive weight)\n          \n            \n            Figure 7: Comparison of with and without adaptive weights G m ()\n          \n          for a video with rapid camera panning. The camera paths on the top plot the x-translation over time.\n          ? parameter ? t to balance the above two terms. Since Equation 5 is quadratic, we can solve it with any linear system solver. Here, we use a Jacobi-based iterative solver [Bronshtein and Semendyayev 1997]:\n          (?+1) 1 2? t ? t,r (?) P (t) = C(t) + P (r), (6) ? ? r?? t ,r=t\n          where ? = 1 + 2? t r?? t ,r=t ? t,r , and ? is an iteration index. At initialization, P (0) (t) = C(t). Once we obtain the optimized path P, we compute the warping transform B(t) = C ?1 (t)P (t) to warp the original video frame to the stabilized result ( Figure 6(b) ). Discontinuity-preserving The adaptive weight ? t,r is important to preserve motion discontinuity. We follow the idea of bilateral filter [Tomasi and Manduchi 1998] and design it by two Gaussian functions:\n          ? t,r = G t ( r ? t ) ? G m ( C(r) ? C(t) ) , (7)\n          where G t () gives larger weight to the nearby frames. G m () measures the changes of two camera poses. We use a large kernel to ensure successful suppression of both highfrequency jitters (e.g., handshake) and low-frequency bounces (e.g., walking). In our implementation, we set ? t to 60 neighboring frames and the standard deviation of G t () to 10. In contrast, previous low-pass filtering based methods [Matsushita et al. 2006] typically need a smaller amount of support (e.g., 10 frames) to avoid aggressive cropping and distortion. But such a small kernel is often insufficient in suppressing low frequency bounces. The reason why we can use a larger kernel lies in G m (). In video stabilization, for rapid camera motion (e.g, caused by fast panning or scene transition), an inappropriate amount of smoothing may lead to excessive cropping, as shown in Figure 7 . In this case, the camera pans quickly, and na?ve Gaussian smoothing (second row) causes the camera path to significantly deviate from its original path, as indicated by the dashed lines in the left plot on top. The corresponding frames shown on the second row will require large cropping. Our adaptive term G m () preserves the sudden camera motions to a certain degree. The result from our adaptive smoothing (bottom row) produces much less cropping. To measure the camera motion, we use the change in translation components ? x (t), ? y (t) extracted from the camera pose C(t), namely |? x (t) ? ? x (r)| + |? y (t) ? ? y (r)|. The frame translation ? x (t), ? y (t) can describe most camera motions in practice except for an in-plane rotation or scale around the principal axis. Cropping and distortion control The above adaptive term ? t,r can give us a certain amount of ability to control cropping and distortion. However, the user may want to have strict control on the cropping ratio and distortion. In principle, we could formulate a constrained optimization to address this issue. But it may be too complex to be solved or reproduced. In this work, we resort to a simple but effective method adaptively adjust the parameter ? t for each frame. We first run the optimization with a global fixed ? t = ? (empirically set to 5) and then check the cropping ratio and distortion of every frame. For any frame that does not satisfy the user requirements (cropping ratio or distortion is smaller than a pre-defined threshold), we decrease its parameter ? t by a step (1/10? t ) and re-run the optimization. Note, according to Equation 6, a smaller ? will make the optimized path closer to the original one, which has less cropping and distortions. The procedure is iterated until all frames satisfy the requirements. We measure the cropping ratio and distortion from the warping transform B(t) = C ?1 (t)P (t). The anisotropic scaling of B(t) measures the distortion. It can be computed by the ratio of the two largest eigenvalues of the affine part of B(t) [Hartley and Zisserman 2003]. We use B(t) to compute the overlapping area of the original video frame and the stabilized frame. The cropping ratio is the ratio of this area and the original frame area. In our experiments, we require the cropping ratio to be larger than 0.8, and the distortion score to be larger than 0.95 for all examples. In principle, we can further measure the perspective distortion by the two perspective components in B(t). But we empirically find they are always too small when compared with the affine components and do not include them.\n        \n        \n          4.2 Optimizing Bundled Paths\n          Our motion model generates a bundle of camera paths. If these paths are optimized independently, neighboring paths could be less consistent, which may generate distortion in the final rendered video. Hence, we do a space-time optimization of all paths by minimizing the following objective function O ({P i (t)}) + P i (t) ? P j (t) 2 , (8) i t j?N (i) where N (i) includes eight neighbors of the grid cell i. The first term is the objective function in Equation 5 for each single path, and the second term enforces the smoothness between neighboring paths. This optimization is also quadratic and the optimum result can be obtained by solving a large sparse linear system. Again, our solution is updated by a Jacobi-based iteration [Bronshtein and Semendyayev 1997]: (?+1) 1 (?) (?) P i (t) = ? (C i (t)+ 2? t w t,r P i (r)+ 2P j (t)), r?? t j?N (i) r=t j=i where ? = 2? t w t,r + 2N (i) ? 1. r?? t ,r=t\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          78:6\n          ?\n          S. Liu et al.\n          We typically iterate 20 times to optimize camera paths.\n          During optimization, the motion-adaptive term G m (?) is evaluated at individual cells, since different cells have different motion. In comparison, ? t is determined from the global path (generated by concatenating the pre-warping global homographies), because it controls the overall cropping and distortion. Then, we use ? t to optimize the camera paths in all cells.  Result synthesis After path optimization, we compute the warping matrix B i (t) for each cell i by B i (t) = C i ?1 (t)P i (t). We then apply B i (t) to warp the i-th cell at the t-th frame to generate the final output video. Usually, applying B i (t) directly generates good results. This is because our motion estimation ensures first order smoothness of the original paths. Furthermore, the bundled optimization in Equation 8 requires nearby optimized paths to be similar. Thus, the smoothness is naturally satisfied by B i (t) most of the time. Sometimes, there are slight distortions (e.g., seams of about 1-pixel width), in which case we perform a bilinear interpolation to fix them.\n        \n        \n          4.3 Correcting Rolling Shutter Effects\n          Our bundled paths model can naturally handle rolling shutter effects without pre-calibration. The principle of our method is similar to that of [Grundmann et al. 2012]. Our system does rolling shutter correction while simultaneously stabilizing the video. In a shaky video, a rolling shutter causes spatially variant high frequency jitters. When smoothing the camera paths, we simultaneously rectify rolling shutter effects and other jitters caused by camera shake.\n        \n      \n      \n        5 Results\n        We run our method on an Intel i7 3.2GHZ Quad-Core machine with 8G RAM. We extract 400-600 SURF features [Bay et al. 2008] per frame. For motion estimation, we always divide the video frame to 16 ? 16 cells. For a video of 1280 ? 720 resolution, our unoptimized system takes 392 milliseconds to process a frame (around 2.5fps). Specifically, we spend 300ms, 50ms, 12ms and 30ms to extract features, estimate motion, optimize camera paths and render the final result. All original and result videos are provided on our webpage 1 .\n        \n          5.1 Algorithm Validation\n          We first verify the effectiveness of different components of the proposed approach. A Global Path vs. Bundled Paths For the example in Figure 1 , the result according to a global path has remaining jitters in some image regions. This is because the parallax makes the global homography motion model invalid, therefore some image regions cannot be stabilized very well. But our bundled paths can handle this kind of typical situation. Please refer to our accompanying video for a visual comparison. Spatially-variant Homographies vs. Homography Mixture Grundmann et al. [2012] proposed a homography mixture model for rolling shutter correction. They divide a video frame into a 1D array of horizontal blocks, and use a Gaussian mixture of homographies for each block. This model is beyond a single 2D transformation and able to partially handle parallax.\n          1 http://www.ece.nus.edu.sg/stfpage/eletp/Projects/Stabilization/Stabili- zationSig13.html\n          \n            \n          \n          (a) original video frame (b) YouTube result\n          \n            \n          \n          (c) our result (d) homography mixtures (our implementation)\n          \n            Figure 8: Comparison with the homography mixture models in [Grundmann et al. 2012]. (a) A sample frame in the original video. (b) The output frame produced by YouTube Stabilizer. (c) The result produced by our method. (d) The result produced using our implementation of homography mixture [Grundmann et al. 2012] (with the same bundled path smoothing).\n          \n          Compared with our 2D mesh-based, spatially-variant homographies, this model has two limitations: 1) it does not address horizontal depth variation; 2) it uses weaker feature points (which apply lower threshold level for feature detection) and a simple Gaussian mixture for the regularization. Weaker feature points may result in larger fitting errors and the ability to use simple Gaussian smoothing is limited. Figure 8 shows a comparison of these two models. In this example, the scene has horizontal depth variation and the sky region lacks feature points. Figure 8 (a) is the result of using YouTube Stabilizer (integrated Homography Mixture feature). We can observe severe geometrical distortions. To further verify our observation, we replace our spatially-variant model with the homography mixture model (our implementation) in our framework and generate the result in Figure 8 (d), where we observe similar distortion. In comparison, our warping-based motion estimation can fundamentally handle depth variation (not limited to vertical direction). Our result ( Figure 8 (c)) does not suffer from such distortion. Please also see the comparison in the accompanying video. Rolling Shutter Handling Figure 9 compares our methods with [Grundmann et al. 2012] on two example videos from their paper. Our model accounts for frame distortions such as skew (left example) and local wobble (right example). More examples are included in the supplementary video, which shows we achieve similar results on correcting rolling shutter distortion as [Grundmann et al. 2012].\n        \n        \n          5.2 Quantitative Evaluation\n          To quantitatively evaluate and measure the result from different aspects, we define three objective metrics. Cropping and distortion Our first two metrics measure cropping ratio and global distortion. We first fit a global homography at each frame between input video and output video. We then compute the cropping ratio and distortion for each frame. The cropping ratio can be directly computed from the scale component of the homography. There is one global cropping ratio for the whole sequence, and each frame provides an estimation. We average these estimations at all frames as the final metric. The distortion is computed as defined in Section 4.1. Because any distortion in a single frame will destroy the perfection of the whole result, we choose their minimum across the whole sequence as the final metric. This ?worst-case? metric  allows us to easily see whether the whole result video is completely successful. For a good result, both metrics should be close to 1. Stability The third metric measures the stability of the result. Designing a good metric is non-trivial because it is hard to compare two different videos. We suggest an empirically good metric using frequency analysis on estimated 2D motion from a video. Our basic assumption is that the more energy is contained in the low frequency part of the motion, the more stable a video is. Computationally, we estimate our bundled camera paths to approximate the true motion (optical flow) in a video. We do not smooth out anything after the estimation. Then, we extract translation and rotation components from each path. Each component is a 1D temporal signal. Finally, we evaluate the energy percentage of the low frequency components (expect for DC component) in these 1D signals to measure the stability. Specifically, we take a few of the lowest (empirically set as from the 2nd to the 6th) frequencies and calculate the energy percentage over full frequencies (excluded by the DC component). Similar to the distortion, we take the smallest measurement among the translation and rotation as the final metric. For a good result, the metric should approach 1 here as well.\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          Bundled Camera Paths for Video Stabilization\n          ?\n          78:7\n          \n            \n          \n          input frames [Grundmann et al. 2012] our results\n          \n            Figure 9: Two rolling shutter removal examples using our method and [Grundmann et al. 2012]. Our results are on par with that from [Grundmann et al. 2012]. Please see video for a full comparison.\n          \n        \n        \n          5.4 Comparison with the State-of-the-Art Systems 5.3 Comparison with Publicly Available Results\n          The purpose of this comparison is to test whether our results are comparable with (if not better than) previous ?successful? results in [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012; Grundmann et al. 2011]. We collect eleven test videos from these papers (thumbnails in Figure 10 ), and compare our results with their published results (all from authors? project webpages). Overall, all methods generate similar stability both subjectively and quantitatively ( Figure 10 ) on these examples, while our results are slightly better on some videos in terms of cropping ratio and distortion. For video (2)-(4), 3D stabilization [Liu et al. 2009] achieves the best stability and distortion scores. It suggests that 3D methods are the first choice (in term of stability and distortion error), when the 3D motion can be successfully estimated. Although our results are slightly worse in stability, the visual difference is quite small (please verify from the supplementary video). Furthermore, the aggressive smoothing in 3D methods sometimes leads to an output FOV that is too small as demonstrated by the cropping score. Our method manages to provide a good trade-off. For video (5-9), [Liu et al. 2011], [Goldstein and Fattal 2012], and our method achieve similar stability, while our method is slightly better in cropping and distortion. For video (10-11) 2 , our method outperforms the L1optimization [Grundmann et al. 2011] in stability (slightly), cropping ratio, and distortion scores. Figure 11 highlights the most challenging video (10) in this dataset. Liu et al. [2011] refer this example as a failure case because a single subspace cannot account for the feature trajectories on both the face and the background. Their results have visible distortion. [Grundmann et al. 2011] produced better result on this example. But in the video result, we still observe large temporal distortion on the background region. (See our accompanying video.) In comparison, our method can successfully handle this example (achieve best in terms of all three metrics) because the warping-based motion model can represent this complicated motion. Due to no publicly available implementation of previous works, we compare our system with two well-known commercial systems ? YouTube Stabilizer and ?Warp Stabilizer? in Adobe After Effects CS6. The YouTube Stabilizer is based on the combination of the L 1 -norm path optimization [Grundmann et al. 2011] and homography mixtures [Grundmann et al. 2012]. The ?Warp Stabilizer? in Adobe After Effects is largely based on subspace stabilization [Liu et al. 2011]. We understand that commercial products are often different from a given research system. But we believe these two systems represent the essential elements of research conducted in this field, and the comparison makes sense for examining strengths or weaknesses and robustness (for various videos using a set of fixed parameters) of our system. Dataset We assemble a comprehensive dataset of 174 short videos (10 ? 60 seconds) from previous publications, Internet, and our own captures. To know the strength and weakness of a method in different situations, we roughly divide our data into 7 categories based on camera motion and scene type. They are: (I) simple, (II) quick rotation, (III) zooming, (IV) large parallax, (V) driving, (VI) crowd, and (VII) running. 2 To better measure stability on background motion (caused by camera shake), we use a manual foreground mask to exclude foreground motion. YouTube Stabilizer is a parameter-free online tool. But ?Warp Stabilizer? is an interactive system, and the user might carefully tune a few parameters. Here, we wish to examine its robustness as an automatic tool by fixing its parameters. We use the example videos in [Liu et al. 2011] to decide the best parameters. Finally, we choose the default parameters (smoothness: 50%, ?Smooth Motion? and ?Subspace Warp?) to produce results. Quantitative Comparison For each category, we compute the average metrics and standard deviation of three systems ( Figure 12 (a)). We discuss the results with regard to each system in detail below. All three systems perform well in category (I) ?simple?, since this category contains videos with relatively smooth camera motion and mild depth variations. Though our method has a minor advantage, the users can safely choose any of three to get a desired result. Among the remaining categories, we want to highlight the category (IV) ?large parallax?. The three systems achieve similar stability, while our system is clearly better in terms of distortion. We show two examples in Figure 12 (b) and (c) for visual comparison of our system and the YouTube Stabilizer. These examples show the limitation of a 1D array of homography mixtures ? it cannot model depth changes in horizontal direction. Warp Stabilizer also generates some shearing/skewing artifacts in some video frames, though in principle this 3D method should be able to handle parallax. Figure 12 (d) shows such an example (please note the shearing of the bookshelf). This is probably due to the subspace analysis failure caused by occlusion. Our method succeeds in all of these examples. Comparison in this category clearly demonstrates the advantages of our warping-based motion model in dealing with a large parallax. Categories (II?III) contain quick rotation or zooming, which are challenging cases for methods requiring long feature tracking. ?Warp Stabilizer? often generates significant cropping. Figure 12(e) is such an example. To alleviate this problem, we try to interactively tune its smoothing parameters. When applying a weaker smoothing, however, we find its result becomes shaky. In comparison, our method generates stable results with much less cropping. For categories (V?VII), the three systems generate similar stability levels (?Warp Stabilizer? is slightly better in category VII), while our sys- tem is consistently better with respect to either cropping ratio or distortion control. We notice that our method generates relatively smaller standard deviations of the three metrics for all categories. It suggests that our method generates more consistent results from various inputs. User Study We further conduct a user study with 40 participants to evaluate and compare our method with the YouTube Stabilizer and the ?Warp Stabilizer? in Adobe AfterEffects CS6. Every participant is required to evaluate results on 28 different input videos (randomly sampled from our dataset), in which there are 4 videos for each category mentioned above (The 4 video are prepared in the way that two of them compare our result to YouTube Stabilizer, and the other two to ?Warp Stabilizer?). In the user study, we use the scheme of forced two-alternative choice. Every participant is asked to pick a better one between the results of our method and YouTube Stabilizer, or between the results of our method and the ?Warp Stabilizer?. These videos are displayed to the subjects in a random order. The subjects are unaware of the video categories. Neither do they know which technique is used to produce the stabilized results. Figure 13 (a) shows such an interface for the user study. The original video is displayed on the top. The two stabilized ones are shown side-by-side below. Users can simultaneously play input video and both two results to better examine the difference. And these videos can be played back and forth, or be paused at a certain frame to help users carefully make their decision. The user can also play each of these videos individually to examine their quality without other distractions. We ask users to disregard differences in aspect ratio, or sharpness since each one may undergo different video codecs or further post-processing which makes uniform treatment difficult. The user study results are shown in 13 (b). For each category, we show the average percentage of user preference. In general, the majority of all users showed significant preference towards our results when compared to any of the other two systems respectively. In particular, the participants prefer the overall quality of our results for category (IV) ?large parallax? over YouTube Stabilizer (72% vs. 28%) and ?Warp Stabilizer? (69% vs. 31%). The result is consistent with our metric evaluation. For category (II?III) containing quick rotation or zooming, users show a strong bias in preference toward our results over ?Warp Stabilizer? (93% vs. 7% for rotation,\n          input frames [Grundmann et al. 2012] our results\n          \n            \n          \n          input [Liu et al. 2011] our result\n          \n            Figure 11: Comparison with a failure case of prior methods.\n          \n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          78:8\n          ?\n          S. Liu et al.\n          1 2 3 4 5\n          1 0.9 cropping 0.8 0.7 0.6 0.5 1 0.9 distortion 0.7 0.8 0.6 0.5 1 0.9 stability 0.7 0.8 0.6 0.5\n          \n            Figure 10: Quantitative comparison with existing stabilization techniques on publicly available data.\n            \n          \n          6 7 8 9 10 11\n          ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n          Bundled Camera Paths for Video Stabilization\n          ?\n          78:9\n          \n            IV\n            (I) simple (II) quick rotation (III) zooming large parallax (V) driving\n          \n          1 1 1 1 1 0.9 0.9 0.9 0.9 0.9 0.8 0.8 0.8 0.8 0.8 (a) 0.7 0.6 0.7 0.6 0.6 0.7 0.6 0.7 0.7 0.6 0.5 0.5 0.5 0.5 0.5 0.4 0.4 0.4 0.4 0.4 0.3 0.3 0.3 0.3 0.3 C D S C D S C D S C D S C D S Google YouTube Adobe After Effect CS6 (?Warp Stabilizer?) Our system C: cropping ratio\n          (b) (c)\n          \n            \n          \n          input video YouTube result our result\n          \n            Figure 12: Comparisons with two popular systems: YouTube Stabilizer and Adobe After Effect ?Warp Stabilizer?. Top: quantitative comparisons by three metrics: cropping (C), distortion (D) and stability (S). Bottom: some sample video frames for visual comparisons.\n          \n          (I) simple (II) quick rotation\n          \n            a\n            100% 90% 80% 70% 60% (b) 50% 40% 30% 20% 10% 0%\n          \n          \n            \n            \n            \n            Figure 13: (a) Pair-wise comparison interface for user study. (b) User study results by comparing our method with two popular systems: YouTube Stabilizer and Adobe After Effect ?Warp Stabilizer?.\n          \n          83% vs. 17% for zooming). This is possibly due to the significant cropping in the results of ?Warp Stabilizer?. For categories (V?VII), more participants prefer our results to the other two systems, although the three systems generate similar stability levels according to our stability metric. It is likely because of the superior distortion and cropping control in our method. In category (I) ?simple?, users express similar preference toward three results.  After the user study, we also ask all participants to articulate the criteria for their feedbacks. We conclude the main criteria for unacceptable videos: 1) the video gets a smaller field of view or even contains frames with visible empty (black) area; 2) the video presents structure distortions in individual frames; 3) the motions in some video frames vibrate or oscillate; 4) the scene transition looks abrupt or not smoothed in the video. From these criteria, our proposed metrics can be partially related with human preferences. And both quantitative evaluation and user study results consistently indicate our system performs better than the other two systems.\n        \n        \n          5.5 Limitations and Discussion\n          We find that when 3D reconstruction is successful, 3D methods often generate the best results. However, our system is more robust as we do not require feature tracking, and it produces comparable or only slightly worse results. It is interesting to note that our adaptive path optimization can also be applied to path smoothing for 3D methods [Liu et al. 2009; Liu et al. 2011; Goldstein and Fattal 2012], which often use low-pass filtering (Gaussian smoothing), or curve fitting for path planning. In comparison, our adaptive camera  path smoothing technique can automatically adjust the smoothness strength by considering discontinuity and distortion. We show such an example video on our project webpage. There are cases where the warping-based motion model fails to handle severe occlusions or dis-occlusions, especially when combined with rolling shutter effects. Figure 14 shows two such examples. Our warping-based motion model chooses a large ? to enforce strong coherence between grid cells. In this way, we can minimize the geometrical distortion, but at the same time, we sacrifice motion accuracy and eventually the stability of the result. In general, we find geometrical distortion is more disruptive than some slight remaining jitters. Our path optimization does not strictly follow cinematography rules, which may be desirable in certain applications. But our discontinuity-preservation optimization produces visually pleasing results in most examples. If necessary, we could apply the strategy in [Gleicher and Liu 2007] as a post-process to solve this problem. We also do not deal with motion blur. Sometimes, the stabilized results contain visible blur artifacts. This problem can be addressed by the recent work [Cho et al. 2012].\n          (VI) crowd (VII) running 1 1 0.9 0.9 0.8 0.8 0.7 0.7 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 C D S C D S D: distortion S: stability\n          (d) (e)\n          \n            \n          \n          (III) zooming (IV) large parallax (V) driving (VI) crowd (VII) running\n          \n            \n            \n            \n          \n          Effect CS6 (?Warp\n          \n            \n          \n          Ours\n        \n      \n      \n        6 Conclusion\n        We have presented a new 2D video stabilization method with a bundled camera paths model. The proposed method can simultaneously generate comparable results to 3D methods while keeping merits of 2D methods. Using image warping techniques for motion representation is an interesting finding in this paper. In the future, we would\n        ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n        78:10\n        ?\n        S. Liu et al.\n        \n          a\n          (b)\n        \n        \n          \n          \n          Figure 14: Two failure cases. The left is due to severe occlusion together with rolling shutter effects. The right is caused by the crowd.\n        \n        extend this kind of representation to other video-based applications.\n      \n      \n        Acknowledgements\n        We thank all the reviewers for their helpful discussions, Kaimo Lin and other subjects for their help in the user study, Nathan Holdstein and Jiangyu Liu for their help in proofreading. This work is also partially supported by the Singapore project R-263-000-620-112.\n      \n      \n        References\n        \n          B AKER , S., B ENNETT , E. P., K ANG , S. B., AND S ZELISKI , R. 2010. Removing rolling shutter wobble. In Proc. CVPR.\n          B AY , H., E SS , A., T UYTELAARS , T., AND V AN G OOL , L. 2008. Speeded-up robust features (surf). Comput. Vis. Image Underst. 110, 3, 346?359.\n          B RONSHTEIN , I. N., AND S EMENDYAYEV , K. A. 1997. Handbook of Mathematics. Springer-Verlag, New York, NY, USA.\n          B ROX , T., B RUHN , A., P APENBERG , N., AND W EICKERT , J. 2004. High accuracy optical flow estimation based on a theory for warping. In Proc. ECCV.\n          B UEHLER , C., B OSSE , M., AND M C M ILLAN , L. 2001. Nonmetric image-based rendering for video stabilization. In Proc.\n        \n      \n      \n        CVPR.\n        C HEN , B.-Y., L EE , K.-Y., H UANG , W.-T., AND L IN , J.-S. 2008. Capturing intention-based full-frame video stabilization. Computer Graphics Forum 27, 7, 1805?1814. C HO , S., W ANG , J., AND L EE , S. 2012. Video deblurring for hand-held cameras using patch-based synthesis. ACM Trans. Graph. (Proc. of SIGGRAPH) 31, 4. F ISCHLER , M. A., AND B OLLES , R. C. 1981. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Commun. ACM 24, 6, 381?395. F ORSS ? N , P.-E., AND R INGABY , E. 2010. Rectifying rolling shutter video from hand-held devices. In CVPR. G AO , J., K IM , S. J., AND B ROWN , M. S. 2011. Constructing image panoramas using dual-homography warping. In Proc. CVPR. G LEICHER , M. L., AND L IU , F. 2007. Re-cinematography: Improving the camera dynamics of casual video. In Proc. of ACM\n      \n      \n        Multimedia.\n        G OLDSTEIN , A., AND F ATTAL , R. 2012. Video stabilization using epipolar geometry. ACM Trans. Graph. (TOG) 31, 5, 126:1? 126:10. G RUNDMANN , M., K WATRA , V., AND E SSA , I. 2011. Autodirected video stabilization with robust l1 optimal camera paths. In Proc. CVPR.  G RUNDMANN , M., K WATRA , V., C ASTRO , D., AND E SSA , I. 2012. Calibration-free rolling shutter removal. In Proc. ICCP. H ARTLEY , R., AND Z ISSERMAN , A. 2003. Multiple View Geometry in Computer Vision, 2 ed. Cambridge University Press, New York, NY, USA. I GARASHI , T., M OSCOVICH , T., AND H UGHES , J. F. 2005. Asrigid-as-possible shape manipulation. ACM Trans. Graph. (Proc. of SIGGRAPH) 24, 3, 1134?1141. K ARPENKO , A., J ACOBS , D., B AEK , J., AND L EVOY , M. 2011. Digital video stabilization and rolling shutter correction using gyroscopes. In Stanford CS Tech Report. L EE , K.-Y., C HUANG , Y.-Y., C HEN , B.-Y., AND O UHYOUNG , M. 2009. Video stabilization using robust feature trajectories. In Proc. ICCV. L IANG , C.-K., C HANG , L.-W., AND C HEN , H. H. 2008. Analysis and compensation of rolling shutter effect. In IEEE Trans. on\n      \n      \n        Image Processing.\n        L IN , W.-Y., L IU , S., M ATSUSHITA , Y., N G , T.-T., AND C HEONG , L.-F. 2011. Smoothly varying affine stitching. In\n        L IU , F., G LEICHER , M., J IN , H., AND A GARWALA , A. 2009. Content-preserving warps for 3d video stabilization.\n      \n      \n        Proc. CVPR. ACM Trans. Graph. (Proc. of SIGGRAPH) 28.\n        L IU , F., G LEICHER , M., W ANG , J., J IN , H., AND A GARWALA , A. 2011. Subspace video stabilization. ACM Trans. Graph. 30. L IU , S., W ANG , Y., Y UAN , L., B U , J., T AN , P., AND S UN , J. 2012. Video stabilization with a depth camera. In Proc. CVPR. L UCAS , B. D., AND K ANADE , T. 1981. An iterative image registration technique with an application to stereo vision. In Proc.\n      \n      \n        of the International Joint Conference on Artificial Intelligence\n        (IJCAI), 674?679. M ATSUSHITA , Y., O FEK , E., G E , W., T ANG , X., AND S HUM , H.Y. 2006. Full-frame video stabilization with motion inpainting. IEEE Trans. Pattern Anal. Mach. Intell. 28, 1150?1163. M ORIMOTO , C., AND C HELLAPPA , R. 1998. Evaluation of image stabilization algorithms. In Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing, 2789 ? 2792. N AKAMURA , J. 2005. Image Sensors and Signal Processing for Digital Still Cameras. CRC Press, Inc. N IR , T., B RUCKSTEIN , A. M., AND K IMMEL , R. 2008. Overparameterized variational optical flow. Int. J. Comput. Vision (IJCV) 76, 2, 205?216. S CHAEFER , S., M C P HAIL , T., AND W ARREN , J. 2006. Image deformation using moving least squares. ACM Trans. Graph. (Proc. of SIGGRAPH) 25, 3, 533?540. S HUM , H.-Y., AND S ZELISKI , R. 2000. Construction of panoramic image mosaics with global and local alignment. Int. J. Comput. Vision (IJCV) 36, 2, 101?130. S MITH , B. M., Z HANG , L., J IN , H., AND A GARWALA , A. 2009. Light field video stabilization. In Proc. ICCV. S ZELISKI , R. 1996. Motion estimation with quadtree splines. IEEE Trans. Pattern Anal. Mach. Intell. 18, 12, 1199?1210. T OMASI , C., AND M ANDUCHI , R. 1998. Bilateral filtering for gray and color images. In Proc. ICCV, 839?846.\n        ACM Transactions on Graphics, Vol. 32, No. 4, Article 78, Publication Date: July 2013\n      \n    \n  ",
  "resources" : [ ]
}