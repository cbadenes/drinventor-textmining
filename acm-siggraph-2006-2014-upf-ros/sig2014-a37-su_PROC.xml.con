{
  "uri" : "sig2014-a37-su_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a37-su_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Estimating Image Depth Using Shape Collections",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Hao-Su",
      "name" : "Hao",
      "surname" : "Su"
    }, {
      "uri" : "http://drinventor/Qixing-Huang",
      "name" : "Qixing",
      "surname" : "Huang"
    }, {
      "uri" : "http://drinventor/Niloy J.-Mitra",
      "name" : "Niloy J.",
      "surname" : "Mitra"
    }, {
      "uri" : "http://drinventor/Yangyan-Li",
      "name" : "Yangyan",
      "surname" : "Li"
    }, {
      "uri" : "http://drinventor/Leonidas J.-Guibas",
      "name" : "Leonidas J.",
      "surname" : "Guibas"
    } ]
  },
  "bagOfWords" : [ "Table", "Figure", "show", "representative", "result", "propose", "approach", "overall", "result", "reasonably", "good", "despite", "obvious", "difficulty", "problem", "68.2", "correspondence", "fall", "below", "0.02", "time", "average", "shape", "diameter", "all", "dataset", "Hausdorff", "distance", "error", "considerably", "lower", "than", "deviation", "error", "shape", "point", "cloud", "drive", "shape", "collection", "show", "use", "shape", "collection", "good", "prior", "distribution", "point", "restricted", "drift", "along", "common", "shape", "space", "deviation", "error", "large", "because", "correspondence", "may", "glide", "along", "shape", "which", "exactly", "same", "we", "next", "discuss", "result", "each", "category", "chair", "table", "we", "evaluate", "chair", "table", "category", "because", "fine", "geometric", "detail", "present", "shape", "like", "other", "man-made", "object", "chair", "table", "usually", "have", "strong", "symmetry", "imply", "lower-dimensional", "deformation", "space", "other", "hand", "four", "leg", "may", "introduce", "match", "ambiguity", "category", "we", "find", "we", "algorithm", "limit", "when", "selfocclusion", "present", "lower", "board", "occlude", "front", "leg", "row", "figure", "consequently", "part", "attach", "leg", "cup", "relatively", "small", "household", "item", "usually", "have", "circular", "symmetrical", "body", "interestingly", "we", "method", "produce", "visually", "more", "please", "result", "compare", "Kinect", "because", "object", "size", "reach", "resolution", "limit", "sensor", "surface", "specular", "which", "challenge", "structural", "light", "mechanism", "Kinect", "we", "choose", "category", "because", "have", "large", "variation", "possible", "shape", "particularly", "curvature", "pole", "can", "see", "we", "algorithm", "succeed", "both", "lamp", "example", "figure", "success", "can", "attribute", "two", "reason", "first", "we", "use", "data-driven", "approach", "implicitly", "combine", "part", "from", "different", "shape", "second", "we", "use", "non-rigid", "deformation", "field", "which", "allow", "bend", "pole", "we", "choose", "category", "common", "outdoor", "object", "have", "fine", "geometric", "detail", "-lrb-", "e.g.", "wheel", "side", "mirror", "-rrb-", "we", "algorithm", "could", "accurately", "estimate", "depth", "car", "other", "hand", "Kinect", "have", "problem", "detect", "window", "wheel", "because", "too", "reflective", "too", "dark", "respectively", "comparison", "Automatic", "Pop-Up", "Automatic", "Pop-Up", "-lsb-", "Hoiem", "et", "al.", "2005", "-rsb-", "automatically", "reconstruct", "3d", "information", "use", "single", "image", "initially", "design", "outdoor", "scene", "use", "plane", "classifier", "software", "assume", "simple", "geometric", "prior", "tend", "work", "poorly", "complicated", "indoor", "object", "thin", "fine", "feature", "we", "show", "effect", "Automatic", "Pop-Up", "chair", "model", "Figure", "use", "pre-trained", "classifier", "we", "algorithm", "visu", "ally", "significantly", "better", "than", "output", "from", "software", "-lrb-", "compare", "last", "column", "figure", "-rrb-", "paper", "we", "have", "present", "data-driven", "algorithm", "add", "depth", "information", "image", "object", "algorithm", "take", "input", "image", "segmented", "object", "collection", "3d", "shape", "same", "object", "class", "compute", "various", "geometric", "prior", "from", "shape", "collection", "optimize", "depth", "estimation", "image", "object", "procedure", "fully", "automatic", "we", "have", "evaluate", "performance", "present", "approach", "benchmark", "consist", "Kinect", "scan", "variety", "common", "object", "take", "under", "different", "lighting", "condition", "experimental", "result", "show", "we", "approach", "produce", "depth", "close", "ground-truth", "superior", "state-of-the-art", "depth", "estimator", "we", "have", "also", "show", "usefulness", "we", "approach", "various", "application", "besides", "application", "demonstrate", "paper", "present", "depth", "estimator", "enable", "variety", "other", "application", "both", "computer", "graphic", "computer", "vision", "example", "shape", "collection", "can", "serve", "hub", "link", "many", "image", "object", "particularly", "useful", "retrieve", "similar", "image", "object", "be", "take", "from", "drastically", "different", "view", "point", "can", "match", "well", "pure", "image", "method", "another", "example", "help", "image-shape", "network", "we", "can", "propagate", "rich", "image", "label", "purpose", "categorize", "shape", "challenging", "problem", "shape", "analysis", "due", "lack", "label", "shape", "datum", "combine", "3d", "shape", "label", "Limitations", "course", "state", "we", "approach", "require", "segmented", "image", "object", "knowledge", "object", "class", "well", "study", "problem", "computer", "vision", "future", "work", "can", "combine", "we", "approach", "present", "method", "work", "best", "man-made", "object", "whose", "3d", "model", "can", "well", "align", "where", "variation", "shape", "pose", "modest", "do", "apply", "well", "object", "high", "variability", "tree", "building", "high", "articulation", "animal", "object", "important", "utilize", "more", "specialized", "domain", "knowledge", "-lrb-", "i.e.", "skeleton", "regular", "structure", "-rrb-", "establish", "correspondence", "estimate", "depth", "finally", "we", "experience", "minimum", "couple", "hundred", "shape", "necessary", "algorithm", "succeed", "intuition", "each", "part", "object", "image", "need", "have", "multiple", "correspondence", "good", "regularization", "future", "work", "ample", "opportunity", "future", "research", "while", "so", "far", "we", "have", "focus", "estimate", "depth", "single", "segmented", "object", "would", "very", "interesting", "generalize", "approach", "estimate", "depth", "entire", "scene", "would", "require", "we", "automate", "object", "detection", "process", "take", "account", "spatial", "relation", "among", "object" ],
  "content" : "Table 1 and Figure 7 shows representative results for the proposed approach. Overall the results are reasonably good despite the obvious difficulty of the problem, with 68.2% correspondences falling below 0.02 times the averaged shape diameter. For all datasets, the Hausdorff distance error is considerably lower than that of the deviation error. As the shape of the point cloud is driven by the shape collection, this shows that using the shape collection as a good prior, the distribution of points is restricted to drift along the common shape space. The deviation error is large because the correspondences may glide along the shapes, which are not exactly the same. We next discuss the results for each category. Chair and tables. We evaluate on the chair and table categories because fine geometric details are present in these shapes. Like other man-made objects, chair and tables usually have strong symmetries, implying a lower-dimensional deformation space. On the other hand, the four legs may introduce matching ambiguities. On these categories, we find that our algorithm is limited when selfocclusion presents: the lower board is occluded by the front leg in Row 6 of Figure 7 and consequently part of it is attached to the leg. Cups are relatively small household items and usually have a circular symmetrical body. Interestingly, our method produces visually more pleasing results compared with the Kinect, because the object size is reaching the resolution limit of the sensor and the surface is specular, which is challenging for the structural light mechanism of the Kinect. We choose this category because it has large variations in the possible shapes, particularly in the curvature of the pole. It can be seen that our algorithm succeeds in both lamp examples in Figure 7 . The success can be attributed to two reasons. First, we use a data-driven approach to implicitly combine parts from different shapes. Second, we use a non-rigid deformation field, which allows the bending of the pole. We choose this category as a common outdoor object having fine geometric details (e.g., wheels, side mirrors). Our algorithm could accurately estimates the depth of cars. On the other hand, the Kinect has problems in detecting windows and wheels, because they are too reflective or too dark respectively. Comparison to Automatic Pop-Up. Automatic Pop-Up [Hoiem et al. 2005] automatically reconstructs 3D information using a single image and was initially designed for outdoor scenes using plane classifiers. The software assumes simple geometric priors and tend to work poorly for complicated indoor objects with thin and fine features. We show the effect of Automatic Pop-Up on a chair model in Figure 9 using the pre-trained classifiers. Our algorithm is visu- ally significantly better than the output from this software (compare the last column of Figures 2 and 9). In this paper, we have presented a data-driven algorithm for adding depth information to an image object. The algorithm takes as input an image of a segmented object and a collection of 3D shapes of the same object class, and computes various geometric priors from the shape collection to optimize the depth estimation of the image object. This procedure is fully automatic. We have evaluated the performance of the presented approach on a benchmark that consists of Kinect scans of a variety of common objects taken under different lighting conditions. Experimental results show that our approach produces depth that is close to the ground-truth, and is superior to state-of-the-art depth estimators. We have also shown the usefulness of the our approach for various applications. Besides the applications demonstrated in this paper, the presented depth estimator enables a variety of other applications in both computer graphics and computer vision. As an example, the shape collection can serve as the hub that links many image objects. This is particularly useful for retrieving similar image objects that were taken from drastically different view points that cannot be matched well by pure image methods. As another example, with the help of the image-shape network, we can propagate rich image labels for the purpose of categorizing shapes ? a challenging problem in shape analysis due to the lack of labeled shapes or of data combining 3D shapes and labels. Limitations. Of course, as stated, our approach requires a segmented image of an object and a knowledge of the object class. These are well studied problems in computer vision and future work can combine these with our approach. The presented method works best with man-made objects whose 3D models can be well aligned and where the variation in shape poses is modest. It does not apply well to objects of high variability, such as trees, or buildings, or of high articulation, such as animals. For these objects, it is important to utilize more specialized domain knowledge (i.e., skeletons and regular structures) to establish correspondences and estimate depth. Finally, in our experience, a minimum of a couple of hundreds of shapes is necessary for the algorithm to succeed. The intuition is that each part of the object in the image needs to have multiple correspondences for good regularization. Future work. There are ample opportunities for future research. While so far we have focused on estimating the depth of a single segmented object, it would be very interesting to generalize this approach to estimate the depth of an entire scene. This would require us to automate the object detection process and to take into account spatial relations among objects.",
  "resources" : [ ]
}