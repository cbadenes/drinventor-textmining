{
  "uri" : "sig2012a-a157-rivers_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2012a/a157-rivers_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Sculpting by Numbers",
    "published" : "2012",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Alec R.-Rivers",
      "name" : "Alec R.",
      "surname" : "Rivers"
    }, {
      "uri" : "http://drinventor/Andrew-Adams",
      "name" : "Andrew",
      "surname" : "Adams"
    }, {
      "uri" : "http://drinventor/Fr?do-Durand",
      "name" : "Fr?do",
      "surname" : "Durand"
    } ]
  },
  "bagOfWords" : [ "we", "use", "projector/camera", "pair", "scan", "work", "progress", "project", "multiple", "form", "guidance", "onto", "object", "itself", "indicate", "which", "area", "need", "more", "material", "which", "need", "less", "where", "any", "ridge", "valley", "depth", "discontinuity", "we", "extend", "technique", "support", "replicate", "sequence", "model", "create", "stop-motion", "video", "we", "employ", "spatially-augmented", "reality", "approach", "-lrb-", "see", "e.g.", "raskar", "et", "al.", "-lsb-", "1998", "-rsb-", "Bimber", "Raskar", "-lsb-", "2005", "-rsb-", "overview", "spatially-augmented", "reality", "-rrb-", "which", "visual", "feedback", "illustrate", "discrepancy", "between", "work", "progress", "target", "3d", "shape", "approach", "first", "propose", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "approach", "projector-camera", "pair", "use", "scan", "object", "be", "create", "use", "structured", "light", "we", "propose", "method", "provide", "guidance", "illustrate", "depth", "disparity", "between", "current", "work", "target", "similar", "approach", "Skeels", "Rehg", "-lsb-", "2007", "-rsb-", "well", "additional", "form", "guidance", "which", "we", "call", "edge", "guidance", "which", "aid", "reproduce", "high-frequency", "surface", "detail", "3d", "printing", "cnc", "method", "highly", "accurate", "suffer", "from", "fabrication", "artifact", "limit", "size", "material", "object", "can", "produce", "recent", "work", "computer", "graphic", "have", "therefore", "focus", "software", "technique", "enable", "alternative", "method", "fabrication", "we", "technique", "fall", "category", "two", "recent", "work", "propose", "method", "generate", "model", "slide", "planar", "slice", "approximate", "target", "3d", "model", "-lsb-", "Hildebrand", "et", "al.", "2012", "McCrae", "et", "al.", "2011", "-rsb-", "Lau", "et", "al.", "-lsb-", "2011", "-rsb-", "propose", "method", "convert", "3d", "furniture", "model", "fabricatable", "planar", "part", "hybrid", "human-computer", "interface", "have", "be", "use", "before", "especially", "medical", "application", "example", "combine", "human", "surgeon?s", "ability", "plan", "motion", "react", "change", "condition", "robotic", "arm?s", "precision", "-lsb-", "kragic", "et", "al.", "2005", "Mako", "Surgical", "-rsb-", "hybrid", "manual-automatic", "method", "have", "also", "be", "use", "2d", "design", "painter", "have", "use", "photograph", "camera", "obscura", "reference", "-lsb-", "Coke", "1964", "Pennsylvania", "Academy", "fine", "art", "et", "al.", "1994", "-rsb-", "Flagg", "Rehg", "-lsb-", "Flagg", "Rehg", "2006", "-rsb-", "present", "ProjectorGuided", "Painting", "system", "digitally", "guide", "artist", "engage", "reproduce", "2d", "painting", "Dixon", "et", "al.", "-lsb-", "2010", "-rsb-", "propose", "hybrid", "method", "task", "sketch", "human", "face", "from", "reference", "photograph", "which", "user", "guide", "automatic", "feedback", "3d", "scanning", "have", "be", "active", "area", "research", "many", "year", "variety", "optical", "method", "be", "propose", "see", "Chen", "et", "al.", "-lsb-", "2000", "-rsb-", "survey", "approach", "require", "two", "main", "technical", "component", "method", "scanning", "3d", "shape", "high", "spatial", "depth", "resolution", "projector", "register", "same", "coordinate", "system", "project", "guidance", "onto", "material", "while", "faster", "scanning", "method", "exist", "e.g.", "Primesense/Microsoft", "Kinect", "typically", "have", "much", "lower", "spatial", "resolution", "which", "more", "issue", "we", "than", "refresh", "rate", "we", "instead", "project", "Gray", "code", "pattern", "usual", "compute", "correspondence", "may", "incorrect", "least", "significant", "bit", "we", "provide", "two", "form", "guidance", "which", "user", "may", "toggle", "between", "depend", "context", "which", "project", "directly", "onto", "object", "be", "create", "user", "become", "familiar", "system", "quickly", "become", "able", "perceive", "how", "shape", "must", "deform", "judge", "when", "initiate", "rescan", "model", "remove", "store", "point", "cloud", "use", "target", "shape", "modeling", "above" ],
  "content" : "We use a projector/camera pair to scan a work in progress, and project multiple forms of guidance onto the object itself that indicate which areas need more material, which need less, and where any ridges, valleys or depth discontinuities are. We extend the technique to support replicating a sequence of models to create stop-motion video. We employ a spatially-augmented reality approach (see e.g. Raskar et al. [1998] or Bimber and Raskar [2005] for an overview of spatially-augmented reality), in which visual feedback illustrates the discrepancy between a work in progress and a target 3D shape. This approach was first proposed by Skeels and Rehg [2007]. In this approach, a projector-camera pair is used to scan the object being created using structured light. Our proposed method provides guidance that illustrates depth disparities between the current work and the target, similar to the approach of Skeels and Rehg [2007], as well as an additional form of guidance, which we call edge guidance, which aids in reproducing high-frequency surface details. 3D printing and CNC methods are highly accurate, but suffer from fabrication artifacts, and are limited in the size and materials of the object they can produce. Recent work in computer graphics has therefore focused on software techniques that enable alternative methods for fabrication, and our technique falls into this category. Two recent works propose methods to generate models of sliding planar slices that approximate target 3D models [Hildebrand et al. 2012; McCrae et al. 2011]. Lau et al. [2011] propose a method to convert 3D furniture models into fabricatable planar parts. Hybrid human-computer interfaces have been used before, especially in medical applications, for example to combine a human surgeon?s ability to plan motions and react to changing conditions with a robotic arm?s precision [Kragic et al. 2005; Mako Surgical ]. Hybrid manual-automatic methods have also been used for 2D design. Painters have used photographs or a camera obscura as references [Coke 1964; Pennsylvania Academy of the Fine Arts et al. 1994]. Flagg and Rehg [Flagg and Rehg 2006] presented ProjectorGuided Painting, a system for digitally guiding an artist engaged in reproducing a 2D painting. Dixon et al. [2010] proposed a hybrid method for the task of sketching a human face from a reference photograph in which the user is guided by automatic feedback. 3D scanning has been an active area of research for many years, with a variety of optical methods being proposed; see Chen et al. [2000] for a survey. This approach requires two main technical components: a method for scanning a 3D shape at high spatial and depth resolution, and a projector registered to the same coordinate system to project guidance onto the material. While faster scanning methods exist, e.g. the Primesense/Microsoft Kinect, they typically have much lower spatial resolution, which is more of an issue to us than refresh rate. We instead project the Gray code patterns as usual to compute a correspondence that may be incorrect in the least significant bits. We provide two forms of guidance, which the user may toggle between depending on the context, and which are projected directly  onto the object being created. As users become familiar with the system, they quickly become able to perceive how a shape must be deformed and judge when to initiate a rescan. The model is then removed and the stored point clouds are used as the target shape for modeling as above.",
  "resources" : [ ]
}