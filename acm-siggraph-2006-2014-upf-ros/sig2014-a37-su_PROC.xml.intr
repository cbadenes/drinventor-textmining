{
  "uri" : "sig2014-a37-su_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2014/a37-su_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Estimating Image Depth Using Shape Collections",
    "published" : "2014",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Hao-Su",
      "name" : "Hao",
      "surname" : "Su"
    }, {
      "uri" : "http://drinventor/Qixing-Huang",
      "name" : "Qixing",
      "surname" : "Huang"
    }, {
      "uri" : "http://drinventor/Niloy J.-Mitra",
      "name" : "Niloy J.",
      "surname" : "Mitra"
    }, {
      "uri" : "http://drinventor/Yangyan-Li",
      "name" : "Yangyan",
      "surname" : "Li"
    }, {
      "uri" : "http://drinventor/Leonidas J.-Guibas",
      "name" : "Leonidas J.",
      "surname" : "Guibas"
    } ]
  },
  "bagOfWords" : [ "image", "remain", "far", "most", "popular", "visual", "medium", "nowadays", "easy", "acquire", "distribute", "contain", "rich", "visual", "detail", "can", "easily", "view", "understand", "result", "ubiquitous", "web", "2d", "projection", "we", "3d", "world", "however", "may", "lack", "certain", "semantical", "information", "example", "important", "part", "object", "may", "occluded", "depth", "datum", "typically", "miss", "pose", "serious", "challenge", "application", "involve", "image", "recognition", "manipulation", "editing", "etc.", "could", "greatly", "benefit", "from", "omit", "information", "hence", "strong", "motivation", "lift", "image", "3d", "infer", "attribute", "lose", "projection", "paper", "we", "specifically", "interested", "infer", "depth", "visible", "object", "area", "key", "coordinate", "miss", "projection", "problem", "recover", "depth", "from", "single", "image", "naturally", "ill-posed", "various", "prior", "have", "be", "propose", "regularization", "most", "common", "classical", "approach", "match", "input", "image", "set", "3d", "object", "database", "-lrb-", "i.e.", "prior", "-rrb-", "use", "best", "matching", "shape", "fill", "miss", "depth", "information", "however", "large-scale", "deployment", "method", "fundamentally", "limit", "because", "only", "limited", "number", "3d", "model", "available", "most", "often", "we", "do", "even", "have", "3d", "model", "same", "sufficiently", "similar", "object", "from", "which", "image", "take", "paper", "we", "consider", "problem", "estimate", "depth", "image", "object", "exploit", "novel", "joint", "fashion", "collection", "3d", "model", "related", "largely", "different", "object", "-lrb-", "see", "Figure", "-rrb-", "key", "we", "approach", "estimation", "correspondence", "between", "image", "multiple", "model", "help", "correspondence", "estimate", "between", "model", "themselves", "we", "address", "depth", "inference", "problem", "its", "purest", "form", "where", "we", "assume", "object", "image", "have", "be", "segmented", "from", "its", "background", "-lrb-", "image", "now", "commonplace", "shopping", "web", "site", "-rrb-", "while", "we", "3d", "model", "typically", "untextured", "come", "from", "shape", "collection", "Trimble", "3d", "warehouse", "we", "image-based", "shape-driven", "modeling", "technique", "fully", "automatic", "reconstruct", "3d", "point", "cloud", "from", "image", "object", "algorithm", "consist", "preprocessing", "stage", "which", "align", "input", "shape", "each", "other", "learn", "deformation", "model", "each", "shape", "reconstruction", "stage", "which", "use", "continuous", "optimization", "recover", "image", "object", "pose", "reconstruct", "point", "cloud", "from", "image", "align", "relevant", "3d", "model", "extract", "from", "collection", "we", "show", "how", "formulate", "appropriate", "objective", "function", "how", "obtain", "initial", "solution", "how", "effectively", "refine", "solution", "use", "alternate", "optimization", "we", "approach", "we", "jointly", "match", "depth-augmented", "image", "i.e.", "popup", "point", "cloud", "image", "group", "related", "shape", "collection", "we", "pose", "task", "joint", "non-rigid", "registration", "problem", "which", "each", "shape", "can", "deform", "formulation", "have", "two", "key", "feature", "first", "contrast", "utilize", "single", "similar", "shape", "incorporate", "collection", "similar", "shape", "offer", "better", "coverage", "relevant", "neighborhood", "shape", "space", "second", "since", "we", "have", "already", "align", "3d", "model", "each", "other", "enable", "we", "apply", "consistency", "constraint", "-lsb-", "Kim", "et", "al.", "2012a", "Huang", "Guibas", "2013", "-rsb-", "regularize", "image", "3d", "model", "matching", "use", "shape-shape", "correspondence", "joint", "non-rigid", "registration", "formulation", "we", "introduce", "key", "concept", "deformation", "prior", "which", "govern", "deformation", "each", "shape", "-lrb-", "c.f.", "-lsb-", "Averkiou", "et", "al.", "2014", "-rsb-", "-rrb-", "intuitively", "we", "aim", "preserve", "key", "structural", "property", "each", "shape", "deformation", "so", "round", "shape", "stay", "round", "left-to-right", "symmetry", "preserve", "etc.", "particular", "instead", "detect", "property", "form", "each", "shape", "alone", "which", "turn", "out", "unreliable", "we", "learn", "they", "from", "optimal", "deformation", "each", "shape", "other", "shape", "test", "performance", "propose", "approach", "we", "have", "create", "benchmark", "dataset", "consist", "Microsoft", "Kinect", "scan", "various", "category", "object", "include", "chair", "table", "lamp", "cup", "experimental", "result", "show", "propose", "approach", "recover", "depth", "information", "close", "kinect", "scan", "significantly", "more", "accurate", "than", "state-of-the-art", "image-based", "modeling", "technique", "moreover", "propose", "approach", "robust", "variation", "texture", "lighting", "condition", "we", "demonstrate", "depth-enhanced", "image", "editing", "illustrate", "possibility", "offer", "we", "approach", "addition", "we", "show", "we", "work", "key", "intermediate", "step", "towards", "goal", "obtain", "full", "3d", "model", "use", "popup", "point", "cloud", "input", "we", "can", "reconstruct", "certain", "case", "full", "mesh", "exploit", "shape", "symmetry", "learn", "from", "shape", "network", "contribution", "we", "present", "first", "best", "we", "knowledge", "fully", "automatic", "method", "utilize", "network", "related", "different", "3d", "object", "order", "reconstruct", "depth", "information", "from", "single", "image", "object", "key", "novelty", "show", "how", "single", "modestly-sized", "shape", "network", "can", "help", "infer", "depth", "information", "variety", "image", "object", "same", "class", "use", "learn", "deformation", "model", "base", "align", "shape", "network", "compensate", "fact", "image", "from", "model", "directly", "present", "database", "regularize", "model", "deformation", "use", "multi-way", "3d", "alignment", "between", "initial", "image", "point", "cloud", "shape", "neighborhood", "shape", "network", "process", "extract", "depth", "information", "image", "we", "also", "discover", "good", "correspondence", "between", "image", "network", "shape", "enable", "we", "connect", "image", "network", "transfer", "complementary", "information", "back", "forth", "example", "information", "transfer", "can", "include", "texture", "segmentation", "material", "property", "label", "etc.", "data-driven", "geometry", "processing", "emergence", "large", "shape", "collection", "provide", "we", "platform", "aggregate", "information", "from", "multiple", "shape", "improve", "analysis", "processing", "individual", "shape", "already", "Trimble", "3d", "warehouse", "contain", "many", "thousand", "example", "model", "per", "category", "most", "indoor", "object", "some", "popular", "outdoor", "category", "car", "airplane", "recently", "we", "have", "witness", "success", "data-driven", "technique", "shape", "analysis", "-lsb-", "Huang", "et", "al.", "2011", "Kim", "et", "al.", "2012a", "Kim", "et", "al.", "2013", "Huang", "et", "al.", "2013", "Wang", "et", "al.", "2013", "-rsb-", "shape", "model", "ing", "-lsb-", "Chaudhuri", "et", "al.", "2011", "Kalogerakis", "et", "al.", "2012", "Averkiou", "et", "al.", "2014", "-rsb-", "shape", "reconstruction", "-lsb-", "Nan", "et", "al.", "2012", "Kim", "et", "al.", "2012b", "Shen", "et", "al.", "2012", "-rsb-", "key", "task", "data-driven", "geometry", "processing", "technique", "establish", "high-quality", "correspondence", "-lrb-", "either", "pointor", "segment-level", "-rrb-", "across", "geometric", "object", "although", "exist", "rich", "technique", "align", "match", "3d", "shape", "problem", "match", "image", "object", "3d", "shape", "which", "major", "focus", "paper", "far", "from", "be", "solve", "image-shape", "matching", "most", "exist", "image-shape", "match", "approach", "-lsb-", "cyr", "Kimia", "2004", "Xu", "et", "al.", "2011", "Wang", "et", "al.", "2013", "-rsb-", "convert", "problem", "image", "matching", "problem", "i.e.", "match", "image", "project", "view", "3d", "shape", "typically", "from", "estimate", "dense", "correspondence", "between", "silhouette", "curve", "interpolate", "correspondence", "interior", "pixel", "-lsb-", "Sun", "et", "al.", "2011", "-rsb-", "use", "icp-like", "approach", "recently", "Wang", "et", "al.", "-lsb-", "2013", "-rsb-", "propose", "technique", "directly", "estimate", "correspondence", "between", "entire", "image", "object", "major", "limitation", "approach", "project", "view", "3d", "shape", "only", "contain", "partial", "information", "from", "original", "shape", "practice", "technique", "limit", "match", "very", "similar", "object", "contrast", "we", "formulate", "image", "shape", "match", "problem", "solve", "non-rigid", "alignment", "problem", "3d", "i.e.", "simultaneously", "estimate", "optimize", "depth", "image", "object", "deformation", "3d", "shape", "align", "they", "3d", "space", "particular", "we", "show", "match", "image", "collection", "3d", "shape", "boost", "matching", "quality", "enforce", "consistency", "between", "image-shape", "map", "shape-shape", "map", "pose", "estimation", "exist", "vast", "body", "work", "determine", "pose", "object", "image", "relative", "calibrate", "camera", "problem", "commonly", "formulate", "feature", "correspondence", "problem", "thus", "can", "distinguish", "type", "local", "image", "feature", "point", "line", "curve", "segment", "whole", "contour", "-lsb-", "Chen", "et", "al.", "2003", "Dalal", "Triggs", "2005", "Oliva", "Torralba", "2006", "-rsb-", "recently", "researcher", "use", "learning-based", "scheme", "cast", "classification", "learn", "good", "feature", "viewpoint", "estimation", "-lsb-", "Zia", "et", "al.", "2013", "-rsb-", "task", "pose", "estimation", "closely", "couple", "other", "task", "image-shape", "matching", "problem", "depth", "estimation", "point", "correspondence", "we", "therefore", "model", "part", "global", "optimization", "problem", "iteratively", "refine", "result", "large", "improvement", "depth", "estimation", "estimate", "depth", "image", "object", "long", "standing", "problem", "computer", "vision", "computer", "graphic", "problem", "ill-pose", "when", "input", "single", "image", "exist", "approach", "typically", "incorporate", "additional", "information", "user", "interaction", "-lsb-", "Wu", "et", "al.", "2008", "-rsb-", "shade", "-lsb-", "Lensch", "et", "al.", "2003", "Goldman", "et", "al.", "2005", "-rsb-", "use", "abstracted", "proxy", "shape", "-lsb-", "Zheng", "et", "al.", "2012", "-rsb-", "however", "approach", "design", "object", "simple", "texture", "shape", "and/or", "under", "specific", "lighting", "condition", "other", "word", "do", "apply", "well", "man-made", "object", "real", "image", "which", "exhibit", "complicated", "geometry", "texture", "availability", "large", "collection", "depth", "image", "recent", "depth", "estimation", "approach", "base", "supervised", "learning", "-lsb-", "Hoiem", "et", "al.", "2005", "Saxena", "et", "al.", "2009", "-rsb-", "give", "exemplar", "depth", "image", "approach", "learn", "conditional", "probabilistic", "distribution", "pixel", "depths", "relative", "depths", "between", "neighbor", "pixel", "apply", "learn", "distribution", "infer", "depth", "information", "new", "image", "we", "take", "different", "approach", "since", "obtain", "3d", "shape", "similar", "global", "structure", "image", "object", "easy", "we", "estimate", "depth", "information", "unsupervised", "manner", "i.e.", "directly", "match", "image", "3d", "shape", "thus", "avoid", "tedious", "task", "perform", "instance", "specific", "learning" ],
  "content" : "Images remain by far the most popular visual medium. Nowadays they are easy to acquire and distribute, contain rich visual detail, can easily be viewed and understood and, as a result, are ubiquitous  in the Web. As 2D projections of our 3D world, however, they may lack certain semantical information. For example, important parts of objects may be occluded, and depth data is typically missing. This poses serious challenges to applications involving image recognition, manipulation, editing, etc. that could greatly benefit from this omitted information. Hence, there is a strong motivation to lift images to 3D by inferring attributes lost in the projection. In this paper we are specifically interested in inferring depth for the visible object areas ? the key coordinate missing in the projection. As the problem of recovering depth from single image is naturally ill-posed, various priors have been proposed for regularization. The most common and classical approach is to match the input image to a set of 3D objects in a database (i.e., priors), and use the best matching shape to fill in missing depth information. However, large-scale deployment of such a method is fundamentally limited because only a limited number of 3D models is available. Most often, we do not even have a 3D model of the same or sufficiently similar object from which the image was taken. In this paper we consider the problem of estimating depth for an image of an object by exploiting, in a novel joint fashion, a collection of 3D models of related but largely different objects (see Figure 1 ). Key to our approach is the estimation of correspondences between the image and multiple models, with the help of correspondences estimated between the models themselves. We address the depth inference problem in its purest form, where we assume that the object image has been segmented from its background (such images are now commonplace in shopping web sites), while our 3D models are typically untextured and come from shape collections, such as the Trimble 3D warehouse. Our image-based but shape-driven modeling technique is fully automatic and reconstructs a 3D point cloud from the imaged object. The algorithm consists of a preprocessing stage, which aligns the input shapes to each other and learns a deformation model for each shape; and a reconstruction stage, which uses a continuous optimization to recover the image object pose and reconstruct a point cloud from the image that aligns with relevant 3D models extracted from the collection. We show how to formulate an appropriate objective function, how to obtain an initial solution, and how to effectively refine the solution using an alternating optimization. In our approach, we jointly match the depth-augmented image, i.e., the popup point cloud of the image, with a group of related shapes in the collection. We pose the task as a joint non-rigid registration problem, in which each shape can be deformed. The formulation has two key features. First, in contrast to utilizing a single similar shape, incorporating a collection of similar shapes offers a better coverage of the relevant neighborhood of shape space. Second, since we have already aligned the 3D models to each other, it enables us to apply consistency constraints [Kim et al. 2012a; Huang and Guibas 2013] to regularize the image to 3D model matching by using the shape-shape correspondences. In the joint non-rigid registration formulation, we introduce the key concept of deformation priors, which govern the deformation of each shape (c.f. , [Averkiou et al. 2014]). Intuitively, we aim to preserve the key structural properties of each shape in the deformation, so that round shapes stay round, left-to-right symmetries are preserved, etc. In particular, instead of detecting these properties form each shape alone, which turns out to be unreliable, we learn them from the optimal deformations of each shape to other shapes. To test the performance of the proposed approach, we have created a benchmark dataset consisting of Microsoft Kinect scans of various categories of objects including chairs, tables, lamps, and cups. Experimental results show that the proposed approach recovers depth information that is close to Kinect scans, and is significantly more accurate than state-of-the-art image-based modeling techniques. Moreover, the proposed approach is robust to variations in textures and lighting conditions. We demonstrate depth-enhanced image editing to illustrate the possibilities offered by our approach. In addition, we show that our work is a key intermediate step towards the goal of obtaining full 3D models. Using a popup point cloud as input, we can reconstruct in certain cases a full mesh by exploiting shape symmetries learned from the shape network. Contributions. We present the first, to the best of our knowledge, fully automatic method to utilize a network of related but different 3D objects in order to reconstruct depth information from a single imaged object. The key novelties are: ? showing how a single modestly-sized shape network can help infer depth information for a variety of image objects of the same class; ? using learned deformation models based on an aligned shape network to compensate for the fact that the image is not from a model directly present in the database; ? regularizing model deformations using multi-way 3D alignment between the initial image point cloud and the shapes in a neighborhood of the shape network; In the process of extracting depth information on an image, we also discover good correspondences between the image and the network shapes, enabling us to connect the image to the network and transfer complementary information back and forth. Example of such information transfer can include textures, segmentations, material properties, labels, etc. Data-driven geometry processing. The emergence of large shape collections provides us with a platform to aggregate information from multiple shapes to improve the analysis and processing of individual shapes. Already the Trimble 3D warehouse contains many thousands of example models per category for most indoor objects and some popular outdoor categories such as car and airplane. Recently, we have witnessed the success of data-driven techniques in shape analysis [Huang et al. 2011; Kim et al. 2012a; Kim et al. 2013; Huang et al. 2013; Wang et al. 2013], shape model ing [Chaudhuri et al. 2011; Kalogerakis et al. 2012; Averkiou et al. 2014] and shape reconstruction [Nan et al. 2012; Kim et al. 2012b; Shen et al. 2012]. The key task in data-driven geometry processing technique is to establish high-quality correspondences (at either pointor segment-level) across geometric objects. Although there exist rich techniques for aligning and matching 3D shapes, the problem of matching image objects and 3D shapes, which is the major focus of this paper, is far from being solved. Image-shape matching. Most existing image-shape matching approaches [Cyr and Kimia 2004; Xu et al. 2011; Wang et al. 2013] convert the problem into an image matching problem, i.e., matching images with projected views of 3D shapes. They typically start from estimating dense correspondences between silhouette curves, and then interpolate correspondences to interior pixels. [Sun et al. 2011] used an ICP-like approach. Recently, Wang et al. [2013] proposed a technique that directly estimates correspondences between entire image objects. The major limitation of these approaches is that a projected view of a 3D shape only contains partial information from the original shape. In practice, these techniques are limited to matching very similar objects. In contrast, we formulate the image shape matching problem as solving a non-rigid alignment problem in 3D, i.e., simultaneously estimating optimizing the depth of the image object and the deformations of 3D shapes to align them in the 3D space. In particular, we show that matching an image with a collection of 3D shapes boosts the matching quality by enforcing consistency between image-shape maps and shape-shape maps. Pose estimation. There exists a vast body of work to determine the pose of an object in an image relative to a calibrated camera. The problem is commonly formulated as a feature correspondence problem. Thus, they can be distinguished by the type of local image features, such as points, lines, curve segments, whole contours [Chen et al. 2003; Dalal and Triggs 2005; Oliva and Torralba 2006]. Recently, researchers used learning-based scheme to cast it as a classification and learn good features for viewpoint estimation [Zia et al. 2013]. The task of pose estimation is closely coupled with other tasks in the image-shape matching problem, such as depth estimation and point correspondence. We therefore model it as a part of the global optimization problem and iteratively refine it, resulting in large improvement. Depth estimation. Estimating the depth of an image object is a long standing problem in computer vision and computer graphics. This problem is ill-posed when the input is a single image, and existing approaches typically incorporate additional information such as user interaction [Wu et al. 2008] and shading [Lensch et al. 2003; Goldman et al. 2005], or using abstracted proxy shapes [Zheng et al. 2012]. However, these approaches are designed for objects with simple textures and shapes and/or under specific lighting conditions. In other words, they do not apply well on man-made objects in real images, which exhibit complicated geometries and textures. With the availability of large collection of depth images, recent depth estimation approaches are based on supervised learning [Hoiem et al. 2005; Saxena et al. 2009]. Given exemplar depth images, these approaches learn conditional probabilistic distributions of pixel depths and relative depths between neighboring pixels, and apply the learned distributions to infer the depth information of new images. We take a different approach. Since obtaining 3D shapes that are similar in global structure to image objects is easy, we estimate depth information in an unsupervised manner, i.e., by directly matching images with 3D shapes, and thus avoiding the tedious task of performing instance specific learning.",
  "resources" : [ ]
}