{
  "uri" : "sig2008a-a112-ye_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2008a/a112-ye_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates",
    "published" : "2008",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Yuting-Ye",
      "name" : "Yuting",
      "surname" : "Ye"
    }, {
      "uri" : "http://drinventor/C. Karen-Liu",
      "name" : "C. Karen",
      "surname" : "Liu"
    } ]
  },
  "bagOfWords" : [ "7228ae0078d02c7dab5d15f1c8a83e5dd840cfff5ece516cb0bb6ed4d0a1fe1c", "mhy", "10.1145", "1409060.1409065", "name", "identification", "possible", "animate", "responsive", "character", "Dynamic", "constraint", "Near-Unactuated", "coordinate", "yuting", "ye", "c.", "Karen", "Liu", "Georgia", "Institute", "Technology", "paper", "present", "technique", "enhance", "kinematically", "control", "virtual", "character", "generic", "class", "dynamic", "response", "small", "perturbation", "give", "input", "motion", "sequence", "we", "technique", "can", "synthesize", "reactive", "motion", "arbitrary", "external", "force", "specific", "style", "customize", "input", "motion", "we", "method", "re-parameterize", "motion", "degree", "freedom", "base", "joint", "actuation", "input", "motion", "only", "enforce", "equation", "motion", "less", "actuated", "coordinate", "we", "approach", "can", "create", "physically", "responsive", "motion", "base", "kinematic", "pose", "control", "without", "explicitly", "compute", "joint", "actuation", "we", "demonstrate", "simplicity", "robustness", "we", "technique", "show", "variety", "example", "generate", "same", "set", "parameter", "we", "formulation", "focus", "type", "perturbation", "significantly", "disrupt", "upper", "body", "pose", "dynamics", "have", "limit", "effect", "whole-body", "balance", "state", "cr", "category", "i.", "3.7", "-lsb-", "Computer", "Graphics", "-rsb-", "three-dimensional", "graphic", "realism?animation", "keyword", "physically", "base", "animation", "Motion", "Capture", "introduction", "create", "virtual", "character", "realistically", "respond", "physical", "perturbation", "interactive", "environment", "remain", "challenging", "problem", "physics-based", "approach", "generate", "motion", "consistent", "simulated", "environment", "inherently", "difficult", "design", "tune", "produce", "realistic", "motion", "across", "wide", "variety", "scenario", "many", "interactive", "application", "video", "game", "simpler", "more", "widely", "use", "approach", "kinematically", "control", "character", "mimic", "she", "dynamic", "reaction", "play", "back", "pre-scripted", "motion", "response", "predefined", "stimulus", "responsiveness", "character", "thus", "largely", "depend", "quality", "scope", "pre-scripted", "motion", "type", "interaction", "allow", "consequently", "most", "application", "only", "focus", "perturbation", "have", "large", "impact", "character", "since", "virtually", "impossible", "predefine", "all", "possible", "perturbation", "animated", "response", "contrast", "large", "perturbation", "often", "force", "character", "re-plan", "she", "high-level", "behavior", "interfere", "balance", "state", "character", "more", "likely", "encounter", "small", "perturbation", "disrupt", "motion", "pattern", "momentarily", "do", "change", "course", "email", "-lcb-", "yuting", "karenliu", "-rcb-", "@cc", "gatech.edu", "ACM", "Reference", "Format", "ye", "Y.", "Liu", "C.", "2008", "animate", "responsive", "character", "Dynamic", "constraint", "Near-Unactuated", "coordinate", "ACM", "Trans", "graph", "27", "Article", "112", "-lrb-", "December", "2008", "-rrb-", "page", "dous", "10.1145", "1409060.1409065", "http://doi.acm.org/10.1145/1409060.1409065", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2008", "ACM", "0730-0301/2008", "05-art112", "5.00", "DOI", "10.1145", "1409060.1409065", "http://doi.acm.org/10.1145/1409060.1409065", "figure", "leave", "respond", "move", "platform", "right", "avoid", "obstacle", "environment", "she", "current", "action", "paper", "describe", "technique", "enhance", "kinematically", "controlled", "character", "generic", "class", "dynamic", "response", "small-scale", "perturbation", "-lrb-", "figure", "-rrb-", "give", "input", "motion", "we", "technique", "can", "synthesize", "motion", "respond", "external", "force", "arbitrary", "direction", "different", "body", "part", "any", "moment", "time", "without", "additional", "datum", "any", "modification", "underlying", "motion", "synthesis", "engine", "although", "we", "focus", "small-scale", "perturbation", "mainly", "affect", "upper", "body", "motion", "we", "technique", "can", "integrate", "seamlessly", "any", "technique", "produce", "balanced", "lower", "body", "motion", "presence", "large", "perturbation", "we", "approach", "motivate", "observation", "less-controlled", "joint", "degree", "freedom", "-lrb-", "dof", "-rrb-", "usually", "more", "compliant", "when", "perturb", "we", "able", "identify", "those", "compliant", "dof", "we", "can", "apply", "hybrid", "method", "only", "consider", "dynamics", "compliant", "dof", "kinematically", "control", "rest", "character", "instead", "determine", "dof", "heuristic", "hand-tune", "physical", "parameter", "we", "use", "Principle", "Component", "analysis", "-lrb-", "pca", "-rrb-", "define", "new", "set", "coordinate", "rank", "level", "joint", "actuation", "input", "motion", "we", "method", "provide", "more", "principled", "way", "identify", "less", "actuated", "coordinate", "-lrb-", "correspond", "eigenvalue", "close", "zero", "-rrb-", "specific", "each", "input", "motion", "sequence", "we", "denote", "those", "dof", "near-unactuated", "coordinate", "synthesize", "input", "motion", "under", "perturbation", "we", "enforce", "dynamic", "equation", "motion", "only", "near-unactuated", "coordinate", "while", "kinematically", "maintain", "original", "joint", "trajectory", "because", "near-unactuated", "coordinate", "use", "very", "little", "internal", "torque", "input", "motion", "enforce", "dynamic", "equation", "zero", "internal", "actuation", "do", "visually", "modify", "input", "motion", "when", "external", "perturbation", "when", "character", "perturb", "however", "near-unactuated", "coordinate", "compliantly", "react", "external", "force", "while", "actuate", "coordinate", "attempt", "maintain", "input", "joint", "position", "because", "lower", "body", "motion", "typically", "less", "compliant", "internal", "joint", "torque", "can", "obtain", "without", "accurate", "measure", "contact", "force", "we", "technique", "only", "consider", "dynamics", "upper", "body", "motion", "we", "modify", "lower", "body", "motion", "simple", "kinematic", "method", "base", "perturbation", "force", "enforce", "dynamic", "constraint", "near-unactuated", "coordinate", "lead", "two", "main", "advantage", "first", "responsive", "motion", "vary", "due", "different", "activity", "style", "individual", "because", "each", "motion", "perturbation", "result", "unique", "response", "base", "specific", "joint", "torque", "usage", "input", "motion", "second", "we", "formulation", "bypass", "problem", "active", "body", "control", "generalize", "coordinate", "we", "parameterization", "align", "mechanical", "joint", "space", "rather", "align", "more", "meaningful", "actuation", "space", "derive", "from", "input", "motion", "choose", "appropriate", "coordinate", "enforce", "equation", "motion", "we", "approach", "can", "create", "physically", "responsive", "motion", "base", "kinematic", "pose", "control", "without", "explicitly", "compute", "joint", "actuation", "practice", "we", "technique", "can", "adapt", "transparently", "any", "kinematically", "control", "framework", "without", "aid", "forward", "simulator", "additional", "motion", "datum", "we", "demonstrate", "simplicity", "robustness", "we", "approach", "show", "wide", "range", "input", "motion", "arbitrary", "perturbation", "we", "result", "show", "realistic", "recovery", "motion", "emerge", "consequence", "interaction", "kinematic", "dynamic", "control", "example", "character", "stick", "she", "arm", "out", "recover", "from", "large", "push", "we", "believe", "behavior", "due", "fact", "objective", "function", "must", "pull", "joint", "back", "original", "trajectory", "without", "use", "any", "internal", "torque", "near-unactuated", "coordinate", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "112", "publication", "date", "December", "2008", "112:2", "Y.", "ye", "et", "al.", "related", "work", "synthesize", "responsive", "character", "animation", "important", "research", "topic", "broad", "range", "application", "researcher", "have", "explore", "different", "approach", "build", "physics-based", "controller", "guide", "kinematically", "specify", "motion", "datum", "various", "tracking", "controller", "have", "be", "apply", "produce", "responsive", "movement", "under", "physical", "perturbation", "include", "upper", "body", "motion", "-lsb-", "Zordan", "Hodgins", "2002", "Yin", "et", "al.", "2003", "-rsb-", "manipulation", "task", "-lsb-", "abe", "Popovi", "2006", "-rsb-", "stand", "motion", "-lsb-", "Kokkevis", "et", "al.", "1996", "Abe", "et", "al.", "2007", "-rsb-", "cyclic", "biped", "motion", "-lsb-", "Sok", "et", "al.", "2007", "Yin", "et", "al.", "2007", "-rsb-", "proper", "physical", "parameter", "controller", "method", "can", "generate", "realistic", "reactive", "motion", "consistent", "simulated", "virtual", "world", "however", "many", "method", "require", "fine", "tuning", "physical", "parameter", "expensive", "pre-computation", "specific", "target", "motion", "skeletal", "model", "da", "Silva", "et", "al.", "-lsb-", "2008", "-rsb-", "introduce", "systematic", "method", "derive", "balance", "controller", "tailor", "input", "motion", "reduce", "effort", "parameter", "tuning", "we", "method", "also", "produce", "responsive", "motion", "preserve", "style", "input", "motion", "however", "we", "approach", "do", "involve", "any", "active", "body", "control", "thereby", "physical", "parameter", "tuning", "require", "furthermore", "we", "focus", "type", "perturbation", "significantly", "disrupt", "upper", "body", "pose", "dynamics", "have", "limit", "effect", "whole-body", "balance", "kinematically", "control", "character", "animation", "more", "preferable", "many", "online", "application", "because", "easier", "implement", "provide", "precise", "user", "controllability", "create", "dynamic", "response", "under", "external", "impact", "Komura", "et", "al.", "-lsb-", "2004", "2005", "-rsb-", "directly", "change", "motion", "respect", "change", "momentum", "biped", "motion", "Oshita", "Makinouchi", "-lsb-", "2001", "-rsb-", "modify", "joint", "acceleration", "base", "dynamic", "control", "balance", "comfort", "combine", "advantage", "kinematic", "motion", "physical", "simulation", "many", "researcher", "have", "also", "propose", "idea", "switching", "between", "dynamic", "simulation", "motion", "capture", "datum", "whenever", "necessary", "-lsb-", "Zordan", "et", "al.", "2005", "Shapiro", "et", "al.", "2003", "Mandel", "2004", "-rsb-", "particular", "Zordan", "et", "al.", "propose", "framework", "use", "minimal", "simulation", "interval", "after", "impact", "rely", "motion", "capture", "alone", "when", "perturbation", "present", "we", "method", "also", "take", "hybrid", "approach", "synthesis", "responsive", "motion", "however", "instead", "divide", "kinematic", "dynamic", "control", "time", "domain", "we", "divide", "they", "spatially", "transform", "space", "span", "set", "basis", "represent", "joint", "actuation", "original", "motion", "directly", "apply", "motion", "capture", "datum", "produce", "natural", "human", "mo", "tion", "rich", "detail", "many", "technique", "have", "successfully", "demonstrate", "pre-recorded", "datum", "can", "adapt", "new", "situation", "response", "online", "user", "control", "-lsb-", "treuille", "et", "al.", "2007", "McCann", "Pollard", "2007", "Cooper", "et", "al.", "2007", "Shin", "oh", "2006", "-rsb-", "few", "method", "extend", "data-driven", "approach", "synthesize", "responsive", "motion", "balance", "recovery", "against", "external", "force", "-lsb-", "Arikan", "et", "al.", "2005", "Yin", "et", "al.", "2005", "-rsb-", "method", "collect", "set", "specific", "interaction", "advance", "procedurally", "generate", "small", "deformation", "from", "record", "motion", "respond", "predefined", "user", "interaction", "we", "method", "synthesize", "responsive", "motion", "via", "dynamic", "constraint", "instead", "motion", "blending", "thereby", "completely", "remove", "dependency", "motion", "database", "moreover", "we", "allow", "direct", "kinematic", "control", "additional", "objective", "PCA", "have", "be", "frequently", "use", "process", "motion", "datum", "application", "computer", "graphic", "robotic", "computer", "vision", "computer", "animation", "pca", "typically", "use", "reduce", "dimensionality", "configuration", "space", "motion", "blending", "recognition", "modeling", "-lsb-", "brand", "hertzmann", "2000", "Jenkins", "Matari", "2002", "Safonova", "et", "al.", "2004", "Barbic", "et", "al.", "2004", "Chai", "Hodgins", "2007", "-rsb-", "biomechanic", "researcher", "have", "apply", "dimension", "reduction", "technique", "muscle", "activation", "datum", "measure", "from", "behavioral", "experiment", "-lsb-", "Tresch", "et", "al.", "2006", "Ting", "2007", "Alexandrov", "et", "al.", "2005", "-rsb-", "ting", "suggest", "limited", "set", "muscle", "synergy", "define", "low-dimensional", "module", "form", "muscle", "activate", "synchrony", "use", "control", "center", "mass", "after", "postural", "perturbation", "we", "method", "inspire", "ting?s", "work", "we", "formulate", "dynamic", "equation", "space", "muscle", "synergy", "rather", "than", "space", "joint", "configuration", "however", "we", "do", "use", "PCA", "tool", "dimension", "reduction", "we", "only", "apply", "pca", "identify", "principle", "component", "correspond", "lower", "eigenvalue", "because", "principle", "component", "represent", "dimension", "motion", "where", "active", "body", "control", "do", "play", "important", "role", "overview", "approach", "we", "entire", "algorithm", "can", "describe", "three", "simple", "step", "give", "input", "motion", "sequence", "apply", "inverse", "dynamics", "method", "obtain", "internal", "joint", "torque", "upper", "body", "apply", "pca", "obtain", "set", "eigenvector", "E.", "define", "set", "near-unactuated", "coordinate", "subset", "smallest", "corresponding", "eigenvalue", "formulate", "constrain", "optimization", "each", "frame", "solve", "pose", "satisfy", "equation", "motion", "while", "maintain", "original", "motion", "m.", "implementation", "we", "represent", "character?s", "skeleton", "transformation", "hierarchy", "18", "body", "node", "24", "dof", "upper", "body", "12", "dof", "lower", "body", "global", "translation", "orientation", "represent", "six", "dof", "root", "hierarchy", "4.1", "preprocess", "preprocessing", "step", "we", "perform", "inverse", "dynamics", "method", "pca", "input", "motion", "identify", "near-unactuated", "coordinate", "one", "cycle", "from", "input", "motion", "manually", "select", "preprocessing", "compute", "joint", "torque", "input", "motion", "we", "express", "la", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "112", "publication", "date", "December", "2008", "animate", "responsive", "character", "Dynamic", "constraint", "Near-Unactuated", "coordinate", "112:3", "grange?s", "equation", "motion", "each", "joint", "dof", "dt", "where", "lagrangian", "dynamic", "system", "internal", "torque", "th", "column", "jacobian", "matrix", "which", "project", "external", "force", "onto", "absence", "external", "force", "apply", "upper", "body", "we", "use", "equation", "-lrb-", "-rrb-", "solve", "internal", "joint", "torque", "upper", "body", "each", "frame", "form", "joint", "torque", "basis", "-lsb-", "-rsb-", "24", "where", "number", "frame", "select", "input", "motion", "cycle", "perform", "pca", "yield", "eigenvector", "-lsb-", "24", "-rsb-", "rank", "from", "largest", "corresponding", "eigenvalue", "smallest", "we", "divide", "two", "set", "-lsb-", "-rsb-", "contain", "first", "24", "eigenvector", "contain", "rest", "eigenvector", "smallest", "corresponding", "eigenvalue", "we", "define", "set", "near-unactuated", "coordinate", "we", "implementation", "set", "10", "all", "example", "except", "Tai", "Chi", "motion", "4.2", "optimization", "we", "discretize", "time", "domain", "interval", "1/60s", "input", "motion", "each", "time", "step", "we", "solve", "upper", "body", "joint", "angle", "next", "interval", "formulate", "constrain", "optimization", "we", "use", "dynamic", "constraint", "ensure", "nearunactuated", "coordinate", "have", "zero", "internal", "actuation", "all", "time", "-lrb-", "-rrb-", "where", "internal", "joint", "torque", "upper", "body", "compute", "via", "equation", "-lrb-", "-rrb-", "express", "function", "f.", "backward", "difference", "use", "compute", "joint", "velocity", "-lrb-", "-rrb-", "Joint", "acceleration", "compute", "central", "differencing", "-lrb-", "+1", "2q", "-rrb-", "perturbation", "-lrb-", "-rrb-", "original", "motion", "close", "satisfy", "when", "perturbation", "occur", "-lrb-", "-rrb-", "however", "character", "must", "adjust", "she", "motion", "maintain", "we", "use", "spring-like", "objective", "track", "input", "motion", "-lrb-", "-rrb-", "damp", "objective", "model", "dissipation", "dynamic", "system", "+1", "+1", "-lrb-", "+1", "-rrb-", "when", "human", "perturb", "unexpectedly", "typically", "delay", "between", "perturbation", "muscle", "activation", "due", "latency", "sensory", "feedback", "-lsb-", "Miall", "et", "al.", "1985", "Georgopoulos", "et", "al.", "1981", "-rsb-", "delay", "arm", "movement", "due", "visual", "sensory", "feedback", "usually", "range", "from", "150-250", "ms.", "we", "incorporate", "delay", "minimize", "torque", "change", "200", "m", "after", "perturbation", "highly", "actuated", "coordinate", "E.", "-lrb-", "-rrb-", "summary", "we", "formulate", "follow", "optimization", "each", "time", "step", "solve", "upper", "body", "motion", "argmin", "subject", "+1", "-lrb-", "-rrb-", "where", "operator", "denote", "element-wise", "multiplication", "two", "vector", "all", "we", "experiment", "we", "set", "every", "element", "200", "three", "element", "correspond", "spine", "ball", "joint", "dof", "30", "rest", "element", "10", "all", "element", "set", "30", "time", "perturbation", "smoothly", "decrease", "zero", "200", "ms.", "weight", "choose", "scale", "objectiv", "comparable", "level", "well", "reflect", "relative", "importance", "value", "do", "depend", "input", "motion", "skeletal", "model", "4.3", "synthesis", "lower", "body", "motion", "although", "we", "method", "focus", "upper", "body", "response", "we", "formulate", "simple", "computation", "root", "lower", "body", "motion", "when", "character", "perturb", "since", "we", "method", "do", "model", "ground", "contact", "friction", "force", "impact", "perturbation", "root", "can", "simply", "model", "impulse", "proportional", "external", "force", "+1", "where", "total", "mass", "character", "consist", "column", "correspond", "root", "movement", "cause", "footskating", "penetration", "ground", "we", "apply", "simple", "inverse", "kinematic", "method", "lower", "body", "fix", "foot", "contact", "result", "we", "apply", "we", "method", "variety", "cyclic", "motion", "different", "style", "perform", "different", "subject", "one", "complete", "cycle", "each", "motion", "sufficient", "compute", "near-unacutated", "coordinate", "upper", "body", "simulation", "run", "20", "frame", "per", "second", "single", "core", "2.8", "GHz", "Intel", "Core", "duo", "processor", "we", "use", "snopt", "-lsb-", "Gill", "et", "al.", "1996", "-rsb-", "solve", "optimization", "problem", "each", "time", "step", "we", "result", "reveal", "dynamic", "constraint", "near-unactuated", "coordinate", "produce", "compliant", "response", "unexpected", "perturbation", "coordinate", "recovery", "motion", "customize", "input", "motion", "all", "example", "be", "generate", "use", "identical", "set", "weight", "describe", "section", "experiment", "show", "wide", "range", "weight", "produce", "similar", "result", "please", "see", "supplemental", "video", "all", "example", "describe", "below", "eigenvector", "analysis", "demonstrate", "importance", "joint", "actuation", "space", "we", "conduct", "several", "experiment", "normal", "walk", "different", "choice", "coordinate", "which", "dynamic", "constraint", "enforce", "we", "first", "simulated", "same", "input", "motion", "different", "value", "number", "dynamic", "constraint", "nearunactuated", "coordinate", "character", "appear", "more", "responsive", "number", "dynamic", "constraint", "increase", "however", "character", "able", "completely", "recover", "from", "perturbation", "when", "more", "than", "12", "dynamic", "constraint", "when", "number", "dynamic", "constraint", "increase", "16", "character", "simply", "fail", "track", "input", "motion", "second", "experiment", "simulated", "motion", "dynamic", "constraint", "coordinate", "correspond", "higher", "eigenvalue", "result", "show", "character", "able", "maintain", "original", "motion", "without", "actuation", "those", "coordinate", "perturbation", "we", "first", "experiment", "apply", "same", "external", "force", "different", "body", "part", "1.7", "80kg", "male", "character", "during", "different", "phase", "normal", "walking", "sequence", "result", "show", "same", "push", "incur", "larger", "response", "during", "single", "support", "than", "double", "support", "moreover", "character", "exhibit", "more", "stability", "when", "push", "apply", "same", "side", "support", "leg", "when", "push", "arm", "character", "react", "more", "compliantly", "than", "when", "push", "head", "shoulder", "-lrb-", "figure", "-rrb-", "second", "experiment", "test", "effect", "different", "external", "force", "direction", "character", "have", "harder", "time", "recover", "from", "backward", "push", "than", "forward", "one", "indicate", "he", "torso", "actuation", "asymmetric", "along", "sagittal", "direction", "addition", "produce", "highly", "coordinate", "reaction", "we", "method", "also", "preserve", "individual", "style", "we", "demonstrate", "large-scale", "arm", "movement", "female", "character", "-lrb-", "1.5", "40kg", "-rrb-", "preserve", "she", "reactive", "motion", "we", "scale", "magnitude", "external", "force", "proportionally", "female", "subject?s", "weight", "we", "method", "also", "allow", "user", "interact", "character", "perturb", "root", "movement", "illustrate", "we", "simulated", "reaction", "character", "step", "fast", "move", "platform", "root", "accelerate", "abruptly", "character?s", "upper", "body", "react", "passively", "gradually", "recover", "original", "motion", "pattern", "style", "coordinate", "actuation", "space", "encode", "muscle", "usage", "coordination", "specific", "input", "motion", "consequently", "each", "motion", "sequence", "react", "unexpected", "perturbation", "unique", "style", "we", "apply", "same", "set", "external", "force", "normal", "walk", "backward", "walk", "sneaky", "walk", "perform", "same", "male", "character", "-lrb-", "figure", "-rrb-", "comparison", "other", "motion", "normal", "walk", "exhibit", "higher", "coordination", "among", "upper", "body", "counteract", "disturbance", "use", "torso", "both", "arm", "simultaneously", "backward", "walk", "motion", "exhibit", "higher", "stability", "against", "forward", "push", "response", "compliantly", "backward", "push", "sneaky", "walk", "character", "maintain", "more", "stable", "posture", "center", "mass", "position", "lower", "than", "other", "motion", "result", "show", "same", "amount", "force", "induce", "smaller", "response", "sneaky", "walk", "compare", "actuation", "among", "style", "individual", "we", "extract", "near", "unactuated", "coordinate", "one", "individual", "perform", "normal", "walk", "apply", "they", "simulate", "another", "individual?s", "normal", "walk", "under", "perturbation", "result", "show", "plausible", "reactive", "motion", "can", "generate", "only", "when", "two", "individual", "have", "similar", "weight", "height", "we", "also", "conduct", "similar", "experiment", "different", "action", "style", "actuation", "sneaky", "walk", "reproduce", "normal", "walk", "faithfully", "without", "disturbance", "generate", "unrealistic", "response", "when", "perturb", "additional", "objective", "we", "formulation", "allow", "animator", "include", "additional", "objective", "enforce", "kinematic", "property", "input", "motion", "example", "we", "capture", "walking", "sequence", "subject", "hold", "cup", "he", "right", "hand", "during", "motion", "synthesis", "additional", "objective", "add", "keep", "cup", "upright", "orientation", "asymmetrical", "muscle", "usage", "left", "right", "arm", "result", "many", "interesting", "behavior", "when", "character", "push", "right", "arm", "he", "maintain", "orientation", "cup", "rotate", "he", "torso", "compensate", "movement", "he", "right", "arm", "contrast", "when", "left", "arm", "push", "same", "force", "he", "stiffen", "he", "torso", "reduce", "its", "movement", "impact", "right", "arm", "similarly", "we", "add", "objective", "repel", "character", "from", "obstacle", "environment", "character", "fail", "completely", "avoid", "obstacle", "external", "force", "apply", "site", "collision", "non-locomotion", "we", "method", "also", "work", "other", "periodic", "motion", "Tai", "Chi", "form", "although", "Tai", "Chi", "motion", "require", "higher", "overall", "internal", "torque", "than", "other", "locomotion", "sequence", "-lrb-", "-rrb-", "highly", "actuated", "coordinate", "mostly", "lie", "frontal", "plane", "moreover", "torque", "usage", "arm", "Tai", "Chi", "motion", "highly", "correlate", "result", "character", "react", "perturbation", "sagittal", "plane", "both", "arm", "move", "fluidly", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "112", "publication", "date", "December", "2008", "112:4", "Y.", "ye", "et", "al.", "-lrb-", "-rrb-", "head", "-lrb-", "-rrb-", "leave", "shoulder", "-lrb-", "-rrb-", "right", "arm", "figure", "perturbation", "-lrb-", "indicate", "red", "arrow", "-rrb-", "different", "body", "part", "-lrb-", "-rrb-", "forward", "walk", "-lrb-", "-rrb-", "backward", "walk", "-lrb-", "-rrb-", "sneaky", "walk", "figure", "different", "style", "recover", "from", "backward", "push", "conclusion", "we", "have", "present", "technique", "synthesize", "generic", "class", "dynamic", "response", "small-scale", "perturbation", "enforce", "dynamic", "constraint", "actuation", "space", "virtual", "character", "respond", "arbitrary", "unexpected", "perturbation", "specific", "style", "encode", "input", "motion", "we", "have", "demonstrate", "simplicity", "robustness", "we", "technique", "show", "variety", "example", "generate", "same", "set", "parameter", "we", "method", "can", "readily", "augmented", "any", "kinematic", "technique", "character", "animation", "motion", "graph", "motion", "blending", "without", "modification", "exist", "implementation", "nor", "additional", "datum", "main", "assumption", "we", "approach", "only", "small", "set", "coordinate", "muscle", "group", "activate", "perform", "rhythmic", "motion", "biomechanic", "researcher", "have", "also", "hypothesize", "postural", "response", "under", "perturbation", "can", "activate", "few", "muscle", "synergy", "-lsb-", "Torres-Oviedo", "Ting", "2007", "-rsb-", "we", "result", "suggest", "same", "muscle", "synergy", "use", "input", "motion", "can", "also", "produce", "reasonable", "recovery", "motion", "from", "small", "perturbation", "thereby", "lend", "support", "hypothesis", "muscle", "synergy", "building", "block", "construct", "motor", "output", "pattern", "however", "we", "method", "robust", "against", "steady", "perturbation", "clear", "whether", "human", "body", "switch", "different", "muscle", "synergy", "when", "present", "sustained", "disturbance", "we", "technique", "focus", "only", "upper", "body", "response", "suitable", "large", "perturbation", "incur", "loss", "balance", "change", "high-level", "behavior", "we", "anticipate", "technique", "can", "apply", "whole", "body", "motion", "we", "can", "accurately", "measure", "ground", "contact", "force", "one", "possibility", "estimate", "ground", "contact", "force", "from", "motion", "capture", "datum", "use", "method", "describe", "Liu", "et", "al.", "-lsb-", "2005", "-rsb-", "another", "promising", "future", "direction", "combine", "we", "technique", "sophisticated", "balance", "controller", "determine", "lower", "body", "root", "movement", "-lsb-", "da", "Silva", "et", "al.", "2008", "Yin", "et", "al.", "2007", "-rsb-", "we", "approach", "use", "inverse", "dynamics", "method", "principle", "component", "analysis", "both", "which", "know", "sensitive", "input", "noise", "fortunately", "we", "method", "do", "directly", "apply", "computed", "torque", "simulate", "motion", "only", "use", "they", "derive", "eigenbasis", "input", "activity", "we", "test", "robustness", "we", "method", "against", "datum", "noise", "randomly", "select", "different", "cycle", "from", "input", "motion", "result", "show", "sporadic", "noise", "motion", "have", "negligible", "effect", "long", "input", "motion", "contain", "sufficient", "clean", "datum", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "112", "publication", "date", "December", "2008", "animate", "responsive", "character", "Dynamic", "constraint", "Near-Unactuated", "coordinate", "112:5", "acknowledgement", "author", "would", "like", "thank", "Satoru", "Ishigaki", "Wei", "Liu", "Shuang", "Hao", "help", "collect", "motion", "datum", "work", "support", "NSF", "grant", "ccf-cise", "0742303", "reference", "Y.", "opovus", "J.", "2006", "interactive", "animation", "dynamic", "manipulation", "Eurographics/SIGGRAPH", "Symposium", "Computer", "Animation", "Y.", "DA", "ILVA", "M.", "opovus", "J.", "2007", "multiobjective", "control", "frictional", "contact", "Eurographics/SIGGRAPH", "Symposium", "Computer", "Animation", "249", "258", "lexandrov", "a.", "rolov", "a.", "orak", "F.", "ARLSON", "UHTA", "P.", "ark", "S.", "2005", "feedback", "equilibrium", "control", "during", "human", "standing", "biological", "cybernetics", "93", "309", "322", "RIKAN", "O.", "O?B", "RIEN", "J.", "F.", "orsyth", "D.", "A.", "2005", "push", "people", "around", "Eurographics/SIGGRAPH", "Symposium", "Computer", "Animation", "59", "66", "arbic", "J.", "afonova", "a.", "J.-Y.", "aloutso", "C.", "OD", "GINS", "J.", "K.", "ollard", "N.", "S.", "2004", "segment", "motion", "capture", "datum", "distinct", "behavior", "Graphics", "Interface", "vol", "62", "185", "194", "rand", "m.", "ertzmann", "a.", "2000", "style", "machine", "SIGGRAPH", "183", "192", "haus", "J.", "odgin", "J.", "K.", "2007", "constraint-based", "motion", "optimization", "use", "statistical", "dynamic", "model", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "-lrb-", "Aug.", "-rrb-", "ooper", "S.", "ERTZMANN", "a.", "opovus", "Z.", "2007", "active", "learning", "real-time", "motion", "controller", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "-lrb-", "Aug.", "-rrb-", "DA", "ILVA", "M.", "Y.", "opovus", "J.", "2008", "interactive", "simuation", "stylized", "human", "locomotion", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "27", "-lrb-", "Aug.", "-rrb-", "eorgopoulo", "a.", "ALASKA", "J.", "assey", "J.", "1981", "spatial", "trajectory", "reaction", "time", "aim", "movement", "effect", "practice", "uncertainty", "change", "target", "location", "Journal", "Neurophysiology", "46", "725", "743", "ill", "P.", "AUNDERS", "M.", "urray", "W.", "1996", "Snopt", "sqp", "algorithm", "large-scale", "constrain", "optimization", "Tech", "Rep.", "NA", "96-2", "University", "California", "San", "Diego", "ENKINS", "O.", "C.", "atarus", "M.", "J.", "2002", "derive", "action", "behavior", "primitive", "from", "human", "motion", "datum", "ieee/rsj", "2551", "2556", "OKKEVIS", "E.", "ETAXAS", "D.", "ADLER", "N.", "I.", "1996", "usercontrolled", "physics-based", "animation", "articulated", "figure", "computer", "animation", "OMURA", "T.", "EUNG", "H.", "UFFNER", "J.", "2004", "animate", "reactive", "motion", "biped", "locomotion", "VRST", "04", "Proceedings", "ACM", "symposium", "virtual", "reality", "software", "technology", "32", "40", "OMURA", "T.", "E.", "S.", "L.", "au", "R.", "W.", "H.", "2005", "animate", "reactive", "motion", "use", "momentum-based", "inverse", "kinematic", "computer", "animation", "virtual", "world", "16", "213", "223", "iu", "C.", "K.", "ERTZMANN", "a.", "opovus", "Z.", "2005", "Learning", "physics-based", "motion", "style", "nonlinear", "inverse", "optimization", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "24", "-lrb-", "July", "-rrb-", "1071", "1081", "andel", "M.", "2004", "versatile", "interactive", "virtual", "human", "hybrid", "use", "data-driven", "dynamics-based", "motion", "synthesis", "ANN", "J.", "ollard", "N.", "S.", "2007", "responsive", "character", "from", "motion", "fragment", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "-lrb-", "Aug.", "-rrb-", "iall", "R.", "C.", "EIR", "D.", "J.", "TEIN", "J.", "F.", "1985", "Visuomotor", "tracking", "delay", "visual", "feedback", "neuroccience", "16", "511", "520", "SHITA", "M.", "akinouchus", "a.", "2001", "dynamic", "motion", "control", "technique", "human-like", "articulated", "figure", "Computer", "Graphics", "Forum", "20", "192", "202", "afonova", "a.", "odgin", "J.", "K.", "ollard", "N.", "S.", "2004", "synthesize", "physically", "realistic", "human", "motion", "lowdimensinal", "behavior-specific", "space", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "23", "-lrb-", "July", "-rrb-", "514", "521", "hapiro", "a.", "ighin", "F.", "H.", "aloutso", "p.", "2003", "hybrid", "control", "interactive", "character", "animation", "Pacific", "Graphics", "456", "461", "hin", "H.", "J.", "H.", "S.", "2006", "Fat", "graph", "construct", "interactive", "character", "continuous", "control", "Eurographics/SIGGRAPH", "Symposium", "Computer", "Animation", "ok", "K.", "W.", "IM", "M.", "ee", "J.", "2007", "simulate", "biped", "behavior", "from", "human", "motion", "datum", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "-lrb-", "Aug.", "-rrb-", "107", "ing", "L.", "H.", "2007", "dimensional", "reducation", "sensorimotor", "system", "computational", "neuroscience", "13", "-lrb-", "apr.", "-rrb-", "103", "136", "orre", "viedo", "G.", "ing", "L.", "H.", "2007", "muscle", "synergy", "characterize", "human", "postural", "response", "Journal", "Neurophysiology", "98", "2144", "2156", "resch", "M.", "C.", "HEUNG", "V.", "C.", "VELLA", "A.", "2006", "Matrix", "factorization", "algorithm", "identification", "muscle", "synergy", "evaluation", "simulated", "experimental", "datum", "set", "Journal", "Neurophysiology", "95", "2199", "2212", "reuille", "a.", "ee", "Y.", "opovus", "Z.", "2007", "near-optimal", "character", "animation", "continuous", "control", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "-lrb-", "Aug.", "-rrb-", "K.", "LINE", "M.", "B.", "aus", "D.", "K.", "2003", "Motion", "perturbation", "base", "simple", "neuromotor", "control", "model", "Pacific", "Graphics", "K.", "aus", "D.", "K.", "VAN", "de", "ANNE", "M.", "2005", "data-driven", "interactive", "balancing", "behavior", "Pacific", "Graphics", "K.", "OKEN", "K.", "VAN", "de", "ANNE", "M.", "2007", "Simbicon", "simple", "biped", "locomotion", "control", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "26", "105", "115", "ORDAN", "V.", "B.", "odgin", "J.", "K.", "2002", "Motion", "capture-driven", "simulation", "hit", "react", "Eurographics/SIGGRAPH", "Symposium", "Computer", "Animation", "89", "96", "ORDAN", "V.", "B.", "AJKOWSKA", "A.", "HIU", "B.", "AST", "M.", "2005", "Dynamic", "response", "motion", "capture", "animation", "ACM", "Trans", "Graphics", "-lrb-", "SIGGRAPH", "-rrb-", "24", "-lrb-", "July", "-rrb-", "697", "701", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "112", "publication", "date", "December", "2008" ],
  "content" : "\n  \n    7228ae0078d02c7dab5d15f1c8a83e5dd840cfff5ece516cb0bb6ed4d0a1fe1c\n    mhy\n    10.1145/1409060.1409065\n    Name identification was not possible. \n  \n  \n    \n      \n        Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates\n      \n      Yuting Ye C. Karen Liu Georgia Institute of Technology ?\n      This paper presents a technique to enhance a kinematically controlled virtual character with a generic class of dynamic responses to small perturbations. Given an input motion sequence, our technique can synthesize reactive motion to arbitrary external forces with a specific style customized to the input motion. Our method re-parameterizes the motion degrees of freedom based on joint actuations in the input motion. By only enforcing the equations of motion in the less actuated coordinates, our approach can create physically responsive motion based on kinematic pose control without explicitly computing the joint actuations. We demonstrate the simplicity and robustness of our technique by showing a variety of examples generated with the same set of parameters. Our formulation focuses on the type of perturbations that significantly disrupt the upper body poses and dynamics, but have limited effect on the whole-body balance state. CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism?Animation Keywords: Physically Based Animation, Motion Capture\n    \n    \n      \n        1 Introduction\n      \n      Creating virtual characters that realistically respond to physical perturbations in an interactive environment remains a challenging problem. Physics-based approaches generate motions consistent with the simulated environment, but are inherently difficult to design and tune to produce realistic motion across a wide variety of scenarios. For many interactive applications, such as video games, a simpler and more widely used approach is to kinematically control a character but mimic her dynamic reactions by playing back a pre-scripted motion in response to predefined stimuli. The responsiveness of a character thus largely depends on the quality and scope of the pre-scripted motions and the types of interactions allowed. Consequently, most applications only focus on perturbations that have a large impact on the character since it is virtually impossible to predefine all possible perturbations and animated responses. In contrast to large perturbations that often force the character to re-plan her high-level behaviors or interfere with the balance state, the character is more likely to encounter small perturbations that disrupt motion patterns momentarily but do not change the course\n      ? email: {yuting,karenliu}@cc.gatech.edu\n      \n        ACM Reference Format\n      \n      Ye, Y., Liu, C. 2008. Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates. ACM Trans. Graph. 27, 5, Article 112 (December 2008), 5 pages. DOI = 10.1145/1409060.1409065 http://doi.acm.org/10.1145/1409060.1409065.\n      \n        Copyright Notice\n      \n      Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2008 ACM 0730-0301/2008/05-ART112 $5.00 DOI 10.1145/1409060.1409065 http://doi.acm.org/10.1145/1409060.1409065\n      \n        \n        Figure 1: Left: Responding to a moving platform. Right: Avoiding obstacles in the environment\n      \n      of her current action. This paper describes a technique that enhances a kinematically controlled character with a generic class of dynamic responses to small-scale perturbations ( Figure 1 ). Given an input motion, our technique can synthesize motions responding to external forces in arbitrary directions on different body parts at any moment in time, without additional data or any modification to the underlying motion synthesis engine. Although we focus on small-scale perturbations that mainly affect the upper body motion, our technique can be integrated seamlessly with any technique that produces balanced lower body motion in the presence of large perturbations. Our approach is motivated by the observation that less-controlled joint degrees of freedom (DOFs) are usually more compliant when perturbed. If we are able to identify those compliant DOFs, we can apply a hybrid method that only considers dynamics in the compliant DOFs and kinematically controls the rest of the character. Instead of determining these DOFs by heuristics and hand-tuning their physical parameters, we use Principle Component Analysis (PCA) to define a new set of coordinates, ranked by the level of joint actuations in the input motion. Our method provides a more principled way to identify the less actuated coordinates (corresponding to eigenvalues close to zero) specific to each input motion sequence. We denote those DOFs as near-unactuated coordinates. To synthesize the input motion under perturbations, we enforce the dynamic equations of motion only in the near-unactuated coordinates while kinematically maintaining the original joint trajectories. Because the near-unactuated coordinates use very little internal torques in the input motion, enforcing the dynamic equations with zero internal actuation does not visually modify the input motion when there is no external perturbation. When the character is perturbed, however, the near-unactuated coordinates will compliantly react to the external force while the actuated coordinates will attempt to maintain the input joint positions. Because the lower body motion is typically less compliant and the internal joint torques cannot be obtained without accurate measure of contact forces, our technique only considers the dynamics of the upper body motion. We modify the lower body motion with a simple kinematic method based on the perturbation force. Enforcing dynamic constraints in the near-unactuated coordinates leads to two main advantages. First, the responsive motion varies due to different activities, styles, and individuals. This is because each motion and perturbation results in a unique response based on the specific joint torque usage in the input motion. Second, our formulation bypasses the problem of active body control. The generalized coordinates in our parameterization are not aligned with the mechanical joint space, but rather aligned with a more meaningful actuation space derived from the input motion. By choosing the appropriate coordinates to enforce the equations of motion, our approach can create physically responsive motion based on kinematic pose control without explicitly computing the joint actuations. In practice, our technique can be adapted transparently to any kinematically controlled framework without the aid of a forward simulator or additional motion data. We demonstrate the simplicity and robustness of our approach by showing a wide range of input motions with arbitrary perturbations. Our results show that realistic recovery motion emerges as a consequence of the interaction of the kinematic and the dynamic control. For example, the character sticks her arms out to recover from a large push. We believe this behavior is due to the fact that the objective function must pull the joints back to the original trajectories without using any internal torques in the near-unactuated coordinates.\n      ACM Transactions on Graphics, Vol. 27, No. 5, Article 112, Publication date: December 2008.\n      112:2 ? Y. Ye et al.\n      \n        2 Related work\n        Synthesizing responsive character animation is an important research topic with a broad range of applications. Researchers have explored different approaches to building physics-based controllers guided by kinematically specified motion data. Various tracking controllers have been applied to produce responsive movements under physical perturbations, including upper body motions [Zordan and Hodgins 2002; Yin et al. 2003], manipulation tasks [Abe and Popovi? 2006], standing motions [Kokkevis et al. 1996; Abe et al. 2007] or cyclic biped motions [Sok et al. 2007; Yin et al. 2007]. With the proper physical parameters for the controllers, these methods can generate realistic reactive motions consistent with the simulated virtual world. However, many of these methods require fine tuning of physical parameters or expensive pre-computation specific to the target motion and skeletal model. da Silva et al. [2008] introduced a systematic method to derive a balance controller tailored to the input motion, reducing the effort on parameter tuning. Our method also produces responsive motion that preserves the ?style? of the input motion. However, our approach does not involve any active body control, thereby no physical parameter tuning is required. Furthermore, we focus on the type of perturbations that significantly disrupt the upper body poses and dynamics, but have limited effect on the whole-body balance. Kinematically controlled character animation is more preferable in many online applications because it is easier to implement and provides precise user controllability. To create dynamic responses under external impacts, Komura et al. [2004; 2005] directly changed the motion with respect to the change of momentum of a biped motion. Oshita and Makinouchi [2001] modified the joint acceleration based on dynamic control of balance and comfort. To combine the advantages of kinematics motion and the physical simulation, many researchers have also proposed the idea of switching between dynamic simulation and motion capture data whenever necessary [Zordan et al. 2005; Shapiro et al. 2003; Mandel 2004]. In particular, Zordan et al. proposed a framework that uses minimal simulation interval after the impact and relies on the motion capture alone when perturbations are not presented. Our method also takes a hybrid approach to synthesis of responsive motions. However, instead of dividing the kinematic and dynamic control in the time domain, we divide them spatially in a transformed space, spanned by a set of basis representing the joint actuation in the original motion. Directly applying motion capture data produces natural human mo tions with rich details. Many techniques have successfully demonstrated that pre-recorded data can be adapted to new situations in response to online user control [Treuille et al. 2007; McCann and Pollard 2007; Cooper et al. 2007; Shin and Oh 2006]. A few methods extended data-driven approaches to synthesizing responsive motions, such as balance recovery against external forces [Arikan et al. 2005; Yin et al. 2005]. These methods collect a set of specific interactions in advance, and procedurally generate small deformations from the recorded motions to respond to predefined user interactions. Our method synthesizes responsive motions via dynamic constraints instead of motion blending, thereby completely removing the dependency on a motion database. Moreover, we allow for direct kinematics control with additional objectives. PCA has been frequently used to process motion data for applications in computer graphics, robotics, and computer vision. In computer animation, PCA is typically used to reduce the dimensionality of the configuration space for motion blending, recognition, or modeling [Brand and Hertzmann 2000; Jenkins and Matari? 2002; Safonova et al. 2004; Barbic et al. 2004; Chai and Hodgins 2007]. Biomechanics researchers have applied dimension reduction techniques to the muscle activation data measured from behavioral experiments [Tresch et al. 2006; Ting 2007; Alexandrov et al. 2005]. Ting suggested that a limited set of muscle synergies, defined as low-dimensional modules formed by muscles activated in synchrony, are used to control the center of mass after postural perturbations. Our method is inspired by Ting?s work in that we formulate the dynamic equations in the space of muscle synergies, rather than the space of joint configurations. However, we do not use PCA as a tool for dimension reduction. We only apply PCA for identifying the principle components corresponding to lower eigenvalues because these principle components represent the dimensions of the motion where active body control does not play an important role.\n      \n      \n        3 Overview of approach\n        Our entire algorithm can be described in three simple steps. 1. Given an input motion sequence M, apply the inverse dynamics method to obtain the internal joint torques U on the upper body. 2. Apply PCA on U to obtain a set of eigenvectors E. Define a set of near-unactuated coordinates E ? as a subset of E with k smallest corresponding eigenvalues. 3. Formulate a constrained optimization at each frame to solve for a pose that satisfies the equations of motion in E, ? while maintaining the original motion M.\n      \n      \n        4 Implementation\n        We represent the character?s skeleton as a transformation hierarchy of 18 body nodes with 24 DOFs on the upper body, q u , and 12 DOFs on the lower body, q l . The global translation and orientation are represented by six DOFs, q r , at the root of the hierarchy.\n        \n          4.1 Preprocessing\n          In the preprocessing step, we perform the inverse dynamics method and PCA on the input motion to identify the near-unactuated coordinates. One cycle from the input motion is manually selected for preprocessing.  To compute the joint torques of the input motion, we express La-\n          ACM Transactions on Graphics, Vol. 27, No. 5, Article 112, Publication date: December 2008.\n          Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates ? 112:3\n          grange?s equation of motion at each joint DOF q j as:\n          \n            1\n            dt d ? ? q  ? L j ? ? ? q L j = u j + J T j f\n          \n          where L is the Lagrangian of the dynamic system, u j is the internal torque in q j , and J j is the j th column of Jacobian matrix J which projects the external force f onto q j . In the absence of external forces applied on the upper body, we use Equation (1) to solve for the internal joint torques u n on the upper body at each frame n, forming a joint torque basis U = [u 1 , u 2 , ? ? ? , u N ] ? R 24?N , where N is the number of frames in the selected input motion cycle. Performing PCA on U yields eigenvectors E = [e 1 , e 2 , ? ? ? , e 24 ], ranked from the largest corresponding eigenvalue to the smallest. We divide E into two sets, E = [ E ? E]: ? E ? contains the first 24 ? k eigenvectors and E ? contains the rest of the eigenvectors with k smallest corresponding eigenvalues. We then define E ? as the set of near-unactuated coordinates. In our implementation, k is set to 10 for all the examples except for the Tai Chi motion.\n        \n        \n          4.2 Optimization ?t =\n          We discretize time domain into intervals of 1/60s as in the input motion. At each time step, we solve for upper body joint angles q u of the next interval n + 1 by formulating a constrained optimization. We use dynamic constraints C D to ensure that the nearunactuated coordinates have zero internal actuation at all times.\n          \n            2\n            C ? E ? T u n (q, q,  ? q,  ? f) = 0\n          \n          \n            2\n            D\n          \n          where the internal joint torques on the upper body u, computed via Equation (1), are expressed as a function of q, q,  ? q  ? and f. Backward differencing is used to compute the joint velocity: q  ? n ? ?t 1 (q n ? q n?1 ). Joint acceleration is computed by central differencing as: q  ? n ? ?t 1 2 (q n+1 ? 2q n + q n?1 ). If there is no perturbation (f = 0), the original motion is close to satisfying C D . When a perturbation occurs (f = 0), however, the character must adjust her motion to maintain C D = 0. We use a spring-like objective to track the input motion M = (m 1 , m 2 ? ? ? , m N ) and a damping objective to model the dissipation in the dynamic system:\n          \n            3\n            G p = q u n+1 ? m n+1 u\n          \n          \n            4\n            G v = ?t 1 (q n+1 u ? q u n )\n          \n          When human is perturbed unexpectedly, there is typically a delay between the perturbation and muscle activation due to the latency in sensory feedback [Miall et al. 1985; Georgopoulos et al. 1981]. The delay on arm movement due to the visual sensory feedback usually ranges from 150-250 ms. We incorporate this delay by minimizing the torque change for 200 ms after the perturbation in the highly actuated coordinates E. ?\n          \n            5\n            G = 1 E ? T (u n ? u n?1 )\n          \n          \n            5\n            u ?t\n          \n          In summary, we formulate the following optimization at each time step to solve for upper body motion: argmin w 1 ? G p + w 2 ? G v + w 3 ? G u subject to C D = 0 q n+1 u (6) where operator ? denotes the element-wise multiplication of two vectors. In all our experiments, we set every element of w 1 to 200,  the three elements of w 2 corresponding to the spine ball joint DOFs to 30, and the rest of the elements in w 2 to 10. All the elements in w 3 are set to 30 1 at the time of the perturbation and then smoothly decreased to zero in 200 ms. These weights are chosen such that they scale the objectivs to a comparable level as well reflect their relative importance. The values do not depend on the input motion or the skeletal model.\n        \n        \n          4.3 Synthesis of lower body motion\n          Although our method focuses on the upper body response, we formulate a simple computation for the root and lower body motion when the character is perturbed. Since our method does not model the ground contact and friction forces, the impact of the perturbation on the root can simply be modeled as an impulse, proportional to the external force f:\n          \n            7\n            ?t q  ? n+1 r = q  ? r n + m J r T f\n          \n          where m is the total mass of the character and J r consists of the columns of J corresponding to q r . If the root movement causes footskating or penetration of the ground, we apply a simple inverse kinematics method on the lower body to fix the foot contacts.\n        \n      \n      \n        5 Results\n        We applied our method to a variety of cyclic motions with different styles performed by different subjects. One complete cycle of each motion is sufficient to compute the near-unacutated coordinates for the upper body. The simulation runs at 20 frames per second on a single core of a 2.8 GHz Intel Core 2 Duo processor. We use SNOPT [Gill et al. 1996] to solve the optimization problem at each time step. Our results reveal that dynamic constraints in the near-unactuated coordinates produce compliant responses to unexpected perturbations and coordinated recovery motions customized to the input motion. All the examples were generated using an identical set of weights described in Section 4. Experiments show that a wide range of weights produce similar results. Please see the supplemental video for all the examples described below.  Eigenvector analysis To demonstrate the importance of the joint actuation space, we conducted several experiments of a normal walk with different choices of coordinates in which dynamic constraints are enforced. We first simulated the same input motion with different values of k, the number of dynamic constraints in the nearunactuated coordinates. The character appears more responsive as the number of dynamic constraints increases. However, the character is not able to completely recover from a perturbation when there are more than 12 dynamic constraints. When the number of dynamic constraints increases to 16, the character simply fails to track the input motion. The second experiment simulated the motion with dynamic constraints in the coordinates corresponding to higher eigenvalues. The result shows that the character is not able to maintain the original motion without actuations in those coordinates. Perturbation Our first experiment applied the same external force on different body parts of a 1.7m, 80kg male character during different phases of a normal walking sequence. The results show that the same push incurs a larger response during the single support than the double support. Moreover, the character exhibits more stability when the push is applied on the same side of the supporting leg. When pushed on the arms, the character reacts more compliantly than when pushed on the head or shoulder ( Figure 2 ). The second experiment tested the effect of different external force directions. The character has a harder time recovering from a backward push than a forward one, indicating that his torso actuation is asymmetric along the sagittal direction. In addition to producing highly coordinated reactions, our method also preserves individual styles. We demonstrated that the large-scale arm movement of a female character (1.5m, 40kg) is preserved in her reactive motions. We scaled the magnitude of the external forces proportionally to the female subject?s weight. Our method also allows the user to interact with the character by perturbing the root movement. To illustrate this, we simulated the reaction of the character stepping on a fast moving platform. As the root accelerates abruptly, the character?s upper body reacts passively and gradually recovers to the original motion pattern. Style The coordinates in the actuation space encode muscle usage and coordination specific to the input motion. Consequently, each motion sequence reacts to the unexpected perturbations with a unique style. We applied the same set of external forces to normal walk, backward walk, and sneaky walk performed by the same male character ( Figure 3 ). In comparison to other motions, the normal walk exhibits higher coordination among the upper body as it counteracts the disturbance using the torso and both arms simultaneously. The backward walking motion exhibits higher stability against a forward push but responses compliantly to a backward push. In the sneaky walk, the character maintains a more stable posture with the center of mass position lower than other motions. The results show that the same amount of force induces smaller responses on a sneaky walk. To compare the actuation among styles of individuals, we extracted the near unactuated coordinates of one individual performing a normal walk, and applied them to simulate another individual?s normal walk under perturbations. The results show that plausible reactive motions can be generated only when the two individuals have similar weight and height. We also conducted similar experiments for different action styles. The actuation of a sneaky walk reproduces a normal walk faithfully without disturbances, but generates unrealistic response when perturbed. Additional objectives Our formulation allows the animator to include additional objectives to enforce kinematic properties of the input motion. For example, we captured a walking sequence with the subject holding a cup in his right hand. During motion synthesis, an additional objective was added to keep the cup in an upright orientation. The asymmetrical muscle usage in the left and the right arms results in many interesting behaviors. When the character is pushed on the right arm, he maintains the orientation of the cup by rotating his torso to compensate for the movement of his right arm. In contrast, when the left arm is pushed by the same force, he stiffens his torso to reduce its movement and the impact on the right arm. Similarly, we added an objective that repels the character from the obstacles in the environment. If the character fails to completely avoid the obstacles, an external force is applied at the site of collision. Non-locomotion Our method also works on other periodic motions such as Tai Chi forms. Although the Tai Chi motion requires higher overall internal torques than other locomotion sequences (k = 5), the highly actuated coordinates mostly lie on the frontal plane. Moreover, the torque usage of arms in the Tai Chi motion are highly correlated. As a result, the character reacts to perturbations on the sagittal plane with both arms moving fluidly.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 112, Publication date: December 2008.\n        112:4 ? Y. Ye et al.\n        \n          \n          \n          \n        \n        (a) head (b) left shoulder (c) right arm\n        \n          Figure 2: Perturbations (indicated by red arrows) on different body parts\n        \n        \n          \n          \n          \n        \n        (a) forward walk (b) backward walk (c) sneaky walk\n        \n          Figure 3: Different styles recovering from a backward push\n        \n      \n      \n        6 Conclusion\n        We have presented a technique that synthesizes a generic class of dynamic responses to small-scale perturbations. By enforcing the dynamic constraints in the actuation space, the virtual character responds to arbitrary unexpected perturbations in a specific style encoded in the input motion. We have demonstrated the simplicity and robustness of our technique by showing a variety of examples generated with the same set of parameters. Our method can be readily augmented to any kinematic technique for character animation, such as motion graphs or motion blending, without modification to the existing implementation nor additional data. The main assumption of our approach is that only a small set of coordinated muscle groups are activated for performing rhythmic motion. Biomechanics researchers have also hypothesized that postural responses under perturbations can be activated by a few muscle synergies [Torres-Oviedo and Ting 2007]. Our results suggest that the same muscle synergies used for the input motion can also produce reasonable recovery motion from a small perturbation, thereby lending support to the hypothesis of muscle synergies as building blocks for constructing motor output patterns. However, our method is not as robust against steady perturbations. It is not clear whether the human body will switch to different muscle synergies when presented with sustained disturbances. Our technique focuses only on the upper body response and is not suitable for large perturbations that incur the loss of balance or changes of high-level behaviors. We anticipate that the technique can be applied to the whole body motion if we can accurately measure the ground contact forces. One possibility is to estimate the ground contact forces from motion capture data using the method described by Liu et al. [2005]. Another promising future direction is to combine our technique with sophisticated balance controllers that determine the lower body and root movements [da Silva et al. 2008; Yin et al. 2007]. Our approach uses inverse dynamics methods and principle component analysis, both of which are known to be sensitive to input noise. Fortunately, our method does not directly apply the computed torques to simulate motion but only uses them to derive the  eigenbasis of the input activity. We tested the robustness of our method against data noise by randomly selecting different cycles from the input motion. The results show that sporadic noise in the motion has negligible effect as long as the input motion contains sufficient clean data.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 112, Publication date: December 2008.\n        Animating Responsive Characters with Dynamic Constraints in Near-Unactuated Coordinates ? 112:5\n      \n      \n        Acknowledgements\n        The authors would like to thank Satoru Ishigaki, Wei Liu and Shuang Hao for their help in collecting motion data. This work is supported by NSF grant CCF-CISE 0742303.\n      \n      \n        References\n        \n          A BE , Y., AND P OPOVI C  ? , J. 2006. Interactive animation of dynamic manipulation. In Eurographics/SIGGRAPH Symposium on Computer Animation.\n          A BE , Y., DA S ILVA , M., AND P OPOVI C  ? , J. 2007. Multiobjective control with frictional contacts. In Eurographics/SIGGRAPH Symposium on Computer Animation, 249?258.\n          A LEXANDROV , A., F ROLOV , A., H ORAK , F., C ARLSON -K UHTA , P., AND P ARK , S. 2005. Feedback equilibrium control during human standing. Biological Cybernetics 93, 5, 309?322.\n          A RIKAN , O., O?B RIEN , J. F., AND F ORSYTH , D. A. 2005. Pushing people around. In Eurographics/SIGGRAPH Symposium on Computer Animation, 59?66.\n          B ARBIC , J., S AFONOVA , A., P AN , J.-Y., F ALOUTSOS , C., H OD GINS , J. K., AND P OLLARD , N. S. 2004. Segmenting motion capture data into distinct behaviors. In Graphics Interface, vol. 62, 185?194.\n          B RAND , M., AND H ERTZMANN , A. 2000. Style machines. In SIGGRAPH, 183?192.\n          C HAI , J., AND H ODGINS , J. K. 2007. Constraint-based motion optimization using a statistical dynamic model. ACM Trans. on Graphics (SIGGRAPH) 26, 3 (Aug.), 8.\n          C OOPER , S., H ERTZMANN , A., AND P OPOVI C  ? , Z. 2007. Active learning for real-time motion controllers. ACM Trans. on Graphics (SIGGRAPH) 26, 3 (Aug.), 5.\n          DA S ILVA , M., A BE , Y., AND P OPOVI C  ? , J. 2008. Interactive simuation of stylized human locomotion. ACM Trans. on Graphics (SIGGRAPH) 27, 3 (Aug.).\n          G EORGOPOULOS , A., K ALASKA , J., AND M ASSEY , J. 1981. Spatial trajectories and reaction times of aimed movements: Effects of practice, uncertainty and change in target location. Journal of Neurophysiology 46, 725?743.\n          G ILL , P., S AUNDERS , M., AND M URRAY , W. 1996. Snopt: An sqp algorithm for large-scale constrained optimization. Tech. Rep. NA 96-2, University of California, San Diego.\n          J ENKINS , O. C., AND M ATARI C  ? , M. J. 2002. Deriving action and behavior primitives from human motion data. In IEEE/RSJ, 2551?2556.\n          K OKKEVIS , E., M ETAXAS , D., AND B ADLER , N. I. 1996. Usercontrolled physics-based animation for articulated figures. In Computer Animation.\n          K OMURA , T., L EUNG , H., AND K UFFNER , J. 2004. Animating reactive motions for biped locomotion. In VRST ?04: Proceedings of the ACM symposium on Virtual reality software and technology, 32?40.\n          K OMURA , T., H O , E. S. L., AND L AU , R. W. H. 2005. Animating reactive motions using momentum-based inverse kinematics. Computer Animation and Virtual Worlds, 16, 213?223.\n          L IU , C. K., H ERTZMANN , A., AND P OPOVI C  ? , Z. 2005. Learning physics-based motion style with nonlinear inverse optimization. ACM Trans. on Graphics (SIGGRAPH) 24, 3 (July), 1071?1081.\n          M ANDEL , M., 2004. Versatile and interactive virtual humans: Hybrid use of data-driven and dynamics-based motion synthesis.\n          M C C ANN , J., AND P OLLARD , N. S. 2007. Responsive characters from motion fragments. ACM Trans. on Graphics (SIGGRAPH) 26, 3 (Aug.).\n          M IALL , R. C., W EIR , D. J., AND S TEIN , J. F. 1985. Visuomotor tracking with delayed visual feedback. Neuroccience 16, 3, 511? 520.\n          O SHITA , M., AND M AKINOUCHI , A. 2001. A dynamic motion control technique for human-like articulated figures. Computer Graphics Forum 20, 3, 192?202.\n          S AFONOVA , A., H ODGINS , J. K., AND P OLLARD , N. S. 2004. Synthesizing physically realistic human motion in lowdimensinal, behavior-specific spaces. ACM Trans. on Graphics (SIGGRAPH) 23, 3 (July), 514?521.\n          S HAPIRO , A., P IGHIN , F. H., AND F ALOUTSOS , P. 2003. Hybrid control for interactive character animation. In Pacific Graphics, 456?461.\n          S HIN , H. J., AND O H , H. S. 2006. Fat graphs: Constructing an interactive character with continuous controls. In Eurographics/SIGGRAPH Symposium on Computer Animation.\n          S OK , K. W., K IM , M., AND L EE , J. 2007. Simulating biped behaviors from human motion data. ACM Trans. on Graphics (SIGGRAPH) 26, 3 (Aug.), 107.\n          T ING , L. H. 2007. Dimensional reducation in sensorimotor systems. Computational Neuroscience 13, 2 (Apr.), 103?136.\n          T ORRES -O VIEDO , G., AND T ING , L. H. 2007. Muscle synergies characterizing human postural responses. Journal of Neurophysiology 98, 2144?2156.\n          T RESCH , M. C., C HEUNG , V. C., AND D ?A VELLA , A. 2006. Matrix factorization algorithms for the identification of muscle synergies: Evaluation on simulated and experimental data sets. Journal of Neurophysiology 95, 2199?2212.\n          T REUILLE , A., L EE , Y., AND P OPOVI C  ? , Z. 2007. Near-optimal character animation with continuous control. ACM Trans. on Graphics (SIGGRAPH) 26, 3 (Aug.).\n          Y IN , K., C LINE , M. B., AND P AI , D. K. 2003. Motion perturbation based on simple neuromotor control models. In Pacific Graphics.\n          Y IN , K., P AI , D. K., AND VAN DE P ANNE , M. 2005. Data-driven interactive balancing behaviors. In Pacific Graphics.\n          Y IN , K., L OKEN , K., AND VAN DE P ANNE , M. 2007. Simbicon: simple biped locomotion control. ACM Trans. on Graphics (SIGGRAPH) 26, 3, 105?115.\n          Z ORDAN , V. B., AND H ODGINS , J. K. 2002. Motion capture-driven simulations that hit and react. In Eurographics/SIGGRAPH Symposium on Computer Animation, 89?96.\n          Z ORDAN , V. B., M AJKOWSKA , A., C HIU , B., AND F AST , M. 2005. Dynamic response for motion capture animation. ACM Trans. on Graphics (SIGGRAPH) 24, 3 (July), 697?701.\n        \n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 112, Publication date: December 2008.\n      \n    \n  ",
  "resources" : [ ]
}