{
  "uri" : "sig2008a-a116-kopf_PROC.xml",
  "url" : "/Users/cbadenes/Documents/OEG/Projects/DrInventor/datasets/acm-siggraph-2006-2014-upf/sig2008a/a116-kopf_PROC.xml",
  "source" : {
    "name" : "SIGGRAPH",
    "uri" : "http://drinventor/SIGGRAPH",
    "url" : "http://drinventor/SIGGRAPH",
    "protocol" : "http"
  },
  "metainformation" : {
    "title" : "Deep Photo: Model-Based Photograph Enhancement and Viewing",
    "published" : "2008",
    "format" : "pdf",
    "language" : "en",
    "rights" : "GPLv2",
    "description" : "",
    "creators" : [ {
      "uri" : "http://drinventor/Johannes-Kopf",
      "name" : "Johannes",
      "surname" : "Kopf"
    }, {
      "uri" : "http://drinventor/Boris-Neubert",
      "name" : "Boris",
      "surname" : "Neubert"
    }, {
      "uri" : "http://drinventor/Billy-Chen",
      "name" : "Billy",
      "surname" : "Chen"
    }, {
      "uri" : "http://drinventor/Michael F.-Cohen",
      "name" : "Michael F.",
      "surname" : "Cohen"
    }, {
      "uri" : "http://drinventor/Daniel-Cohen-Or",
      "name" : "Daniel",
      "surname" : "Cohen-Or"
    }, {
      "uri" : "http://drinventor/Oliver-Deussen",
      "name" : "Oliver",
      "surname" : "Deussen"
    }, {
      "uri" : "http://drinventor/Matthew-Uyttendaele",
      "name" : "Matthew",
      "surname" : "Uyttendaele"
    }, {
      "uri" : "http://drinventor/Dani-Lischinski",
      "name" : "Dani",
      "surname" : "Lischinski"
    } ]
  },
  "bagOfWords" : [ "efc8617114bb30764aab5424e0445e88edabca3163f2f644594fff5d2b6bc4ce", "ouj", "10.1145", "1409060.1409069", "name", "identification", "possible", "deep", "Photo", "model-based", "Photograph", "enhancement", "view", "Johannes", "Kopf", "Boris", "Neubert", "Billy", "Chen", "University", "Konstanz", "University", "Konstanz", "Microsoft", "Oliver", "Deussen", "Matt", "Uyttendaele", "University", "Konstanz", "Microsoft", "Research", "Original", "Dehazed", "Figure", "some", "application", "Deep", "Photo", "system", "paper", "we", "introduce", "novel", "system", "browse", "enhance", "manipulate", "casual", "outdoor", "photograph", "combine", "they", "already", "exist", "georeferenced", "digital", "terrain", "urban", "model", "simple", "interactive", "registration", "process", "use", "align", "photograph", "model", "once", "photograph", "model", "have", "be", "register", "abundance", "information", "depth", "texture", "GIS", "datum", "become", "immediately", "available", "we", "system", "information", "turn", "enable", "variety", "operation", "range", "from", "dehaze", "relight", "photograph", "novel", "view", "synthesis", "overlay", "geographic", "information", "we", "describe", "implementation", "number", "application", "discuss", "possible", "extension", "we", "result", "show", "augment", "photograph", "already", "available", "3d", "model", "world", "support", "wide", "variety", "new", "way", "we", "experience", "interact", "we", "everyday", "snapshot", "keyword", "image-based", "modeling", "image-based", "rendering", "image", "completion", "dehazing", "relighting", "photo", "browse", "ACM", "Reference", "Format", "Kopf", "J.", "Neubert", "B.", "Chen", "B.", "Cohen", "M.", "Cohen-Or", "D.", "Deussen", "O.", "Uyttendaele", "M.", "Lischinski", "D.", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "ACM", "Trans", "graph", "27", "Article", "116", "-lrb-", "December", "2008", "-rrb-", "10", "page", "dous", "10.1145", "1409060.1409069", "http://doi.acm.org/10.1145/1409060.1409069", "copyright", "Notice", "permission", "make", "digital", "hard", "copy", "part", "all", "work", "personal", "classroom", "use", "grant", "without", "fee", "provide", "copy", "make", "distribute", "profit", "direct", "commercial", "advantage", "copy", "show", "notice", "fus", "rst", "page", "initial", "screen", "display", "along", "full", "citation", "copyright", "component", "work", "own", "other", "than", "ACM", "must", "honor", "abstract", "credit", "permit", "copy", "otherwise", "republish", "post", "server", "redistribute", "list", "use", "any", "component", "work", "other", "work", "require", "prior", "specific", "permission", "and/or", "fee", "permission", "may", "request", "from", "Publications", "Dept.", "ACM", "Inc.", "Penn", "Plaza", "Suite", "701", "New", "York", "NY", "10121-0701", "fax", "+1", "-lrb-212-rrb-Â 869-0481", "permissions@acm.org", "2008", "ACM", "0730-0301/2008", "05-art116", "5.00", "DOI", "10.1145", "1409060.1409069", "http://doi.acm.org/10.1145/1409060.1409069", "Michael", "Cohen", "Daniel", "Cohen-Or", "Microsoft", "Research", "Tel", "Aviv", "University", "Dani", "Lischinski", "Hebrew", "University", "Relighted", "Annotated", "introduction", "despite", "increase", "ubiquity", "digital", "photography", "metaphor", "we", "use", "browse", "interact", "we", "photograph", "have", "change", "much", "few", "exception", "we", "still", "treat", "they", "2d", "entity", "whether", "display", "computer", "monitor", "print", "hard", "copy", "well", "understand", "augment", "photograph", "depth", "can", "open", "way", "variety", "new", "exciting", "manipulation", "however", "infer", "depth", "information", "from", "single", "image", "capture", "ordinary", "camera", "still", "longstanding", "unsolved", "problem", "computer", "vision", "luckily", "we", "witness", "great", "increase", "number", "accuracy", "geometric", "model", "world", "include", "terrain", "building", "register", "photograph", "model", "depth", "become", "available", "each", "pixel", "Deep", "Photo", "system", "describe", "paper", "consist", "number", "application", "afford", "newfound", "depth", "value", "well", "many", "other", "type", "information", "typically", "associate", "model", "deep", "Photo", "motivate", "several", "recent", "trend", "now", "reach", "critical", "mass", "first", "trend", "geo-tagged", "photo", "many", "photo", "share", "web", "site", "now", "enable", "user", "manually", "add", "location", "information", "photo", "some", "digital", "camera", "RICOH", "Caplio", "500se", "Nokia", "N95", "feature", "built-in", "GPS", "allow", "automatic", "location", "tagging", "also", "number", "manufacturer", "offer", "small", "gp", "unit", "allow", "photo", "easily", "geo-tag", "software", "synchronize", "gp", "log", "photo", "addition", "location", "tag", "can", "enhance", "digital", "compass", "able", "measure", "orientation", "-lrb-", "tilt", "head", "-rrb-", "camera", "expect", "future", "more", "camera", "have", "functionality", "most", "photograph", "geo-tagged", "second", "trend", "widespread", "availability", "accurate", "digital", "terrain", "model", "well", "detailed", "urban", "model", "thanks", "commercial", "project", "Google", "Earth", "Microsoft?s", "Virtual", "Earth", "both", "quantity", "quality", "model", "rapidly", "increase", "public", "domain", "NASA", "provide", "detailed", "satellite", "imagery", "-lrb-", "e.g.", "landsat", "-lsb-", "NASA", "2008a", "-rsb-", "-rrb-", "elevation", "model", "-lrb-", "e.g.", "Shuttle", "Radar", "Topography", "Mission", "-lsb-", "NASA", "2008b", "-rsb-", "-rrb-", "also", "number", "city", "around", "world", "create", "detailed", "3d", "model", "cityscape", "-lrb-", "e.g.", "Berlin", "3d", "-rrb-", "combination", "geo-tagging", "availability", "fairly", "accurate", "3d", "model", "allow", "many", "photograph", "precisely", "georegister", "we", "envision", "near", "future", "automatic", "georegistration", "available", "online", "service", "thus", "although", "we", "briefly", "describe", "simple", "interactive", "geo-registration", "technique", "we", "currently", "employ", "emphasis", "paper", "application", "enable", "include", "dehazing", "-lrb-", "add", "haze", "-rrb-", "image", "approximate", "change", "lighting", "novel", "view", "synthesis", "expand", "field", "view", "add", "new", "object", "image", "integration", "GIS", "datum", "photo", "browser", "we", "goal", "work", "have", "be", "enable", "application", "single", "outdoor", "image", "take", "casual", "manner", "without", "require", "any", "special", "equipment", "any", "particular", "setup", "thus", "we", "system", "applicable", "large", "body", "exist", "outdoor", "photograph", "so", "long", "we", "know", "rough", "location", "where", "each", "photograph", "take", "we", "choose", "New", "York", "City", "Yosemite", "National", "Park", "two", "many", "location", "around", "world", "which", "detailed", "textured", "model", "already", "available", "we", "demonstrate", "we", "approach", "combine", "number", "photograph", "-lrb-", "obtain", "from", "flickr", "tm", "-rrb-", "model", "should", "note", "while", "model", "we", "use", "fairly", "detail", "still", "far", "cry", "from", "degree", "accuracy", "level", "detail", "one", "would", "need", "order", "use", "model", "directly", "render", "photographic", "image", "thus", "one", "we", "challenge", "work", "have", "be", "understand", "how", "best", "leverage", "3d", "information", "afford", "use", "model", "while", "same", "time", "preserve", "photographic", "quality", "original", "image", "addition", "explore", "application", "list", "above", "paper", "also", "make", "number", "specific", "technical", "contribution", "two", "main", "one", "new", "data-driven", "stable", "dehazing", "procedure", "new", "model-guided", "layered", "depth", "image", "completion", "technique", "novel", "view", "synthesis", "before", "continue", "we", "should", "note", "some", "limitation", "Deep", "Photo", "its", "current", "form", "example", "we", "show", "outdoor", "scene", "we", "count", "available", "model", "describe", "distant", "static", "geometry", "scene", "we", "can", "expect", "have", "access", "geometry", "nearby", "-lrb-", "possibly", "dynamic", "-rrb-", "foreground", "object", "people", "car", "tree", "etc.", "we", "current", "implementation", "foreground", "object", "mat", "out", "before", "combine", "rest", "photograph", "model", "may", "composit", "back", "onto", "photograph", "later", "stage", "so", "some", "image", "user", "must", "spend", "some", "time", "interactive", "matting", "fidelity", "some", "we", "manipulation", "foreground", "may", "reduce", "say", "we", "expect", "kind", "application", "we", "demonstrate", "scale", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "116:2", "J.", "Kopf", "et", "al.", "Yosemite", "we", "use", "elevation", "datum", "from", "Shuttle", "Radar", "Topography", "Mission", "-lsb-", "NASA", "2008b", "-rsb-", "landsat", "imagery", "-lsb-", "NASA", "2008a", "-rsb-", "datum", "available", "entire", "Earth", "model", "similar", "NYC", "currently", "available", "dozen", "city", "include", "any", "improvement", "automatic", "computer", "vision", "algorithm", "depth", "acquisition", "technology", "related", "work", "we", "system", "touch", "upon", "quite", "few", "distinct", "topic", "computer", "vision", "computer", "graphic", "thus", "comprehensive", "review", "all", "related", "work", "feasible", "due", "space", "constraint", "below", "we", "attempt", "provide", "some", "representative", "reference", "discuss", "detail", "only", "one", "most", "closely", "related", "we", "goal", "technique", "image-based", "modeling", "recent", "year", "much", "work", "have", "be", "do", "image-based", "modeling", "technique", "which", "create", "high", "quality", "3d", "model", "from", "photograph", "one", "example", "pioneering", "Fa", "ade", "system", "-lsb-", "Debevec", "et", "al.", "1996", "-rsb-", "design", "interactive", "modeling", "building", "from", "collection", "photograph", "other", "system", "use", "panoramic", "mosaic", "-lsb-", "Shum", "et", "al.", "1998", "-rsb-", "combine", "image", "range", "datum", "-lsb-", "stamo", "Allen", "2000", "-rsb-", "merge", "ground", "aerial", "view", "-lsb-", "fr?h", "Zakhor", "2003", "-rsb-", "name", "few", "any", "approach", "may", "use", "create", "kind", "textured", "3d", "model", "we", "use", "we", "system", "however", "work", "we", "concern", "creation", "model", "rather", "way", "which", "combination", "single", "photograph", "may", "useful", "casual", "digital", "photographer", "one", "might", "say", "rather", "than", "attempt", "automatically", "manually", "reconstruct", "model", "from", "single", "photo", "we", "exploit", "availability", "digital", "terrain", "urban", "model", "effectively", "replace", "difficult", "3d", "reconstruction/modeling", "process", "much", "simpler", "registration", "process", "recent", "research", "have", "show", "various", "challenging", "task", "image", "completion", "insertion", "object", "photograph", "-lsb-", "Hays", "Efros", "2007", "Lalonde", "et", "al.", "2007", "-rsb-", "can", "greatly", "benefit", "from", "availability", "enormous", "amount", "photograph", "have", "already", "be", "capture", "philosophy", "behind", "we", "work", "somewhat", "similar", "we", "attempt", "leverage", "large", "amount", "textured", "geometric", "model", "have", "already", "be", "create", "unlike", "image", "database", "which", "consist", "mostly", "unrelated", "item", "geometric", "model", "we", "use", "all", "anchor", "world", "surround", "we", "dehaze", "weather", "other", "atmospheric", "phenomenon", "haze", "greatly", "reduce", "visibility", "distant", "region", "image", "outdoor", "scene", "remove", "effect", "haze", "dehazing", "challenging", "problem", "because", "degree", "effect", "each", "pixel", "depend", "depth", "corresponding", "scene", "point", "some", "haze", "removal", "technique", "make", "use", "multiple", "image", "e.g.", "image", "take", "under", "different", "weather", "condition", "-lsb-", "Narasimhan", "Nayar", "2003a", "-rsb-", "different", "polarizer", "orientation", "-lsb-", "Schechner", "et", "al.", "2003", "-rsb-", "since", "we", "interested", "dehaze", "single", "image", "take", "without", "any", "special", "equipment", "method", "suitable", "we", "need", "several", "work", "attempt", "remove", "effect", "haze", "fog", "etc.", "from", "single", "image", "use", "some", "form", "depth", "information", "example", "Oakley", "Satherley", "-lsb-", "1998", "-rsb-", "dehaze", "aerial", "imagery", "use", "estimate", "terrain", "model", "however", "method", "involve", "estimate", "large", "number", "parameter", "quality", "report", "result", "unlikely", "satisfy", "today?s", "digital", "photography", "enthusiast", "Narasimhan", "Nayar", "-lsb-", "2003b", "-rsb-", "dehaze", "single", "image", "base", "rough", "depth", "approximation", "provide", "user", "derive", "from", "satellite", "orthophoto", "very", "latest", "dehazing", "method", "-lsb-", "fattal", "2008", "Tan", "2008", "-rsb-", "able", "dehaze", "single", "image", "make", "various", "assumption", "about", "color", "scene", "we", "work", "differ", "from", "previous", "single", "image", "dehaze", "method", "leverage", "availability", "more", "accurate", "3d", "model", "use", "novel", "data-driven", "dehaze", "procedure", "result", "we", "method", "capable", "effective", "stable", "high-quality", "contrast", "restoration", "even", "extremely", "distant", "region", "novel", "view", "synthesis", "have", "be", "long", "recognize", "add", "depth", "information", "photograph", "provide", "means", "alter", "viewpoint", "classic", "tour", "Picture", "system", "-lsb-", "Horry", "et", "al.", "1997", "-rsb-", "demonstrate", "fitting", "simple", "mesh", "scene", "sometimes", "enough", "enable", "compelling", "3d", "navigation", "experience", "subsequent", "papers", "Kang", "-lsb-", "1998", "-rsb-", "Criminisi", "et", "al.", "-lsb-", "2000", "-rsb-", "oh", "et", "al.", "-lsb-", "2001", "-rsb-", "Zhang", "et", "al.", "-lsb-", "2002", "-rsb-", "extend", "provide", "more", "sophisticated", "user-guided", "3d", "modelling", "technique", "more", "recently", "Hoiem", "et", "al.", "-lsb-", "2005", "-rsb-", "use", "machine", "learn", "technique", "order", "construct", "simple", "pop-up", "3d", "model", "completely", "automatically", "from", "single", "photograph", "system", "despite", "simplicity", "model", "3d", "experience", "can", "quite", "compelling", "work", "we", "use", "already", "available", "3d", "model", "order", "add", "depth", "photograph", "we", "present", "new", "model-guided", "image", "completion", "technique", "enable", "we", "expand", "field", "view", "perform", "high-quality", "novel", "view", "synthesis", "relighting", "number", "sophisticated", "relight", "system", "have", "be", "propose", "various", "researcher", "over", "year", "-lrb-", "e.g.", "-lsb-", "Yu", "Malik", "1998", "Yu", "et", "al.", "1999", "Loscos", "et", "al.", "2000", "Debevec", "et", "al.", "2000", "-rsb-", "-rrb-", "typically", "system", "make", "use", "highly", "accurate", "geometric", "model", "and/or", "collection", "photograph", "often", "take", "under", "different", "lighting", "condition", "give", "input", "often", "able", "predict", "appearance", "scene", "under", "novel", "lighting", "condition", "very", "high", "degree", "accuracy", "realism", "another", "alternative", "use", "time-lapse", "video", "sequence", "-lsb-", "Sunkavalli", "et", "al.", "2007", "-rsb-", "we", "case", "we", "assume", "availability", "geometric", "model", "have", "just", "one", "photograph", "work", "furthermore", "although", "model", "might", "detail", "typically", "quite", "far", "from", "perfect", "match", "photograph", "example", "tree", "cast", "shadow", "nearby", "building", "typically", "absent", "from", "we", "model", "thus", "we", "can", "hope", "correctly", "recover", "reflectance", "each", "pixel", "photograph", "which", "necessary", "order", "perform", "physically", "accurate", "relighting", "therefore", "work", "we", "propose", "very", "simple", "relighting", "approximation", "which", "nevertheless", "able", "produce", "fairly", "compelling", "result", "Photo", "browse", "also", "related", "Photo", "Tourism", "system", "-lsb-", "Snavely", "et", "al.", "2006", "-rsb-", "which", "enable", "browse", "explore", "large", "collection", "photograph", "certain", "location", "use", "3d", "interface", "browse", "experience", "we", "provide", "very", "different", "moreover", "contrast", "Photo", "Tourism", "we", "system", "require", "only", "single", "geo-tagged", "photograph", "make", "applicable", "even", "location", "without", "many", "available", "photo", "Photo", "Tourism", "system", "also", "demonstrate", "transfer", "annotation", "from", "one", "register", "photograph", "another", "Deep", "Photo", "photograph", "register", "model", "world", "make", "possible", "tap", "much", "richer", "source", "information", "work", "geo-referenced", "image", "once", "photo", "register", "geo-referenced", "datum", "map", "3d", "model", "plethora", "information", "become", "available", "example", "Cho", "-lsb-", "Cho", "2007", "-rsb-", "note", "absolute", "geo-location", "can", "assign", "individual", "pixel", "gi", "annotation", "building", "street", "name", "may", "project", "onto", "image", "plane", "deep", "Photo", "support", "similar", "labeling", "well", "several", "additional", "visualization", "contrast", "cho?s", "system", "do", "so", "dynamically", "context", "interactive", "photo", "browse", "application", "furthermore", "discuss", "earlier", "also", "enable", "variety", "other", "application", "addition", "enhance", "photo", "location", "also", "useful", "organize", "visualize", "photo", "collection", "system", "develop", "Toyama", "et", "al.", "-lsb-", "2003", "-rsb-", "enable", "user", "browse", "large", "collection", "geo-referenced", "photo", "2d", "map", "map", "serve", "both", "visualization", "device", "well", "way", "specify", "spatial", "query", "i.e.", "all", "photo", "within", "region", "contrast", "DeepPhoto", "focus", "enhance", "browse", "single", "photograph", "two", "system", "actually", "complementary", "one", "focus", "organize", "large", "photo", "collection", "other", "enhance", "view", "single", "photograph", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "116:3", "registration", "mat", "we", "assume", "photograph", "have", "be", "capture", "simple", "pinhole", "camera", "whose", "parameter", "consist", "position", "pose", "focal", "length", "-lrb-", "seven", "parameter", "total", "-rrb-", "register", "photograph", "3d", "geometric", "model", "scene", "suffice", "specify", "four", "more", "corresponding", "pair", "point", "-lsb-", "Gruen", "Huang", "2001", "-rsb-", "assume", "rough", "position", "from", "which", "photograph", "take", "available", "-lrb-", "either", "from", "geotag", "provide", "user", "-rrb-", "we", "able", "render", "model", "from", "roughly", "correct", "position", "let", "user", "specify", "sufficiently", "many", "correspondence", "recover", "parameter", "solve", "nonlinear", "system", "equation", "-lsb-", "nister", "Stewenius", "2007", "-rsb-", "detail", "user", "interface", "we", "registration", "system", "describe", "technical", "report", "-lsb-", "Chen", "et", "al.", "2008", "-rsb-", "image", "depict", "foreground", "object", "contain", "model", "we", "ask", "user", "matte", "out", "foreground", "application", "demonstrate", "paper", "matte", "do", "have", "too", "accurate", "so", "long", "conservative", "-lrb-", "i.e.", "all", "foreground", "pixel", "contain", "-rrb-", "we", "create", "matte", "Soft", "Scissors", "system", "-lsb-", "Wang", "et", "al.", "2007", "-rsb-", "process", "take", "about", "1-2", "minute", "per", "photo", "every", "result", "produce", "use", "matte", "we", "show", "matte", "next", "input", "photograph", "image", "enhancement", "many", "typical", "image", "we", "take", "spectacular", "often", "well", "know", "landscape", "cityscape", "unfortunately", "many", "case", "lighting", "condition", "weather", "optimal", "when", "photograph", "take", "result", "may", "dull", "hazy", "have", "sufficiently", "accurate", "match", "between", "photograph", "geometric", "model", "offer", "new", "possibility", "enhance", "photograph", "we", "able", "easily", "remove", "haze", "unwanted", "color", "shift", "experiment", "alternative", "lighting", "condition", "4.1", "dehaze", "atmospheric", "phenomenon", "haze", "fog", "can", "reduce", "visibility", "distant", "region", "image", "outdoor", "scene", "due", "atmospheric", "absorption", "scattering", "only", "part", "light", "reflect", "from", "distant", "object", "reach", "camera", "furthermore", "light", "mix", "airlight", "-lrb-", "scatter", "ambient", "light", "between", "object", "camera", "-rrb-", "thus", "distant", "object", "scene", "typically", "appear", "considerably", "lighter", "featureless", "compare", "nearby", "one", "depth", "each", "image", "pixel", "know", "theory", "should", "easy", "remove", "effect", "haze", "fitting", "analytical", "model", "-lrb-", "e.g.", "-lsb-", "McCartney", "1976", "Nayar", "Narasimhan", "1999", "-rsb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "here", "observe", "hazy", "intensity", "pixel", "original", "intensity", "reflect", "towards", "camera", "from", "corresponding", "scene", "point", "airlight", "-lrb-", "-rrb-", "exp", "-lrb-", "??", "-rrb-", "attenuation", "intensity", "function", "distance", "due", "outscattering", "thus", "after", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "116:4", "J.", "Kopf", "et", "al.", "Input", "Model", "texture", "final", "dehazed", "result", "figure", "dehazing", "note", "artifact", "model", "texture", "significant", "deviation", "estimate", "haze", "curve", "from", "exponential", "shape", "Input", "Dehazed", "Figure", "more", "dehaze", "example", "estimate", "parameter", "original", "intensity", "may", "recover", "invert", "model", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "point", "out", "Narasimhan", "Nayar", "-lsb-", "2003a", "-rsb-", "model", "assume", "single-scattering", "homogeneous", "athmosphere", "thus", "more", "suitable", "short", "range", "distance", "might", "fail", "correctly", "approximate", "attenuation", "scene", "point", "more", "than", "few", "kilometer", "away", "furthermore", "since", "exponential", "attenuation", "go", "quickly", "down", "zero", "noise", "might", "severely", "amplify", "distant", "area", "both", "artifact", "may", "observe", "inversion", "result", "Figure", "while", "reduce", "degree", "dehaze", "-lsb-", "Schechner", "et", "al.", "2003", "-rsb-", "regularization", "-lsb-", "schechner", "Averbuch", "2007", "Kaftory", "et", "al.", "2007", "-rsb-", "may", "use", "alleviate", "problem", "we", "approach", "estimate", "stable", "value", "haze", "curve", "-lrb-", "-rrb-", "directly", "from", "relationship", "between", "color", "photograph", "those", "model", "texture", "more", "specifically", "we", "compute", "curve", "-lrb-", "-rrb-", "airlight", "eq", "-lrb-", "-rrb-", "would", "map", "average", "color", "photograph", "corresponding", "average", "-lrb-", "color-corrected", "-rrb-", "model", "texture", "color", "note", "although", "we", "-lrb-", "-rrb-", "have", "same", "physical", "interprertation", "previous", "approach", "due", "we", "estimation", "process", "subject", "constraint", "physicially-based", "model", "since", "we", "estimate", "single", "curve", "represent", "possibly", "spatially", "0.8", "0.6", "intensity", "0.4", "0.2", "2000Â 4000Â 6000Â 8000", "Depth", "estimate", "haze", "curve", "-lrb-", "-rrb-", "Input", "Dehazed", "vary", "haze", "can", "also", "contain", "non-monotonicity", "all", "parameter", "estimate", "completely", "automatically", "robustness", "we", "operate", "average", "color", "over", "depth", "range", "each", "value", "we", "compute", "average", "model", "texture", "color", "-lrb-", "-rrb-", "all", "pixel", "whose", "depth", "-lsb-", "-rsb-", "well", "average", "hazy", "image", "color", "-lrb-", "-rrb-", "same", "pixel", "we", "implementation", "depth", "interval", "parameter", "set", "500", "meter", "all", "image", "we", "experiment", "averaging", "make", "we", "approach", "less", "sensitive", "model", "texture", "artifact", "registration", "stitching", "error", "bad", "pixel", "contain", "shadow", "cloud", "before", "explain", "detail", "we", "method", "we", "would", "like", "point", "out", "model", "texture", "typically", "have", "global", "color", "bias", "example", "Landsat", "use", "seven", "sensor", "whose", "spectral", "response", "differ", "from", "typical", "RGB", "camera", "sensor", "thus", "color", "result", "texture", "only", "approximation", "one", "would", "have", "be", "capture", "camera", "-lrb-", "see", "Figure", "-rrb-", "we", "correct", "color", "bias", "measure", "ratio", "between", "photo", "texture", "color", "foreground", "-lrb-", "each", "channel", "-rrb-", "use", "ratio", "correct", "color", "entire", "texture", "more", "precisely", "we", "compute", "global", "multiplicative", "correction", "vector", "lum", "-lrb-", "-rrb-", "lum", "-lrb-", "-rrb-", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "116:5", "Input", "Fattal?s", "result", "figure", "comparison", "other", "dehaze", "method", "second", "row", "show", "full-resolution", "zoom", "region", "indicate", "red", "rectangle", "input", "photo", "see", "supplementary", "material", "more", "comparison", "image", "where", "average", "-lrb-", "-rrb-", "similarly", "compute", "average", "model", "texture", "lum", "-lrb-", "-rrb-", "denote", "luminance", "color", "we", "set", "1600", "meter", "all", "we", "image", "now", "we", "ready", "explain", "how", "compute", "haze", "curve", "-lrb-", "-rrb-", "ignore", "moment", "physical", "interpretation", "-lrb-", "-rrb-", "note", "eq", "-lrb-", "-rrb-", "simply", "stretch", "intensity", "image", "around", "use", "scale", "coefficient", "-lrb-", "-rrb-", "we", "goal", "find", "-lrb-", "-rrb-", "would", "map", "hazy", "photo", "color", "-lrb-", "-rrb-", "color-corrected", "texture", "color", "-lrb-", "-rrb-", "substitute", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "eq", "-lrb-", "-rrb-", "we", "get", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "different", "choice", "result", "different", "scale", "curve", "-lrb-", "-rrb-", "we", "set", "since", "guarantee", "-lrb-", "-rrb-", "use", "would", "result", "larger", "value", "-lrb-", "-rrb-", "hence", "less", "contrast", "dehazed", "image", "use", "might", "prone", "instability", "figure", "show", "-lrb-", "-rrb-", "curve", "estimate", "describe", "above", "recover", "haze", "curve", "-lrb-", "-rrb-", "allow", "effectively", "restore", "contrast", "photo", "however", "color", "background", "might", "undergo", "color", "shift", "we", "compensate", "adjust", "while", "keep", "-lrb-", "-rrb-", "fix", "after", "change", "dehaze", "preserve", "color", "photo", "background", "adjust", "we", "first", "compute", "average", "background", "color", "photo", "average", "-lrb-", "-rrb-", "similarly", "compute", "average", "model", "texture", "we", "set", "5000m", "all", "we", "image", "color", "background", "preserve", "ratio", "-lrb-", "-rrb-", "have", "same", "value", "every", "color", "channel", "thus", "we", "rewrite", "eq", "-lrb-", "-rrb-", "obtain", "set", "max", "-lrb-", "red", "red", "green", "green", "blue", "blue", "-rrb-", "particular", "choice", "result", "maximum", "guarantee", "finally", "we", "use", "eq", "-lrb-", "-rrb-", "recovered", "-lrb-", "-rrb-", "adjust", "dehaze", "photograph", "Inversion", "result", "we", "result", "figure", "show", "various", "image", "dehaze", "we", "method", "Figure", "compare", "we", "method", "other", "approach", "comparison", "we", "focus", "method", "applicable", "we", "context", "work", "single", "image", "only", "fattal?s", "method", "-lsb-", "2008", "-rsb-", "dehaze", "image", "nicely", "up", "certain", "distance", "-lrb-", "particularly", "consider", "method", "do", "require", "any", "input", "addition", "image", "itself", "-rrb-", "unable", "effectively", "dehaze", "more", "distant", "part", "closer", "horizon", "Inversion", "result", "obtain", "via", "eq", "-lrb-", "-rrb-", "exponential", "haze", "curve", "how", "dehaze", "perform", "number", "papers", "e.g.", "-lsb-", "Schechner", "et", "al.", "2003", "Narasimhan", "Nayar", "2003a", "Narasimhan", "Nayar", "2003b", "-rsb-", "here", "we", "use", "we", "accurate", "depth", "map", "instead", "use", "multiple", "image", "user-provided", "depth", "approximation", "airlight", "color", "set", "sky", "color", "near", "horizon", "optical", "depth", "adjust", "manually", "result", "suffer", "from", "amplify", "noise", "distance", "break", "down", "next", "horizon", "contrast", "we", "result", "manage", "remove", "more", "haze", "than", "two", "other", "approach", "while", "preserve", "natural", "color", "input", "photo", "note", "practice", "one", "might", "want", "remove", "haze", "completely", "we", "have", "do", "because", "haze", "sometimes", "provide", "perceptually", "significant", "depth", "cue", "also", "dehaze", "typically", "amplify", "some", "noise", "region", "where", "little", "visible", "detail", "remain", "original", "image", "still", "almost", "every", "image", "benefit", "from", "some", "degree", "dehazing", "have", "obtain", "model", "haze", "photograph", "we", "can", "insert", "new", "object", "scene", "more", "seamless", "fashion", "apply", "model", "object", "well", "-lrb-", "accordance", "depth", "suppose", "-rrb-", "do", "simply", "invert", "eq", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "demonstrate", "companion", "video", "4.2", "Relighting", "one", "can", "underestimate", "importance", "role", "lighting", "play", "creation", "interesting", "photograph", "particular", "landscape", "photography", "vast", "majority", "breathtaking", "photograph", "take", "during", "golden", "hour", "after", "sunrise", "before", "sunset", "-lsb-", "Reichmann", "2001", "-rsb-", "unfortunately", "most", "we", "outdoor", "snapshot", "take", "under", "rather", "boring", "lighting", "Deep", "Photo", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "116:6", "J.", "Kopf", "et", "al.", "input", "relight", "Relighted", "Input", "Relighted", "Figure", "relighting", "result", "produce", "we", "system", "original", "Relighted", "Lit", "Model", "Figure", "comparison", "between", "original", "photo", "its", "relight", "version", "rendering", "underlie", "model", "under", "same", "illumination", "possible", "modify", "lighting", "photograph", "approximate", "what", "scene", "might", "look", "like", "another", "time", "day", "explain", "earlier", "we", "goal", "work", "single", "image", "augment", "detailed", "yet", "completely", "accurate", "geometric", "model", "scene", "setup", "do", "allow", "we", "correctly", "recover", "reflectance", "each", "pixel", "thus", "we", "use", "following", "simple", "workflow", "which", "only", "approximate", "appearance", "lighting", "change", "scene", "we", "begin", "dehaze", "image", "describe", "previous", "section", "modulate", "color", "use", "lightmap", "compute", "novel", "lighting", "original", "sky", "replace", "new", "one", "simulate", "desire", "time", "day", "-lrb-", "we", "use", "Vue", "infinite", "-lsb-", "e-on", "Software", "2008", "-rsb-", "synthesize", "new", "sky", "-rrb-", "finally", "we", "add", "haze", "back", "use", "eq", "-lrb-", "-rrb-", "after", "multiply", "haze", "curve", "-lrb-", "-rrb-", "global", "color", "mood", "transfer", "coefficient", "global", "color", "mood", "transfer", "coefficient", "compute", "each", "color", "channel", "two", "sky", "dome", "compute", "one", "corresponding", "Input", "Relighted", "Relighted", "Input", "Relighted", "actual", "-lrb-", "known", "estimate", "-rrb-", "time", "day", "photograph", "take", "other", "correspond", "desire", "sun", "position", "let", "ref", "new", "average", "color", "two", "sky", "dome", "color", "mood", "transfer", "coefficient", "give", "new", "ref", "lightmap", "may", "compute", "variety", "way", "we", "current", "implementation", "offer", "user", "set", "control", "various", "aspect", "lighting", "include", "atmosphere", "parameter", "diffuse", "ambient", "color", "etc.", "we", "compute", "lightmap", "simple", "local", "shade", "model", "scale", "color", "mood", "coefficient", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "where", "-lsb-", "shadow", "-rsb-", "shadow", "coefficient", "indicate", "amount", "light", "attenuation", "due", "shadow", "ambient", "coefficient", "diffuse", "coefficient", "point", "normal", "direction", "sun", "final", "result", "obtain", "simply", "multiply", "image", "L.", "note", "we", "do", "attempt", "remove", "exist", "illumination", "before", "apply", "new", "one", "however", "we", "find", "even", "basic", "procedure", "yield", "convincing", "change", "lighting", "-lrb-", "see", "Figure", "dynamic", "relight", "sequence", "video", "-rrb-", "Figure", "demonstrate", "relight", "geo-registered", "photo", "generate", "completely", "different", "-lrb-", "more", "realistic", "-rrb-", "effect", "than", "simply", "render", "underlie", "geometric", "model", "under", "desire", "lighting", "novel", "View", "synthesis", "one", "compelling", "feature", "Deep", "Photo", "ability", "modify", "viewpoint", "from", "which", "original", "photograph", "take", "bring", "static", "photo", "life", "manner", "significantly", "enhance", "photo", "browse", "experience", "show", "companion", "video", "assume", "photograph", "have", "be", "register", "sufficiently", "accurate", "geometric", "model", "scene", "challenge", "change", "viewpoint", "reduce", "complete", "miss", "texture", "area", "either", "occluded", "simply", "outside", "original", "view", "frustum", "we", "use", "image", "completion", "-lsb-", "efro", "Leung", "1999", "Drori", "et", "al.", "2003", "-rsb-", "fill", "miss", "area", "texture", "from", "other", "part", "photograph", "we", "image", "completion", "process", "similar", "texture-by-numbers", "-lsb-", "Hertzmann", "et", "al.", "2001", "-rsb-", "where", "instead", "hand-painted", "label", "map", "we", "use", "guidance", "map", "derive", "from", "texture", "3d", "model", "rural", "area", "typically", "aerial", "image", "terrain", "while", "urban", "model", "texture", "map", "building", "texture", "synthesize", "over", "cylindrical", "layered", "depth", "image", "-lrb-", "LDI", "-rrb-", "-lsb-", "shade", "et", "al.", "1998", "-rsb-", "center", "around", "original", "camera", "position", "LDI", "image", "store", "each", "pixel", "depths", "normal", "scene", "point", "intersect", "corresponding", "ray", "from", "viewpoint", "we", "use", "datum", "structure", "since", "able", "represent", "both", "visible", "occluded", "part", "scene", "-lrb-", "we", "example", "we", "use", "LDI", "four", "depth", "layer", "per", "pixel", "-rrb-", "color", "frontmost", "layer", "each", "pixel", "take", "from", "original", "photograph", "provide", "inside", "original", "view", "frustum", "while", "remain", "color", "synthesize", "we", "guide", "texture", "transfer", "we", "begin", "texture", "transfer", "process", "compute", "guide", "value", "all", "layer", "each", "pixel", "guide", "value", "vector", "-lrb-", "-rrb-", "where", "chrominance", "value", "corresponding", "point", "model", "texture", "distance", "corresponding", "scene", "point", "from", "location", "camera", "we", "experiment", "we", "try", "various", "other", "feature", "include", "terrain", "normal", "slope", "height", "combination", "thereof", "we", "achieve", "best", "result", "however", "realtively", "simple", "feature", "vector", "above", "include", "distance", "feature", "vector", "bias", "synthesis", "towards", "generate", "texture", "correct", "scale", "normalize", "so", "distance", "from", "5000", "meter", "map", "-lsb-", "-rsb-", "we", "only", "include", "chrominance", "information", "feature", "vector", "-lrb-", "luminance", "-rrb-", "alleviate", "problem", "associate", "exist", "transient", "feature", "shade", "shadow", "model", "texture", "texture", "synthesis", "carry", "out", "multi-resolution", "manner", "first", "-lrb-", "coarsest", "-rrb-", "level", "synthesize", "grow", "texture", "outwards", "from", "known", "region", "each", "unknown", "pixel", "we", "examine", "square", "neighborhood", "around", "exhaustively", "search", "best", "matching", "neighborhood", "from", "known", "region", "-lrb-", "use", "norm", "-rrb-", "since", "we", "neighborhood", "contain", "miss", "pixel", "we", "can", "apply", "pca", "compression", "other", "speed-up", "structure", "straight", "forward", "way", "however", "first", "level", "sufficiently", "coarse", "its", "synthesis", "rather", "fast", "synthesize", "each", "next", "level", "we", "upsample", "result", "previous", "level", "perform", "small", "number", "k-coherence", "synthesis", "pass", "-lsb-", "ashikhmin", "2001", "-rsb-", "refine", "result", "here", "we", "use", "look-ahead", "region", "total", "synthesis", "time", "about", "minute", "per", "image", "total", "texture", "size", "typically", "order", "4800", "1600", "pixel", "time", "four", "layer", "should", "note", "when", "work", "ldi", "concept", "pixel?s", "neighborhood", "must", "adjust", "account", "existence", "multiple", "depth", "layer", "each", "pixel", "we", "define", "neighborhood", "following", "way", "each", "depth", "layer", "pixel", "have", "up", "pixel", "surround", "neighbor", "pixel", "have", "multiple", "depth", "layer", "pixel", "layer", "closest", "depth", "value", "assign", "immediate", "neighbor", "render", "image", "from", "novel", "viewpoint", "we", "use", "shader", "project", "LDI", "image", "onto", "geometric", "model", "compute", "distance", "model", "camera", "use", "pixel", "color", "from", "depth", "layer", "closest", "distance", "significant", "change", "viewpoint", "eventually", "cause", "texture", "distortion", "one", "keep", "use", "texture", "from", "photograph", "alleviate", "problem", "we", "blend", "photograph?s", "texture", "model?s", "texture", "new", "virtual", "camera", "get", "farther", "away", "from", "original", "viewpoint", "we", "find", "significantly", "improve", "3d", "view", "experience", "even", "drastic", "view", "change", "go", "bird?s", "eye", "view", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "116:7", "figure", "extend", "field", "view", "red", "rectangle", "indicate", "boundary", "original", "photograph", "companion", "video", "demonstrate", "change", "viewpoint", "thus", "texture", "color", "each", "terrain", "point", "give", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "photo", "-lrb-", "-rrb-", "-lrb-", "-lrb-", "-rrb-", "-rrb-", "model", "-lrb-", "-rrb-", "where", "blending", "factor", "-lrb-", "-rrb-", "determine", "respect", "current", "view", "accord", "follow", "principle", "-lrb-", "-rrb-", "pixel", "original", "photograph", "which", "correspond", "surface", "face", "camera", "consider", "more", "reliable", "than", "those", "oblique", "surface", "-lrb-", "ii", "-rrb-", "pixel", "original", "photograph", "also", "prefer", "whenever", "corresponding", "scene", "point", "view", "from", "same", "direction", "current", "view", "original", "one", "specifically", "let", "-lrb-", "-rrb-", "denote", "surface", "normal", "original", "camera", "position", "from", "which", "photograph", "take", "new", "current", "camera", "position", "next", "let", "-lrb-", "-rrb-", "denote", "normalize", "vector", "from", "scene", "point", "original", "camera", "position", "similarly", "new", "-lrb-", "new", "-rrb-", "new", "10", "-lrb-", "-rrb-", "max", "-lrb-", "-lrb-", "-rrb-", "new", "-rrb-", "other", "word", "define", "greater", "among", "cosine", "angle", "between", "normal", "original", "view", "direction", "cosine", "angle", "between", "two", "view", "direction", "finally", "we", "also", "apply", "re-hazing", "on-the-fly", "first", "we", "remove", "haze", "from", "texture", "completely", "describe", "section", "4.1", "we", "add", "haze", "back", "time", "use", "distance", "from", "current", "camera", "position", "result", "may", "see", "figure", "video", "information", "visualization", "have", "register", "photograph", "model", "have", "GIS", "datum", "associate", "allow", "display", "various", "information", "about", "scene", "while", "browse", "photograph", "we", "have", "implement", "simple", "application", "demonstrate", "several", "type", "information", "visualization", "application", "photograph", "show", "sideby-side", "top", "view", "model", "refer", "map", "view", "view", "frustum", "correspond", "photograph", "display", "map", "view", "update", "dynamically", "whenever", "view", "change", "-lrb-", "describe", "section", "-rrb-", "move", "cursor", "either", "two", "view", "highlight", "corresponding", "location", "other", "view", "map", "view", "user", "able", "switch", "between", "street", "map", "orthographic", "photo", "combination", "thereof", "etc.", "addition", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "116:8", "J.", "Kopf", "et", "al.", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "different", "information", "visualization", "mode", "we", "system", "-lrb-", "a-b", "-rrb-", "coupled", "map", "photo", "view", "user", "move", "mouse", "over", "one", "view", "corresponding", "location", "show", "other", "view", "well", "profile", "horizontal", "scanline", "map", "view", "-lrb-", "-rrb-", "show", "superimpose", "over", "terrain", "photo", "view", "-lrb-", "-rrb-", "since", "location", "mouse", "cursor", "occlude", "mountain", "photo", "its", "location", "photo", "view", "indicate", "use", "semi-transparent", "arrow", "-lrb-", "-rrb-", "name", "landmark", "automatically", "superimpose", "photo", "-lrb-", "d-e", "-rrb-", "coupled", "photo", "map", "view", "superimpose", "street", "network", "street", "under", "mouse", "cursor", "highlight", "both", "view", "text", "label", "also", "possible", "superimpose", "graphical", "map", "element", "road", "directly", "onto", "photo", "view", "ability", "demonstrate", "figure", "companion", "video", "various", "database", "geo-tagged", "media", "available", "web", "we", "able", "highlight", "location", "both", "view", "-lrb-", "photo", "map", "-rrb-", "particular", "interest", "geo-tagged", "Wikipedia", "article", "about", "various", "landmark", "we", "display", "small", "Wikipedia", "icon", "location", "which", "open", "browser", "window", "corresponding", "article", "when", "click", "also", "demonstrate", "companion", "video", "another", "nice", "visualization", "feature", "we", "system", "ability", "highlight", "object", "under", "mouse", "photo", "view", "can", "useful", "example", "when", "view", "night", "time", "photograph", "urban", "scene", "shot", "night", "building", "under", "cursor", "may", "show", "use", "daylight", "texture", "from", "underlie", "model", "discussion", "conclusion", "we", "present", "Deep", "Photo", "novel", "system", "editing", "browse", "outdoor", "photograph", "leverage", "high", "quality", "3d", "model", "earth", "now", "become", "widely", "available", "we", "have", "demonstrate", "once", "simple", "geo-registration", "photo", "perform", "model", "can", "use", "many", "interesting", "photo", "manipulation", "range", "from", "deand", "rehazing", "relighting", "integrate", "gi", "information", "application", "we", "show", "vary", "haze", "removal", "challenging", "problem", "due", "fact", "haze", "function", "depth", "we", "have", "show", "now", "depth", "available", "geo-registered", "photograph", "excellent", "haze", "editing", "can", "achieve", "similarly", "have", "underlie", "geometric", "model", "make", "possible", "generate", "convincing", "relighted", "photograph", "dynamically", "change", "view", "finally", "we", "demonstrate", "enormous", "wealth", "information", "available", "online", "can", "now", "use", "annotate", "help", "browse", "photograph", "within", "we", "framework", "we", "use", "model", "obtain", "from", "virtual", "Earth", "manual", "registration", "do", "within", "minute", "mat", "out", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "-lrb-", "-rrb-", "figure", "failure", "case", "some", "describe", "application", "produce", "artifact", "badly", "register", "-lrb-", "leave", "-rrb-", "and/or", "insuffienctly", "accurate", "model", "-lrb-", "right", "-rrb-", "case", "dehaze", "application", "generate", "halo", "around", "misaligned", "depth", "edge", "because", "use", "wrong", "depth", "value", "same", "artifact", "can", "observe", "zoom", "full", "image", "figure", "foreground", "also", "easy", "task", "use", "state-of-the-art", "technique", "Soft", "Scissors", "-lsb-", "Wang", "et", "al.", "2007", "-rsb-", "all", "other", "operation", "dehazing", "relighting", "run", "interactive", "speed", "however", "compute", "very", "detailed", "shadow", "map", "relighting", "can", "time", "consuming", "can", "expect", "always", "some", "difference", "misalignment", "between", "photograph", "model", "may", "arise", "due", "insufficiently", "accurate", "model", "also", "due", "fact", "photograph", "be", "capture", "ideal", "pinhole", "camera", "although", "can", "lead", "some", "artifact", "-lrb-", "see", "Figure", "-rrb-", "we", "find", "many", "case", "difference", "less", "problematic", "than", "one", "might", "fear", "however", "automatically", "resolve", "difference", "certainly", "challenging", "interesting", "topic", "future", "work", "we", "believe", "application", "present", "here", "represent", "just", "small", "fraction", "possible", "geo-photo", "editing", "operation", "many", "exist", "digital", "photography", "product", "could", "greatly", "enhance", "use", "geo", "information", "Operations", "could", "encompass", "noise-reduction", "image", "sharpen", "3d", "model", "prior", "postcapture", "refocussing", "object", "recovery", "under", "over-exposed", "area", "well", "illumination", "transfer", "between", "photograph", "gi", "database", "contain", "wealth", "information", "which", "we", "have", "just", "use", "small", "amount", "water", "grass", "pavement", "build", "material", "etc", "can", "all", "potentially", "automatically", "label", "use", "improve", "photo", "tone", "adjustment", "label", "can", "transfer", "automatically", "from", "one", "image", "other", "again", "have", "single", "consistent", "3d", "model", "we", "photograph", "provide", "much", "more", "than", "just", "depth", "value", "per", "pixel", "paper", "we", "mostly", "deal", "single", "image", "most", "application", "we", "demonstrate", "become", "even", "stronger", "when", "combine", "multiple", "input", "photo", "particularly", "interesting", "direction", "might", "combine", "Deep", "Photo", "Photo", "Tourism", "system", "once", "Photo", "Tour", "geo-registered", "coarse", "3d", "information", "generate", "Photo", "Tourism", "could", "use", "enhance", "online", "3d", "datum", "vice-versa", "information", "visualization", "novel", "view", "synthesis", "application", "we", "demonstrate", "here", "could", "combine", "Photo", "Tourism", "viewer", "idea", "fuse", "multiple", "image", "could", "even", "extend", "video", "could", "register", "model", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "deep", "Photo", "model-based", "Photograph", "enhancement", "viewing", "116:9", "acknowledgement", "research", "support", "part", "grant", "from", "follow", "funding", "agency", "Lion", "foundation", "gif", "foundation", "Israel", "Science", "Foundation", "DFG", "graduiertenkolleg/1042", "explorative", "analysis", "visualization", "large", "information", "space", "University", "Konstanz", "Germany", "reference", "SHIKHMIN", "M.", "2001", "synthesize", "natural", "texture", "Proceedings", "2001", "symposium", "interactive", "3d", "graphic", "-lrb-", "i3d", "-rrb-", "217", "226", "hen", "B.", "AMOS", "G.", "FEK", "E.", "OHEN", "M.", "RUCKER", "S.", "ister", "D.", "2008", "interactive", "technique", "register", "image", "digital", "terrain", "building", "model", "Microsoft", "Research", "Technical", "Report", "MSR-TR-2008-115", "ho", "P.", "L.", "2007", "3d", "organization", "2d", "urban", "imagery", "Proceedings", "36th", "Applied", "Imagery", "Pattern", "recognition", "Workshop", "riminisus", "a.", "eid", "i.", "D.", "isserman", "a.", "2000", "single", "view", "metrology", "International", "Journal", "Computer", "Vision", "40", "123", "148", "ebevec", "P.", "E.", "aylor", "C.", "J.", "ALIK", "J.", "1996", "modeling", "render", "architecture", "from", "photograph", "hybrid", "geometryand", "image-based", "approach", "Proceedings", "SIGGRAPH", "96", "11", "20", "ebevec", "P.", "AWKINS", "T.", "CHOU", "C.", "UIKER", "H.-P.", "arokin", "W.", "AGAR", "M.", "2000", "acquire", "reflectance", "field", "human", "face", "Proceedings", "SIGGRAPH", "2000", "145", "156", "rorus", "i.", "ohen", "D.", "eshurun", "H.", "2003", "fragment-based", "image", "completion", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2003", "-rrb-", "22", "303", "312", "EON", "oftware", "2008", "Vue", "Infinite", "http", "www.e-onsoftware.com/products/vue/vue_", "6_infinite", "fro", "a.", "a.", "eung", "T.", "K.", "1999", "texture", "synthesis", "non-parametric", "sampling", "Proceedings", "IEEE", "International", "Conference", "Computer", "Vision", "-lrb-", "iccv", "-rrb-", "99", "1033", "1038", "attal", "R.", "2008", "single", "image", "dehazing", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2008", "-rrb-", "27", "73", "UH", "C.", "akhor", "a.", "2003", "construct", "3d", "city", "model", "merge", "aerial", "ground", "view", "IEEE", "Computer", "Graphics", "application", "23", "52", "61", "ruen", "a.", "uang", "T.", "S.", "2001", "calibration", "orientation", "Cameras", "Computer", "Vision", "Springer-Verlag", "Secaucus", "NJ", "USA", "ay", "J.", "fro", "a.", "A.", "2007", "scene", "completion", "use", "million", "photograph", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2007", "-rrb-", "26", "ertzmann", "a.", "ACOBS", "C.", "E.", "liver", "N.", "URLESS", "B.", "ALESIN", "D.", "H.", "2001", "image", "analogy", "Proceedings", "SIGGRAPH", "2001", "327", "340", "oiem", "D.", "FROS", "A.", "A.", "EBERT", "M.", "2005", "Automatic", "photo", "pop-up", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2005", "-rrb-", "24", "577", "584", "orry", "Y.", "njyo", "k.-i.", "raus", "K.", "1997", "tour", "picture", "use", "spidery", "mesh", "interface", "make", "animation", "from", "single", "image", "Proceedings", "SIGGRAPH", "97", "225", "232", "AFTORY", "R.", "chechner", "Y.", "Y.", "eevus", "Y.", "Y.", "2007", "variational", "distance-dependent", "image", "restoration", "Proceedings", "IEEE", "Conference", "Computer", "Vision", "Pattern", "recognition", "-lrb-", "cvpr", "-rrb-", "2007", "ang", "S.", "B.", "1998", "depth", "painting", "image-based", "rendering", "application", "Tech", "rep.", "Compaq", "Cambridge", "Research", "Lab", "alonde", "j.-f.", "oiem", "D.", "FROS", "A.", "A.", "OTHER", "C.", "INN", "J.", "riminisus", "a.", "2007", "Photo", "clip", "art", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2007", "-rrb-", "26", "osco", "C.", "RETTAKIS", "G.", "OBERT", "L.", "2000", "interactive", "virtual", "relighting", "real", "scene", "IEEE", "transaction", "visualization", "computer", "graphic", "289", "305", "artney", "E.", "J.", "1976", "optics", "Atmosphere", "scatter", "Molecules", "Particles", "John", "Wiley", "Sons", "New", "York", "NY", "USA", "arasimhan", "S.", "G.", "AYAR", "S.", "K.", "2003", "contrast", "restoration", "weather", "degrade", "image", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "25", "713", "724", "arasimhan", "S.", "G.", "AYAR", "S.", "K.", "2003", "interactive", "-lrb-", "de", "-rrb-", "weather", "image", "use", "physical", "model", "IEEE", "Workshop", "Color", "Photometric", "method", "computer", "Vision", "NASA", "2008", "landsat", "program", "http://landsat.gsfc", "nasa.gov", "NASA", "2008", "shuttle", "radar", "topography", "mission", "http://www2", "jpl.nasa.gov", "srtm", "ayar", "S.", "K.", "ARASIMHAN", "S.", "G.", "1999", "Vision", "bad", "weather", "Proceedings", "IEEE", "International", "Conference", "Computer", "Vision", "-lrb-", "iccv", "-rrb-", "99", "820", "827", "ister", "D.", "TEWENIUS", "H.", "2007", "minimal", "solution", "generalise", "3-point", "pose", "problem", "Journal", "Mathematical", "Imaging", "Vision", "27", "67", "79", "akley", "J.", "P.", "atherley", "B.", "L.", "1998", "improving", "image", "quality", "poor", "visibility", "condition", "use", "physical", "model", "contrast", "degradation", "IEEE", "transaction", "image", "processing", "167", "179", "B.", "M.", "HEN", "M.", "ORSEY", "J.", "URAND", "F.", "2001", "image-based", "modeling", "photo", "editing", "Proceedings", "ACM", "SIGGRAPH", "2001", "433", "442", "eichmann", "M.", "2001", "art", "photography", "http://www.luminous-landscape.com/essays/", "theartof.shtml", "chechner", "Y.", "Y.", "VERBUCH", "Y.", "2007", "regularize", "image", "recovery", "scatter", "media", "IEEE", "transaction", "Pattern", "Analysis", "Machine", "Intelligence", "29", "1655", "1660", "chechner", "Y.", "Y.", "ARASIMHAN", "S.", "G.", "AYAR", "S.", "K.", "2003", "polarization-based", "vision", "through", "haze", "Applied", "Optics", "42", "511", "525", "hade", "J.", "ORTLER", "S.", "L.-W.", "ZELISKI", "R.", "1998", "layered", "depth", "image", "Proceedings", "SIGGRAPH", "98", "231", "242", "hum", "h.-y.", "M.", "ZELISKI", "R.", "1998", "interactive", "construction", "3-d", "model", "from", "panoramic", "mosaic", "Proceedings", "IEEE", "Conference", "Computer", "Vision", "Pattern", "recognition", "-lrb-", "cvpr", "-rrb-", "1998", "427", "433", "navely", "N.", "eitz", "S.", "M.", "ZELISKI", "R.", "2006", "Photo", "tourism", "explore", "photo", "collection", "3d", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2006", "-rrb-", "25", "835", "846", "tamo", "i.", "llen", "P.", "K.", "2000", "3-d", "model", "construction", "use", "range", "image", "datum", "Proceedings", "IEEE", "Conference", "Computer", "Vision", "Pattern", "recognition", "-lrb-", "cvpr", "-rrb-", "1998", "531", "536", "UNKAVALLI", "K.", "atusik", "W.", "fister", "H.", "usinkiewicz", "S.", "2007", "factored", "time-lapse", "video", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2007", "-rrb-", "26", "101", "R.", "T.", "2008", "visibility", "bad", "weather", "from", "single", "image", "Proceedings", "IEEE", "Conference", "Computer", "Vision", "Pattern", "recognition", "-lrb-", "cvpr", "-rrb-", "2008", "appear", "oyama", "K.", "OGAN", "R.", "oseway", "a.", "2003", "geographic", "location", "tag", "digital", "image", "Proceedings", "11th", "acm", "international", "conference", "Multimedia", "156", "166", "ang", "J.", "GRAWALA", "M.", "OHEN", "M.", "F.", "2007", "soft", "scissors", "interactive", "tool", "realtime", "high", "quality", "matting", "ACM", "transaction", "graphic", "-lrb-", "Proceedings", "SIGGRAPH", "2007", "-rrb-", "26", "Y.", "ALIK", "J.", "1998", "recover", "photometric", "property", "architectural", "scene", "from", "photograph", "Proceedings", "SIGGRAPH", "98", "207", "217", "Y.", "EBEVEC", "P.", "ALIK", "J.", "AWKINS", "T.", "1999", "inverse", "global", "illumination", "recover", "reflectance", "model", "real", "scene", "from", "photograph", "Proceedings", "SIGGRAPH", "99", "215", "224", "hang", "L.", "UGAS", "hocion", "G.", "AMSON", "J.-S.", "EITZ", "S.", "M.", "2002", "single-view", "modelling", "free-form", "scene", "Journal", "visualization", "computer", "animation", "13", "225", "235", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008", "116:10", "J.", "Kopf", "et", "al.", "ACM", "transaction", "Graphics", "Vol", "27", "no.", "Article", "116", "publication", "date", "December", "2008" ],
  "content" : "\n  \n    efc8617114bb30764aab5424e0445e88edabca3163f2f644594fff5d2b6bc4ce\n    ouj\n    10.1145/1409060.1409069\n    Name identification was not possible. \n  \n  \n    \n      \n        Deep Photo: Model-Based Photograph Enhancement and Viewing\n      \n      Johannes Kopf Boris Neubert Billy Chen University of Konstanz University of Konstanz Microsoft Oliver Deussen Matt Uyttendaele University of Konstanz Microsoft Research\n      \n        \n      \n      Original Dehazed\n      \n        Figure 1: Some of the applications of the Deep Photo system.\n      \n      In this paper, we introduce a novel system for browsing, enhancing, and manipulating casual outdoor photographs by combining them with already existing georeferenced digital terrain and urban models. A simple interactive registration process is used to align a photograph with such a model. Once the photograph and the model have been registered, an abundance of information, such as depth, texture, and GIS data, becomes immediately available to our system. This information, in turn, enables a variety of operations, ranging from dehazing and relighting the photograph, to novel view synthesis, and overlaying with geographic information. We describe the implementation of a number of these applications and discuss possible extensions. Our results show that augmenting photographs with already available 3D models of the world supports a wide variety of new ways for us to experience and interact with our everyday snapshots. Keywords: image-based modeling, image-based rendering, image completion, dehazing, relighting, photo browsing\n      ACM Reference Format Kopf, J., Neubert, B., Chen, B., Cohen, M., Cohen-Or, D., Deussen, O., Uyttendaele, M., Lischinski, D. 2008. Deep Photo: Model-Based Photograph Enhancement and Viewing. ACM Trans. Graph. 27, 5, Article 116 (December 2008), 10 pages. DOI = 10.1145/1409060.1409069 http://doi.acm.org/10.1145/1409060.1409069. Copyright Notice Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or direct commercial advantage and that copies show this notice on the fi rst page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701, fax +1 (212) 869-0481, or permissions@acm.org . ? 2008 ACM 0730-0301/2008/05-ART116 $5.00 DOI 10.1145/1409060.1409069 http://doi.acm.org/10.1145/1409060.1409069\n      Michael Cohen Daniel Cohen-Or Microsoft Research Tel Aviv University Dani Lischinski The Hebrew University\n      Relighted Annotated\n    \n    \n      \n        1 Introduction\n      \n      Despite the increasing ubiquity of digital photography, the metaphors we use to browse and interact with our photographs have not changed much. With few exceptions, we still treat them as 2D entities, whether they are displayed on a computer monitor or printed as a hard copy. It is well understood that augmenting a photograph with depth can open the way for a variety of new exciting manipulations. However, inferring the depth information from a single image that was captured with an ordinary camera is still a longstanding unsolved problem in computer vision. Luckily, we are witnessing a great increase in the number and the accuracy of geometric models of the world, including terrain and buildings. By registering photographs to these models, depth becomes available at each pixel. The Deep Photo system described in this paper, consists of a number of applications afforded by these newfound depth values, as well as the many other types of information that are typically associated with such models.  Deep Photo is motivated by several recent trends now reaching critical mass. The first trend is that of geo-tagged photos. Many photo sharing web sites now enable users to manually add location information to photos. Some digital cameras, such as the RICOH Caplio 500SE and the Nokia N95, feature a built-in GPS, allowing automatic location tagging. Also, a number of manufacturers offer small GPS units that allow photos to be easily geo-tagged by software that synchronizes the GPS log with the photos. In addition, location tags can be enhanced by digital compasses that are able to measure the orientation (tilt and heading) of the camera. It is expected that, in the future, more cameras will have such functionality, and that most photographs will be geo-tagged. The second trend is the widespread availability of accurate digital terrain models, as well as detailed urban models. Thanks to commercial projects, such as Google Earth and Microsoft?s Virtual Earth, both the quantity and the quality of such models is rapidly increasing. In the public domain, NASA provides detailed satellite imagery (e.g., Landsat [NASA 2008a]) and elevation models (e.g., Shuttle Radar Topography Mission [NASA 2008b]). Also, a number of cities around the world are creating detailed 3D models of their cityscape (e.g., Berlin 3D). The combination of geo-tagging and the availability of fairly accurate 3D models allows many photographs to be precisely georegistered. We envision that in the near future automatic georegistration will be available as an online service. Thus, although we briefly describe the simple interactive geo-registration technique that we currently employ, the emphasis of this paper is on the applications that it enables, including: ? dehazing (or adding haze to) images, ? approximating changes in lighting, ? novel view synthesis, ? expanding the field of view, ? adding new objects into the image, ? integration of GIS data into the photo browser. Our goal in this work has been to enable these applications for single outdoor images, taken in a casual manner without requiring any special equipment or any particular setup. Thus, our system is applicable to a large body of existing outdoor photographs, so long as we know the rough location where each photograph was taken. We chose New York City and Yosemite National Park as two of the many locations around the world, for which detailed textured models are already available 1 . We demonstrate our approach by combining a number of photographs (obtained from flickr TM ) with these models. It should be noted that while the models that we use are fairly detailed, they are still a far cry from the degree of accuracy and the level of detail one would need in order to use these models directly to render photographic images. Thus, one of our challenges in this work has been to understand how to best leverage the 3D information afforded by the use of these models, while at the same time preserving the photographic qualities of the original image. In addition to exploring the applications listed above, this paper also makes a number of specific technical contributions. The two main ones are a new data-driven stable dehazing procedure, and a new model-guided layered depth image completion technique for novel view synthesis. Before continuing, we should note some of the limitations of Deep Photo in its current form. The examples we show are of outdoor scenes. We count on the available models to describe the distant static geometry of the scene, but we cannot expect to have access to the geometry of nearby (and possibly dynamic) foreground objects, such as people, cars, trees, etc. In our current implementation such foreground objects are matted out before combining the rest of the photograph with a model, and may be composited back onto the photograph at a later stage. So, for some images, the user must spend some time on interactive matting, and the fidelity of some of our manipulations in the foreground may be reduced. That said, we expect the kinds of applications we demonstrate will scale to\n      ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n      116:2 ? J. Kopf et al.\n      1 For Yosemite, we use elevation data from the Shuttle Radar Topography Mission [NASA 2008b] with Landsat imagery [NASA 2008a]. Such data is available for the entire Earth. Models similar to that of NYC are currently available for dozens of cities.\n      include any improvements in automatic computer vision algorithms and depth acquisition technologies.\n      \n        2 Related Work\n        Our system touches upon quite a few distinct topics in computer vision and computer graphics; thus, a comprehensive review of all related work is not feasible due to space constraints. Below, we attempt to provide some representative references, and discuss in detail only the ones most closely related to our goals and techniques.  Image-based modeling. In recent years, much work has been done on image-based modeling techniques, which create high quality 3D models from photographs. One example is the pioneering Fa  ?ade system [Debevec et al. 1996], designed for interactive modeling of buildings from collections of photographs. Other systems use panoramic mosaics [Shum et al. 1998], combine images with range data [Stamos and Allen 2000], or merge ground and aerial views [Fr?h and Zakhor 2003], to name a few. Any of these approaches may be used to create the kinds of textured 3D models that we use in our system; however, in this work we are not concerned with the creation of such models, but rather with the ways in which their combination with a single photograph may be useful for the casual digital photographer. One might say that rather than attempting to automatically or manually reconstruct the model from a single photo, we exploit the availability of digital terrain and urban models, effectively replacing the difficult 3D reconstruction/modeling process by a much simpler registration process. Recent research has shown that various challenging tasks, such as image completion and insertion of objects into photographs [Hays and Efros 2007; Lalonde et al. 2007] can greatly benefit from the availability of the enormous amounts of photographs that had already been captured. The philosophy behind our work is somewhat similar: we attempt to leverage the large amount of textured geometric models that have already been created. But unlike image databases, which consist mostly of unrelated items, the geometric models we use are all anchored to the world that surrounds us. Dehazing. Weather and other atmospheric phenomena, such as haze, greatly reduce the visibility of distant regions in images of outdoor scenes. Removing the effect of haze, or dehazing, is a challenging problem, because the degree of this effect at each pixel depends on the depth of the corresponding scene point. Some haze removal techniques make use of multiple images; e.g., images taken under different weather conditions [Narasimhan and Nayar 2003a], or with different polarizer orientations [Schechner et al. 2003]. Since we are interested in dehazing single images, taken without any special equipment, such methods are not suitable for our needs. There are several works that attempt to remove the effects of haze, fog, etc., from a single image using some form of depth information. For example, Oakley and Satherley [1998] dehaze aerial imagery using estimated terrain models. However, their method involves estimating a large number of parameters, and the quality of the reported results is unlikely to satisfy today?s digital photography enthusiasts. Narasimhan and Nayar [2003b] dehaze single images based on a rough depth approximation provided by the user, or derived from satellite orthophotos. The very latest dehazing methods [Fattal 2008; Tan 2008] are able to dehaze single images by making various assumptions about the colors in the scene. Our work differs from these previous single image dehazing methods in that it leverages the availability of more accurate 3D models, and uses a novel data-driven dehazing procedure. As a result, our method is capable of effective, stable high-quality contrast restoration even of extremely distant regions. Novel view synthesis. It has been long recognized that adding depth information to photographs provides the means to alter the viewpoint. The classic ?Tour Into the Picture? system [Horry et al. 1997] demonstrates that fitting a simple mesh to the scene is sometimes enough to enable a compelling 3D navigation experience. Subsequent papers, Kang [1998], Criminisi et al. [2000], Oh et al. [2001], Zhang et al. [2002], extend this by providing more sophisticated, user-guided 3D modelling techniques. More recently Hoiem et al. [2005] use machine learning techniques in order to construct a simple ?pop-up? 3D model, completely automatically from a single photograph. In these systems, despite the simplicity of the models, the 3D experience can be quite compelling. In this work, we use already available 3D models in order to add depth to photographs. We present a new model-guided image completion technique that enables us to expand the field of view and to perform high-quality novel view synthesis. Relighting. A number of sophisticated relighting systems have been proposed by various researchers over the years (e.g., [Yu and Malik 1998; Yu et al. 1999; Loscos et al. 2000; Debevec et al. 2000]). Typically, such systems make use of a highly accurate geometric model, and/or a collection of photographs, often taken under different lighting conditions. Given this input they are often able to predict the appearance of a scene under novel lighting conditions with a very high degree of accuracy and realism. Another alternative to use a time-lapse video sequence [Sunkavalli et al. 2007]. In our case, we assume the availability of a geometric model, but have just one photograph to work with. Furthermore, although the model might be detailed, it is typically quite far from a perfect match to the photograph. For example, a tree casting a shadow on a nearby building will typically be absent from our model. Thus, we cannot hope to correctly recover the reflectance at each pixel of the photograph, which is necessary in order to perform physically accurate relighting. Therefore, in this work we propose a very simple relighting approximation, which is nevertheless able to produce fairly compelling results. Photo browsing. Also related is the ?Photo Tourism? system [Snavely et al. 2006], which enables browsing and exploring large collections of photographs of a certain location using a 3D interface. But, the browsing experience that we provide is very different. Moreover, in contrast to ?Photo Tourism?, our system requires only a single geo-tagged photograph, making it applicable even to locations without many available photos. The ?Photo Tourism? system also demonstrates the transfer of annotations from one registered photograph to another. In Deep Photo, photographs are registered to a model of the world, making it possible to tap into a much richer source of information. Working with geo-referenced images. Once a photo is registered to geo-referenced data such as maps and 3D models, a plethora of information becomes available. For example, Cho [Cho 2007] notes that absolute geo-locations can be assigned to individual pixels and that GIS annotations, such as building and street names, may be projected onto the image plane. Deep Photo supports similar labeling, as well as several additional visualizations, but in contrast to Cho?s system, it does so dynamically, in the context of an interactive photo browsing application. Furthermore, as discussed earlier, it also enables a variety of other applications. In addition to enhancing photos, location is also useful in organizing and visualizing photo collections. The system developed by Toyama et al. [2003] enables a user to browse large collections of geo-referenced photos on a 2D map. The map serves as both a visualization device, as well as a way to specify spatial queries, i.e., all photos within a region. In contrast, DeepPhoto focuses on enhancing and browsing of a single photograph; the two systems are actually complementary, one focusing on organizing large photo collections, and the other on enhancing and viewing single photographs.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        Deep Photo: Model-Based Photograph Enhancement and Viewing ? 116:3\n      \n      \n        3 Registration and Matting\n        We assume that the photograph has been captured by a simple pinhole camera, whose parameters consist of position, pose, and focal length (seven parameters in total). To register such a photograph to a 3D geometric model of the scene, it suffices to specify four or more corresponding pairs of points [Gruen and Huang 2001]. Assuming that the rough position from which the photograph was taken is available (either from a geotag, or provided by the user), we are able to render the model from roughly the correct position, let the user specify sufficiently many correspondences, and recover the parameters by solving a nonlinear system of equations [Nister and Stewenius 2007]. The details and user interface of our registration system are described in a technical report [Chen et al. 2008]. For images that depict foreground objects not contained in the model, we ask the user matte out the foreground. For the applications demonstrated in this paper the matte does not have to be too accurate, so long as it is conservative (i.e., all the foreground pixels are contained). We created mattes with the Soft Scissors system [Wang et al. 2007]. The process took about 1-2 minutes per photo. For every result produced using a matte we show the matte next to the input photograph.\n      \n      \n        4 Image Enhancement\n        Many of the typical images we take are of a spectacular, often well known, landscape or cityscape. Unfortunately in many cases the lighting conditions or the weather are not optimal when the photographs are taken, and the results may be dull or hazy. Having a sufficiently accurate match between a photograph and a geometric model offers new possibilities for enhancing such photographs. We are able to easily remove haze and unwanted color shifts and to experiment with alternative lighting conditions.\n      \n      \n        4.1 Dehazing\n        Atmospheric phenomena, such as haze and fog can reduce the visibility of distant regions in images of outdoor scenes. Due to atmospheric absorption and scattering, only part of the light reflected from distant objects reaches the camera. Furthermore, this light is mixed with airlight (scattered ambient light between the object and camera). Thus, distant objects in the scene typically appear considerably lighter and featureless, compared to nearby ones. If the depth at each image pixel is known, in theory it should be easy to remove the effects of haze by fitting an analytical model (e.g., [McCartney 1976; Nayar and Narasimhan 1999]):\n        \n          1\n          I h = I o f (z) + A (1 ? f (z)) .\n        \n        Here I h is the observed hazy intensity at a pixel, I o is the original intensity reflected towards the camera from the corresponding scene point, A is the airlight, and f (z) = exp(?? z) is the attenuation in intensity as a function of distance due to outscattering. Thus, after\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        116:4 ? J. Kopf et al.\n        \n          \n        \n        Input Model textures Final dehazed result\n        \n          Figure 2: Dehazing. Note the artifacts in the model texture, and the significant deviation of the estimated haze curves from exponential shape.\n          \n        \n        Input\n        Dehazed\n        \n          Figure 3: More dehazing examples.\n        \n        estimating the parameters A and ? the original intensity may be recovered by inverting the model:\n        \n          2\n          1 I o = A + (I h ? A) . f (z)\n        \n        As pointed out by Narasimhan and Nayar [2003a], this model assumes single-scattering and a homogeneous athmosphere. Thus, it is more suitable for short ranges of distance and might fail to correctly approximate the attenuation of scene points that are more than a few kilometers away. Furthermore, since the exponential attenuation goes quickly down to zero, noise might be severely amplified in the distant areas. Both of these artifacts may be observed in the ?inversion result? of Figure 4 . While reducing the degree of dehazing [Schechner et al. 2003] and regularization [Schechner and Averbuch 2007; Kaftory et al. 2007] may be used to alleviate these problems, our approach is to estimate stable values for the haze curve f (z) directly from the relationship between the colors in the photograph and those of the model textures. More specifically, we compute a curve f (z) and an airlight A, such that eq. (2) would map averages of colors in the photograph to the corresponding averages of (color-corrected) model texture colors. Note that although our f (z) has the same physical interprertation as in the previous approaches, due to our estimation process it is not subject to the constraints of a physicially-based model. Since we estimate a single curve to represent the possibly spatially\n        1 0.8 0.6 Intensity 0.4 0.2 0 2000 4000 6000 8000 Depth Estimated haze curves f (z)\n        Input\n        Dehazed\n        varying haze it can also contain non-monotonicities. All of the parameters are estimated completely automatically.  For robustness, we operate on averages of colors over depth ranges. For each value of z, we compute the average model texture color I ? m (z) for all pixels whose depth is in [z ? ? , z + ? ], as well as the average hazy image color I ? h (z) for the same pixels. In our implementation, the depth interval parameter ? is set to 500 meters, for all images we experimented with. The averaging makes our approach less sensitive to model texture artifacts, such as registration and stitching errors, bad pixels, or contained shadows and clouds. Before explaining the details of our method, we would like to point out that the model textures typically have a global color bias. For example, Landsat uses seven sensors whose spectral responses differ from the typical RGB camera sensors. Thus, the colors in the resulting textures are only an approximation to ones that would have been captured by a camera (see Figure 2 ). We correct this color bias by measuring the ratio between the photo and the texture colors in the foreground (in each channel), and using these ratios to correct the colors of the entire texture. More precisely, we compute a global multiplicative correction vector C as\n        \n          3\n          F h F m C = / , lum(F h ) lum(F m )\n        \n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        Deep Photo: Model-Based Photograph Enhancement and Viewing ? 116:5\n        \n          \n        \n        Input Fattal?s Result\n        \n          \n          Figure 4: Comparison with other dehazing methods. The second row shows full-resolution zooms of the region indicated with a red rectangle in the input photo. See the supplementary materials for more comparison images.\n        \n        where F h is the average of I ? h (z) with z < z F , and F m is a similarly computed average of the model texture. lum(c) denotes the luminance of a color c. We set z F to 1600 meters for all our images. Now we are ready to explain how to compute the haze curve f (z). Ignoring for the moment the physical interpretation of A and f (z), note that eq. (2) simply stretches the intensities of the image around A, using the scale coefficient f (z) ?1 . Our goal is to find A and f (z) that would map the hazy photo colors I ? h (z) to the color-corrected texture colors C I ? m (z). Substituting I ? h (z) for I h , and C I ? m (z) for I o , in eq. (2) we get\n        \n          4\n          I ? h (z) ? A f (z) = . ?\n        \n        \n          4\n          C I m (z) ? A\n        \n        Different choices of A will result in different scaling curves f (z). We set A = 1 since this guarantees f (z) ? 0. Using A > 1 would result in larger values of f (z), and hence less contrast in the dehazed image, and using A < 1 might be prone to instabilities. Figure 2 shows the f (z) curve estimated as described above. The recovered haze curve f (z) allows to effectively restore the contrasts in the photo. However, the colors in the background might undergo a color shift. We compensate for this by adjusting A, while keeping f (z) fixed, such that after the change the dehazing preserves the colors of the photo in the background. To adjust A, we first compute the average background color B h of the photo as the average of I ? h (z) with z > z B , and a similarly computed average of the model texture B m . We set z B to 5000m for all our images. The color of the background is preserved, if the ratio\n        \n          5\n          A + (B h ? A) ? f ?1 B h ? 1 R = , f = , B h B m ? 1\n        \n        has the same value for every color channel. Thus, we rewrite eq. (5) to obtain A as\n        \n          6\n          R ? f ?1 A = B h 1 ? f ?1 ,\n        \n        and set R = max(B m,red /B h,red , B m,green /B h,green , B m,blue /B h,blue ). This particular choice of R results in the maximum A that guarantees A ? 1. Finally, we use eq. (2) with the recovered f (z) and the adjusted A to dehaze the photograph.\n        Inversion Result Our Result\n        Figures 2 and 3 show various images dehazed with our method. Figure 4 compares our method with other approaches. In this comparison we focused on methods that are applicable in our context of working with a single image only. Fattal?s method [2008] dehazes the image nicely up to a certain distance (particularly considering that this method does not require any input in addition to the image itself), but it is unable to effectively dehaze the more distant parts, closer to the horizon. The ?Inversion Result? was obtained via eq. (2) with an exponential haze curve. This is how dehazing was performed in a number of papers, e.g., [Schechner et al. 2003; Narasimhan and Nayar 2003a; Narasimhan and Nayar 2003b]. Here, we use our accurate depth map instead of using multiple images or user-provided depth approximations. The airlight color was set to the sky color near the horizon, and the optical depth ? was adjusted manually. The result suffers from amplified noise in the distance, and breaks down next to the horizon. In contrast, our result manages to remove more haze than the two other approaches, while preserving the natural colors of the input photo. Note that in practice one might not want to remove the haze completely as we have done, because haze sometimes provides perceptually significant depth cues. Also, dehazing typically amplifies some noise in regions where little or no visible detail remain in the original image. Still, almost every image benefits from some degree of dehazing. Having obtained a model for the haze in the photograph we can insert new objects into the scene in a more seamless fashion by applying the model to these objects as well (in accordance with the depth they are supposed to be at). This is done simply by inverting eq. (2):\n        \n          7\n          I h = A + (I o ? A) f (z).\n        \n        This is demonstrated in the companion video.\n      \n      \n        4.2 Relighting\n        One cannot underestimate the importance of the role that lighting plays in the creation of an interesting photograph. In particular, in landscape photography, the vast majority of breathtaking photographs are taken during the ?golden hour?, after sunrise, or before sunset [Reichmann 2001]. Unfortunately most of our outdoor snapshots are taken under rather boring lighting. With Deep Photo\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        116:6 ? J. Kopf et al.\n        \n          \n        \n        Input Relighted Relighted\n        \n          \n        \n        Input Relighted\n        \n          Figure 5: Relighting results produced with our system.\n          \n        \n        Original Relighted Lit Model\n        \n          Figure 6: A comparison between the original photo, its relighted version, and a rendering of the underlying model under the same illumination.\n        \n        it is possible to modify the lighting of a photograph, approximating what the scene might look like at another time of day.  As explained earlier, our goal is to work on single images, augmented with a detailed, yet not completely accurate geometric model of the scene. This setup does not allow us to correctly recover the reflectance at each pixel. Thus, we use the following simple workflow, which only approximates the appearance of lighting changes in the scene. We begin by dehazing the image, as described in the previous section, and modulate the colors using a lightmap computed for the novel lighting. The original sky is replaced by a new one simulating the desired time of day (we use Vue 6 Infinite [E-on Software 2008] to synthesize the new sky). Finally, we add haze back in using Eq. (7), after multiplying the haze curves f (z) by a global color mood transfer coefficient. The global color mood transfer coefficient L G is computed for each color channel. Two sky domes are computed, one corresponding\n        Input Relighted Relighted\n        Input Relighted\n        to the actual (known or estimated) time of day the photograph was taken, and the other corresponding to the desired sun position. Let I ref and I new be the average colors of the two sky domes. The color mood transfer coefficients are then given by L G = I new /I ref . The lightmap may be computed in a variety of ways. Our current implementation offers the user a set of controls for various aspects of the lighting, including atmosphere parameters, diffuse and ambient colors, etc. We then compute the lightmap with a simple local shading model and scale it by the color mood coefficient:\n        \n          8\n          L = L G ? L S ? (L A + L D ? (n ? l)) ,\n        \n        where L S ? [I shadow , 1] is the shadow coefficient that indicates the amount of light attenuation due to shadows, L A is the ambient coefficient, L D is the diffuse coefficient, n the point normal, and l the direction to the sun. The final result is obtained simply by multiplying the image by L. Note that we do not attempt to remove the existing illumination before applying the new one. However, we found even this basic procedure yields convincing changes in the lighting (see Figure 5, and the dynamic relighting sequences in the video). Figure 6 demonstrates that relighting a geo-registered photo generates a completely different (and more realistic) effect than simply rendering the underlying geometric model under the desired lighting.\n      \n      \n        5 Novel View Synthesis\n        One of the compelling features of Deep Photo is the ability to modify the viewpoint from which the original photograph was taken. Bringing the static photo to life in this manner significantly enhances the photo browsing experience, as shown in the companion video. Assuming that the photograph has been registered with a sufficiently accurate geometric model of the scene, the challenge in changing the viewpoint is reduced to completing the missing texture in areas that are either occluded, or are simply outside the original view frustum. We use image completion [Efros and Leung 1999; Drori et al. 2003] to fill the missing areas with texture from  other parts of the photograph. Our image completion process is similar to texture-by-numbers [Hertzmann et al. 2001], where instead of a hand-painted label map we use a guidance map derived from the textures of the 3D model. In rural areas these are typically aerial images of the terrain, while in urban models these are the texture maps of the buildings. The texture is synthesized over a cylindrical layered depth image (LDI) [Shade et al. 1998], centered around the original camera position. The LDI image stores, for each pixel, the depths and normals of scene points intersected by the corresponding ray from the viewpoint. We use this data structure, since it is able to represent both the visible and the occluded parts of the scene (in our examples we used a LDI with four depth layers per pixel). The colors of the frontmost layer in each pixel are taken from the original photograph provided that they are inside the original view frustum, while the remaining colors are synthesized by our guided texture transfer. We begin the texture transfer process by computing the guiding value for all of the layers at each pixel. The guiding value is a vector (U,V, D), where U and V are the chrominance values of the corresponding point in the model texture, and D is the distance to the corresponding scene point from the location of the camera. In our experiments, we tried various other features, including terrain normal, slope, height, and combinations thereof. We achieved the best results, however, with the realtively simple feature vector above. Including the distance D in the feature vector biases the synthesis towards generating textures at the correct scale. D is normalized so that distances from 0 to 5000 meters map to [0, 1]. We only include chrominance information in the feature vector (and not luminance) to alleviate problems associated with existing transient features such as shading and shadows in the model textures. Texture synthesis is carried out in a multi-resolution manner. The first (coarsest) level is synthesized by growing the texture outwards from the known regions. For each unknown pixel we examine a square neighborhood around it, and exhaustively search for the best matching neighborhood from the known region (using the L 2 norm). Since our neighborhoods contain missing pixels we cannot apply PCA compression and other speed-up structures in a straight forward way. However, the first level is sufficiently coarse and its synthesis is rather fast. To synthesize each next level we upsample the result of the previous level and perform a small number of k-coherence synthesis passes [Ashikhmin 2001] to refine the result. Here we use a 5 ? 5 look-ahead region and k = 4. The total synthesis time is about 5 minutes per image. The total texture size is typically on the order of 4800 ? 1600 pixels, times four layers. It should be noted that when working with LDIs the concept of a pixel?s neighborhood must be adjusted to account for the existence of multiple depth layers at each pixel. We define the neighborhood in the following way: On each depth layer, a pixel has up to 8 pixels surrounding it. If the neighboring pixel has multiple depth layers, the pixel on the layer with the closest depth value is assigned as the immediate neighbor. To render images from novel viewpoints, we use a shader to project the LDI image onto the geometric model by computing the distance of the model to the camera and using the pixel color from the depth layer closest to this distance. Significant changes in the viewpoint eventually cause texture distortions if one keeps using the texture from the photograph. To alleviate this problem, we blend the photograph?s texture into the model?s texture as the new virtual camera gets farther away from the original viewpoint. We found this to significantly improve the 3D viewing experience, even for drastic view changes, such as going to bird?s eye view.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        Deep Photo: Model-Based Photograph Enhancement and Viewing ? 116:7\n        \n          \n          Figure 7: Extending the field of view. The red rectangle indicates the boundaries of the original photograph. The companion video demonstrates changing the viewpoint.\n        \n        Thus, the texture color T at each terrain point x is given by\n        \n          9\n          T (x) = g(x) T photo (x) + (1 ? g(x)) T model (x),\n        \n        where the blending factor g(x) is determined with respect to the current view, according to the following principles: (i) pixels in the original photograph which correspond to surfaces facing camera are considered more reliable than those on oblique surfaces; and, (ii) pixels in the original photograph are also preferred whenever the corresponding scene point is viewed from the same direction in the current view, as it was in the original one. Specifically, let n(x) denote the surface normal, C 0 the original camera position from which the photograph was taken, and C new the current camera position. Next, let v 0 = (C 0 ? x)/ C 0 ? x denote the normalized vector from the scene point to the original camera position, and similarly v new = (C new ? x)/ C new ? x . Then\n        \n          10\n          g(x) = max (n(x) ? v 0 , v new ? v 0 ) .\n        \n        In other words, g is defined as the greater among the cosine of the angle between the normal and the original view direction, and the cosine of the angle between the two view directions. Finally, we also apply re-hazing on-the-fly. First, we remove haze from the texture completely as described in Section 4.1. Then, we add haze back in, this time using the distances from the current camera position. The results may be seen in Figure 7 and in the video.\n      \n      \n        6 Information Visualization\n        Having registered a photograph with a model that has GIS data associated with it allows displaying various information about the scene, while browsing the photograph. We have implemented a simple application that demonstrates several types of information visualization. In this application, the photograph is shown sideby-side with a top view of the model, referred to as the map view. The view frustum corresponding to the photograph is displayed in the map view, and is updated dynamically whenever the view is changed (as described in Section 5). Moving the cursor in either of the two views highlights the corresponding location in the other view. In the map view, the user is able to switch between a street map, an orthographic photo, a combination thereof, etc. In addition\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        116:8 ? J. Kopf et al.\n        \n          \n        \n        (a) (b)\n        \n          Figure 8: Different information visualization modes in our system. (a-b) Coupled map and photo views. As the user moves the mouse over one of the views, the corresponding location is shown in the other view as well. The profile of a horizontal scanline in the map view (a) is shown superimposed over the terrain in the photo view (b). Since the location of the mouse cursor is occluded by a mountain in the photo, its location in the photo view is indicated using semi-transparent arrows. (c) Names of landmarks are automatically superimposed on the photo. (d-e) Coupled photo and map views with superimposed street network. The streets under the mouse cursor are highlighted in both views.\n        \n        to text labels it is also possible to superimpose graphical map elements, such as roads, directly onto the photo view. These abilities are demonstrated in Figures 1 and 8 and in the companion video. There are various databases with geo-tagged media available on the web. We are able to highlight these locations in both views (photo and map). Of particular interest are geo-tagged Wikipedia articles about various landmarks. We display a small Wikipedia icon at such locations, which opens a browser window with the corresponding article, when clicked. This is also demonstrated in the companion video. Another nice visualization feature of our system is the ability to highlight the object under the mouse in the photo view. This can be useful, for example, when viewing night time photographs: in an urban scene shot at night, the building under the cursor may be shown using daylight textures from the underlying model.\n      \n      \n        7 Discussion and Conclusions\n        We presented Deep Photo, a novel system for editing and browsing outdoor photographs. It leverages the high quality 3D models of the earth that are now becoming widely available. We have demonstrated that once a simple geo-registration of a photo is performed, the models can be used for many interesting photo manipulations that range from deand rehazing and relighting to integrating GIS information. The applications we show are varied. Haze removal is a challenging problem due to the fact that haze is a function of depth. We have shown that now that depth is available in a geo-registered photograph, excellent ?haze editing? can be achieved. Similarly, having an underlying geometric model makes it possible to generate convincing relighted photographs, and dynamically change the view. Finally, we demonstrate that the enormous wealth of information available online can now be used to annotate and help browse photographs. Within our framework we used models obtained from Virtual Earth. The manual registration is done within a minute, matting out the\n        (c) (d) (e)\n        \n          \n          Figure 9: Failure cases: some of the described applications produce artifacts for badly registered (left) and/or insuffienctly accurate models (right). In this case the dehazing application generated halos around misaligned depth edges because it used wrong depth values there. The same artifacts can be observed by zooming into the full images in Figures 2 and 3.\n        \n        foreground is also an easy task using state-of-the-art techniques such as Soft Scissors [Wang et al. 2007]. All other operations such as dehazing and relighting run at interactive speeds; however, computing very detailed shadow maps for the relighting can be time consuming. As can be expected, there are always some differences and misalignments between the photograph and the model. The may arise due to insufficiently accurate models, and also due to the fact that the photographs were not captured with an ideal pinhole camera. Although they can lead to some artifacts (see Figure 9 ), we found that in many cases these differences are less problematic than one might fear. However, automatically resolving such differences is certainly a challenging and interesting topic for future work. We believe that the applications presented here represent just a small fraction of possible geo-photo editing operations. Many of the existing digital photography products could be greatly enhanced with the use of geo information. Operations could encompass noise-reduction and image sharpening with 3D model priors, postcapture refocussing, object recovery in under or over-exposed areas as well as illumination transfer between photographs. GIS databases contain a wealth of information, of which we have just used a small amount. Water, grass, pavement, building materials, etc, can all potentially be automatically labeled and used to improve photo tone adjustment. Labels can be transferred automatically from one image to others. Again, having a single consistent 3D model for our photographs provides much more than just a depth value per pixel. In this paper we mostly dealt with single images. Most of the applications that we demonstrated become even stronger when combining multiple input photos. A particularly interesting direction might be to combine Deep Photo with the Photo Tourism system. Once a Photo Tour is geo-registered, the coarse 3D information generated by Photo Tourism could be used to enhance online 3D data and vice-versa. The information visualization and novel view synthesis applications we demonstrate here could be combined with the Photo Tourism viewer. This idea of fusing multiple images could even be extended to video that could be registered to the models.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        Deep Photo: Model-Based Photograph Enhancement and Viewing ? 116:9\n      \n      \n        Acknowledgements\n        This research was supported in parts by grants from the the following funding agencies: the Lion foundation, the GIF foundation, the Israel Science Foundation, and by DFG Graduiertenkolleg/1042 ?Explorative Analysis and Visualization of Large Information Spaces? at University of Konstanz, Germany.\n      \n      \n        References\n        \n          A SHIKHMIN , M. 2001. Synthesizing natural textures. Proceedings of the 2001 symposium on Interactive 3D graphics (I3D), 217? 226.\n          C HEN , B., R AMOS , G., O FEK , E., C OHEN , M., D RUCKER , S., AND N ISTER , D. 2008. Interactive techniques for registering images to digital terrain and building models. Microsoft Research Technical Report MSR-TR-2008-115.\n          C HO , P. L. 2007. 3D organization of 2D urban imagery. Proceedings of the 36th Applied Imagery Pattern Recognition Workshop, 3?8.\n          C RIMINISI , A., R EID , I. D., AND Z ISSERMAN , A. 2000. Single view metrology. International Journal of Computer Vision 40, 2, 123?148.\n          D EBEVEC , P. E., T AYLOR , C. J., AND M ALIK , J. 1996. Modeling and rendering architecture from photographs: A hybrid geometryand image-based approach. Proceedings of SIGGRAPH ?96, 11?20.\n          D EBEVEC , P., H AWKINS , T., T CHOU , C., D UIKER , H.-P., S AROKIN , W., AND S AGAR , M. 2000. Acquiring the reflectance field of a human face. Proceedings of SIGGRAPH 2000, 145?156.\n          D RORI , I., C OHEN -O R , D., AND Y ESHURUN , H. 2003. Fragment-based image completion. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2003) 22, 3, 303?312.\n          EON S OFTWARE , 2008. Vue 6 Infinite. http: //www.e-onsoftware.com/products/vue/vue_ 6_infinite.\n          E FROS , A. A., AND L EUNG , T. K. 1999. Texture synthesis by non-parametric sampling. Proceedings of IEEE International Conference on Computer Vision (ICCV) ?99 2, 1033?1038.\n          F ATTAL , R. 2008. Single image dehazing. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2008) 27, 3, 73.\n          F R UH  ? , C., AND Z AKHOR , A. 2003. Constructing 3D city models by merging aerial and ground views. IEEE Computer Graphics and Applications 23, 6, 52?61.\n          G RUEN , A., AND H UANG , T. S. 2001. Calibration and Orientation of Cameras in Computer Vision. Springer-Verlag, Secaucus, NJ, USA.\n          H AYS , J., AND E FROS , A. A. 2007. Scene completion using millions of photographs. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2007) 26, 3, 4.\n          H ERTZMANN , A., J ACOBS , C. E., O LIVER , N., C URLESS , B., AND S ALESIN , D. H. 2001. Image analogies. Proceedings of SIGGRAPH 2001, 327?340.\n          H OIEM , D., E FROS , A. A., AND H EBERT , M. 2005. Automatic photo pop-up. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2005) 24, 3, 577?584.\n          H ORRY , Y., A NJYO , K.-I., AND A RAI , K. 1997. Tour into the picture: using a spidery mesh interface to make animation from a single image. Proceedings of SIGGRAPH ?97, 225?232.\n          K AFTORY , R., S CHECHNER , Y. Y., AND Z EEVI , Y. Y. 2007. Variational distance-dependent image restoration. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2007, 1?8.\n          K ANG , S. B. 1998. Depth painting for image-based rendering applications. Tech. rep., Compaq Cambridge Research Lab.\n          L ALONDE , J.-F., H OIEM , D., E FROS , A. A., R OTHER , C., W INN , J., AND C RIMINISI , A. 2007. Photo clip art. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2007) 26, 3, 3.\n          L OSCOS , C., D RETTAKIS , G., AND R OBERT , L. 2000. Interactive virtual relighting of real scenes. IEEE Transactions on Visualization and Computer Graphics 6, 4, 289?305.\n          M C C ARTNEY , E. J. 1976. Optics of the Atmosphere: Scattering by Molecules and Particles. John Wiley and Sons, New York, NY, USA.\n          N ARASIMHAN , S. G., AND N AYAR , S. K. 2003. Contrast restoration of weather degraded images. IEEE Transactions on Pattern Analysis and Machine Intelligence 25, 6, 713?724.\n          N ARASIMHAN , S. G., AND N AYAR , S. K. 2003. Interactive (de)weathering of an image using physical models. IEEE Workshop on Color and Photometric Methods in Computer Vision.\n          NASA, 2008. The landsat program. http://landsat.gsfc . nasa.gov/.\n          NASA, 2008. Shuttle radar topography mission. http://www2 . jpl.nasa.gov/srtm/.\n          N AYAR , S. K., AND N ARASIMHAN , S. G. 1999. Vision in bad weather. Proceedings of IEEE International Conference on Computer Vision (ICCV) ?99, 820?827.\n          N ISTER , D., AND S TEWENIUS , H. 2007. A minimal solution to the generalised 3-point pose problem. Journal of Mathematical Imaging and Vision 27, 1, 67?79.\n          O AKLEY , J. P., AND S ATHERLEY , B. L. 1998. Improving image quality in poor visibility conditions using a physical model for contrast degradation. IEEE Transactions on Image Processing 7, 2, 167?179.\n          O H , B. M., C HEN , M., D ORSEY , J., AND D URAND , F. 2001. Image-based modeling and photo editing. Proceedings of ACM SIGGRAPH 2001, 433?442.\n          R EICHMANN , M., 2001. The art of photography. http://www.luminous-landscape.com/essays/ theartof.shtml.\n          S CHECHNER , Y. Y., AND A VERBUCH , Y. 2007. Regularized image recovery in scattering media. IEEE Transactions on Pattern Analysis and Machine Intelligence 29, 9, 1655?1660.\n          S CHECHNER , Y. Y., N ARASIMHAN , S. G., AND N AYAR , S. K. 2003. Polarization-based vision through haze. Applied Optics 42, 3, 511?525.\n          S HADE , J., G ORTLER , S., H E , L.-W., AND S ZELISKI , R. 1998. Layered depth images. Proceedings of SIGGRAPH ?98, 231? 242.\n          S HUM , H.-Y., H AN , M., AND S ZELISKI , R. 1998. Interactive construction of 3-d models from panoramic mosaics. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 1998, 427?433.\n          S NAVELY , N., S EITZ , S. M., AND S ZELISKI , R. 2006. Photo tourism: exploring photo collections in 3d. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2006) 25, 3, 835?846.\n          S TAMOS , I., AND A LLEN , P. K. 2000. 3-D model construction using range and image data. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 1998, 531? 536.\n          S UNKAVALLI , K., M ATUSIK , W., P FISTER , H., AND R USINKIEWICZ , S. 2007. Factored time-lapse video. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2007) 26, 3, 101.\n          T AN , R. T. 2008. Visibility in bad weather from a single image. Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2008, to appear.\n          T OYAMA , K., L OGAN , R., AND R OSEWAY , A. 2003. Geographic location tags on digital images. Proceedings of the 11th ACM international conference on Multimedia, 156?166.\n          W ANG , J., A GRAWALA , M., AND C OHEN , M. F. 2007. Soft scissors: an interactive tool for realtime high quality matting. ACM Transactions on Graphics (Proceedings of SIGGRAPH 2007) 26, 3.\n          Y U , Y., AND M ALIK , J. 1998. Recovering photometric properties of architectural scenes from photographs. Proceedings of SIGGRAPH ?98, 207?217.\n          Y U , Y., D EBEVEC , P., M ALIK , J., AND H AWKINS , T. 1999. Inverse global illumination: recovering reflectance models of real scenes from photographs. Proceedings of SIGGRAPH ?99, 215? 224.\n          Z HANG , L., D UGAS -P HOCION , G., S AMSON , J.-S., AND S EITZ , S. M. 2002. Single-view modelling of free-form scenes. The Journal of Visualization and Computer Animation 13, 4, 225? 235.\n        \n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n        116:10 ? J. Kopf et al.\n        ACM Transactions on Graphics, Vol. 27, No. 5, Article 116, Publication date: December 2008.\n      \n    \n  ",
  "resources" : [ ]
}