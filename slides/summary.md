Some conclusions about SIGGraph corpus:  
- **About Documents**: (see ***corpus*** chart) There are 101 annotated documents based on *context*, i.e. the `siggraph paper selections - v2.docx` file sent by mail specifying a `title`, `authors`, `doi` and `domain` for each pdf file, and 86 annotated documents based on *content*, i.e using the *Dr Inventor Text Mining Framework* (DRI-TMF) released by UPF. Also, there are 2 documents with the same `file-name` and different `title` and `authors` based on context information or content information:
    - `a26-kerr.pdf`: Context-based title is: *Clone attack! Perception of crowd variety* different from content-based title: *Toward Evaluating Lighting Design Interface Paradigms for Novice Users*  
    - `a34-ben-chen.pdf`: Context-based title is: *Real-time, all-frequency shadows in dynamic scenes* different from content title: *Variational Harmonic Maps for Space Deformation*  
And 1 file with a wrong parsed document: `a33-gal.pdf`. So, finally, there are 83 documents, from the SIGGraph corpus, with valid context and content-based annotations to be analyzed.

- **About Categories**: (see ***categories-context*** chart) Taking into account the context information, in that corpus exists 8 *unbalanced* categories: rendering(17), modeling(19), animation(20), image(21), 3dprinting(1), light(1), game(1) and perception(3).

- **About Content**: We have considered 12 logic divisions for every document based on three point of views: *sections*, *sentences* and *terms*.
Based on *sections* we have considered `full`, i.e using the full-text of the document, `abs`, i.e using only the abstract section, `intr`, i.e. using only the sections (and inner sections) with title containing words such as: *introduction*, *related work*, *previous work* or *background*, and `con`, i.e using only the sections (and inner sections) with title containing words such as: *results*, *conclusion* or *future work*.
Based on *sentences* we have considered `app`, i.e. sentences marked as `DRI_Approach` by the DRI-TMF, `bkg`, i.e. sentences marked as `DRI_Background` by the DRI-TMF, `chl`, i.e. sentences marked as `DRI_Challenge` by the DRI-TMF, `fwork`, i.e. sentences marked as `DRI_FutureWork` by the DRI-TMF, `out` , i.e. sentences marked by the DRI-TMF, `csum`, i.e. sentences marked by the `CENTROID_SECT` constant in the DRI-TMF, and `tsum`, i.e. sentences marked by the `TITILE_SIM` constant in the DRI-TMF.  
Based on *terms* we have considered `term`, i.e. a list of terms extracted from the document by the DRI-TMF.

For each logic division, from now on *sections*, some statistics about the number of sentences (see ***num-sentences***), tokens (see ***num-tokens***), words (see ***num-words***), lemmas (see ***num-lemmas***) and part-of-speech elements (see ***num-in***, ***num-jj** ..) have been calculated. The `bkg` section, for example, referred to sentences with a background knowledge, is empty, i.e. has no words, for all the documents. No sentences have been discovered with this type of information in the corpus. Furthermore, taking into account the number of words and lemmas in a document, the `full` section has similar distribution to other sections such as `out` section (word-pearson-correlation=0.84821 , lemma-pearson-correlation=0.82431), `app` section (word-pearson-correlation=0.67681 , lemma-pearson-correlation=0.68094) and `term` section (word-pearson-correlation=0.87648 , lemma-pearson-correlation=0.82727).

Every category identified in the context-based annotations (see ***categories-context***) have been represented by the word2vec model created from every logic division (see ***w2v-<category>-<section>***). However, some of them are empty because that category word does not appear in the texts of the sections, for example `3dprinting` for all the sections (see ***w2v-3dprinting-<section>), or `game` in the `abs` section (see ***w2v-game-abs***). Here is interesting to see how the same category is represented in different sections, i.e. in different contexts. For example, the `animation` category identified in the `full` section (see ***w2v-animation-full***) is represented as: *input*, *produce*, *quality*, *brighter*, *diffusion*, etc,  in the `out` section is represented as: *internal*, *machine*, *utilize*, *behind*, *van*, etc and in the `int` section as: *accuracy*, *actually*, *shadow*, *hard*, *conservative*, etc. So, different sections represent in a different way the same word, in this case the same category.

Following with this idea, every section has a different number of topics (see ***topics-<section>***). Thus, for the `full` section have been identified 11 topics (see ***topics-full***), whereas the `abs` section has 6 topics (see ***topics-abs***) and 12 topic were discovered for the `out` section (see ***topics-out***). Now, Is interesting to compare the list of the most relevant words of each topic in the `full` section (see ***topics-full***) (image, motion, shape, point, sample, scene, fluid, user and simulation) with the list of categories  previously identified (see ***categories-context***)(image, animation, modeling, rendering, perception, game, light, 3dprinting). Some of these words are related such as animation and motion, modeling and shape, perception and point and so on. Taking into account only the abstracts of the papers, i.e. the `abs` section, 6 topics are discovered and the most relevant words are: image, shape, method, motion, user and filter (see ***topics-abs***).

Finally, **precision**, **recall** and **f-measure** have been calculated for each category in every section (see ***eval-<category***). Here, only the categories with a high number of documents, i.e. image, animation, modeling and rendering, show interesting results. However, the behavior is highly different between them. In my opinion, there are not enough documents to obtain relevant results.
